%!TEX program = pdflatex
%!BIB program = bibtex
\documentclass[12pt]{article}
\usepackage{geometry}
% \geometry{left=2.0cm,right=2.0cm,top=2.5cm,bottom=2.5cm}


\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{lipsum}
\usepackage{multirow}  % for multirow command used in the table
\usepackage{setspace}
\usepackage{ulem}  % 用于添加下划线
\usepackage{enumitem}
\usepackage{color}
\usepackage{amssymb}  % 用于打对号
\urlstyle{same}

\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}


\begin{document}
\setstretch{1.5}
\linespread{1}
\title{Response to Reviewers of TCSVT-07024-2021: A Simple and Strong Baseline for Universal Targeted Attacks on Siamese Visual Tracking}
\author{Zhenbang Li, Yaya Shi, Jin Gao, Shaoru Wang, \\Bing Li, Pengpeng Liang, Weiming Hu}
\date{}
\maketitle

\noindent Dear Editors:

We would like to express our heartfelt gratitude to you and the reviewers for the insightful and helpful comments. When we revised the paper, we carefully considered and followed all the comments and suggestions provided by you and the reviewers. To summarize, we have made the following revisions:

(1) We have added the advantages \& limitations of the proposed method after experimental analysis and the future work to the conclusion.

(2) We have carefully considered and followed all the comments and suggestions related to the clarity of the writing, and made the new material a thoroughly revised manuscript.

% 消融实验
(3) We have added a new experiment to attack in the YCbCr color space to reduce the perceptibility of the perturbations.

% % 消融实验
% (4) We have used a new way to generate the fake trajectory.

% 消融实验：噪声攻击。
(4) We have added a new experiment to evaluate the attack performance of the gaussian random noise.

% 消融实验
(5) We have added a new experiment to evaluate the attack performance when only adding the perturbation to the search/template image.

% 引用论文
(6) We have added 3 papers published in the IEEE Transactions on Circuits and Systems for Video Technology, which are most closely related to our manuscript, and analysed what is distinctive / new about our current manuscript related to these previously published papers.

\uline{
Research topics related to adversarial attacks includes digital watermarking \cite{9343885}, 3D face presentation attacks \cite{9294085} and adversarial defense \cite{9169672}.
Xiong et. al. \cite{9343885} propose to generate the digital watermarking by slightly modifying the pixel values of video frames to protect video content from unauthorized access. Compared with \cite{9343885}, our purpose of modifying pixel values of video frames is attacking the tracker, instead of detecting the illegal distribution of a digital movie.
Jia et. al. \cite{9294085} propose to generate 3D face artifacts to attack the face recognition network. Compared with \cite{9294085}, our method directly modifies the input of the network instead of changing the input of cameras using 3D face artifacts.
Wang et. al. \cite{9169672} propose the white-box attack/defense methods for image classifiers. Compared with \cite{9169672}, we focus on attacking the object tracking networks instead of the image classification networks.
}

We hope that our revised manuscript is now appropriate for publication in IEEE Transactions on Circuits and Systems for Video Technology. Specific responses to all the comments of each reviewer are included in the rest of this document and highlighted using bold font after the comments of each reviewer for the convenience of cross-reference. To make the changes easier to identify where necessary, we also have underlined most of the revised parts in the manuscript and provide an underlined version for the convenience of second review.\\[10pt]
\indent We are looking forward to your reply.\\[10pt]
\noindent Yours sincerely,\\
\noindent Zhenbang Li, Yaya Shi, Jin Gao, Shaoru Wang, Bing Li, Pengpeng Liang, Weiming Hu
\\
\\
\\
\noindent Dr. Jin Gao (Contact author)\\
\noindent National Laboratory of Pattern Recognition (NLPR)\\
\noindent Institute of Automation, Chinese Academy of Sciences (CASIA)\\
\noindent Address: No. 95, Zhongguancun East Road, Haidian District,\\
\noindent Beijing 100190, P. R. China\\
\noindent Email: jin.gao@nlpr.ia.ac.cn

%%%%%%%%%%%%%%%%% 审稿人 1 %%%%%%%%%%%%%%%%%
\newpage
{\centering\section*{Response Letter to Reviewer \#1}}
\noindent Dear Reviewer \#1:

Thank you very much for your thorough review. Your insightful comments are very helpful for us to improve the quality of the paper. According to your comments and suggestions, we have carefully and extensively revised the manuscript. The main revised parts are highlighted by underlines in the underlined version for your convenience. You will find that all your comments and suggestions are considered and followed. We hope that our revised manuscript is now appropriate for publication in IEEE Transactions on Circuits and Systems for Video Technology.
In addition, point-to-point responses to your comments are given below and highlighted using bold font in line with your comments in order to facilitate cross-referencing.\\[10pt]
\indent We are looking forward to your reply.\\[10pt]
\noindent Yours sincerely,\\
\noindent Zhenbang Li, Yaya Shi, Jin Gao, Shaoru Wang, Bing Li, Pengpeng Liang, Weiming Hu
\\
\\
\\
\noindent Dr. Jin Gao (Contact author)\\
\noindent National Laboratory of Pattern Recognition (NLPR)\\
\noindent Institute of Automation, Chinese Academy of Sciences (CASIA)\\
\noindent Address: No. 95, Zhongguancun East Road, Haidian District,\\
\noindent Beijing 100190, P. R. China\\
\noindent Email: jin.gao@nlpr.ia.ac.cn

\newpage
\textit{This paper addresses the task of attacking Siamese network-based trackers in a simple yet effective fashion. Unlike other methods that operate in the video-specific attacking regime (which resides on network inference for generating perturbations while tracking), this method is the first to perform universal targeted attacks for Siamese trackers utilizing both the translucent perturbation and the adversarial patch together. By adding the perturbation to the template and adding the patch to the search image while performing tracking, this work fools the Siamese trackers to the fake target region and thus makes them fail in tracking the real target object. Overall, this is an interesting paper, and it is well written and organized. As it is a resubmitted manuscript, I notice that the authors have made substantial changes to the previous manuscript, which are able to appropriately respond to the comments made by the previous reviewers. Although the template perturbation and adversarial patch are both easy to observe for human eyes as the previous reviewers have pointed out, and the SSIMs for them are also lower than the video-specific attacking method, e.g., FAN \cite{FAN}, this reviewer believes that this proposed new framework can be a new configuration of adversarial attack on visual tracking for its achieved balance between the attack efficiency and the perturbation perceptibility. This new configuration will attract increasing attention from the visual tracking attack community to study on more efficient attack methods.}

\textbf{Many thanks for your positive comments on the strength of our paper and the novelty of the proposed attack method.}

%%%% 问题 1.1 %%%%
\textit{In addition, I suggest the authors add more experiments to demonstrate the practicability of the attack method when the ground truth box information is missing in the training data. The experimental results show that it is effective to use the predicted boxes instead of ground truth boxes for training perturbations.}

\textbf{Thanks for the good comment. As suggessted, we have added more experiments to demonstrate the practicability of the attack method when the ground truth box information is missing in the training data. Specifically, we evaluate the untargeted and targeted attack performance on the following datasets and metrics: (1) AO and precision on OTB-2015, (2) SR and AO on GOT-Val, and (3) precision, normalized precision and AO on LaSOT.
% 新增加的实验结果分析位于某处，展示如下。
The detailed analysis of new added experiments is added in Section IV.D of the revised manuscript. For your convenience in cross-checking, the new text is given as follows.}

\begin{table}[t]
  \renewcommand\thetable{IX}
  \centering
  \caption{Attack results on OTB-15, GOT-Val and LaSOT with or without the ground truth information. w/ GT represents that we train perturbations using the groud truth bounding box information of the trainig dataset. w/o GT represents that we train perturbations using the predicted boxes instead of ground truth boxes of the training dataset.}
  \begin{tabular}{rrcccc}
  \toprule
  \multirow{2}{*}[-2pt]{Benchmarks} & \multirow{2}{*}[-2pt]{Metrics} & \multicolumn{2}{c}{Unargeted Attack} & \multicolumn{2}{c}{Targeted Attack} \\ \cmidrule{3-6}
                              &                          & w/ GT  & \multicolumn{1}{l}{w/o GT}  & w/ GT  & \multicolumn{1}{l}{w/o GT} \\ \midrule
  \multirow{2}{*}{OTB-15}     & AO                       & 0.063  & 0.056                       & 0.759  & 0.752                      \\
                              & Precision                & 0.092  & 0.080                       & 0.795  & 0.794                      \\ \midrule
  \multirow{2}{*}{GOT-Val}    & SR                       & 0.123  & 0.121                       & 0.890  & 0.893                      \\
                              & AO                       & 0.153  & 0.160                       & 0.840  & 0.833                      \\ \midrule
  \multirow{3}{*}{LaSOT}      & Precision                & 0.046  & 0.043                       & 0.605  & 0.531                      \\
                              & Norm. Prec.              & 0.048  & 0.044                       & 0.702  & 0.660                      \\
                              & AO                       & 0.069  & 0.063                       & 0.691  & 0.646                      \\ \bottomrule
  \end{tabular}
  \label{tab:agent_GT}
\end{table}

\uline{We evaluate the untargeted and targeted attack performance on the following datasets and metrics: (1) AO and precision on OTB-2015, (2) SR and AO on GOT-Val, and (3) precision, normalized precision and AO on LaSOT.
Experimental results are shown in Table \ref{tab:agent_GT}.
In terms of AO on GOT-Val, the untargeted attack performance is 0.153 when the ground truth is used to train perturbations and 0.160 when the ground truth is not used.
In terms of the precision score on OTB-2015, the targeted attack performance is 0.795 when the ground truth is used to train perturbations and 0.794 when the ground truth is not used.
In terms of the normalized precision score on LaSOT, the targeted attack performance is 0.702 when the ground truth is used to train perturbations and 0.666 when the ground truth is not used.
Therefore, it is effective to use the predicted boxes instead of ground truth boxes for training perturbations.
}

%%%% 问题 1.2 %%%%
\textit{A small question is that it will be better if the authors can provide some pseudo code for the untargeted attack and targeted attack processes in addition to the training process. This will facilitate the understanding of the attacking process while performing tracking.}

\begin{algorithm}[t]
    \renewcommand\thealgorithm{2}
    \caption{Attack Process}
    \label{alg:algorithm_attack}
    \textbf{Input}: The trained perturbations $\delta$ and $p$, Siamese tracker $f$, video $V=\{I_i\}_1^T$. $b^{gt}_1$ is the position of the real target in the first frame. $B^{fake}=\{b^{fake}_i\}_1^{T}$ is the trajectory we hope the tracker to output.\\
    \textbf{Output}: $B^{pred}=\{b^{pred}_i\}_1^{T}$
    \begin{algorithmic}[1] %[1] enables line numbers
      \STATE Generate the clean template image $\textbf{z}_1$ according to $I_1$ and $b^{gt}_1$.
      \STATE Generate the perturbed template image $\tilde{\textbf z}_1=\textbf z_1+\delta$.
      \STATE Let $i = 2$.
    \WHILE{$i \le T$}
    \STATE Generate clean search image $\textbf{x}_i$ according to $I_i$ and $b^{pred}_{i-1}$.
    \STATE $b^{fake}_i=\{x_{0_i}, y_{0_i}, x_{1_i}, y_{1_i}\}$
    \STATE Generate the perturbed search image $\tilde{\textbf x}_i = A_{\text{add}}(\textbf x_i, p, \{x_{0_i}, y_{0_i}, x_{1_i}, y_{1_i}\}).$
    \STATE $\textbf{C, R, Q} = f(\tilde {\textbf x}_i, \tilde{\textbf z}_1).$
    \STATE Generate the predicted bounding box $b^{pred}_i$ according to $\textbf{C, R, Q}$.
    \STATE $i = i + 1.$
    \ENDWHILE
    \STATE \textbf{return} $B^{pred}$
    \end{algorithmic}
\end{algorithm}

\textbf{
Thanks for the good comment. As suggested, we have provided the algorithm for the attack process in Section III.C of the revised manuscript.
Both the targeted attack and untargeted attack process follow the same steps as shown in Alg. \ref{alg:algorithm_attack}.
%The difference between the target attack and the untargeted attack is the evaluation. For the targeted attack, we compare the AO between $B^{pred}=\{b^{pred}_i\}_1^{T}$ and $B^{fake}=\{b^{fake}_i\}_1^{T}$. For the untargeted attack, we compare the AO between $B^{pred}=\{b^{pred}_i\}_1^{T}$ and $B^{gt}=\{b^{gt}_i\}_1^{T}$.
}

%%%% 问题 1.3 %%%%
\textit{In addition, the font size in Fig. 9 is too small to read on my computer, which needs to be improved.}

% \textbf{Thanks for the good comment. As suggested, we have increased the font size of texts in Fig. \ref{fig:attr} to make it easier to read on the computer.}


% \newgeometry{left=1cm,right=1cm,top=3cm,bottom=3cm}

% \begin{figure*}[t]
%   \renewcommand\thefigure{9}
%   \begin{center}
%     \includegraphics[width=0.32\textwidth]{images_imperceptible/OTB2015/success_plot_OPE_OTB100_BC.png}
%     \includegraphics[width=0.32\textwidth]{images_imperceptible/OTB2015/success_plot_OPE_OTB100_DEF.png}
%     \includegraphics[width=0.32\textwidth]{images_imperceptible/OTB2015/success_plot_OPE_OTB100_FM.png}
%     \includegraphics[width=0.32\textwidth]{images_imperceptible/OTB2015/success_plot_OPE_OTB100_IPR.png}
%     \includegraphics[width=0.32\textwidth]{images_imperceptible/OTB2015/success_plot_OPE_OTB100_IV.png}
%     \includegraphics[width=0.32\textwidth]{images_imperceptible/OTB2015/success_plot_OPE_OTB100_LR.png}
%     \includegraphics[width=0.32\textwidth]{images_imperceptible/OTB2015/success_plot_OPE_OTB100_MB.png}
%     \includegraphics[width=0.32\textwidth]{images_imperceptible/OTB2015/success_plot_OPE_OTB100_OCC.png}
%     \includegraphics[width=0.32\textwidth]{images_imperceptible/OTB2015/success_plot_OPE_OTB100_OPR.png}
%     \includegraphics[width=0.32\textwidth]{images_imperceptible/OTB2015/success_plot_OPE_OTB100_OV.png}
%     \includegraphics[width=0.32\textwidth]{images_imperceptible/OTB2015/success_plot_OPE_OTB100_SV.png}
%   \end{center}
%       \caption{Untargeted attack results of the different trackers under the 11 attributes: out-of-view (OV), occlusion (OCC), illumination variation (IV), out-of-plane rotation (OPR), scale variation (SV),deformation (DEF), low resolution (LR), fast motion (FM), background clutters (BC), motion blur (MB), and in-plane rotation (IPR). The results show good transferability of our attacks to different tracking architectures, even if the generated perturbations are applied to anchor-based trackers.}
%   \label{fig:attr}
% \end{figure*}

% \restoregeometry

\textbf{Sorry for the inconvenience of reading Fig. 9 caused by the small font size.
We are willing to adjust Fig. 9 font size to make it easier to read on the computer, but because we have added many ablation experiments to revised manuscript, the total number of pages exceeds the maximum page limit for TCSVT (i.e., 14 pages). Therefore we have to remove the analysis of the attack performance for each attribute of the OTB as well as Fig. 9.
}

%%%% 问题 1.4 %%%%
\textit{Also, some recent papers are valued to be referred (2020-2021), to enhance the quality.}

\textbf{Thanks for the good comments. We have added several 2020-2021 related papers to enhance the quality of the revised manuscript. For your convenience of cross review, the new text is given as follows.}

\uline{Research topics related to adversarial attacks includes digital watermarking \cite{9343885}, 3D face presentation attacks \cite{9294085} and adversarial defense \cite{9169672}.
Xiong et. al. \cite{9343885} propose to generate the digital watermarking by slightly modifying the pixel values of video frames to protect video content from unauthorized access. Compared with \cite{9343885}, our purpose of modifying pixel values of video frames is attacking the tracker, instead of detecting the illegal distribution of a digital movie.
Jia et. al. \cite{9294085} propose to generate 3D face artifacts to attack the face recognition network. Compared with \cite{9294085}, our method directly modifies the input of the network instead of changing the input of cameras using 3D face artifacts.
Wang et. al. \cite{9169672} propose the white-box attack/defense methods for image classifiers. Compared with \cite{9169672}, we focus on attacking the object tracking networks instead of the image classification networks.}

%%%%%%%%%%%%%%%%% 审稿人 2 %%%%%%%%%%%%%%%%%
\newpage
{\centering\section*{Response Letter to Reviewer \#2}}
\noindent Dear Reviewer \#2:

Thank you very much for your thorough review. Your insightful comments are very helpful for us to improve the quality of the paper. According to your comments and suggestions, we have carefully and extensively revised the manuscript. The main revised parts are highlighted by underlines in the underlined version for your convenience. You will find that all your comments and suggestions are considered and followed. We hope that our revised manuscript is now appropriate for publication in IEEE Transactions on Circuits and Systems for Video Technology.
In addition, point-to-point responses to your comments are given below and highlighted using bold font in line with your comments in order to facilitate cross-referencing.\\[10pt]
\indent We are looking forward to your reply.\\[10pt]
\noindent Yours sincerely,\\
\noindent Zhenbang Li, Yaya Shi, Jin Gao, Shaoru Wang, Bing Li, Pengpeng Liang, Weiming Hu
\\
\\
\\
\noindent Dr. Jin Gao (Contact author)\\
\noindent National Laboratory of Pattern Recognition (NLPR)\\
\noindent Institute of Automation, Chinese Academy of Sciences (CASIA)\\
\noindent Address: No. 95, Zhongguancun East Road, Haidian District,\\
\noindent Beijing 100190, P. R. China\\
\noindent Email: jin.gao@nlpr.ia.ac.cn

\newpage
\textit{In this paper, the authors train a universal adversarial patch to add on both template and search regions of a Siamese based tracker to deteriorate its original performance. The proposed perturbations are video-agnostic, leading to a low computational cost during attack. The experiment validations show that the proposed method achieves favorable attack results on OTB2015, GOT-10k, LaSOT, UAV123, VOT2016, VOT2018 and VOT2019. In addition, the generated perturbations transfer well on other Siamese trackers as well. The idea of this paper is interesting and the experiments are thorough.}

\textbf{Many thanks for your positive comments on the strength of our paper and the novelty of the proposed attack method.}

%%%% 问题 2.1 %%%%
\textit{However, there are some concerns over the implementation, performance and writing. 1. The authors state that training with Ep. 4 leads to an obvious patch on the images while using Eq.5 into the training process results in a less obvious patch. The reviewer considers that giving a constraint (e.g. $l_{\infty}$) on the $p_x$ in Eq.4 can make the perturbation imperceptible intuitively. Please give more analysis on this setting.
Besides, the reviewer hopes to know the reason why give an extra perturbation on the template region. The perturbations on template and search regions look similar, while the authors say that they are different. Please state the difference between the patch application operator on search examples and the operator on template examples.
In addition, the denotations of $A_{paste}$ in Eq.4 and $A_{add}$ in Eq.5 seem like the same one.
}

\textbf{
Thanks for the good comment.
Before analysis if giving constraint (e.g. $L_{\infty}$) on the $p_x$ in Eq.4 is really lead to imperceptible or not, let us first clarify the concept of the $A_{paste}$ and $A_{add}$, which was not explained well in the previous manuscript.}

\textbf{% 解释paste（说明不需要加 constraint）
$A_{paste}$ means that,
% we use the pixel value in in every place of the perturbation to replace the original image pixel value at the corresponding position.
% The final pixel value is changed from the original value to a new value.
in the region where the perturbation is pasted, the pixel values of the original image are replaced with the pixel values of the perturbation.
% 说明加了约束也没用。
% As a result, even though we add constraint to the perturbation and it is small, when we paste it to the image, it shows nearly black but not imperceptible.
Thus, even if we add constraints to the patch so that the perturbation values are small, when the patch is pasted to the image, it will look like an almost black region, which is obvious rather than imperceptible.}
% 解释加法
\textbf{The operator $A_{add}$ refers to the injection of small additive noise into the image. The injected noises are small and sometimes invisible to human eyes.
%In other words, in the region where the perturbation is added, the pixel values of the perturbation are added to the pixel values of the original image.
}

%\textbf{The operator $A_{\text{add}}(\textbf{x}, p_\textbf{x}, b^{fake}_{\textbf{x}})$ and $\textbf{z} + \delta_\textbf{z}$ is similar. The only difference is that, $A_{\text{add}}$ means add the perturbation to a specific region because the patch size is less than the image, while $+$ means add the perturbation to the full image because the size is the same.}

\textbf{Next, let us explain the difference between the patch $p$ added to the search image and the perturbation $\delta$ added to the template image. $p$ has a smaller size of $64\times 64$, and added to a larger ($303\times 303$) search image as a fake target.
$\delta$ is the same size ($127\times 127$) as the template image.
% p 通常被加到搜索图像的背景区域，并不会改变真实前景目标区域的像素值。因此如果我们不扰动模板图像的话，总会在真实目标区域有较高的预测。
The adversarial patch is usually added to the background region of the search image and does not change the pixel values in the foreground region where the real target exists. Therefore if we do not add the perturbation to the template image, the response values of the region where the real target exists on the heatmaps are always high. Thus it is necessary to perturb the template image to cooling down hot regions where the real target exists on the heatmaps and increasing the responses at the position of the fake target.
}
\textbf{For your convenience of cross checking, the new added text is shown as follows:}

\uline{
$A_{paste}$ means that, in the region where the perturbation is pasted, the pixel values of the original image are replaced with the pixel values of the perturbation.
% 解释加法
The operator $A_{add}$ refers to the injection of small additive noise into the image. The injected noises are small and sometimes invisible to human eyes.}

\uline{
The adversarial patch is usually added to the background region of the search image and does not change the pixel values in the foreground region where the real target exists. Therefore if we do not add the perturbation to the template image, the response values of the region where the real target exists on the heatmaps are always high. Thus it is necessary to perturb the template image to cooling down hot regions where the real target exists on the heatmaps and increasing the responses at the position of the fake target.
}


%%%% 问题 2.2 %%%%
\textit{2. I agree with reviewer 3, the ground truth boxes are inaccessible to trackers during the inference. It seems that the authors use the ground truth boxes to generate the fake trajectory in lines 51-57 on page 7. Please clarify it.}

\textbf{
Thanks for the good comment. As suggest, we consider an alternative way to generate the \textit{fake trajectory} without the use of the ground truth boxes. Specifically, the attacker forces the tracker to follow a fixed direction. We illustrate this with 4 different directions, consisting of shifting the box by $(\pm 3, \pm 3)$ pixels in each consecutive frames, corresponding to the four directions $45^{\circ}, -45^{\circ}, 135^{\circ}, -135^{\circ}$.
% 介绍在什么数据集上做的实验，用的什么评价指标。
The attack performance is evaluated according to AO/SR on GOT-Val.
The detailed analysis of new added experiment is added in Section IV.D of the revised manuscript. For your convenience in cross-checking, the new text is given as follows.}

\uline{
% 介绍轨迹生成方式
\textit{Ablation Study: An alternative way to generate the fake trajectory}  Here, we consider an alternative way to generate the \textit{fake trajectory}. Specifically, the attacker forces the tracker to follow a fixed direction. We illustrate this with 4 different directions, consisting of shifting the box by $(\pm 3, \pm 3)$ pixels in each consecutive frames, corresponding to the four directions $45^{\circ}, -45^{\circ}, 135^{\circ}, -135^{\circ}$.
% 介绍在什么数据集上做的实验，用的什么评价指标。
The attack performance is evaluated according to AO/SR on GOT-Val.
% 分析实验结果。简单分析即可。
As shown in Table \ref{table:direction}, our attack algorithm achieves effective attacks under two different fake trajectories. Specifically, when the fake trajectory follows a fixed direction, the AO of the untargeted attack is 0.175, and the AO of the targeted attack is 0.845.
}

\begin{table}[t]
  \renewcommand\thetable{X}
  \centering
  \caption{Influence of different ways to generate the fake trajectory. One way is the fake trajectory to follow a fixed direction. The other way is the fake trajectory to follow the real target. The attack performance is evaluated according to AO/SR on GOT-Val.}
  \begin{tabular}{@{}rcccc@{}}
  \toprule
  \multirow{2}{*}[-2pt]{Type of the Fake Trajectory} & \multicolumn{2}{c}{Untargeted Attack} & \multicolumn{2}{c}{Targeted Attack} \\ \cmidrule{2-5}
                              & AO                & SR                & AO               & SR               \\ \midrule
  Fixed direction             & 0.175             & 0.144             & 0.845            & 0.897            \\
  Follow the real target      & 0.153             & 0.123             & 0.840            & 0.890            \\ \bottomrule        
  \end{tabular}
  \label{table:direction}
\end{table}

%%%% 问题 2.3 %%%%
\textit{3. For the experiments, the authors should conduct the ablation study on only adding perturbations on the template images or the search regions to show the impact of $p$ and $\delta$.}

\textbf{Thanks for your advice and we have conducted the ablation study on only adding perturbations on the template images or the search regions to show the impact of $p$ and $\delta$.
The detailed analysis of new added experiment is added in Section IV.D of the revised manuscript. For your convenience in cross-checking, the new text is given as follows.}

\uline{\textit{Ablation Study: Only perturb the template or search image} To evaluate the impact of $p$ and $\delta$, we add the ablation study on only adding perturbations on the template images or the search regions. 
The attack performance is evaluated according to AO/SR on GOT-Val.
The result is shown in Table \ref{table:one_branch}.
% 对比无目标攻击性能。
For the untargeted attack, only perturbing the template/search image leads to the AO of 0.510/0.714, while perturbing both the template and the search image leads to the AO of 0.153, which is more effective.
% 有目标
For the targeted attack, only perturbing the templat/search branch leads to the AO of 0.156/0.160, while perturbing both the template and the search image leads to the AO of 0.840. Experimental results show that attacking two branches can achieve better results than attacking one branch.}

\begin{table}[]
  \renewcommand\thetable{XI}
  \centering
  \caption{Attack performance when only adding the perturbation to the search/template image. The first row refers to only perturbing the template image. The second row refers to only perturb the search image. The third row refers to perturb both the template and the search image. The attack performance is evaluated according to AO/SR on GOT-Val.}
  \label{table:one_branch}
  \begin{tabular}{@{}cccccc@{}}
  \toprule
  \multirow{2}{*}[-2pt]{Template} & \multirow{2}{*}[-2pt]{Search} & \multicolumn{2}{c}{Untargeted Attack} & \multicolumn{2}{c}{Targeted Attack} \\ \cmidrule{3-6}
                                  &                               & AO                & SR                & AO               & SR               \\ \midrule
  \checkmark                      &                               & 0.510             & 0.567             & 0.156            & 0.106            \\
                                  & \checkmark                    & 0.714             & 0.841             & 0.160            & 0.132            \\
  \checkmark                      & \checkmark                    & 0.153             & 0.123             & 0.840            & 0.890            \\
  \bottomrule
  \end{tabular}
\end{table}

%%%% 问题 2.4 %%%%
\textit{4. As reviewer 1 and reviewer 2 say, the perturbations added to template regions and research regions are not imperceptible, which may be helpful to misguide the tracker. The reviewer considers that adding a similar random pattern on the template and search regions to further illustrate the effectiveness of the proposed method.}

\textbf{To illustrate the effectiveness of the proposed method, we add a random pattern on the template and search regions. 
% 我们使用高斯噪声代替训练好的扰动来攻击SiamFC++\_GoogleNet。我们在GOT-Val上进行实验。
Specifically, we replace the trained perturbations with the gaussion random noise to attack SiamFC++\_GoogleNet. The mean value of the random noise is 0. The standard deviation is changed from 1.0 to 100.0. The experiment is performed on GOT-Val.
% 实验结果的分析
As shown in Table \ref{table:noise}, Gaussian noise cannot effectively attack the tracker even if the standard deviation is up to 100.
For your convenience in cross-checking, the new text is given as follows.
}

% \begin{table}[t]
%   \renewcommand\thetable{XII}
%   \centering
%   \caption{Attack performance using the gaussian random noise. We replace the trained perturbations with the gaussion random noise to attack SiamFC++\_GoogleNet. The mean value of the random noise is 0. The standard deviation is changed from 1.0 to 100.0. The experiment is performed on GOT-Val.}
%   \begin{tabular}{@{}rccccc@{}}
%   \toprule
%   \multirow{2}{*}[-2pt]{std} & \multirow{2}{*}[-2pt]{SSIM}& \multicolumn{2}{c}{Untargeted Attack} & \multicolumn{2}{c}{Targeted Attack} \\ \cmidrule{3-6}
%                              &                            & AO                & SR                & AO               & SR               \\ \midrule
%   1.0                        & 0.98                       & 0.760             & 0.896             & 0.142            & 0.101            \\
%   10.0                       & 0.38                       & 0.761             & 0.898             & 0.143            & 0.102            \\
%   100.0                      & 0.01                       & 0.715             & 0.850             & 0.152            & 0.112            \\ \bottomrule        
%   \end{tabular}
%   \label{table:noise}
% \end{table}

\begin{table}[t]
  \renewcommand\thetable{XII}
  \centering
  \caption{Attack performance using the gaussian random noise. We replace the trained perturbations with the gaussion random noise to attack SiamFC++\_GoogleNet. The mean value of the random noise is 0. The standard deviation is changed from 1.0 to 100.0. The experiment is performed on GOT-Val.}
  \begin{tabular}{@{}rcccc@{}}
  \toprule
  \multirow{2}{*}[-2pt]{std} & \multicolumn{2}{c}{Untargeted Attack} & \multicolumn{2}{c}{Targeted Attack} \\ \cmidrule{2-5}
                             & AO                & SR                & AO               & SR               \\ \midrule
  1.0                        & 0.760             & 0.896             & 0.142            & 0.101            \\
  10.0                       & 0.761             & 0.898             & 0.143            & 0.102            \\
  100.0                      & 0.715             & 0.850             & 0.152            & 0.112            \\ \bottomrule        
  \end{tabular}
  \label{table:noise}
\end{table}

\uline{\textit{Ablation Study: Attack performance of the random noise} To illustrate the effectiveness of the proposed method, we add a random pattern on the template and search regions. 
% 我们使用高斯噪声代替训练好的扰动来攻击SiamFC++\_GoogleNet。我们在GOT-Val上进行实验。
Specifically, we replace the trained perturbations with the gaussion random noise to attack SiamFC++\_GoogleNet. The mean value of the random noise is 0. The standard deviation is changed from 1.0 to 100.0. The experiment is performed on GOT-Val.
% 实验结果的分析
As shown in Table \ref{table:noise}, Gaussian noise cannot effectively attack the tracker even if the standard deviation is up to 100.
}

%%%% 问题 2.5 %%%%
\textit{
5. There are some minor problems, grammar errors and typos in this paper. The reviewer hopes the authors polish this paper again.}

\textit{- On page 2, ‘1016’ $\rightarrow$ ‘2016’ in line 56. There is a same one in line 47 on page 6.}

\textit{- The denotation of $B_x^{fake}$ in Eq.4 is not clear enough, even though it can be inferred by the later part.}

\textit{- On page 4, ‘imperceptible’ $\rightarrow$ ‘imperceptibly’ in line 57.}

\textit{- The reinitialization of VOT-toolkit should be mentioned in the part of ‘experimental setup’.}

\textit{- On page 7, ‘.(see Table I)’ is a typo.}

\textbf{Thanks for the good comment. We have carefully checked through the whole text and corrected the grammar mistakes and typos. Specifically,}
\textbf{We have replaced ``Experiment results on OTB2015 \cite{OTB}, GOT-10k \cite{GOT-10k}, LaSOT \cite{GOT-10k}, UAV123 \cite{UAV123}, VOT1016 \cite{VOT2016}, VOT2018 \cite{VOT2018} and VOT2019 \cite{VOT2019}. benchmarks demonstrate the effectiveness and efficiency of our approach.'' with}
``\uline{Experiment results on OTB2015 \cite{OTB}, GOT-10k \cite{GOT-10k}, LaSOT \cite{LaSOT}, UAV123 \cite{UAV123}, VOT2016 \cite{VOT2016}, VOT2018 \cite{VOT2018} and VOT2019 \cite{VOT2019}. benchmarks demonstrate the effectiveness and efficiency of our approach.}''
\textbf{in Section I of the revised manuscript.}

\textbf{We have added the definition of $B_x^{fake}$ of Eq.4 as follow:}
``\uline{$b^{fake} = \{x_0, y_0, x_1, y_1\}$ denotes the coordinates of the upper-left and lower-right corners of the fake target on the search image.}''
\textbf{in Section III.A of the revised manuscript.}

\textbf{We have replaced ``In SiamFC++, the tracker first transforms the paired reference frame $I_1$ and annotation $b_1^{gt}$ to get an template image $\textbf z_1$, and transforms the search frame $I_i$ to get the search image $\textbf x_i$ centered at the position estimated in the previous frame.'' with}
``\uline{In SiamFC++, the tracker first transforms the paired reference frame $I_1$ and annotation $b_1^{gt}$ to get a template image $\textbf z_1$, and transforms the search frame $I_i$ to get the search image $\textbf x_i$ centered at the position estimated in the previous frame.}''
\textbf{in Section III.A of the revised manuscript.}

\textbf{We have replaced ``However, CNN attacks are usually expected imperceptible but the above method has to paste an obviously noticeable fake target patch to tracking frames, which raises the risk of being suspected.'' with}
``\uline{However, CNN attacks are usually expected imperceptibly but the above method has to paste an obviously noticeable fake target patch to tracking frames, which raises the risk of being suspected.}''
\textbf{in Section III.A of the revised manuscript.}

\textbf{We have  replaced ``$A_add$ adds the patch into the search image $\textbf x$ at location $(\frac{x_0+x_1}{2},\frac{y_0+y_1}{2})$.'' with}
``\uline{$A_{add}$ adds the patch into the search image $\textbf x$ at location $(\frac{x_0+x_1}{2},\frac{y_0+y_1}{2})$.}''
\textbf{in Section III.B of the revised manuscript.}

\textbf{We have replaced ``We evaluate our video-agnostic perturbations for targeted attacks on several tracking benchmarks, i.e., OTB2015 \cite{OTB}, GOT-10k \cite{GOT-10k}, LaSOT \cite{LaSOT}, UAV123 \cite{UAV123}, VOT1016 \cite{VOT2016}, VOT2018 \cite{VOT2018} and VOT2019 \cite{VOT2019}.'' with}
``\uline{We evaluate our video-agnostic perturbations for targeted attacks on several tracking benchmarks, i.e., OTB2015 \cite{OTB}, GOT-10k \cite{GOT-10k}, LaSOT \cite{LaSOT}, UAV123 \cite{UAV123}, VOT2016 \cite{VOT2016}, VOT2018 \cite{VOT2018} and VOT2019 \cite{VOT2019}.}''
\textbf{in Section IV.A of the revised manuscript.}

\textbf{We have replaced ``We adopt COCO \cite{COCO}, ILSVRC-VID \cite{VID} and the training splits of GOT-10k \cite{GOT-10k} and LaSOT \cite{LaSOT} as our training set. (see Table \ref{tab:dataset})'' with}
``\uline{We adopt COCO \cite{COCO}, ILSVRC-VID \cite{VID} and the training splits of GOT-10k \cite{GOT-10k} and LaSOT \cite{LaSOT} as our training set.}''
\textbf{in Section IV.B of the revised manuscript.}

\textbf{We have mentioned the reinitialization of of VOT-toolkit in Section IV.A of the revised manuscript:}
\uline{Different from other datasets, the VOT dataset has a reinitialization module. When the tracker loses the target (i.e., the overlap is zero between the predicted result and the annotation), the tracker will be reinitialized with the ground truth.}

%%%%%%%%%%%%%%%%% 审稿人 3 %%%%%%%%%%%%%%%%%
\clearpage
\newpage
{\centering\section*{Response Letter to Reviewer \#3}}
\noindent Dear Reviewer \#3:

Thank you very much for your thorough review. Your insightful comments are very helpful for us to improve the quality of the paper. According to your comments and suggestions, we have carefully and extensively revised the manuscript. The main revised parts are highlighted by underlines in the underlined version for your convenience. You will find that all your comments and suggestions are considered and followed. We hope that our revised manuscript is now appropriate for publication in IEEE Transactions on Circuits and Systems for Video Technology.
In addition, point-to-point responses to your comments are given below and highlighted using bold font in line with your comments in order to facilitate cross-referencing.\\[10pt]
\indent We are looking forward to your reply.\\[10pt]
\noindent Yours sincerely,\\
\noindent Zhenbang Li, Yaya Shi, Jin Gao, Shaoru Wang, Bing Li, Pengpeng Liang, Weiming Hu
\\
\\
\\
\noindent Dr. Jin Gao (Contact author)\\
\noindent National Laboratory of Pattern Recognition (NLPR)\\
\noindent Institute of Automation, Chinese Academy of Sciences (CASIA)\\
\noindent Address: No. 95, Zhongguancun East Road, Haidian District,\\
\noindent Beijing 100190, P. R. China\\
\noindent Email: jin.gao@nlpr.ia.ac.cn

\newpage

%%%% 问题 3.1 %%%%
\textit{This paper employs the universal perturbation attacks on Siamese visual trackers. There are still the following concerns about the proposed method. 1. As stated in the paper, the proposed method ``does not require gradient optimization or network inference". However, this is a double-edged sword since it resulted in suspicious attacks. Prior works commonly train a network to prevent not only suspicious attacks but also modifying every pixel. I think it's a major problem with this work. I suggest considering a proper strategy to remove/reduce it.}

\textbf{
Thanks for your advice.
% 承认已有问题：相比于现有的目标跟踪攻击算法，我们方法的不足之处是扰动值较为显眼。
The shortcoming of our method is the more conspicuous perturbation values compared to existing target tracking attack algorithms.
% 解释我们的扰动显眼的原因：其他方法在整个搜索图像添加扰动，而我们必须要在$64 \times 64$这个很小的区域内添加扰动，这增加了学习的难度，使得扰动值大。
Other methods add perturbations over the entire search image, whereas we need to add perturbations over a very small region of $64 \times 64$, which increases the learning difficulty and makes the perturbation values large.
% 解释我们不用全图攻击的原因：我们仅在小区域内而不是在整个搜索图像上添加扰动，因为必须指定轨迹在哪。
% 解决方案：为了既享受全图扰动带来的低感知性，以及局部扰动用于指定目标位置，我们考虑在不同的图像通道执行不同的扰动：首先将图像转为YCbCr通道，在Y通道做全图攻击，在CbCr通道做局部攻击。or: 为了在有目标攻击的同时降低可感知性，...
To reduce perceptibility while having a targeted attack, we consider a new way to perturb the search image: first we convert the search image into YCbCr color space, then add perturbations over the entire search image in the Y channel, and add perturbations over a very small region of $64 \times 64$ in the CbCr channel.
% 介绍什么是 YCbCr
Different from RGB color space, YCbCr color space encodes a color image similar to human eyes’ retina, which separates the RGB components into a luminance component (Y) and two chrominance components (Cb as blue projection and Cr as red projection).
% 解释为什么用YCbCr而不是RGB。选择 YCbCr 而不是 RGB 的原因是更加独立。
We choose YCbCr color space since its color channels are less correlated and quite independent than RGB.
% 解释为什么在CbCr上做局部攻击在Y上做全图攻击，而不是其他方式：有研究证明Y通道可感知性更强而CbCr弱一些。因为我们希望补丁区域扰动值更小，所以我们在CbCr做局部攻击。
While considering the color sensitivity of human vision system, the CbCr channels are least sensitive that the Y channel. This means that the patch added in the CbCr channels has better performance of transparency.
% 实验细节：迭代了多少次？损失函数各项的权重是什么？模板图像怎么操作。
% 实验结果。
The detailed analysis of new added experiment is added in Section IV.D of the revised manuscript.
For your convenience in cross-checking, the new text is given as follows.}

\begin{figure}[t]
  \centering
  \includegraphics[width=.8\textwidth]{images_imperceptible/1.pdf}
  \caption{Visualization of the perturbations.
  % 介绍什么是 RGB Attack
  Figure (a) demonstrates the perturbations trained by the RGB attack method, i.e., the attack attack method proposed in Section III.
  % 介绍什么是 YCbCr Attack
  Figure (b) demonstrates the perturbations trained by the YCbCr attack represents the attack method: first we convert the search image into YCbCr color space, then add perturbations over the entire search image in the Y channel, and add perturbations over a very small region of $64 \times 64$ in the CbCr channel. 
  For the template image, we convert it into YCbCr color space, then add the perturbation over the entire template image in all YCbCr channels.
  The perturbed template and search images are then converted into RGB color scpace and fed into the tracking network.
  The other steps of the training process are the same as the attack method in Section III.
  the YCbCr attack method can increase SSIM of the search image from 0.56 to 0.78. The template SSIM remains unchanged at 0.79.}
  \label{fig:YCbCr1}
\end{figure}

\begin{table}[t]
  \centering
  \caption{Influence of different ways to perturb the search image. One way is adding a small patch on the RGB channel. The other way is adding perturbation on the whole search image on the Y channel and add a small patch on the CbCr channel. The attack performance is evaluated according to AO/SR on GOT-Val.}
  \label{table:perturb}
  \begin{tabular}{@{}rcccccc@{}}
  \toprule
  \multirow{2}{*}[-1pt]{\begin{tabular}[c]{@{}c@{}}Way to perturb\\ the search image\end{tabular}} & \multicolumn{2}{c}{Untargeted Attack} & \multicolumn{2}{c}{Targeted Attack} & \multicolumn{2}{c}{SSIM} \\ \cmidrule{2-7}
                                                         & AO                                      & SR                               & AO                & SR                   & $\delta$          & $p$  \\ \midrule
  GRB Attack                                             & 0.246                                   & 0.227                            & 0.682             & 0.756                & 0.56              & 0.79 \\
  YCbCr Attack                                           & 0.153                                   & 0.123                            & 0.840             & 0.890                & 0.78              & 0.79 \\ \bottomrule        
  \end{tabular}
\end{table}

\uline{\textit{Attack in YCbCr Space}
% 承认已有问题：相比于现有的目标跟踪攻击算法，我们方法的不足之处是扰动值较为显眼。
The shortcoming of our method is the more conspicuous perturbation values compared to existing target tracking attack algorithms.
% 解释我们的扰动显眼的原因：其他方法在整个搜索图像添加扰动，而我们必须要在$64 \times 64$这个很小的区域内添加扰动，这增加了学习的难度，使得扰动值大。
Other methods add perturbations over the entire search image, whereas we need to add perturbations over a very small region of $64 \times 64$, which increases the learning difficulty and makes the perturbation values large.
% 解释我们不用全图攻击的原因：我们仅在小区域内而不是在整个搜索图像上添加扰动，因为必须指定轨迹在哪。
% 解决方案：为了既享受全图扰动带来的低感知性，以及局部扰动用于指定目标位置，我们考虑在不同的图像通道执行不同的扰动：首先将图像转为YCbCr通道，在Y通道做全图攻击，在CbCr通道做局部攻击。or: 为了在有目标攻击的同时降低可感知性，...
To reduce perceptibility while having a targeted attack, we consider a new way to perturb the search image: first we convert the search image into YCbCr color space, then add the perturbation over the entire search image in the Y channel, and add perturbations over a very small region of $64 \times 64$ in the CbCr channel.%The perturbed search image is then converted into RGB color space and fed into the tracking network.
% 介绍什么是 YCbCr
Different from RGB color space, YCbCr color space encodes a color image similar to human eyes’ retina, which separates the RGB components into a luminance component (Y) and two chrominance components (Cb as blue projection and Cr as red projection).
% 解释为什么用YCbCr而不是RGB。选择 YCbCr 而不是 RGB 的原因是更加独立。
We choose YCbCr color space since its color channels are less correlated and quite independent than RGB \cite{8630918}.
% 解释为什么在CbCr上做局部攻击在Y上做全图攻击，而不是其他方式：有研究证明Y通道可感知性更强而CbCr弱一些。因为我们希望补丁区域扰动值更小，所以我们在CbCr做局部攻击。
While considering the color sensitivity of human vision system, the CbCr channels are least sensitive that the Y channel \cite{8630918}. This means that the patch added in the CbCr channels has better performance of transparency.
% 介绍模板图像
For the template image, we convert it into YCbCr color space, then add the perturbation over the entire template image in all YCbCr channels.
The perturbed template and search images are then converted into RGB color scpace and fed into the tracking network.
% 实验细节：迭代了多少次？损失函数各项的权重是什么？模板图像怎么操作。
The other steps of the training process are the same as the attack method in Section III.
% 实验结果。
The experimental result is shown in Table \ref{table:perturb}. As we can see,
% 分析攻击性能
the attack performance is slightly degraded. In terms of AO on GOT-VAL, the targeted attack performance changes from 0.840 to 0.680, and the untargeted attack performance changes from 0.153 to 0.246.
% 分析可感知性
% As shown in Figure \ref{fig:YCbCr}, 
As shown in Table \ref{table:perturb},
our new method can increase SSIM of the search image from 0.56 to 0.78. The template SSIM remains unchanged at 0.79.
}

% \begin{table}[]
%   \begin{tabular}{lllllll}
%   \caption{Performance of the new attack method. We can see SSIM increases.}
%           & \multicolumn{2}{l}{Untargeted Attack} & \multicolumn{2}{l}{Targeted Attack} & \multirow{2}{*}{SSIM x} & \multirow{2}{*}{SSIM z} \\
%           & AO                & SR                & AO               & SR               &                         &                         \\
%   method1 & 0.1747            & 0.1442            & 0.8448           & 0.8972           & 0.56                    & 0.79                    \\
%   method2 &                   &                   &                  &                  & 0.78                    & 0.79                   
%   \end{tabular}
% \end{table}

%%%% 问题 3.2 %%%%
\textit{2. The proposed method and offline training phase should be explained more clearly. For instance, the termination of offline training or offline optimization is missed affecting perturbation values.}

\textbf{Thanks for the good comment. As suggested, we have explained in detail the perturbation update process in offline training.
The detailed analysis is added in Section III.B of the revised manuscript.
For your convenience in cross-checking, the new text is given as follows.}

\uline{Before introducing the the perturbation update process in offline training, we first revisit the popular adversarial example generation methods. One of the simplest methods to generate adversarial images $I^{adv}$ works by linearizing loss function in $L_{\infty}$ neighbourhood of a clean image and finds exact maximum of linearized function using following closed-form equation \cite{FGSM}:}
\begin{equation}
    I^{adv} = I + \epsilon \cdot \text{ sign} \bigl( \nabla_I L(I, y_{true})  \bigr),
    \vspace{-0.1cm}
\end{equation}
\uline{where $I$ is the input image, and the values of the pixels are integer numbers in the range [0, 255]. $y_{true}$ is the true label for the image $I$. $L(I, y)$ is the cost function of the neural network for the attack purpose, given image $I$ and label $y$. $\epsilon$ is a hyper-parameter to be chosen. A straightforward way to extend the above method is applying it multiple times with small step size, and clipping pixel values of intermediate results after each step to ensure that they are in an $\epsilon$-neighbourhood of the original image.  This leads to the Basic Iterative Method (BIM) introduced in \cite{kurakin2017adversarial}:}
\begin{equation}
    \begin{gathered}
        I_0^{adv} = I, \\
        I_{N+1}^{adv} = I_N^{adv}+\epsilon \cdot \text{ sign}(\nabla_I L(I_N^{adv},y_{true})),
    \end{gathered}
\end{equation}
%\uline{where $Clip_{I, \epsilon} \left\{ I' \right\}$ is the function which performs per-pixel clipping of the image $I'$, so that the result will be in $L_{\infty}$ $\epsilon$-neighbourhood of the source image $I$.}
\uline{The BIM can be easily made into an attacker for a specific desired target class, called the Iterative Target Class Method \cite{kurakin2017adversarial}:}
\begin{equation}
  \begin{gathered}
      I_0^{adv} = I,\\
      I_{N+1}^{adv} = I_N^{adv}-\epsilon \cdot \text{ sign}(\nabla_I L(I_N^{adv},y_{target})).
  \end{gathered}
  \label{equ:itcm}
\end{equation}

\uline{
We utilize the Iterative Target Class Method to update the perturbation values during the training process. At each training step, the perturbations are updated as follows:}

\begin{gather}
  \delta_{k+1} = \delta_{k} - \epsilon_1 \cdot \text{sign}(\nabla_{\delta_k}L)\\
  p_{k+1} = p_{k} - \epsilon_2 \cdot \text{sign}(\nabla_{p_k}L),
  \end{gather}

\textit{3. The descriptions of figures and tables are not self-explanatory.}

\textbf{Thanks for your advice and we have replaced the descriptions of figures and tables. The modification is shown as follows for the convience of cross checking.}

% table 2
\textbf{In Table II, we have replaced ``Compared with baselines on perturbed GOT-Val dataset.'' with}
\uline{``Comparison of attack performance with 2 baseline methods on GOT-Val in terms of AO. Baseline1 performs untargeted attacks based on the UAP \cite{UAP} method. Baseline2 performs targeted attacks based on the adversarial patch \cite{patch} method.''}
\textbf{in Section IV.C of the revised manuscript.}

% table 6
\textbf{In Table VI, we have replaced ``Contribution of each loss on GOT-Val.'' with}
\uline{``Analysis of the impact of each loss component on GOT-Val.''}
\textbf{in Section IV.D of the revised manuscript.}

% table 7
\textbf{In Table VII, we have replaced ``Transferability to different backbones on GOT-Val.'' with}
\uline{``Transferability to different backbones on GOT-Val. The original perturbations are trained on SiamFC++\_GoogleNet, then we directly apply the perturbations to two more different backbones of SiamFC++, i.e., ShuffleNet \cite{ShuffleNet} and AlexNet \cite{AlexNet}.''}
\textbf{in Section IV.E of the revised manuscript.}

% table 8
\textbf{In Table VIII, we have replaced ``Transferability to different tracking architectures on OTB2015.'' with}
\uline{``Transferability to different tracking architectures on OTB2015. The original perturbations are trained on SiamFC++\_GoogleNet, then we directly apply the perturbations to two more state-of-the-art anchor-based trackers: AlexNet-based SiamRPN \cite{SiamRPN} and ResNet-based SiamRPN++ \cite{SiamRPN++}.''}
\textbf{in Section IV.E of the revised manuscript.}

% table 10
\textbf{In Table X, we have replaced ``Untargeted attack: Precision score on OTB2015.'' with}
\uline{``State-of-the-art comparison of untargeted attack performance on OTB2015 in terms of precision score.''}
\textbf{in Section IV.F of the revised manuscript.}

% table 11
\textbf{In Table XI, we have replaced ``Targeted attack: Precision score on OTB2015.'' with}
\uline{``State-of-the-art comparison of targeted attack performance on OTB2015 in terms of precision score.''}
\textbf{in Section IV.F of the revised manuscript.}

% figure 8
\textbf{In Figure VIII, we have replaced ``Adversarial patches of different sizes.'' with}
\uline{``Influence of the Adversarial Patch Size. In our experiment, we produce three different sizes of the adversarial patch, namely, 64-by-64, 32-by-32 and 16-by-16 to test the efficiency of their attacks. We evaluate the AO score of SiamFC++ on the perturbed GOT-Val dataset.''}
\textbf{in Section IV.D of the revised manuscript.}

\textit{4. I suggest adding the advantages \& limitations of the proposed method after experimental analysis and future works to the conclusion.}

\textbf{Thanks for the good commenet. As suggested, we have added the advantages \& limitations of the proposed method after experimental analysis and future works to the conclusion. For your convenience in cross-checking, the new text is given as follows.}

\uline{
In summary, compared with other attack methods, our method has two key advantages. First,
  % 通用的，也就是不需要进行网络推断或者梯度计算，因此使得可以在不消耗计算资源的同时执行攻击成为可能。
because our video-agnostic feature, we are fast, no need to network interferes or gradient calculation, making it possible to attack a real-world online-tracking system when we can not get access to the limited computational resources.
Second, the proposed perturbations show good transferability to other anchor free or anchor based trackers. 
  %(1) Physical attack. We are the digital attack, which means ... Physical attack is ... The advantage of it is ... Our work assumes a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as input.
  % 这不就是 ICCV2019 的工作吗？它们也通用？好像是。EOT是通用的吗？
The main limitation of our work is the values of our universal perturbations are not as small as other non-universal perturbations due to the task difficulty.
% Future work:
  % (1) We need to do the physical attack.
  % (2) We need to be imperceptible.
  % 使用同一个扰动同时攻击多个视觉任务，包括图像分类，检测，视频目标跟踪和分割。
  In future work, we expect that it will be possible to demonstrate the existence of a single perturbation to attack multi vision tasks, including image classification, object detection and video object tracking/segementation.
  % 以更小的扰动取得更高的攻击效果。（呼应缺点之：扰动大）
  % Use more imperceptible perturbations to achieve high attack performance.
  % 本文所提出的对抗性信息被添加在数字图像上，从而影响跟踪器的跟踪效果。然而所提出的对抗性信息难以用于执行孪生跟踪器的物理攻击，即将对抗扰动打印出来添加到物理世界的真实目标上，从而影响孪生跟踪系统的性能。因为对抗扰动的打印过程可能改变扰动的值，且相机传感器可能无法感知较小的扰动。可能的解决方案是借助EOT生成针对相机距离和角度具有鲁棒性的物理扰动，使得在各种成像条件下，跟踪系统都无法对带有对抗性信息的真实目标进行正确跟踪。
  % 通过 *** 方式（比如，借助 *** 信息）实现物理攻击。
  % 防御？
}

\textit{5. Some of the experiments require more explanations. For example, transferability has been investigated by different backbones \& architectures. It's needed to mention these experiments are with/without the training phase or not.}

\textbf{Thanks for the good comment. These experiments are without the training phase. These experiments do not need the training phase and we have clarified this in Section IV.E of the revised manuscript. For your convenience in cross-checking, the new text is given as follows.}

\uline{In this sub-section, we analyze the transferability of the proposed attack method. Specifically, the perterbations are trained on SiamFC++\_GoogleNet, then we directly apply the perturbations to other tracking networks including SiamFC++\_ShuffleNet, SiamFC++\_\\AlexNet, SiamPRN and SiamRPN++.}

\textit{6. In experiments, I suggest considering two scenarios of different directions and trajectories.}

\textbf{Thanks for the good comment. As suggest, we consider an alternative way to generate the \textit{fake trajectory} without the use of the ground truth boxes. Specifically, the attacker forces the tracker to follow a fixed direction. We illustrate this with 4 different directions, consisting of shifting the box by $(\pm 3, \pm 3)$ pixels in each consecutive frames, corresponding to the four directions $45^{\circ}, -45^{\circ}, 135^{\circ}, -135^{\circ}$.
% 介绍在什么数据集上做的实验，用的什么评价指标。
The attack performance is evaluated according to AO/SR on GOT-Val.
The detailed analysis of new added experiment is added in Section IV.D of the revised manuscript. For your convenience in cross-checking, the new text is given as follows.}

\uline{\textit{Ablation Study: An alternative way to generate the fake trajectory} Here, we consider an alternative way to generate the \textit{fake trajectory}. Specifically, the attacker forces the tracker to follow a fixed direction. We illustrate this with 4 different directions, consisting of shifting the box by $(\pm 3, \pm 3)$ pixels in each consecutive frames, corresponding to the four directions $45^{\circ}, -45^{\circ}, 135^{\circ}, -135^{\circ}$.
% 介绍在什么数据集上做的实验，用的什么评价指标。
The attack performance is evaluated according to AO/SR on GOT-Val.
% 分析实验结果。简单分析即可。
As shown in Table \ref{table:direction}, our attack algorithm achieves effective attacks under two different fake trajectories. Specifically, when the fake trajectory follows a fixed direction, the AO of the untargeted attack is 0.175, and the AO of the targeted attack is 0.845.}

\begin{table}[t]
  \renewcommand\thetable{\ref{table:direction}}
  \centering
  \caption{Influence of different ways to generate the fake trajectory. One way is the fake trajectory to follow a fixed direction. The other way is the fake trajectory to follow the real target. The attack performance is evaluated according to AO/SR on GOT-Val.}
  \begin{tabular}{@{}rcccc@{}}
  \toprule
  \multirow{2}{*}[-2pt]{Type of the Fake Trajectory} & \multicolumn{2}{c}{Untargeted Attack} & \multicolumn{2}{c}{Targeted Attack} \\ \cmidrule{2-5}
                              & AO                & SR                & AO               & SR               \\ \midrule
  Fixed direction             & 0.175             & 0.144             & 0.845            & 0.897            \\
  Follow the real target      & 0.153             & 0.123             & 0.840            & 0.890            \\ \bottomrule        
  \end{tabular}
  %\label{table:direction1}
\end{table}

\textit{7. There are still some typo and grammar mistakes in the paper.}

\textbf{Thanks for the good comment. We have carefully checked through the whole text and corrected the grammar mistakes and typos. Specifically,}
\textbf{We have replaced ``Experiment results on OTB2015 \cite{OTB}, GOT-10k \cite{GOT-10k}, LaSOT \cite{GOT-10k}, UAV123 \cite{UAV123}, VOT1016 \cite{VOT2016}, VOT2018 \cite{VOT2018} and VOT2019 \cite{VOT2019}. benchmarks demonstrate the effectiveness and efficiency of our approach.'' with}
``\uline{Experiment results on OTB2015 \cite{OTB}, GOT-10k \cite{GOT-10k}, LaSOT \cite{LaSOT}, UAV123 \cite{UAV123}, VOT2016 \cite{VOT2016}, VOT2018 \cite{VOT2018} and VOT2019 \cite{VOT2019}. benchmarks demonstrate the effectiveness and efficiency of our approach.}''
\textbf{in Section I of the revised manuscript.}

\textbf{We have replaced ``In SiamFC++, the tracker first transforms the paired reference frame $I_1$ and annotation $b_1^{gt}$ to get an template image $\textbf z_1$, and transforms the search frame $I_i$ to get the search image $\textbf x_i$ centered at the position estimated in the previous frame.'' with}
``\uline{In SiamFC++, the tracker first transforms the paired reference frame $I_1$ and annotation $b_1^{gt}$ to get a template image $\textbf z_1$, and transforms the search frame $I_i$ to get the search image $\textbf x_i$ centered at the position estimated in the previous frame.}''
\textbf{in Section III.A of the revised manuscript.}

\textbf{We have replaced ``However, CNN attacks are usually expected imperceptible but the above method has to paste an obviously noticeable fake target patch to tracking frames, which raises the risk of being suspected.'' with}
``\uline{However, CNN attacks are usually expected imperceptibly but the above method has to paste an obviously noticeable fake target patch to tracking frames, which raises the risk of being suspected.}''
\textbf{in Section III.A of the revised manuscript.}

\textbf{We have  replaced ``$A_add$ adds the patch into the search image $\textbf x$ at location $(\frac{x_0+x_1}{2},\frac{y_0+y_1}{2})$.'' with}
``\uline{$A_{add}$ adds the patch into the search image $\textbf x$ at location $(\frac{x_0+x_1}{2},\frac{y_0+y_1}{2})$.}''
\textbf{in Section III.B of the revised manuscript.}

\textbf{We have replaced ``We evaluate our video-agnostic perturbations for targeted attacks on several tracking benchmarks, i.e., OTB2015 \cite{OTB}, GOT-10k \cite{GOT-10k}, LaSOT \cite{LaSOT}, UAV123 \cite{UAV123}, VOT1016 \cite{VOT2016}, VOT2018 \cite{VOT2018} and VOT2019 \cite{VOT2019}.'' with}
``\uline{We evaluate our video-agnostic perturbations for targeted attacks on several tracking benchmarks, i.e., OTB2015 \cite{OTB}, GOT-10k \cite{GOT-10k}, LaSOT \cite{LaSOT}, UAV123 \cite{UAV123}, VOT2016 \cite{VOT2016}, VOT2018 \cite{VOT2018} and VOT2019 \cite{VOT2019}.}''
\textbf{in Section IV.A of the revised manuscript.}

\textbf{We have replaced ``We adopt COCO \cite{COCO}, ILSVRC-VID \cite{VID} and the training splits of GOT-10k \cite{GOT-10k} and LaSOT \cite{LaSOT} as our training set. (see Table \ref{tab:dataset})'' with}
``\uline{We adopt COCO \cite{COCO}, ILSVRC-VID \cite{VID} and the training splits of GOT-10k \cite{GOT-10k} and LaSOT \cite{LaSOT} as our training set.}''
\textbf{in Section IV.B of the revised manuscript.}

\textit{8. Missing key ref : "Deep Learning for Visual Tracking: A Comprehensive Survey," in IEEE Transactions on Intelligent Transportation Systems, 2021.}

\textbf{Thanks for the good comment. We have cited this paper at Section II of the revised manuscript.  For your convenience in cross-checking, the new text is given as follows.}

\uline{A comprehensive survey of the related trackers is beyond the scope of this paper, so we only briefly review trackers that are most relevant to our work: correlation-filtering based trackers and Siamese trackers. Please refer to \cite{9339950} for a thorough survey on visual object tracking methods.}

%%%%%%%%%%%%%%%%% 审稿人 4 %%%%%%%%%%%%%%%%%
\clearpage
\newpage
{\centering\section*{Response Letter to Reviewer \#4}}
\noindent Dear Reviewer \#4:

Thank you very much for your thorough review. Your insightful comments are very helpful for us to improve the quality of the paper. According to your comments and suggestions, we have carefully and extensively revised the manuscript. The main revised parts are highlighted by underlines in the underlined version for your convenience. You will find that all your comments and suggestions are considered and followed. We hope that our revised manuscript is now appropriate for publication in IEEE Transactions on Circuits and Systems for Video Technology.
In addition, point-to-point responses to your comments are given below and highlighted using bold font in line with your comments in order to facilitate cross-referencing.\\[10pt]
\indent We are looking forward to your reply.\\[10pt]
\noindent Yours sincerely,\\
\noindent Zhenbang Li, Yaya Shi, Jin Gao, Shaoru Wang, Bing Li, Pengpeng Liang, Weiming Hu
\\
\\
\\
\noindent Dr. Jin Gao (Contact author)\\
\noindent National Laboratory of Pattern Recognition (NLPR)\\
\noindent Institute of Automation, Chinese Academy of Sciences (CASIA)\\
\noindent Address: No. 95, Zhongguancun East Road, Haidian District,\\
\noindent Beijing 100190, P. R. China\\
\noindent Email: jin.gao@nlpr.ia.ac.cn

\newpage
\textit{This paper proposes a universal targeted attacks method on Siamese visual tracking task. It seems that the method is feasible and somewhat novel.}

\textbf{Many thanks for your positive comments on the strength of our paper and the novelty of the proposed attack method.}

\textit{My major concern is that the writing and organization need to be carefully modified and optimized. Besides, 1. Authors are focused on the anchor-free tracker in the experiments, but the anchor-free Siamese trackers are not well mentioned. I suggest the authors to include the discussion of state-of-the-art of other similar approaches (e.g., FCOT, SiamCAR, OCEAN, et. al.).}

\textbf{We have discussed several state-of-the-art anchor free Siamese trackers at the related work. For your convenience of cross review, the new text is given as follows.}

\uline{
% Ocean
Ocean \cite{zhang2020ocean} directly predicts the position and scale of target objects in an anchor-free fashion. Since each pixel in groundtruth boxes is well trained, the tracker is capable of rectifying inexact predictions of target objects during inference.
% FCOT
FCOT \cite{cui2020fully} introduces an online regression model generator (RMG) based on the carefully designed anchor-free box regression branch, which enables the tracker to be more effective in handling target deformation during tracking procedure.
%  SiamCAR
SiamCAR \cite{9157720} is both proposal and anchor free, and decomposes tracking into two subproblems: one classification problem and one regression task. SiamCAR takes one unique response map to predict object location and bounding box directly.
}

\textbf{Besides, we have evaluated the transferability to the anchor free tracker OCEAN in Setcion IV.E of the revised manuscript. For your convenience of cross review, the new text is given as follows.}

\uline{We evaluate the transferability to Ocean on OTB2015 in terms of success. As shown in table \ref{tab:arch}, the tracking performance drops form 0.902 to 0.282.}

\begin{table}[t!]
  \centering
  \caption{Transferability to different tracking architectures on OTB2015. The original perturbations are trained on SiamFC++\_GoogleNet, then we directly apply the perturbations to two more state-of-the-art anchor-based trackers: AlexNet-based SiamRPN \cite{SiamRPN} and ResNet-based SiamRPN++ \cite{SiamRPN++}.}
  \begin{tabular}{rcccc} 
  \toprule
  \multirow{2}{*}[-2pt]{Trackers} & \multicolumn{2}{c}{Before Attack} & \multicolumn{2}{c}{Untargeted Attack}  \\
  \cmidrule{2-5}
                            & AO & Precision              & AO    & Precision \\ \midrule
  SiamRPN++                 & 0.676   & 0.879             & 0.418 & 0.556     \\
  SiamRPN                   & 0.666   & 0.876             & 0.483 & 0.643     \\
  Ocean                     & 0.672   & 0.902             & 0.237 & 0.282     \\ \bottomrule
  \end{tabular}
  \label{tab:arch}
\end{table}

\textit{2. The model is trained based on Eq.11 and Eq.12. It is not clear why the sign function is introduced. It is also suggested to describe the derivation of these two equations in detail.}

\textbf{Thanks for the good comment. As suggested, we have explained in detail the perturbation update process in offline training.
The detailed analysis is added in Section III.B of the revised manuscript.
For your convenience in cross-checking, the new text is given as follows.}

\uline{Before introducing the the perturbation update process in offline training, we first revisit the popular adversarial example generation methods. One of the simplest methods to generate adversarial images $I^{adv}$ works by linearizing loss function in $L_{\infty}$ neighbourhood of a clean image and finds exact maximum of linearized function using following closed-form equation \cite{FGSM}:}
\begin{equation}
    I^{adv} = I + \epsilon \cdot \text{ sign} \bigl( \nabla_I L(I, y_{true})  \bigr),
    \vspace{-0.1cm}
\end{equation}
\uline{where $I$ is the input image, and the values of the pixels are integer numbers in the range [0, 255]. $y_{true}$ is the true label for the image $I$. $L(I, y)$ is the cost function of the neural network for the attack purpose, given image $I$ and label $y$. $\epsilon$ is a hyper-parameter to be chosen. A straightforward way to extend the above method is applying it multiple times with small step size, and clipping pixel values of intermediate results after each step to ensure that they are in an $\epsilon$-neighbourhood of the original image.  This leads to the Basic Iterative Method (BIM) introduced in \cite{kurakin2017adversarial}:}
\begin{equation}
    \begin{gathered}
        I_0^{adv} = I, \\
        I_{N+1}^{adv} = I_N^{adv}+\epsilon \cdot \text{ sign}(\nabla_I L(I_N^{adv},y_{true})),
    \end{gathered}
\end{equation}
%\uline{where $Clip_{I, \epsilon} \left\{ I' \right\}$ is the function which performs per-pixel clipping of the image $I'$, so that the result will be in $L_{\infty}$ $\epsilon$-neighbourhood of the source image $I$.}
\uline{The BIM can be easily made into an attacker for a specific desired target class, called the Iterative Target Class Method \cite{kurakin2017adversarial}:}
\begin{equation}
  \begin{gathered}
      I_0^{adv} = I,\\
      I_{N+1}^{adv} = I_N^{adv}-\epsilon \cdot \text{ sign}(\nabla_I L(I_N^{adv},y_{target})).
  \end{gathered}
  \label{equ:itcm}
\end{equation}

\uline{We utilize the Iterative Target Class Method to update the perturbation values during the training process. At each training step, the perturbations are updated as follows:}

\begin{gather}
  \delta_{k+1} = \delta_{k} - \epsilon_1 \cdot \text{sign}(\nabla_{\delta_k}L)\\
  p_{k+1} = p_{k} - \epsilon_2 \cdot \text{sign}(\nabla_{p_k}L),
\end{gather}


\textit{3. The format of all the Tables is suggested to be unified.}

\textbf{Thanks for the good comment. We have unified the format of all the Tables in the revised manuscript.}

\textit{4. There are many typos in the paper, including but not limited to the following errors:}

\textit{(a) In the third line below Eq.7, $A_{a}dd$ should be $A_{add}$.}

\textit{(b) In page 2, difference datasets GOT-10K[12], LaSOT \cite{LaSOT} cite the same reference.}

\textit{(c) In page 3, Subsection A of Section 3, “to get an template image” should be “to get a template image”.}

\textbf{Thanks for the good comment. We have carefully checked through the whole text and corrected the grammar mistakes and typos. Specifically,}
\textbf{We have replaced ``Experiment results on OTB2015 \cite{OTB}, GOT-10k \cite{GOT-10k}, LaSOT \cite{GOT-10k}, UAV123 \cite{UAV123}, VOT1016 \cite{VOT2016}, VOT2018 \cite{VOT2018} and VOT2019 \cite{VOT2019}. benchmarks demonstrate the effectiveness and efficiency of our approach.'' with}
``\uline{Experiment results on OTB2015 \cite{OTB}, GOT-10k \cite{GOT-10k}, LaSOT \cite{LaSOT}, UAV123 \cite{UAV123}, VOT2016 \cite{VOT2016}, VOT2018 \cite{VOT2018} and VOT2019 \cite{VOT2019}. benchmarks demonstrate the effectiveness and efficiency of our approach.}''
\textbf{in Section I of the revised manuscript.}

\textbf{We have replaced ``In SiamFC++, the tracker first transforms the paired reference frame $I_1$ and annotation $b_1^{gt}$ to get an template image $\textbf z_1$, and transforms the search frame $I_i$ to get the search image $\textbf x_i$ centered at the position estimated in the previous frame.'' with}
``\uline{In SiamFC++, the tracker first transforms the paired reference frame $I_1$ and annotation $b_1^{gt}$ to get a template image $\textbf z_1$, and transforms the search frame $I_i$ to get the search image $\textbf x_i$ centered at the position estimated in the previous frame.}''
\textbf{in Section III.A of the revised manuscript.}

\textbf{We have replaced ``However, CNN attacks are usually expected imperceptible but the above method has to paste an obviously noticeable fake target patch to tracking frames, which raises the risk of being suspected.'' with}
``\uline{However, CNN attacks are usually expected imperceptibly but the above method has to paste an obviously noticeable fake target patch to tracking frames, which raises the risk of being suspected.}''
\textbf{in Section III.A of the revised manuscript.}

\textbf{We have  replaced ``$A_add$ adds the patch into the search image $\textbf x$ at location $(\frac{x_0+x_1}{2},\frac{y_0+y_1}{2})$.'' with}
``\uline{$A_{add}$ adds the patch into the search image $\textbf x$ at location $(\frac{x_0+x_1}{2},\frac{y_0+y_1}{2})$.}''
\textbf{in Section III.B of the revised manuscript.}

\textbf{We have replaced ``We evaluate our video-agnostic perturbations for targeted attacks on several tracking benchmarks, i.e., OTB2015 \cite{OTB}, GOT-10k \cite{GOT-10k}, LaSOT \cite{LaSOT}, UAV123 \cite{UAV123}, VOT1016 \cite{VOT2016}, VOT2018 \cite{VOT2018} and VOT2019 \cite{VOT2019}.'' with}
``\uline{We evaluate our video-agnostic perturbations for targeted attacks on several tracking benchmarks, i.e., OTB2015 \cite{OTB}, GOT-10k \cite{GOT-10k}, LaSOT \cite{LaSOT}, UAV123 \cite{UAV123}, VOT2016 \cite{VOT2016}, VOT2018 \cite{VOT2018} and VOT2019 \cite{VOT2019}.}''
\textbf{in Section IV.A of the revised manuscript.}

\textbf{We have replaced ``We adopt COCO \cite{COCO}, ILSVRC-VID \cite{VID} and the training splits of GOT-10k \cite{GOT-10k} and LaSOT \cite{LaSOT} as our training set. (see Table \ref{tab:dataset})'' with}
``\uline{We adopt COCO \cite{COCO}, ILSVRC-VID \cite{VID} and the training splits of GOT-10k \cite{GOT-10k} and LaSOT \cite{LaSOT} as our training set.}''
\textbf{in Section IV.B of the revised manuscript.}

\normalem
\bibliographystyle{IEEEtran}
\bibliography{ref.bib}

\end{document}


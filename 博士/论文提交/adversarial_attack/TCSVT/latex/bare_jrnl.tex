%!TEX program = pdflatex
%\iffalse
%!BIB program = bibtex
%\fi

\documentclass[journal]{IEEEtran}

\usepackage{graphicx}  % 用于 includegraphics
\usepackage{booktabs}  % 用于表格 midrule
\usepackage{subfigure}  % show sub-figure
\usepackage{algorithm}  % for algrithm
\usepackage{algorithmic}  % for algrithm
\usepackage{multirow}  % for multirow command used in the table
\usepackage{amsfonts}  % for mathbb
\usepackage{color,xcolor}  % 用于改变字体颜色
\usepackage{amssymb}  % 用于打对号
\usepackage{amsfonts}  % 用于 mathbb
\usepackage{url}  % 用于使用连接
\usepackage{amsmath}
\newcommand{\eg}{e.g.}
\newcommand{\ie}{i.e.}

% 题目
\hyphenation{op-tical net-works semi-conduc-tor}
\begin{document}
\title{Video-Agnostic Perturbations: Efficient Targeted Attacks for Siamese Visual Tracking}
\author{Zhenbang Li, Bing Li, Jin Gao, Liang Li, Weiming Hu
\thanks{This work is supported by the National Key R\&D Program of China (No. 2018AAA0102802, No. 2018AAA0102803, No. 2018AAA0102800), the NSFC-General Technology Collaborative Fund for Basic Research (Grant No. U1636218), the Natural Science Foundation of China (Grant No. 61751212, 61721004, 61772225, 61972394), the Key Research Program of Frontier Sciences, CAS (Grant No. QYZDJ-SSW-JSC040), and the National Natural Science Foundation of Guangdong (No. 2018B030311046). \textit{(Corresponding author: Jin Gao.)}}
\thanks{Z. Li, B. Li, J. Gao are with the National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China, and also with School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, 100190, China (e-mail: zhenbang.li@nlpr.ia.ac.cn; bli@nlpr.ia.ac.cn; jin.gao@nlpr.ia.ac.cn).}
\thanks{L. Li is with the Brain Science Center, Beijing Institute of Basic Medical Sciences, Beijing, China (e-mail: liang.li.brain@aliyun.com).}
\thanks{W. Hu is with CAS Center for Excellence in Brain Science and Intelligence Technology, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China, and also with School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, 100190, China (e-mail: wmhu@nlpr.ia.ac.cn).}
}
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
\maketitle

% 摘要
\begin{abstract}
Siamese trackers are shown to be vulnerable to adversarial attacks recently. However, the existing attack methods craft the perturbations for each video independently, which comes at a non-negligible computational cost. The question is what if we can not get access to the limited computational resources in the real-world online-tracking phase.
In this paper, we show the existence of video-agnostic perturbations that can enable the targeted attack, e.g., forcing a tracker to follow the ground-truth trajectory with specified offsets, to be universal and free from inference in a network. Specifically, we attack a tracker by adding a universal imperceptible perturbation to the template image and pasting a \textit{fake target}, i.e., a small universal adversarial patch, into the search images adhering to the predefined trajectory, so that the tracker outputs the location and size of the \textit{fake target} instead of the real target. Our approach allows perturbing a novel video to come at no additional cost except the mere addition and pasting operations -- and not require gradient optimisation or network inference. Experimental results on several datasets demonstrate that our approach can effectively fool the Siamese trackers in a targeted attack manner. The attack performance is state-of-the-art: drop the presion score of OTB2015 from 0.861 to 0.048, and 0.928 for the fake trajectory. The black box attack performance is also good. We will make our code publicly available.
\end{abstract}

% 关键字
\begin{IEEEkeywords}
IEEE, IEEEtran, journal, \LaTeX, paper, template.
\end{IEEEkeywords}
\IEEEpeerreviewmaketitle

% 引言
\section{Introduction}
\IEEEPARstart{T}{his} Given an arbitrary detected or annotated object of interest in the initial video frame, visual object tracking is aimed at {\it recognizing} and {\it localizing} other instances of the same object in subsequent frames. This paradigm of tracking visual objects from a single initial exemplar in the online-tracking phase has been broadly cast as a Siamese network-based one-shot problem recently \cite{SiamFC,SiamRPN,SiamRPN++,SiamFC++}, which is termed as Siamese visual tracking and recognised as highly effective and efficient for visual tracking.

\begin{figure}[htbp]
  \centering
  %\subfigure{\includegraphics[width=0.2\textwidth]{images/x.jpg}} \qquad
  %\subfigure{\includegraphics[width=0.2\textwidth]{images/z.jpg}}
  \includegraphics[width=0.45\textwidth]{images/1_v8.pdf}
  \caption{An illustration of our attacks to SiamFC++ on some example tracking sequences from GOT-10k benchmark. Our approach generates the video-agnostic perturbations which can force SiamFC++ to follow a complicated trajectory at virtually no cost. This is realized by off-line training the perturbations so that the tracker mistakenly believes the \textit{fake target} area contains the object to be tracked (see the left heatmap). Moreover, the quality assessment branch in SiamFC++ is also misled to confirm this result (see the right heatmap). The \textit{fake target} size is gradually decreased.} 
  %Being universal, the generated perturbations can be conveniently exploited to perturb videos on-the-fly without extra computations. 
  %Red represents the ground-truth bounding box, and green represents the predicted bounding box by the tracker.
  %The purple dashed box marks the universal adversarial patch. The left heatmap represents the probability of each spatial position to contain the \textit{fake target}, and the right heatmap represents the target state estimation quality.}
  \label{fig:1}
\end{figure}

Recently, the robustness of Siamese trackers have attracted much attention in the sense of testing their vulnerability to adversarial attacks, and the focus has been directed towards more efficient and low-cost attacks \cite{TTP,FAN,SPARK}. Despite their success, these attack methods are not suitable for attacking the real-world online-tracking systems built upon the small edge platforms with limited computational resources. The reason is that they still need to craft the perturbations for each video independently based on either iterative optimisation or adversarial network inference, for which the adequacy of computational resources may be not ensured by the computationally intensive tracking systems. 

Universal adversarial perturbations (UAPs) proposed in \cite{UAP} can fool most images from a data distribution in an image-agnostic manner. Being universal, UAPs can be conveniently exploited to perturb unseen data on-the-fly without extra computations. Therefore, UAPs are particularly useful when attacking real-time applications deployed on platforms with limited computational resources. However, no existing work has touched the topic of attacking the Siamese trackers using UAPs, because it is hard to apply existing UAPs to attack Siamese trackers directly. The main reason lies in the fact that, (a) most UAPs are designed for typical neural networks with one image as input while Siamese networks accept both the template and search images, and (b) the goal of existing UAP methods is to disturb unary or binary model outputs for single instance while we need to use universal perturbations to mislead Siamese trackers to follow a specified trajectory.
  
In this paper, we make the first attempt in finding the video-agnostic perturbations that fool a state-of-the-art Siamese tracker, \ie, SiamFC++ \cite{SiamFC++}, in a targeted attack manner, which comes at virtually no cost in the online-tracking phase. Specifically, we aim to attack the trackers by adding a universal imperceptible perturbation to the template image and pasting a \textit{fake target}, \ie, a small universal adversarial patch, into the search image adhering to the predefined trajectory (as shown in Figure \ref{fig:1}), so that the tracker outputs the location and size of the \textit{fake target} instead of the real target. Our generated video-agnostic perturbations allow perturbing a novel video to come at no additional cost except the mere addition and pasting operations -- and not require gradient optimization or network inference. Experiment results on OTB2015 \cite{OTB}, GOT-10k \cite{GOT-10k} and LaSOT \cite{GOT-10k} benchmarks demonstrate the effectiveness and efficiency of our approach.

\section{Related Work}

\subsection{Siamese Visual Tracking}

Siamese visual tracking is a fundamental research direction in template matching-based tracking besides the correlation filter-based methods. Both of them are aimed to "causally" estimate the positions of a template cropped from the initial video frame in the subsequent frames. Siamese trackers formulate visual tracking as learning cross-correlation similarities between a target template and the candidates in search region in a convolution fashion. Tracking is then performed by locating the object in the search image region based on the highest visual similarity. This paradigm is formulated as a local one-shot detection task.

Recently, some Siamese trackers~\cite{SiamRPN,SiamRPN++,SiamFC++} have demonstrated a significant performance improvement in visual tracking. 
In particular, SiamRPN \cite{SiamRPN} consists of one Siamese subnetwork for feature extraction and another region proposal subnetwork including the classification and regression branches separately. Based on its success in the decomposition of classification and state estimation, SiamRPN++ \cite{SiamRPN++} further breaks the restriction of strict translation invariance through a simple yet effective spatial aware sampling strategy and successfully trains a ResNet-driven Siamese tracker with significant performance gains. Apart from these anchor-based methods, an anchor-free tracker SiamFC++ \cite{SiamFC++} is further designed by considering non-ambiguous scoring, prior target scale/ratio distribution knowledge-free and estimation quality assessment guidelines.
In our experiments, we are focused on the anchor-free SiamFC++ tracker, whereas the transferability of our generated adversarial attacks to the previous anchor-based trackers is also studied.

\begin{figure}[t]
  \centering
  \subfigure{\includegraphics[width=0.2\textwidth]{images/x.jpg}} \qquad
  \subfigure{\includegraphics[width=0.2\textwidth]{images/z.jpg}}
  \caption{Visualization of the generated perturbations. Left: the template perturbations. Right: the adversarial patch.}
  \label{fig:vis_perturbations}
\end{figure}

\subsection{Adversarial Attacks}

Adversarial attacks to image classification were first investigated in \cite{intriguing}, which is aimed to identify the vulnerability of modern deep networks to imperceptible perturbations. 
Recent studies also emerge to investigate the adversarial attacks to other diverse types of tasks such as natural language processing \cite{generating} and object detection \cite{wei2019transferable}.
Scenarios of possible adversarial attacks can be categorized along different dimensions.

\textit{Write box attack v.s. Black box attack} In the write box attack setting, the attacker knows every thing about the system, including the parameters of the network, the training/testing dataset and so on. In the black box setting, the attacker do not know the parameters of the network. This paper focus on the write box attack. However, although we train perturbations using the known paramters, we surprisely find that the trained perturbations can do black attack on different trackers.

\textit{Un-universal attacks v.s. Universal attacks} In the setting of un-universal attacks, the attacker has to generate one perturbation of every new data, which is time costing. In the setting of the universal attacks, only one perturbation is generated and used to every data point of the dataset. In this paper, we focus on generating universal perturbations for siamese trackers, which is no time/computation cost.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.7\textwidth]{images/network_v5.pdf}
  \caption{The training pipeline of the proposed method. We aim to train an imperceptible perturbation $\delta$ for the template image $\textbf z$, and an adversarial patch $p$ for the search image $\textbf x$. After adding $\delta$ to $\textbf z$ and pasting the \textit{fake target} $p$ into $\textbf x$, the tracker outputs the location and size of the \textit{fake target} instead of the real target.}
  \label{fig:net}
\end{figure*}

\textit{Optimisation based attack and GAN based attack} The optimisation based attack do loops to generate perturbations. The loop is time costing. To avoid this, the GAN based method use GAN to generate perturbation for one datapoint without loop. However, these methods need to run a GAN network for every datapoint, which occupy the GPU memory and time. In this paper, we generate perturbations in an off-line manner, which do not need the loop and network feed forward.

\textit{Imperceptible Perturbations v.s. Adversarial Patch} The imperceptible perturbations most commonly modify each pixel by a small amount and can be found using a number of optimisation strategies such as Limited-memory BFGS \cite{intriguing} and PGD \cite{PGD}.
Different from the imperceptible perturbations, the adversarial patch is extremely salient to a neural network. The adversarial patch can be placed anywhere into the input image to cause the network to misbehave, and thus is commonly used for universal attacks \cite{patch}.
To the best of our knowledge, we are the first to attack object trackers utilizing both the imperceptible perturbation and the adversarial patch together, which are jointly trained in an end-to-end manner.
Note that our adversarial patch works in the network domain instead of the image domain. In the network-domain case, the noise is allowed to take any value and is not restricted to the dynamic range of image value as in the image-domain case \cite{karmon2018lavan}.

\textit{Untargeted Attacks v.s. Targeted Attacks} In the case of untargeted attacks, the adversary's goal is to cause the network to predict any incorrect label and whatever the incorrect label is does not matter, \eg, pushing the object location estimation just outside the true search region in visual tracking.
Targeted attacks, however, aim to change the network's prediction to some specific target label. In visual tracking, the targeted attacks aim to intentionally drive trackers to output specified object locations following a predefined trajectory.

\subsection{Adversarial Attacks in Visual Tracking}

Recently, there are several explorations of the adversarial attacks to the visual tracking task. For example, PAT \cite{PAT} generates physical adversarial textures via white-box attacks to steer the tracker to lock on the texture when a tracked object moves in front of it. However, PAT validates its method by attacking a light deep regression tracker GOTURN \cite{GOTURN}, which has low tracking accuracy on modern benchmarks. In this paper, we aim to attack the state-of-the-art Siamese trackers.
RTAA \cite{RTAA} takes temporal motion into consideration when generating lightweight perturbations over the estimated tracking results frame-by-frame. However, RTAA only performs the untargeted attacks for trackers, which is less challenging than the targeted attacks in this paper, as we aim to create arbitrary, complex trajectories at test time. 

Targeted attacks to follow an erroneous path which looks realistic are crucial to deceive the tracking system without raising any suspicion in the real-world applications.
SPARK \cite{SPARK} computes incremental perturbations by using information from the past frames to perform targeted attacks to Siamese trackers. However, SPARK needs to generate distinct adversarial example for every search image through heavy iterative schemes, which is time-consuming to attack online-tracking in real time. The recent real-time attacker in \cite{TTP} exclusively uses the template image to generate temporally-transferable perturbation in a one-shot manner, and then adds it to every search image. However, this method still needs to generate perturbations for each individual video, and its targeted attack setting requires diverse perturbations from several runs of network inference. It is ill-suited to attack a real-world online-tracking system when we can not get access to the limited computational resources. In this paper, however, we propose video-agnostic perturbations which allow perturbing a novel video to come at no additional cost except the mere addition and pasting operations.

% 方法
\section{Method}

\textcolor{black}{In this section, we introduce our video-agnostic targeted attack framework for Siamese trackers. We aim to attack the tracker by adding an imperceptible perturbation to the template image and pasting a \textit{fake target}, i.e., an adversarial patch, into the search images adhering to the predefined trajectory, so that the tracker outputs the location and size of the \textit{fake target} instead of the real target. Below, we formalize our targeted attacks to SiamFC++ \cite{SiamFC++}, and then introduce our perturbation strategy in detail.}
 
\subsection{Problem Definition}

Let $V=\{I_i\}_1^T$ denote the frames of a video sequence of length $T$. $B^{gt}=\{b^{gt}_i\}_1^T$ is used to represent the target's ground-truth position in each frame. The visual object tracking aims to predict the position $B^{pred}=\{b^{pred}_i\}_1^T$ of this target in the subsequent frames given its initial state. In SiamFC++, the tracker first transforms the paired reference frame $I_1$ and annotation $b_1^{gt}$ to get an template image $\textbf z$, and transforms the search frame $I_i$ to get the search image $\textbf x_i$ centered at the position estimated in the previous frame. At each time-step, the template image $\textbf z$ and the search image $\textbf x_i$ are first passed individually through a shared backbone network and then fused using a channel-by-channel mutual correlation operation:
\begin{equation}
  f_{i}(\mathbf{z}, \mathbf{x})=\psi_{i}(\phi(\mathbf{z})) \star \psi_{i}(\phi(\mathbf{x})), i \in\{\mathrm{cls}, \text { reg }\}
\end{equation}
where $\star$ denotes the channel-by-channel correlation operation, $\phi(\cdot)$ denotes the feature extractor of the twin network, $\psi_i(\cdot)$ denotes the layer specific to the classification or regression task, and $i$ denotes the specific task type ($\mathrm{cls}$ denotes the classification task and $\mathrm{reg}$ denotes the regression task). $\psi_{\mathrm{cls}}$ and $\psi_{\mathrm{reg}}$ are both designed as two convolutional layers for adapting generic features to the feature space specific to the classification/regression task. $\psi_{\mathrm{cls}}$ and $\psi_{\mathrm{reg}}$ extracted features have the same size. The fused features then act as input to a head network, which predicts a classification map $\textbf{C}$, a bounding box regression map $\textbf{R}$, and a quality assessment map $\textbf{Q}$ in an anchor-free manner. In short, $\textbf C$ encodes the probability of each spatial position to contain the target, $\textbf R$ regresses the bounding box of the target, and $\textbf Q$ predicts the target state estimation quality. The final bounding box is then generated according to $\textbf{C}$, $\textbf{R}$ and $\textbf{Q}$.

Formally, we aim to train an imperceptible perturbation $\delta$ for the template image $\textbf z$, and an adversarial patch $p$ for the search image $\textbf x_i$. After adding $\delta$ to $\textbf z$ and pasting the fake target $p$ into $\textbf x_i$, the tracker outputs the location and size of the adversarial patch instead of the real target (see Figure \ref{fig:net}). Both $\delta$ and $p$ are universal (\ie, video-agnostic), which means perturbing a novel video only involves the mere addition and pasting of the perturbations to the template and search images -- and does not require gradient optimisation or network inference.

\subsection{Generating Video-Agnostic Perturbations}

In this subsection, we show how to train the video-agnostic perturbations $(\delta, p)$ for Siamese trackers.
At beginning, each element in $\delta$ is initialized to 0.
$p$ is initialized with a normal distribution, with a mean of 127 and a standard deviation of 1.
During the $k$-th iteration of training, a video $V=\{I_i\}_1^T$ is randomly selected from the training dataset $\mathcal V$. Assuming the template perturbation at the $k$-th iteration is $\delta_k \in \mathbb{R}^{127\times 127 \times 3}$, and the adversarial patch is $p_k \in \mathbb{R}^{128\times 128\times 3}$. We first randomly pick paired frames $I_t, I_s$ from $V$.
The clean template image $\textbf z\in \mathbb{R}^{127\times 127 \times 3}$ is generated according to $I_t$ and $b^{gt}_t$, and the perturbed template image is:
\begin{equation}
\tilde {\textbf z} = \textbf z + \delta_k.
\end{equation}
Similarly, the clean search image $\textbf x \in \mathbb{R}^{303\times 303 \times 3}$ is generated according to $I_s$ and $b^{gt}_s$.
As mentioned before, the patch is regarded as a \textit{fake target} and pasted into the search images. We force the center position of the \textit{fake target} to near the center position of the real target within a shift range of 64 pixels, where shift is defined as the maximum range of translation generated from a uniform distribution.
The width/height of the \textit{fake target} is randomly selected between 32 pixels and 128 pixels.
The perturbed search image is generated as follows:
\begin{equation}
\tilde{\textbf x} = A(\textbf x, p_k, (l^x, l^y), (w, h)),
\end{equation}
where $(l^x, l^y)$ and $(w, h)$ represent the position and size of the \textit{fake target} with respect to the search image, respectively. $A$ is a patch application operator \cite{patch} which first resizes the patch $p_k \in \mathbb{R}^{128\times 128\times 3}$ to $\hat{p}_k \in \mathbb{R}^{w\times h\times 3}$, and then pastes the resized patch $\hat{p}_k$ into the search image $\textbf x$ at location $(l^x,l^y)$.

Subsequently, the SiamFC++ tracker $\phi(\cdot)$ takes $\tilde {\textbf x}$ and $\tilde{\textbf  z}$ as input and makes predictions as follows:
\begin{equation}
\textbf{C, R, Q} = \phi(\tilde {\textbf x}, \tilde{\textbf z}).
\end{equation}

\begin{algorithm}[tb]
  \caption{Training Process}
  \label{alg:algorithm}
  \textbf{Input}: Training dataset $\mathcal{V}$, Siamese tracker $\phi$, and max iteration number $N$.\\
  \textbf{Output}: Imperceptible perturbation $\delta$, and adversarial patch $p$.
  \begin{algorithmic}[1] %[1] enables line numbers
  \STATE Let $k = 0$.
  \WHILE{$k < N$}
  \STATE Randomly pick a video $V\in \mathcal{V}$. The corresponding ground truth is $B^{gt}=\{b^{gt}_i\}^T_1$.
  \STATE Randomly pick paired frames $I_t, I_s$ from $V$.
  \STATE Generate template image $\textbf{z}$ according to $I_t$ and $b^{gt}_t$.
  \STATE $\tilde{\textbf{z}} = \textbf{z} + \delta_k.$
  \STATE Generate search image $\textbf{x}$ according to $I_s$ and $b^{gt}_s$.
  \STATE Calculate the \textit{fake target} position $\{l^x, l^y, w, h\}$ with respect to the search image.
  \STATE $\tilde{\textbf x} = A(\textbf x, p_k, (l^x, l^y), (w, h)).$
  \STATE $\textbf{C, R, Q} = \phi(\tilde {\textbf x}, \tilde{\textbf z}).$
  \STATE Generate fake labels $\textbf{C}^*,\textbf{R}^*,\textbf{Q}^*$ using $\{l^x, l^y, w, h\}$.
  \STATE Calculate loss $L(\textbf{C, R, Q}, \textbf{C}^*, \textbf{R}^*, \textbf{Q}^*)$ using Equ. \ref{eq:loss}.
  \STATE $\delta_{k+1} = \delta_{k} - \epsilon_1 \cdot \text{sign}(\nabla_{\delta_k}L).$
  \STATE $p_{k+1} = p_{k} - \epsilon_2 \cdot \text{sign}(\nabla_{p_k}L).$
  \STATE $k = k + 1.$
  \ENDWHILE
  \STATE \textbf{return} $\delta_N, p_N.$
  \end{algorithmic}
  \label{alg}
\end{algorithm}

\begin{equation}
  \mathrm{IoU}^{*}=\frac{\text { Intersection }\left(B, B^{*}\right)}{\operatorname{Union}\left(B, B^{*}\right)}
\end{equation}  

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\textwidth]{images/vis_v4.pdf}
  \caption{Our targeted attack results following a predefined \textit{fake trajectory} (indicated by the yellow targeted bounding box).}
  \label{fig:vis}
\end{figure*}

\textit{Target Generation} The target is used to calcutate the loss. The target composed of three part: classification target, regression target and quality estimation target. Specifically, for classification, location $(x,y)$ on feature map $\psi_{\mathrm{cls}}$ is considered as a positive sample if its corresponding location $\left(\left\lfloor\frac{s}{2}\right\rfloor+x s,\left\lfloor\frac{s}{2}\right\rfloor+y s\right)$ on the input image falls into the fake bounding box, and vice versa are considered as negative samples. $s=8$ denotes the total step length of the feature extraction network.

In the regression branch, the last convolution layer predicts the distance of position $\left(\left\lfloor\frac{s}{2}\right\rfloor+x s,\left\lfloor\frac{s}{2}\right\rfloor+y s\right)$ to the four edges of the fake target labeled box, denoted as the four-dimensional vector $\boldsymbol{t}^{*}=\left(l^{*}, t^{*}, r^{*}, b^{*}\right)$. Thus, the regression label of position $(x,y)$ can be expressed as:

\begin{equation}
  \begin{array}{ll}
  l^{*}=\left(\left\lfloor\frac{s}{2}\right\rfloor+x s\right)-x_{0}, \quad t^{*}=\left(\left\lfloor\frac{s}{2}\right\rfloor+y s\right)-y_{0} \\
  r^{*}=x_{1}-\left(\left\lfloor\frac{s}{2}\right\rfloor+x s\right), \quad b^{*}=y_{1}-\left(\left\lfloor\frac{s}{2}\right\rfloor+y s\right)
  \end{array}
\end{equation}
where $(x_0, y_0)$ and $(x_1, y_1)$ denote the coordinates of the upper-left and lower-right corners of the fake target labeling box $B^*$, respectively.

SiamFC++ assumes that feature pixels near the center of the target will have higher importance than other pixels.Following the design of SiamFC++, we use a $1 \times 1$ convolutional layer for quality assessment, i.e., to learn the intersection over union (IoU) score of the predicted border to the fake target labeled box:

\textit{Training Objective} The loss function is calculated as follows:
\begin{equation}
\begin{array}{l}
\begin{aligned}
L&=\frac{\alpha}{N_{\mathrm{pos}}} \sum_{x, y} L_{\mathrm{cls}}\left(\textbf{C}_{x, y}, \textbf{C}_{x, y}^{*}\right) \\
&+\frac{\beta}{N_{\mathrm{pos}}} \sum_{x, y} \textbf{1}_{\left\{\textbf{C}_{x, y}^{*}>0\right\}} L_{\mathrm{quality}}\left(\textbf{Q}_{x, y}, \textbf{Q}_{x, y}^{*}\right) \\
&+\frac{\gamma}{N_{\mathrm{pos}}} \sum_{x, y} \textbf{1}_{\left\{\textbf{C}_{x, y}^{*}>0\right\}} L_{\mathrm{reg}}\left(\textbf{R}_{x, y}, \textbf{R}_{x, y}^{*}\right) \\
&+\eta \cdot ||\delta_k||_2^2 +  \sigma \cdot ||p_k||^2_2,
\end{aligned}
\end{array}
\label{eq:loss}
\end{equation}
where $\textbf{C}_{x, y}, \textbf{R}_{x, y}, \textbf{Q}_{x, y}$ represent the values of $\textbf{C}, \textbf{R}, \textbf{Q}$ at location $(x, y)$, respectively. $\textbf{C}^*, \textbf{R}^*, \textbf{Q}^*$ are the fake labels generated according to the position and size of the \textit{fake target}. $\textbf 1$ is the indicator function that takes 1 if the condition in subscribe holds and takes 0 if not, $N_{\mathrm{pos}}$ denotes the number of positive samples in the training phase, $L_{\mathrm{cls}}$ denotes the focal loss \cite{focal} for classification result:

\begin{equation}
  \mathrm{FL}\left(p_{\mathrm{t}}\right)=-\alpha_{\mathrm{t}}\left(1-p_{\mathrm{t}}\right)^{\gamma} \log \left(p_{\mathrm{t}}\right)
\end{equation}

$L_{\mathrm{quality}}$ denotes the binary cross entropy (BCE) loss for quality assessment:

\begin{equation}
\begin{array}{ll}
\ell(x, y)=L=\left\{l_{1}, \ldots, l_{N}\right\}^{\top},\\
l_{n}=-w_{n}\left[y_{n} \cdot \log x_{n}+\left(1-y_{n}\right) \cdot \log \left(1-x_{n}\right)\right]
\end{array}
\end{equation}

and $L_{\mathrm{reg}}$ denotes the GIoU loss \cite{GIoU} for bounding box regression.

\begin{equation}
G I o U=I o U-\frac{|C \backslash(A \cup B)|}{|C|}
\end{equation}

Following SiamFC++, we assign 1 to $\textbf{C}_{x, y}^{*}$ if $(x, y)$ is considered as a positive sample, and 0 if as a negative sample.
  
\textit{Optimisation} At each training step, the perturbations are updated as follows:
\begin{gather}
\delta_{k+1} = \delta_{k} - \epsilon_1 \cdot \text{sign}(\nabla_{\delta_k}L)\\
p_{k+1} = p_{k} - \epsilon_2 \cdot \text{sign}(\nabla_{p_k}L),
\end{gather}
where $\epsilon_1$ is used to ensure that the perturbation added to the template image is imperceptible, and $\epsilon_2$ is used to maintain the training stability.
During training, we only optimize the values of perturbations $(\delta, p)$ and the parameters of the Siamese network remain intact. We outline this training procedure in Algorithm \ref{alg}.

\subsection{Discussion: Difference between our method and the origional UAP/patch method} We firstly show that these two different attack method can be optimisated together. Then, as shown in the Experiment section, the comine of both attacks is necessary, just use one can not get good result. The patch works as a fake target, it tells the network where to look. This is necessary for targeted attack for trackers, which need to specify a wrong path. Use a fake target to mislead the path is our contribution. Compared to other work, which pre-compute several destinatios and match the similar path. This is sub-optimal because it only an approximation for the path. As shown in experiments, this method works not as good as ours. The target perturbation is used to find the fake target. It hide the real feature of the target, generating signals specific for the fake target.

\subsection{Attacking the Tracker at Inference Time}

Once the perturbations $(\delta, p)$ is trained, we can use them to perturb the template image and search images of any novel video for attacking. Both $\delta$ and $p$ are universal (i.e., video-agnostic), which means perturbing a novel video only involves the mere addition and pasting of the perturbations to the template and search images -- and does not require gradient optimisation or network inference.
Assume $B^{fake}=\{b^{fake}_i\}_1^{T}$ is the trajectory we hope the tracker to output.
During tracking the $i$-th frame of the video $V=\{I_i\}_1^T$, we need to transform the bounding box $b^{fake}_i$ (with respect to the original frame $I_i$) into the box $\hat b_i=\{l^x_i, l^y_i, w_i, h_i\}$ with respect to the search image $\textbf x_i$, and paste $p$ into $\textbf x_i$ according to $\hat b_i$:
\begin{equation}
\tilde{\textbf x}_i = A(\textbf x_i, p, (l^x_i, l^y_i), (w_i, h_i)).
\end{equation}
The tracker then takes $\tilde{\textbf z}_i=\textbf z_i+\delta$ and $\tilde{ \textbf x}_i$ as input, and the subsequent tracking procedure remains the same as SiamFC++.

% 实验
\section{Experiments}

\subsection{Experimental Setup}

\textit{Evaluation Benchmarks} We evaluate our video-agnostic perturbations for targeted attacks on several tracking benchmarks, \ie, OTB2015 \cite{OTB}, GOT-10k \cite{GOT-10k}, and LaSOT \cite{LaSOT}. Generally speaking, OTB2015 is a typical tracking benchmark which is widely used for evaluation for several years, GOT-10k has the advantage of magnitudes wider coverage of object classes, and LaSOT has much longer video sequences with average duration of 84 seconds. They all follow the One-Pass Evaluation (OPE) protocol and their evaluation methodologies are similar as the measurement is mostly based on the success and precision of the trackers over the test videos. For instance, they all measure the success based on the fraction of frames in a sequence where the intersection-over-union (IoU) overlap of the predicted and ground truth rectangles exceeds a given threshold, and then the trackers are ranked using the area-under-the-curve (AUC) criterion. Since the average of IoU overlaps (AO) over all the test video frames is recently proved to be equivalent to the AUC criterion, we thus denote the success measurement as AO in the following. Besides AO, a success rate (SR) metric is also directly used to measure the percentage of successfully tracked frames given a threshold as in GOT-10k. As for the precision, it encodes the proportion of frames for which the center of the predicted rectangle is within 20 pixels of the ground truth center. Since the precision metric is sensitive to the resolution of the images and the size of the bounding boxes, a metric of normalized precision over the size of the ground truth bounding box is proposed and the trackers are then ranked using the AUC for normalized precision between 0 and 0.5.

\textit{Generating the Fake Trajectory} We need to predefine a specific trajectory $B^{fake}=\{b^{fake}_i\}_1^{T}$ for each video to achieve targeted attack in the online-tracking phase, which we call the \textit{fake trajectory}. We denote the \textit{real trajectory} as $B^{gt}=\{b^{gt}_i\}_1^T$ with the bounding box ground truth.
It is possible to manually label arbitrary $B^{fake}$ for each video, however, it will be time-consuming in our experimental evaluation. So we generate $B^{fake}$ based on $B^{gt}$. Specifically, the \textit{fake trajectory} follows the \textit{real trajectory} and the adjacent boundaries of $b^{fake}_i$ and $b^{gt}_i$ are 16 pixels apart.
%The size of $b^{fake}_0$ is the same as $B^{gt}_0$ and the size of $b^{fake}_i$
The bounding box size of the \textit{fake trajectory} gradually changes from the size of $b^{gt}_1$ to $64\times 64$. Note that the annotations of GOT-10k's test data are kept private, so we only use its validation set for our evaluation and denote it as GOT-Val.

\subsubsection{Q: How can we know what is the object that the application is tracking?}

In academic research, the target position of the first frame is given. In practical applicaiton, the target position of the first frame is often obtained by running an object detector on this frame. Once the target position of the first frame is obtained, we can put the adversarial patch near the target to mislead the tracker. In the next frames, we can specify an arbitrary \textit{fake trajectory} to place the adversarial patch.

\textit{Image Quality Assessment} We use structural similarity (SSIM) \cite{SSIM} to evaluate the quality of the generated imperceptible template perturbation $\delta$. It is difficult to be found when SSIM is close to 1 (see Table \ref{tab:iter}).

\begin{table}
\centering
%\footnotesize
%\tabcolsep=2.0pt
\caption{Overall attack results on the evaluation benchmarks.}
\begin{tabular}{c c | c | c | c}
\toprule
\multirow{2}{*}[-2pt]{Benchmarks} & \multirow{2}{*}[-2pt]{Metrics} & Clean Videos    & \multicolumn{2}{c}{Perturbed Videos}  \\
\cmidrule{3-5}
                          &                         & Real Traj. & Real Traj. & Fake Traj.     \\ 
\midrule
\multirow{2}{*}{OTB-15} 
& AO   & 0.642 & 0.035 & 0.842\\
& Precision & 0.861 & 0.048 & 0.928\\
\midrule
\multirow{2}{*}{GOT-Val} 
& SR & 0.897 & 0.023 & 0.890\\
& AO 				   & 0.760 & 0.035 & 0.818 \\
\midrule
\multirow{3}{*}{LaSOT} 
& Precision       & 0.514 & 0.013 & 0.820\\
& Norm. Prec. & 0.551 & 0.015 & 0.788\\
& AO & 0.525 & 0.022 & 0.767\\
%& Succ. rate  & 0.626 & 0.016 & 0.834\\
\midrule
\multicolumn{2}{c|}{FPS} & 58 & 58 & 58\\
\bottomrule
\end{tabular}
\label{tab:benchmark results}
\end{table}

\subsection{Implementation Details}

In our evaluation, the backbone Siamese network of our base tracker SiamFC++ \cite{SiamFC++} adopts GoogLeNet \cite{GoogLeNet}.
We implement our approach in Pytorch and train our perturbations using three GTX 1080Ti GPUs.
We adopt COCO \cite{COCO}, ILSVRC-VID \cite{VID} and the training splits of GOT-10k \cite{GOT-10k} and LaSOT \cite{LaSOT} as our training set.
We train the perturbations for 32768 iterations with a mini-batch of 96 images (32 images per GPU).
The learning rate of the template perturbation $\epsilon_1$ is set to 0.1, and for the adversarial patch it is $\epsilon_2 = 0.5$.
We generate training samples following the practice in SiamFC++.
During both the training and online-tracking phase, the off-the-shelf SiamFC++ tracking network model\footnote{It is exclusively trained on the training split of GOT-10k by the authors of SiamFC++ and can be downloaded from \url{https://drive.google.com/file/d/1BevcIEZr_kgyFjhxayOFw08DFl2u5Zi7/view}} is fixed and used for the whole evaluation, the spatial size of the template image is set to $127\times 127$, and the search image is $303\times 303$.
In Equ. \ref{eq:loss}, we set $\alpha=1, \beta=1, \gamma=1, \eta=0.005$, and $\sigma=10^{-5}$.

\begin{table}
\centering
%\footnotesize
%\tabcolsep=2.5pt
\caption{Contribution of each loss on GOT-Val.}
\begin{tabular}{ccc|cc|cc} 
\toprule
\multirow{2}{*}[-2pt]{$L_{\text{cls}}$}     & \multirow{2}{*}[-2pt]{$L_{\text{quality}}$} & \multirow{2}{*}[-2pt]{$L_{\text{reg}}$} & \multicolumn{2}{c|}{Fake Traj.}          & \multicolumn{2}{c}{Real Traj.}           \\ 
\cmidrule{4-7}
                       &                    &                    & AO                    & SR                    & AO                    & SR                     \\ 
\midrule
\checkmark   &    &    & 0.718  & 0.824    & 0.086 & 0.083   \\
   & \checkmark   &    & 0.044  & 0.044    & 0.703 & 0.842   \\
   &    & \checkmark   & 0.664  & 0.726    & 0.165 & 0.184   \\
\checkmark   & \checkmark   & \checkmark   & 0.818  & 0.890    & 0.035 & 0.023   \\ \bottomrule
\end{tabular}
\label{tab:loss}
\end{table}

\subsection{Attack Results on the Evaluation Benchmarks}

\textit{Overall Attack Results} We test the performance of our targeted attack method on the evaluation benchmarks and gather the overall results in Table \ref{tab:benchmark results}. It is shown that the base tracker SiamFC++ can achieve state-of-the-art performance on all the evaluation benchmarks and run in real time (at about 58 fps on an GTX 1080Ti GPU). However this real-time performance requires the computationally intensive tracking system to occupy most of the computational resources, and thus it is appealing to develop a virtually costless attack method to fool the tracking system without scrambling for the resources. As shown in Table \ref{tab:benchmark results}, our attack method can satisfy this appealing demand and fool the SiamFC++ tracker effectively by misleading the tracker to follow a predefined \textit{fake trajectory}. Moreover, the high AO and Precision performance with respect to the \textit{fake trajectory} indicates a more effective attack without raising any suspicion (see Figure \ref{fig:vis}).

\begin{table*}
\centering
\footnotesize
\tabcolsep=3.0pt
\caption{Influence of the training iteration number on GOT-Val.}
\begin{tabular}{cc|cccccccccccccccc} 
\toprule
\multicolumn{2}{c|}{Iterations}     & 1     & 2     & 4     & 8     & 16    & 32    & 64    & 128   & 256   & 512   & 1024  & 2048  & 4096  & 8192  & 16384 & 32768  \\ 
\midrule
\multirow{2}{*}{Fake Traj.} & AO    & 0.002 & 0.002 & 0.002 & 0.002 & 0.002 & 0.003 & 0.007 & 0.042 & 0.299 & 0.668 & 0.746 & 0.781 & 0.798 & 0.820 & 0.821 & 0.818  \\
                                 & SR    & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.001 & 0.005 & 0.044 & 0.335 & 0.749 & 0.822 & 0.855 & 0.872 & 0.895 & 0.897 & 0.890  \\ 
\midrule
\multirow{2}{*}{Real Traj.} & AO    & 0.757 & 0.756 & 0.757 & 0.757 & 0.758 & 0.759 & 0.753 & 0.720 & 0.474 & 0.150 & 0.095 & 0.071 & 0.041 & 0.032 & 0.032 & 0.035  \\
                                 & SR    & 0.894 & 0.891 & 0.893 & 0.891 & 0.893 & 0.896 & 0.888 & 0.852 & 0.559 & 0.164 & 0.098 & 0.066 & 0.031 & 0.021 & 0.022 & 0.023  \\ 
\midrule
\multicolumn{2}{c|}{SSIM of $\delta$}                        & 1.00  & 1.00  & 1.00  & 1.00  & 0.99  & 0.99  & 0.97  & 0.93  & 0.86  & 0.86  & 0.87  & 0.88  & 0.88  & 0.88  & 0.88  & 0.88   \\
%\multicolumn{2}{c|}{MSE}                         & 0.51  & 0.26  & 0.32  & 0.37  & 0.48  & 0.84  & 2.03  & 5.65  & 15.10 & 25.43 & 23.70 & 21.89 & 20.69 & 20.49 & 20.03 & 20.87  \\
\bottomrule
\end{tabular}
\label{tab:iter}
\end{table*}

\begin{table}
\centering
%\footnotesize
%\tabcolsep=2.0pt
\caption{Transferability to different backbones on GOT-Val.}
\begin{tabular}{c|cc|cc|cc} 
\toprule
\multirow{3}{*}[-6pt]{Backbone} & \multicolumn{2}{c|}{Clean Videos}    & \multicolumn{4}{c}{Perturbed Videos}                                        \\ 
\cmidrule{2-7}
                          & \multicolumn{2}{c|}{Real Traj.} & \multicolumn{2}{c|}{Real Traj.} & \multicolumn{2}{c}{Fake Traj.}  \\ 
\cmidrule{2-7}
                          & AO    & SR                           & AO    & SR                           & AO    & SR                           \\ 
\midrule
GoogLeNet                 & 0.760 & 0.897                        & 0.035 & 0.023                        & 0.818 & 0.890                        \\
AlexNet                   & 0.720 & 0.850                        & 0.196 & 0.227                        & 0.572 & 0.640                        \\
ShuffleNet                & 0.766 & 0.888                        & 0.554 & 0.656                        & 0.135 & 0.134                        \\
\bottomrule
\end{tabular}
\label{tab:backbone}
\end{table}

\begin{table}
\centering
%\footnotesize
%\tabcolsep=2.0pt
\caption{Transferability to different tracking architectures on OTB2015.}
\begin{tabular}{c|cc|cc} 
\toprule
\multirow{2}{*}[-2pt]{Trackers} & \multicolumn{2}{c|}{Clean Videos} & \multicolumn{2}{c}{Perturbed Videos}  \\
\cmidrule{2-5}
                          & AO & Precision              & AO & Precision                   \\
\midrule
SiamRPN++                 & 0.676   & 0.879                  & 0.518   & 0.691                       \\
SiamRPN                   & 0.666   & 0.876                  & 0.379   & 0.506                       \\
\bottomrule
\end{tabular}
\label{tab:arch}
\end{table}

\textit{Ablation Study: Influence of Training Iterations} Table \ref{tab:iter} demonstrates the effect of the number of training iterations of the algorithm on the effectiveness of the attack. It can be observed that as the number of iterations increases, the AO value relative to the false trajectory increases significantly, while the AO value relative to the real trajectory decreases significantly. This demonstrates the effectiveness of the end-to-end training process proposed in this chapter.
After about 30,000 training iterations, the resulting perturbation prevents the tracker from tracking most of the targets in GOT-Val, and the AO relative to the true trajectory decreases from 0.760 to 0.035.
We note that the AO decreases significantly faster at the beginning of the training cycle (training iterations less than 2048) than later. This demonstrates the fast convergence capability of our end-to-end training scheme.

To verify whether the perturbations generated for the template images are difficult to perceive, we evaluate and analyze the quality of the template images after the perturbations. The quality of the perturbed template images is evaluated by the metric SSIM. As shown in Table \ref{tab:iter}, the SSIM value of the perturbed template image gradually decreases as the training proceeds, which indicates that the difference between the perturbed template image and the original template image gradually increases due to the learned adversarial perturbation being added to the clean template image. The SSIM value ranges from 0 to 1. If the SSIM is close to 1, the difference between the perturbed template image and the original template image is smaller. If SSIM is closer to 0, it means that the difference between the perturbed template image and the original template image is larger, which means that the perturbation generated by the template image is easier to perceive. The SSIM value after convergence of this algorithm is 0.88, which means that the difference between the perturbed template image and the original template image is smaller, that is, the perturbation generated by the template image is harder to perceive (see Fig. \ref{fig:1}).

\textit{Ablation Study: Influence of Training Loss} We implemented a series of experiments to analyze and evaluate the contribution of each loss component.
In table \ref{tab:loss}, we report the attack results on the GOT-Val dataset. The results reflect the differences in attack performance for different perturbations obtained by training using different combinations of the loss terms in Eq. \ref{eq:loss}.
The AO value of the tracker relative to the true trajectory decreases from 0.760 to 0.703 when only adversarial information is generated using quality assessment loss, indicating that quality assessment loss can cause a slight degradation in the performance of the tracker. This is because the perturbation information generated using quality assessment loss interferes with the tracker's ability to select the best target frame.
However, the tracker's AO value relative to the spurious trajectory is only 0.044, indicating that the perturbations generated using quality assessment loss alone can barely cause the tracker to generate tracking frames along the specified trajectory.
When only the adversarial information is generated using regression loss, the AO value of the tracker relative to the true trajectory decreases from 0.760 to 0.165, indicating that regression loss can cause a large degradation in the tracker's performance. This is because the perturbation information generated using regression loss corrupts the tracker's ability to predict the true target border position.
When only adversarial information is generated using classification loss, the tracker's AO value decreases from 0.760 to 0.086 relative to the true trajectory and up to 0.718 relative to the false trajectory. this is because the perturbation information generated using classification loss causes the tracker to localize to the false target instead of the true target's location.
In conclusion, all loss terms are beneficial, and the classification term is more important than the quality assessment term.

\subsection{Transferability Analysis}

\textit{Transferability to Different Backbones} We evaluate the transferability of our attacks when applying the perturbations to two more different backbones of SiamFC++, \ie, ShuffleNet \cite{ShuffleNet} and AlexNet \cite{AlexNet}.
The experimental results are shown in Table \ref{tab:backbone}. In the case of SiamFC++-AlexNet, the AO with respect to the \textit{real trajectory} drops from 0.72 to 0.196. However, our perturbations do not generalize well to SiamFC++-ShuffleNet, which we conjecture to be due to the particular group convolution and channel shuffle operations in ShuffleNet.

\textit{Transferability to Different Tracking Architectures} We evaluate the transferability of our attacks when applying the perturbations to two more state-of-the-art anchor-based trackers: AlexNet-based SiamRPN \cite{SiamRPN} and ResNet-based SiamRPN++ \cite{SiamRPN++} to verify the transferability to different tracking architectures.
SiamRPN uses an RPN network to perform location regression and target classification on the response maps. SiamRPN++ performs layer-wise and depth-wise aggregation to improve accuracy based on the effective training using the ResNet backbone. The experimental results are shown in Table \ref{tab:arch}. In the case of SiamRPN, the AO with respect to the \textit{real trajectory} drops from 0.666 to 0.379 while the performance of SiamRPN++ is decreased from 0.676 to 0.518. The results show good transferability of our attacks to different tracking architectures, even if the generated perturbations are applied to anchor-based trackers.

\subsection{Comparison with Other Methods}

We compare the twin network attack method proposed in this chapter with the latest attack methods, including the AlexNet-based untargeted attack method CSA \cite{CSA}, and two other targeted attack methods: the AlexNet-based FAN \cite{FAN} and the ResNet50 -based TTP \ cite{TTP}. We report the accuracy scores relative to the real/false trajectories in table \ref{tab:untargeted}. CSA \cite{CSA} is a twin tracking attack method called cooling-shrinking attack. CSA can suppress the peak region on the heat map reflecting the target location, which is used to attack the tracker's targeting ability; CSA can also simultaneously force the tracker's predicted bounding box to shrink, which is used to attack the tracker's the border regression capability of the tracker. However, the method requires running a generative network for each frame to obtain adversarial information, so the attack time is as high as 4720 ms, making it difficult to meet the demand for real-time tracking. CSA reduces the accuracy score relative to the real trajectory from 0.851 to 0.458, and the attack performance is more limited. Due to the limitations of the algorithm design, CSA cannot perform targeted attacks for target tracking. In the literature \cite{FAN}, Liang et al. proposed a fast attack network (FAN) for attacking SiamFC trackers. To perform untargeted attacks, the FAN proposes a drift loss that shifts the tracker's prediction of the target's position. Over time, the tracking error accumulates until the tracker loses the target completely. To perform targeted attacks, FAN proposes embedding feature loss for improving the similarity between the features of the adversarial sample and the features of a particular trajectory region.FAN can reduce the accuracy score relative to the real trajectory from 0.720 to 0.180, which has better performance for non-targeted attacks. However, the accuracy score is only 0.420 with respect to false trajectories, which has a more limited targeted attack performance. Similar to SCA, FAN also requires running a generative network for each frame to obtain adversarial information. In the literature \cite{TTP}, Nakka et al. propose a temporally transferable perturbations (TTP) for attacking the SiamRPN++ tracker.TTP generates a single adversarial perturbation from the template image only and adds this perturbation to each search image of the video TTP reduces the accuracy score relative to the real trajectory from 0.910 to 0.080, which has a better performance for non-targeted attacks. However, the accuracy score of this method is only 0.692 with respect to the false trajectory, and the performance of targeted attacks is more limited. In addition, the method needs to run a generative network for each video to obtain adversarial information, and thus requires the computational and storage resources of the target tracking platform, which is difficult to deploy to resource-constrained embedded platforms. Our method can reduce the accuracy score from 0.861 to 0.048 with respect to real trajectories and up to 0.928 with respect to false trajectories, which is significantly better than other methods, and only needs to perform additive operations and image paste operations to generate adversarial information for any video without gradient optimization or network inference.

Translated with www.DeepL.com/Translator (free version)

\begin{table}[]
\centering
%\footnotesize
%\tabcolsep=2.0pt
\caption{Precision score with respect to the \textit{real / fake trajectory} on OTB2015.}
\begin{tabular}{@{}cccccc@{}}
\toprule
\multirow{2}{*}[-2pt]{Method} & \multirow{2}{*}[-2pt]{Tracker} & \multirow{2}{*}[-2pt]{\begin{tabular}[c]{@{}c@{}}Attack\\ Cost(ms)\end{tabular}} & \multirow{2}{*}[-2pt]{\begin{tabular}[c]{@{}c@{}}Clean\\ Videos\end{tabular}} & \multicolumn{2}{c}{Perturbed Videos} \\ \cmidrule(l){5-6} 
 &  &  &  & Real Traj. & Fake Traj. \\ \midrule
CSA & SiamRPN & 4720 & 0.851 & 0.458 & - \\
FAN & SiamFC & 10 & 0.720 & 0.180&0.420 \\
TTP & SiamRPN++ & 8 & 0.910 & 0.080&0.692 \\
\midrule
Ours & SiamFC++ & 0 & 0.861 & 0.048&0.928 \\ \bottomrule
\end{tabular}
\label{tab:untargeted}
\end{table}

\section{Conclusion}

In this paper, we propose a video-agnostic targeted attack method for Siamese trackers. 
We aim to attack the tracker by adding an imperceptible perturbation to the template image and pasting a \textit{fake target}, \ie, a small adversarial patch, into the search image adhering to the predefined trajectory, so that the tracker outputs the location and size of the \textit{fake target} instead of the real target. Being universal, the generated perturbations can be conveniently exploited to perturb videos on-the-fly without extra computations.
Experiments on several popular datasets show that our method can effectively fool the Siamese trackers in a targeted attack manner.


% 引文
\bibliographystyle{IEEEtran}
\bibliography{ref}

\end{document}



<!DOCTYPE html>
<!-- saved from url=(0055)https://openreview.net/group?id=ICLR.cc/2021/Conference -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin="true"><link rel="stylesheet" href="./ICLR 2021 Conference _ OpenReview_files/css"><link rel="stylesheet" href="./ICLR 2021 Conference _ OpenReview_files/bootstrap.min.css"><link rel="icon" href="https://openreview.net/favicon.ico"><script type="text/javascript" async="" src="./ICLR 2021 Conference _ OpenReview_files/analytics.js.下载"></script><script async="" src="./ICLR 2021 Conference _ OpenReview_files/js"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag() { dataLayer.push(arguments); }
            gtag('js', new Date());
            gtag('config', 'UA-108703919-1', {
              page_path: window.location.pathname + window.location.search,
              transport_type: 'beacon'
            });</script><meta name="viewport" content="width=device-width, initial-scale=1"><meta property="og:image" content="https://openreview.net/images/openreview_logo_512.png"><meta property="og:type" content="website"><meta property="og:site_name" content="OpenReview"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@openreviewnet"><title>ICLR 2021 Conference | OpenReview</title><meta name="description" content="Welcome to the OpenReview homepage for ICLR 2021 Conference"><meta property="og:title" content="ICLR 2021 Conference"><meta property="og:description" content="Welcome to the OpenReview homepage for ICLR 2021 Conference"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/styles.38b7829b.chunk.css" as="style"><link rel="stylesheet" href="./ICLR 2021 Conference _ OpenReview_files/styles.38b7829b.chunk.css" data-n-g=""><noscript data-n-css="true"></noscript><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/main-6b56d8b97b908e519b08.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/webpack-d7b2fb72fb7257504a38.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/framework.b11cd6ab3c62dae3dfb8.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/29107295.c7a36c5cb4964dc936e4.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/b637e9a5.d9354c88856ddbaf0be2.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/42e8bd60.2605cce5b4c2063fa3d3.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/55e0983c962a44019f922501f82fb92be7dc6038.b1fe41cb81db81e91237.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/181a3372a136420bacd8143b1a96dfd629357ddc.7ec9ffd88886a5000e00.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/a070395ba8e9275b849fcd4310593532831ac4b7.8744cf13ed507c350a25.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/1220f4ffb4828c6d46a13072dc033abc74519993.3d3dd7c19e661b4a6bfa.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/styles.c669c7da917090bc8543.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/_app-0232e1e8985093b3faa8.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/group-11f7765f7db1e915182b.js.下载" as="script"><script src="./ICLR 2021 Conference _ OpenReview_files/tex-chtml-full.js.下载" async="" crossorigin="anonymous"></script><style type="text/css">.simpread-theme-root{font-size:62.5%!important}sr-rd-content,sr-rd-desc,sr-rd-title{width:100%}sr-rd-title{display:-webkit-box;margin:1em 0 .5em;overflow:hidden;text-overflow:ellipsis;text-rendering:optimizelegibility;-webkit-line-clamp:3;-webkit-box-orient:vertical}sr-rd-content{text-align:left;word-break:break-word}sr-rd-desc{text-align:justify;line-height:2.4;margin:0 0 1.2em;box-sizing:border-box}sr-rd-content{font-size:25.6px;font-size:1.6rem;line-height:1.6}sr-rd-content h1,sr-rd-content h1 *,sr-rd-content h2,sr-rd-content h2 *,sr-rd-content h3,sr-rd-content h3 *,sr-rd-content h4,sr-rd-content h4 *,sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{word-break:break-all}sr-rd-content div,sr-rd-content p{display:block;float:inherit;line-height:1.6;font-size:25.6px;font-size:1.6rem}sr-rd-content div,sr-rd-content p,sr-rd-content pre,sr-rd-content sr-blockquote{margin:0 0 1.2em;word-break:break-word}sr-rd-content a{padding:0 5px;vertical-align:baseline;vertical-align:initial}sr-rd-content a,sr-rd-content a:link{color:inherit;font-size:inherit;font-weight:inherit;border:none}sr-rd-content a:hover{background:transparent}sr-rd-content img{margin:10px;padding:5px;max-width:100%;background:#fff;border:1px solid #bbb;box-shadow:1px 1px 3px #d4d4d4}sr-rd-content figcaption{text-align:center;font-size:14px}sr-rd-content sr-blockquote{display:block;position:relative;padding:15px 25px;text-align:left;line-height:inherit}sr-rd-content sr-blockquote:before{position:absolute}sr-rd-content sr-blockquote *{margin:0;font-size:inherit}sr-rd-content table{width:100%;margin:0 0 1.2em;word-break:keep-all;word-break:normal;overflow:auto;border:none}sr-rd-content table td,sr-rd-content table th{border:none}sr-rd-content ul{margin:0 0 1.2em;margin-left:1.3em;padding:0;list-style:disc}sr-rd-content ol{list-style:decimal;margin:0;padding:0}sr-rd-content ol li,sr-rd-content ul li{font-size:inherit;list-style:disc;margin:0 0 1.2em}sr-rd-content ol li{list-style:decimal;margin-left:1.3em}sr-rd-content ol li *,sr-rd-content ul li *{margin:0;text-align:left;text-align:initial}sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em}sr-rd-content li ul{list-style:circle}sr-rd-content pre{font-family:Consolas,Monaco,Andale Mono,Source Code Pro,Liberation Mono,Courier,monospace;display:block;padding:15px;line-height:1.5;word-break:break-all;word-wrap:break-word;white-space:pre;overflow:auto}sr-rd-content pre,sr-rd-content pre *,sr-rd-content pre div{font-size:17.6px;font-size:1.1rem}sr-rd-content li pre code,sr-rd-content p pre code,sr-rd-content pre{background-color:transparent;border:none}sr-rd-content pre code{margin:0;padding:0}sr-rd-content pre code,sr-rd-content pre code *{font-size:17.6px;font-size:1.1rem}sr-rd-content pre p{margin:0;padding:0;color:inherit;font-size:inherit;line-height:inherit}sr-rd-content li code,sr-rd-content p code{margin:0 4px;padding:2px 4px;font-size:17.6px;font-size:1.1rem}sr-rd-content mark{margin:0 5px;padding:2px;background:#fffdd1;border-bottom:1px solid #ffedce}.sr-rd-content-img{width:90%;height:auto}.sr-rd-content-img-load{width:48px;height:48px;margin:0;padding:0;border-style:none;border-width:0;background-repeat:no-repeat;background-image:url(data:image/gif;base64,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)}.sr-rd-content-center{text-align:center;display:-webkit-box;-webkit-box-align:center;-webkit-box-pack:center;-webkit-box-orient:vertical}.sr-rd-content-center-small{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex}.sr-rd-content-center-small img{margin:0;padding:0;border:0;box-shadow:none}.sr-rd-content-nobeautify{margin:0;padding:0;border:0;box-shadow:0 0 0}sr-rd-mult{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin:0 0 16px;padding:16px 0 24px;width:100%;background-color:#fff;border-radius:4px;box-shadow:0 1px 2px 0 rgba(60,64,67,.3),0 2px 6px 2px rgba(60,64,67,.15)}sr-rd-mult:hover{-webkit-transition:all .45s 0ms;transition:all .45s 0ms;box-shadow:1px 1px 8px rgba(0,0,0,.16)}sr-rd-mult sr-rd-mult-content{padding:0 16px;overflow:auto}sr-rd-mult sr-rd-mult-avatar,sr-rd-mult sr-rd-mult-content{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sr-rd-mult sr-rd-mult-avatar{-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 15px}sr-rd-mult sr-rd-mult-avatar span{display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;max-width:75px;overflow:hidden;text-overflow:ellipsis;text-align:left;font-size:16px;font-size:1rem}sr-rd-mult sr-rd-mult-avatar img{margin-bottom:0;max-width:50px;max-height:50px;width:50px;height:50px;border-radius:50%}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center{margin:0}sr-page{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;width:100%}</style><style type="text/css">sr-rd-theme-github{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{position:relative;margin-top:1em;margin-bottom:1pc;font-weight:700;line-height:1.4;text-align:left;color:#363636}sr-rd-content h1{padding-bottom:.3em;font-size:57.6px;font-size:3.6rem;line-height:1.2}sr-rd-content h2{padding-bottom:.3em;font-size:44.8px;font-size:2.8rem;line-height:1.225}sr-rd-content h3{font-size:38.4px;font-size:2.4rem;line-height:1.43}sr-rd-content h4{font-size:32px;font-size:2rem}sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h6{color:#777}sr-rd-content ol,sr-rd-content ul{list-style-type:disc;padding:0;padding-left:2em}sr-rd-content ol ol,sr-rd-content ul ol{list-style-type:lower-roman}sr-rd-content ol ol ol,sr-rd-content ol ul ol,sr-rd-content ul ol ol,sr-rd-content ul ul ol{list-style-type:lower-alpha}sr-rd-content table{width:100%;overflow:auto;word-break:normal;word-break:keep-all}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{padding:6px 13px;border:1px solid #ddd}sr-rd-content table tr{background-color:#fff;border-top:1px solid #ccc}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{border-left:4px solid #ddd}.simpread-theme-root{background-color:#fff;color:#333}sr-rd-title{font-family:PT Sans,SF UI Display,\.PingFang SC,PingFang SC,Neue Haas Grotesk Text Pro,Arial Nova,Segoe UI,Microsoft YaHei,Microsoft JhengHei,Helvetica Neue,Source Han Sans SC,Noto Sans CJK SC,Source Han Sans CN,Noto Sans SC,Source Han Sans TC,Noto Sans CJK TC,Hiragino Sans GB,sans-serif;font-size:54.4px;font-size:3.4rem;font-weight:700;line-height:1.3}sr-rd-desc{position:relative;margin:0;margin-bottom:30px;padding:25px;padding-left:56px;font-size:28.8px;font-size:1.8rem;color:#777;background-color:rgba(0,0,0,.05);box-sizing:border-box}sr-rd-desc:before{content:"\201C";position:absolute;top:-28px;left:16px;font-size:80px;font-family:Arial;color:rgba(0,0,0,.15)}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#363636;font-weight:400;line-height:1.8}sr-rd-content b *,sr-rd-content strong,sr-rd-content strong * sr-rd-content b{-webkit-animation:none 0s ease 0s 1 normal none running;animation:none 0s ease 0s 1 normal none running;-webkit-backface-visibility:visible;backface-visibility:visible;background:transparent none repeat 0 0/auto auto padding-box border-box scroll;border:medium none currentColor;border-collapse:separate;-o-border-image:none;border-image:none;border-radius:0;border-spacing:0;bottom:auto;box-shadow:none;box-sizing:content-box;caption-side:top;clear:none;clip:auto;color:#000;-webkit-columns:auto;-moz-columns:auto;columns:auto;-webkit-column-count:auto;-moz-column-count:auto;column-count:auto;-webkit-column-fill:balance;-moz-column-fill:balance;column-fill:balance;-webkit-column-gap:normal;-moz-column-gap:normal;column-gap:normal;-webkit-column-rule:medium none currentColor;-moz-column-rule:medium none currentColor;column-rule:medium none currentColor;-webkit-column-span:1;-moz-column-span:1;column-span:1;-webkit-column-width:auto;-moz-column-width:auto;column-width:auto;content:normal;counter-increment:none;counter-reset:none;cursor:auto;direction:ltr;display:inline;empty-cells:show;float:none;font-family:serif;font-size:medium;font-style:normal;font-variant:normal;font-weight:400;font-stretch:normal;line-height:normal;height:auto;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;left:auto;letter-spacing:normal;list-style:disc outside none;margin:0;max-height:none;max-width:none;min-height:0;min-width:0;opacity:1;orphans:2;outline:medium none invert;overflow:visible;overflow-x:visible;overflow-y:visible;padding:0;page-break-after:auto;page-break-before:auto;page-break-inside:auto;-webkit-perspective:none;perspective:none;-webkit-perspective-origin:50% 50%;perspective-origin:50% 50%;position:static;right:auto;-moz-tab-size:8;-o-tab-size:8;tab-size:8;table-layout:auto;text-align:left;text-align-last:auto;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;top:auto;-webkit-transform:none;transform:none;-webkit-transform-origin:50% 50% 0;transform-origin:50% 50% 0;-webkit-transform-style:flat;transform-style:flat;-webkit-transition:none 0s ease 0s;transition:none 0s ease 0s;unicode-bidi:normal;vertical-align:baseline;visibility:visible;white-space:normal;widows:2;width:auto;word-spacing:normal;z-index:auto;all:initial}sr-rd-content a,sr-rd-content a:link{color:#4183c4;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#4183c4;text-decoration:underline}sr-rd-content pre{background-color:#f7f7f7;border-radius:3px}sr-rd-content li code,sr-rd-content p code{background-color:rgba(0,0,0,.04);border-radius:3px}.simpread-multi-root{background:#f8f9fa}</style><style type="text/css">sr-rd-theme-newsprint{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-weight:700}sr-rd-content h1{font-size:48px;font-size:3rem;line-height:1.6em;margin-top:2em}sr-rd-content h2,sr-rd-content h3{font-size:32px;font-size:2rem;line-height:1.15;margin-top:2.285714em;margin-bottom:1.15em}sr-rd-content h3{font-weight:400}sr-rd-content h4{font-size:28.8px;font-size:1.8rem;margin-top:2.67em}sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h1{border-bottom:1px solid;margin-bottom:1.875em;padding-bottom:.8125em}sr-rd-content ol,sr-rd-content ul{margin:0 0 1.5em 1.5em}sr-rd-content ol li{list-style-type:decimal;list-style-position:outside}sr-rd-content ul li{list-style-type:disc;list-style-position:outside}sr-rd-content table{width:100%;margin-bottom:1.5em;font-size:25.6px;font-size:1.6rem}sr-rd-content thead th,tfoot th{padding:.25em .25em .25em .4em;text-transform:uppercase}sr-rd-content th{text-align:left}sr-rd-content td{vertical-align:top;padding:.25em .25em .25em .4em}sr-rd-content thead{background-color:#dadada}sr-rd-content tr:nth-child(2n){background:#e8e7e7}sr-rd-content sr-blockquote{padding:10px 15px;border-left-style:solid;border-left-width:10px;border-color:#d6dbdf;background:none repeat scroll 0 0 rgba(102,128,153,.05);text-align:left}sr-rd-content sr-blockquote:before{content:""}.simpread-multi-root,.simpread-theme-root{background-color:#f3f2ee;color:#2c3e50}sr-rd-title{font-family:PingFang SC,Hiragino Sans GB,Microsoft Yahei,WenQuanYi Micro Hei,sans-serif;line-height:1.5;font-weight:500;font-size:48px;font-size:3rem;color:#07b;border-bottom:1px solid;margin-bottom:1.875em;padding-bottom:.8125em}sr-rd-desc{color:rgba(102,128,153,.6);background-color:rgba(102,128,153,.075);border-radius:4px;margin-bottom:1em;padding:15px;font-size:32px;font-size:2rem;line-height:1.5;text-align:center}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{line-height:1.8;color:#2c3e50}sr-rd-content a,sr-rd-content a:link{color:#08c;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#5ba4e5}sr-rd-content li code,sr-rd-content p code,sr-rd-content pre{background-color:#dadada}sr-rd-mult{background-color:rgba(102,128,153,.075)}</style><style type="text/css">sr-rd-theme-gothic{display:none}sr-rd-content h1{line-height:64px;line-height:4rem;margin:64px 0 28px;margin:4rem 0 1.75rem;padding:20px 30px}sr-rd-content h1,sr-rd-content h2{font-weight:400;text-align:center;text-transform:uppercase}sr-rd-content h2{line-height:48px;line-height:3rem;margin:48px 0 31px;margin:3rem 0 1.9375rem;padding:0 30px}sr-rd-content h3,sr-rd-content h4,sr-rd-content h5{font-weight:400}sr-rd-content h6{font-weight:700}sr-rd-content h1{font-size:57.6px;font-size:3.6rem}sr-rd-content h2{font-size:51.2px;font-size:3.2rem}sr-rd-content h3{font-size:40px;font-size:2.5rem}sr-rd-content h4{font-size:35.2px;font-size:2.2rem}sr-rd-content h5{font-size:30.4px;font-size:1.9rem}sr-rd-content h6{font-size:27.2px;font-size:1.7rem}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{margin-top:1.2em;margin-bottom:.6em;color:#111}sr-rd-content ol,sr-rd-content ul{list-style-type:disc;margin-left:3em}sr-rd-content ol ol,sr-rd-content ul ol{list-style-type:lower-roman}sr-rd-content ol ol ol,sr-rd-content ol ul ol,sr-rd-content ul ol ol,sr-rd-content ul ul ol{list-style-type:lower-alpha}sr-rd-content table{margin-bottom:20px}sr-rd-content table td,sr-rd-content table th{padding:8px;line-height:20px;line-height:1.25rem;vertical-align:top;border-top:1px solid #ddd}sr-rd-content table th{font-weight:700}sr-rd-content table thead th{vertical-align:bottom}sr-rd-content table caption+thead tr:first-child td,sr-rd-content table caption+thead tr:first-child th,sr-rd-content table colgroup+thead tr:first-child td,sr-rd-content table colgroup+thead tr:first-child th,sr-rd-content table thead:first-child tr:first-child td,sr-rd-content table thead:first-child tr:first-child th{border-top:0}sr-rd-content table tbody+tbody{border-top:2px solid #ddd}sr-rd-content sr-blockquote{margin:0 0 17.777px;margin:0 0 1.11111rem;padding:8px 17.777px 0 16.888px;padding:.5rem 1.11111rem 0 1.05556rem;border-left:1px solid gray}sr-rd-content sr-blockquote,sr-rd-content sr-blockquote p{line-height:2;color:#6f6f6f}.simpread-multi-root,.simpread-theme-root{background:#fcfcfc;color:#333}sr-rd-title{font-weight:400;line-height:64px;line-height:4rem;text-align:center;text-transform:uppercase;color:#111;font-size:51.2px;font-size:3.2rem}sr-rd-desc{margin:0 0 17.777px;margin:0 0 1.11111rem;padding:8px 17.777px 0 16.888px;padding:.5rem 1.11111rem 0 1.05556rem;font-size:32px;font-size:2rem;line-height:2;color:#6f6f6f;border-left:1px solid gray}sr-rd-content{font-weight:400;color:#333}sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#333}sr-rd-content a,sr-rd-content a:link{color:#900;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#900;text-decoration:underline}sr-rd-content li code,sr-rd-content p code,sr-rd-content pre{background-color:transparent;border:1px solid #ccc}sr-rd-mult{background-color:#f2f2f2}</style><style type="text/css">sr-rd-theme-engwrite{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{margin:20px 0 10px;padding:0;font-weight:500;-webkit-font-smoothing:antialiased}sr-rd-content h1{font-weight:300;text-align:center;font-size:44.8px;font-size:2.8rem;color:#933d3f}sr-rd-content h2{font-size:38.4px;font-size:2.4rem;border-bottom:1px solid #ccc;color:#000}sr-rd-content h3{font-size:28.8px;font-size:1.8rem}sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h6{color:#777}sr-rd-content ol,sr-rd-content ul{padding-left:30px}sr-rd-content ol li>:first-child,sr-rd-content ol li ol:first-of-type,sr-rd-content ol li ul:first-of-type,sr-rd-content ul li>:first-child,sr-rd-content ul li ol:first-of-type,sr-rd-content ul li ul:first-of-type{margin-top:0}sr-rd-content ol ol,sr-rd-content ol ul,sr-rd-content ul ol,sr-rd-content ul ul{margin-bottom:0}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{border:1px solid #ccc;padding:6px 13px}sr-rd-content table tr{border-top:1px solid #ccc;background-color:#fff}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{text-align:left;border-top:1px dotted #cdc7bc;border-bottom:1px dotted #cdc7bc;background-color:#f8edda;color:#777}sr-blockquote>:first-child{margin-top:0}sr-blockquote>:last-child{margin-bottom:0}.simpread-multi-root,.simpread-theme-root{background-color:#fcf5ed;color:#333}sr-rd-title{font-weight:300;text-align:center;font-size:44.8px;font-size:2.8rem;color:#933d3f}sr-rd-desc{padding:10px;background-color:#f8edda;color:#777;font-size:32px;font-size:2rem;text-align:center;border-top:1px dotted #cdc7bc;border-bottom:1px dotted #cdc7bc}sr-rd-content{padding:20px 0;margin:0 auto}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#333;line-height:1.8}sr-rd-content a,sr-rd-content a:link{color:#ae3737;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{text-decoration:underline}sr-rd-content pre{background-color:transparent;border:1px solid #ccc;border-radius:3px}sr-rd-content li code,sr-rd-content p code{border:1px solid #eaeaea;background-color:#f4ece3;border-radius:3px}sr-rd-mult{background-color:#f8edda}</style><style type="text/css">sr-rd-theme-octopress{display:none}sr-rd-content h1{font-size:56.32px;font-size:3.52rem;line-height:30.72px;line-height:1.92rem}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{text-rendering:optimizelegibility;margin-bottom:20.8px;margin-bottom:1.3rem;font-weight:700}sr-rd-content h2{font-size:38.4px;font-size:2.4rem}sr-rd-content h3{font-size:33.28px;font-size:2.08rem}sr-rd-content h4{font-size:28.8px;font-size:1.8rem}sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h1,sr-rd-content h2{padding-top:27.2px;padding-top:1.7rem;padding-bottom:19.2px;padding-bottom:1.2rem;background:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAABCAYAAACsXeyTAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAFUlEQVQIHWNIS0sr/v//PwMMDzY+ADqMahlW4J91AAAAAElFTkSuQmCC") 0 100% repeat-x}sr-rd-content h2{padding-top:20.8px;padding-top:1.3rem;padding-bottom:0}sr-rd-content ul{list-style-type:disc}sr-rd-content ul ul{list-style-type:circle;margin-bottom:0}sr-rd-content ul ul ul{list-style-type:square;margin-bottom:0}sr-rd-content ol{list-style-type:decimal}sr-rd-content ol ol{list-style-type:lower-alpha;margin-bottom:0}sr-rd-content ol ol ol{list-style-type:lower-roman;margin-bottom:0}sr-rd-content ol,sr-rd-content ol ol,sr-rd-content ol ul,sr-rd-content ul,sr-rd-content ul ol,sr-rd-content ul ul{margin-left:1.3em}sr-rd-content ol ol,sr-rd-content ol ul,sr-rd-content ul ol,sr-rd-content ul ul{margin-bottom:0}sr-rd-content table{width:100%;overflow:auto;word-break:normal;word-break:keep-all}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{padding:6px 13px;border:1px solid #ddd}sr-rd-content table tr{background-color:#fff;border-top:1px solid #ccc}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{font-style:italic;font-size:inherit;line-height:2;padding-left:1em;border-left:4px solid hsla(0,0%,67%,.5)}.simpread-multi-root,.simpread-theme-root{background:#f8f8f8 url("data:image/png;base64,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") 0 0;color:#333}sr-rd-title{font-size:56.32px;font-size:3.52rem;line-height:64px;line-height:4rem;font-weight:700;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAABCAYAAACsXeyTAAAACXBIW…sTAAALEwEAmpwYAAAAFUlEQVQIHWNIS0sr/v//PwMMDzY+ADqMahlW4J91AAAAAElFTkSuQmCC) 0 100% repeat-x}sr-rd-desc{font-style:italic;font-size:30.72px;font-size:1.92rem;line-height:2;padding-left:1em;border-left:4px solid hsla(0,0%,67%,.5)}sr-rd-content{margin:0 auto;padding:1em 0}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{line-height:2;color:#333}sr-rd-content a,sr-rd-content a:link{color:#1863a1;text-decoration:underline}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#0181eb;text-decoration:underline}sr-rd-content pre{color:#586e75;background-color:#fdf6e3;border-radius:.4em;border:1px solid #e7dec3}sr-rd-content li code,sr-rd-content p code{color:#555;background-color:transparent;border:1px solid #ddd}sr-rd-mult{background-color:#ededed}</style><style type="text/css">sr-rd-theme-pixyii{display:none}sr-rd-content h1,sr-rd-content h1 *,sr-rd-content h2,sr-rd-content h2 *,sr-rd-content h3,sr-rd-content h3 *,sr-rd-content h4,sr-rd-content h4 *,sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{color:inherit;font-weight:900;line-height:1.2;margin:1em 0}sr-rd-content h1,sr-rd-content h1 *{font-size:62.72px;font-size:3.92rem}sr-rd-content h2,sr-rd-content h2 *{font-size:58.24px;font-size:3.64rem}sr-rd-content h3,sr-rd-content h3 *{font-size:36.4px;font-size:2.275rem}sr-rd-content h4,sr-rd-content h4 *{font-size:29.12px;font-size:1.82rem}sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{font-size:25.168px;font-size:1.573rem}sr-rd-content ol,sr-rd-content ul{font-size:28px;font-size:1.75rem;line-height:24px;line-height:1.5rem}sr-rd-content li{font-size:25.2px;font-size:1.575rem;line-height:1.8;margin:0;position:relative}sr-rd-content table{width:100%;font-size:25.2px;font-size:1.575rem}sr-rd-content table>tbody>tr>td,sr-rd-content table>tbody>tr>th,sr-rd-content table>tfoot>tr>td,sr-rd-content table>tfoot>tr>th,sr-rd-content table>thead>tr>td,sr-rd-content table>thead>tr>th{padding:12px;line-height:1.2;vertical-align:top;border-top:1px solid #333}sr-rd-content table>thead>tr>th{vertical-align:bottom;border-bottom:2px solid #333}sr-rd-content table>caption+thead>tr:first-child>td,sr-rd-content table>caption+thead>tr:first-child>th,sr-rd-content table>colgroup+thead>tr:first-child>td,sr-rd-content table>colgroup+thead>tr:first-child>th,sr-rd-content table>thead:first-child>tr:first-child>td,sr-rd-content table>thead:first-child>tr:first-child>th{border-top:0}sr-rd-content table>tbody+tbody{border-top:2px solid #333}sr-rd-content sr-blockquote{margin:16px 0;margin:1rem 0;padding:1.33em;font-style:italic;border-left:5px solid #7a7a7a;color:#555}.simpread-theme-root{background-color:#fff;color:#555}sr-rd-title{font-family:PingFang SC,Hiragino Sans GB,Microsoft Yahei,WenQuanYi Micro Hei,sans-serif;font-size:67.2px;font-size:4.2rem;font-weight:900;line-height:1.2}sr-rd-desc{margin:16px 0;margin:1rem 0;padding:1.33em;font-style:italic;font-size:32px;font-size:2rem;line-height:2;border-left:5px solid #7a7a7a;color:#555}sr-rd-content{font-size:33.6px;font-size:2.1rem;line-height:1.8;font-weight:400;color:#555}sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#555;font-size:28px;font-size:1.75rem;line-height:1.8;font-weight:300}sr-rd-content b,sr-rd-content b *,sr-rd-content strong,sr-rd-content strong *{font-weight:700}sr-rd-content a,sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover,sr-rd-content a:link{color:#463f5c;text-decoration:underline}sr-rd-content sr-blockquote code{font-size:inherit}sr-rd-content pre{border:1px solid #7a7a7a}sr-rd-content li code,sr-rd-content p code,sr-rd-content pre{color:#7a7a7a;background-color:transparent}.simpread-multi-root{background:#f8f9fa}</style><style type="text/css">sr-rd-theme-monospace{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-weight:700;color:#6363ac}sr-rd-content h1{font-size:35.2px;font-size:2.2rem}sr-rd-content h2{font-size:32px;font-size:2rem}sr-rd-content h3{font-size:28.8px;font-size:1.8rem}sr-rd-content h4{font-size:25.6px;font-size:1.6rem}sr-rd-content h5{font-size:22.4px;font-size:1.4rem}sr-rd-content h6{font-size:20.8px;font-size:1.3rem}sr-rd-content strong{color:#b5302e}sr-rd-content em{font-style:italic;color:#400469}sr-rd-content ol,sr-rd-content ul{list-style-type:none}sr-rd-content ol li,sr-rd-content ul li{margin:0}sr-rd-content table{line-height:25.6px;line-height:1.6rem;border-collapse:collapse;border-spacing:0;empty-cells:show;border:1px solid #cbcbcb}sr-rd-content thead{background-color:#e0e0e0;color:#000;text-align:left;vertical-align:bottom}sr-rd-content td,sr-rd-content th{border-left:1px solid #cbcbcb;border-width:0 0 0 1px;margin:0;overflow:visible;padding:.5em 1em}sr-rd-content sr-blockquote{background-color:hsla(0,0%,50%,.05);border-top-right-radius:5px;border-bottom-right-radius:5px;border-left:8px solid #979797;line-height:2}sr-rd-content sr-blockquote *{line-height:inherit}.simpread-theme-root{color:#333;background:#fff}sr-rd-title{font-size:44.8px;font-size:2.8rem;line-height:1.2;font-weight:700;color:#6363ac}sr-rd-desc{padding:10px;background-color:hsla(0,0%,50%,.05);font-size:28.8px;font-size:1.8rem;text-align:center;border-top-right-radius:5px;border-bottom-right-radius:5px;border-left:8px solid #979797}sr-rd-content{color:#333}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{line-height:1.7}sr-rd-content a,sr-rd-content a:link{color:#005dad;text-decoration:underline}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#fff;background-color:#2a6496;text-decoration:none}sr-rd-content pre{color:#e9eded;background-color:#263238}sr-rd-content li code,sr-rd-content p code{color:#949415;background-color:transparent}.simpread-multi-root{background:#f8f9fa}</style><style type="text/css">sr-rd-theme-night{display:none}sr-rd-content h1{margin-top:2em}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{color:#dedede;font-weight:400;clear:both;-ms-word-wrap:break-word;word-wrap:break-word;margin:0;padding:0}sr-rd-content h1{font-size:57.6px;font-size:3.6rem;line-height:64px;line-height:4rem;margin-bottom:38.4px;margin-bottom:2.4rem;letter-spacing:-1.5px}sr-rd-content h2{font-size:38.4px;font-size:2.4rem;line-height:48px;line-height:3rem}sr-rd-content h2,sr-rd-content h3{margin-bottom:38.4px;margin-bottom:2.4rem;letter-spacing:-1px}sr-rd-content h3{font-size:28.8px;font-size:1.8rem;line-height:38.4px;line-height:2.4rem}sr-rd-content h4{font-size:25.6px;font-size:1.6rem;line-height:35.2px;line-height:2.2rem;margin-bottom:38.4px;margin-bottom:2.4rem}sr-rd-content h5{font-size:16px;font-size:1rem;line-height:20px;line-height:1.25rem;margin-bottom:24px;margin-bottom:1.5rem}sr-rd-content h6{font-size:25.6px;font-size:1.6rem;line-height:25.6px;line-height:1.6rem;margin-bottom:12px;margin-bottom:.75rem;font-weight:700}sr-rd-content ol,sr-rd-content ul{padding:0 0 0 30px;padding:0 0 0 1.875rem}sr-rd-content ul{list-style:square}sr-rd-content ol{list-style:decimal}sr-rd-content ol ol,sr-rd-content ol ul,sr-rd-content ul ol,sr-rd-content ul ul{margin:0}sr-rd-content li div{padding-top:0}sr-rd-content li,sr-rd-content li p{margin:0;position:relative}sr-rd-content table{margin-top:0;margin-bottom:24px;margin-bottom:1.5rem;border-collapse:collapse;border-spacing:0;page-break-inside:auto;text-align:left}sr-rd-content table a{color:#dedede}sr-rd-content thead{display:table-header-group}sr-rd-content table td,sr-rd-content table th{border:1px solid #474d54}sr-rd-content sr-blockquote{margin:0 0 30px 30px;margin:0 0 1.875rem 1.875rem;border-left:2px solid #474d54;padding-left:30px;margin-top:35px;line-height:2}.simpread-multi-root,.simpread-theme-root{background:#363b40;color:#b8bfc6}sr-rd-title{color:#dedede;font-size:50.4px;font-size:3.15rem;line-height:56px;line-height:3.5rem;letter-spacing:-1.5px}sr-rd-desc{margin:35px;margin-left:0;padding-left:30px;padding-left:1.875rem;font-size:32px;font-size:2rem;line-height:2;border-left:2px solid #474d54}sr-rd-content,sr-rd-desc{color:#b8bfc6}sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#b8bfc6;margin-top:0;line-height:2}sr-rd-content a,sr-rd-content a:link{color:#e0e0e0;text-decoration:underline;-webkit-transition:all .2s ease-in-out;transition:all .2s ease-in-out}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#fff}sr-rd-content pre{background-color:transparent;border:1px solid}sr-rd-content li code,sr-rd-content p code{background:rgba(0,0,0,.05)}sr-rd-mult{background-color:#2d3034}panel{background-color:#2e2e2e!important}panel panel-tab span{color:#fff}panel sr-opt-gp sr-opt-label{color:rgba(108,255,240,.8);font-weight:400}panel text-field-float{color:rgba(108,255,240,.8)!important;font-weight:400!important}panel list-field{background-color:transparent!important}panel list-field:hover list-field-name{color:#fff!important;font-weight:700}panel input,panel list-field-name{color:hsla(35,10%,76%,.87)!important}panel list-view{background-color:#2e2e2e!important}panel-tabs{border-bottom-color:#393d40!important}sr-annote{color:#333}</style><style type="text/css">sr-rd-theme-dark{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-weight:700;color:#dbdbfd}sr-rd-content h1{font-size:48px;font-size:3rem}sr-rd-content h2{font-size:44.8px;font-size:2.8rem}sr-rd-content h3{font-size:40px;font-size:2.5rem}sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{color:#549ad8}sr-rd-content h5{font-size:32px;font-size:2rem}sr-rd-content h6{font-size:28.8px;font-size:1.8rem}sr-rd-content strong{color:#ffffc5}sr-rd-content em{color:#c885f5}sr-rd-content table{width:100%;line-height:25.6px;line-height:1.6rem;border-collapse:collapse;border-spacing:0;empty-cells:show;border:1px solid #cbcbcb}sr-rd-content thead{background-color:#263238;color:#f5f5f5;text-align:left;vertical-align:bottom}sr-rd-content table td,sr-rd-content table th{border-left:1px solid #cbcbcb;border-width:0 0 0 1px;margin:0;overflow:visible;padding:.5em 1em}sr-rd-content sr-blockquote{background-color:hsla(0,0%,50%,.05);border-top-right-radius:5px;border-bottom-right-radius:5px;border-left:8px solid #979797;color:#ebebeb}.simpread-multi-root,.simpread-theme-root{color:#ebebeb;background:#222}sr-rd-title{padding-bottom:.3em;font-size:44.8px;font-size:2.8rem;font-weight:700;line-height:1.2;color:#dbdbfd;border-bottom:1px solid #eee}sr-rd-desc{margin:20px;margin-left:0;padding:5px 20px;font-size:28.8px;font-size:1.8rem;background-color:hsla(0,0%,50%,.05);color:#ebebeb;border-top-right-radius:5px;border-bottom-right-radius:5px;border-left:8px solid #979797}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{line-height:1.7;color:#ebebeb}sr-rd-content a,sr-rd-content a:link{color:#8ac9ff;text-decoration:underline}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{background-color:#2a6496;color:#fff;text-decoration:none}sr-rd-content pre{color:#e9eded;background-color:#263238}sr-rd-content li code,sr-rd-content p code{color:#caca16;background-color:transparent}sr-rd-mult{background-color:hsla(0,0%,50%,.1)}panel{background-color:#222!important}panel panel-tab span{color:#fff}panel sr-opt-gp sr-opt-label{color:rgba(108,255,240,.8);font-weight:400}panel text-field-float{color:rgba(108,255,240,.8)!important;font-weight:400!important}panel list-field{background-color:transparent!important}panel list-field:hover list-field-name{color:#fff!important;font-weight:700}panel input,panel list-field-name{color:hsla(35,10%,76%,.87)!important}panel list-view{background-color:#222!important}panel text-field input{color:rgba(108,255,240,.8)!important}panel-tabs{border-bottom-color:#393d40!important}sr-annote{color:#333}</style><style type="text/css">sr-rd-theme-mail{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{position:relative;margin-top:1em;margin-bottom:1pc;font-weight:700;line-height:1.4;text-align:left;color:#363636}sr-rd-content h1{padding-bottom:.3em;font-size:36p;line-height:1.2}sr-rd-content h2{padding-bottom:.3em;font-size:28p;line-height:1.225}sr-rd-content h3{font-size:24p;line-height:1.43}sr-rd-content h4{font-size:2p}sr-rd-content h5{font-size:16px}sr-rd-content h6{font-size:16px;color:#777}sr-rd-content ol,sr-rd-content ul{list-style-type:disc;padding:0;padding-left:2em}sr-rd-content ol ol,sr-rd-content ul ol{list-style-type:lower-roman}sr-rd-content ol ol ol,sr-rd-content ol ul ol,sr-rd-content ul ol ol,sr-rd-content ul ul ol{list-style-type:lower-alpha}sr-rd-content table{width:100%;overflow:auto;word-break:normal;word-break:keep-all}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{padding:6px 13px;border:1px solid #ddd}sr-rd-content table tr{background-color:#fff;border-top:1px solid #ccc}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{border-left:4px solid #ddd}.simpread-theme-root{background-color:#fff;color:#333}.sr-header{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:left;-ms-flex-pack:left;justify-content:left;width:100%;margin:10px 0;height:41px;border-bottom:1px solid #e0e0e0;padding-bottom:10px}.sr-header,.sr-header a{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#195bf7}.sr-header a{display:-webkit-box;display:-ms-flexbox;display:flex;text-decoration:none}.sr-header .sr-name{height:41px;line-height:41px;font-size:20px;font-weight:700;text-decoration:none}.sr-header .sr-logo{display:block;width:41px;height:41px;background-repeat:no-repeat;background-position:50%;background-image:url(https://simpread-1254315611.file.myqcloud.com/favicon/favicon-32x32.png);margin-right:5px}.sr-header .sr-slogan{height:41px;line-height:44px;font-weight:700;font-size:15px}.sr-rd-footer{font-size:14px;text-align:center;color:#363636}.sr-rd-footer-group{display:-webkit-box;display:-ms-flexbox;display:flex;height:20px}.sr-rd-footer-line{width:100%;border-top:1px solid #e0e0e0}.sr-rd-footer-text{min-width:150px;line-height:0;text-align:center}.sr-rd-footer-copywrite{margin:10px 0 0;color:#363636}.sr-rd-footer-copywrite abbr{-webkit-font-feature-settings:normal;font-feature-settings:normal;font-variant:normal;text-decoration:none}.sr-rd-footer-copywrite .second{margin:10px 0}.sr-rd-footer-copywrite .third a:hover{border:none!important}.sr-rd-footer-copywrite .third a:first-child{margin-right:50px}.sr-rd-footer-copywrite .sr-icon{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:33px;height:33px;opacity:.8;-webkit-transition:opacity .5s ease;transition:opacity .5s ease;cursor:pointer}.sr-rd-footer-copywrite .sr-icon:hover{opacity:1}.sr-rd-footer-copywrite a,.sr-rd-footer-copywrite a:link,.sr-rd-footer-copywrite a:visited{margin:0;padding:0;color:inherit;background-color:transparent;font-size:inherit!important;line-height:normal;text-decoration:none;vertical-align:baseline;vertical-align:initial;border:none!important;box-sizing:border-box}.sr-rd-footer-copywrite a:focus,.sr-rd-footer-copywrite a:hover,.sr-rd-footer a:active{color:inherit;text-decoration:none;border-bottom:1px dotted!important}.sr-rd-content-desc{margin:0;padding:0 0 0 1em;color:#363636;line-height:2;font-size:18px;border-left:4px solid hsla(0,0%,67%,.5)}sr-rd-content{font-size:16px;line-height:1.6}sr-rd-content h1,sr-rd-content h1 *,sr-rd-content h2,sr-rd-content h2 *,sr-rd-content h3,sr-rd-content h3 *,sr-rd-content h4,sr-rd-content h4 *,sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{word-break:break-all}sr-rd-content div,sr-rd-content p{display:block;float:inherit;line-height:1.6;font-size:16px}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#363636;font-weight:400;line-height:1.8}sr-rd-content strong,sr-rd-content strong *{-webkit-animation:none 0s ease 0s 1 normal none running;animation:none 0s ease 0s 1 normal none running;-webkit-backface-visibility:visible;backface-visibility:visible;background:transparent none repeat 0 0/auto auto padding-box border-box scroll;border:medium none currentColor;border-collapse:separate;-o-border-image:none;border-image:none;border-radius:0;border-spacing:0;bottom:auto;box-shadow:none;box-sizing:content-box;caption-side:top;clear:none;clip:auto;color:#000;-webkit-columns:auto;-moz-columns:auto;columns:auto;-webkit-column-count:auto;-moz-column-count:auto;column-count:auto;-webkit-column-fill:balance;-moz-column-fill:balance;column-fill:balance;-webkit-column-gap:normal;-moz-column-gap:normal;column-gap:normal;-webkit-column-rule:medium none currentColor;-moz-column-rule:medium none currentColor;column-rule:medium none currentColor;-webkit-column-span:1;-moz-column-span:1;column-span:1;-webkit-column-width:auto;-moz-column-width:auto;column-width:auto;content:normal;counter-increment:none;counter-reset:none;cursor:auto;direction:ltr;display:inline;empty-cells:show;float:none;font-family:serif;font-size:medium;font-style:normal;font-variant:normal;font-weight:400;font-stretch:normal;line-height:normal;height:auto;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;left:auto;letter-spacing:normal;list-style:disc outside none;margin:0;max-height:none;max-width:none;min-height:0;min-width:0;opacity:1;orphans:2;outline:medium none invert;overflow:visible;overflow-x:visible;overflow-y:visible;padding:0;page-break-after:auto;page-break-before:auto;page-break-inside:auto;-webkit-perspective:none;perspective:none;-webkit-perspective-origin:50% 50%;perspective-origin:50% 50%;position:static;right:auto;-moz-tab-size:8;-o-tab-size:8;tab-size:8;table-layout:auto;text-align:left;text-align-last:auto;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;top:auto;-webkit-transform:none;transform:none;-webkit-transform-origin:50% 50% 0;transform-origin:50% 50% 0;-webkit-transform-style:flat;transform-style:flat;-webkit-transition:none 0s ease 0s;transition:none 0s ease 0s;unicode-bidi:normal;vertical-align:baseline;visibility:visible;white-space:normal;widows:2;width:auto;word-spacing:normal;z-index:auto;all:initial}sr-rd-content a,sr-rd-content a:link{color:#4183c4;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#4183c4;text-decoration:underline}sr-rd-content figure{margin:0;padding:0}sr-rd-content img{display:inline-block;padding:0;height:auto;line-height:100%;max-width:50%;text-decoration:none;vertical-align:text-bottom;border-radius:10px;outline:none}sr-rd-content pre{background-color:#f7f7f7;border-radius:3px}sr-rd-content pre *{font-size:1.1px}sr-rd-content li code,sr-rd-content p code{background-color:rgba(0,0,0,.04);border-radius:3px}.simpread-multi-root{background:#f8f9fa}.sr-rd-mult{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin:0 0 16px;padding:16px 0 24px;width:100%;background-color:#fff;border-bottom:1px solid #e0e0e0}.sr-rd-mult .sr-rd-mult-content{padding:0 16px;overflow:auto}.sr-rd-mult .sr-rd-mult-avatar{margin:0 15px}.sr-rd-mult .sr-rd-mult-avatar span{display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;max-width:75px;overflow:hidden;text-overflow:ellipsis;text-align:left;font-size:16px;font-size:1rem}.sr-rd-mult .sr-rd-mult-avatar img{margin-bottom:0;max-width:50px;max-height:50px;width:50px;height:50px;border-radius:50%}.sr-rd-mult .sr-rd-mult-avatar .sr-rd-content-center{margin:0}sr-rd-content.embed *{font-size:medium}sr-rd-content.embed img{max-width:100%}sr-rd-content.embed a,sr-rd-content.embed a:hover{color:inherit;font-size:medium}sr-rd-content.embed a:hover{background-color:inherit}sr-rd-content.embed .MathJax_Processed,sr-rd-content.embed math{display:none}sr-rd-content.embed pre{color:#000;color:initial;background-color:transparent}sr-rd-content.embed pre,sr-rd-content.embed pre *{font-size:13px!important}</style><style type="text/css">@media (pointer:coarse){sr-read{margin:20px 5%!important;min-width:0!important;max-width:90%!important}sr-rd-title{margin-top:0;font-size:2.7rem}sr-rd-content sr-blockquote,sr-rd-desc{margin:10 0!important;padding:0 0 0 10px!important;width:90%;font-size:1.8rem;font-style:normal;line-height:1.7;text-align:justify}sr-rd-content{font-size:1.75rem;font-weight:300}sr-rd-content figure{margin:0;padding:0;text-align:center}sr-rd-content a,sr-rd-content a:link,sr-rd-content li code,sr-rd-content p code{font-size:inherit}sr-rd-footer{margin-top:20px}sr-blockquote,sr-blockquote *{margin:5px!important;padding:5px!important}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6,sr-rd-title{font-family:PingFang SC,Verdana,Helvetica Neue,Microsoft Yahei,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#000;font-weight:100;line-height:1.35}sr-rd-content-h1,sr-rd-content-h2,sr-rd-content-h3,sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{margin-top:1.2em;margin-bottom:.6em;line-height:1.35}sr-rd-content-h1,sr-rd-content h1{font-size:1.8em}sr-rd-content-h2,sr-rd-content h2{font-size:1.6em}sr-rd-content-h3,sr-rd-content h3{font-size:1.4em}sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-size:1.2em}sr-rd-content-ul,sr-rd-content ul{margin-left:1.3em!important;list-style:disc}sr-rd-content-ol,sr-rd-content ol{list-style:decimal;margin-left:1.9em!important}sr-rd-content-ol ol,sr-rd-content-ol ul,sr-rd-content-ul ol,sr-rd-content-ul ul,sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em!important}sr-rd-content img{margin:0;padding:0;border:0;max-width:100%!important;height:auto;box-shadow:0 20px 20px -10px rgba(0,0,0,.1)}sr-rd-mult{min-width:0;background-color:#fff;box-shadow:0 1px 6px rgba(32,33,36,.28);border-radius:8px}sr-rd-mult sr-rd-mult-avatar div{margin:0}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center-small{margin:7px 0!important}sr-rd-mult sr-rd-mult-avatar span{display:block}sr-rd-mult sr-rd-mult-content{padding-left:0}@media only screen and (max-device-width:1024px){.simpread-theme-root,html.simpread-theme-root{font-size:80%!important}sr-rd-mult sr-rd-mult-avatar img{width:50px;height:50px;min-width:50px;min-height:50px}}@media only screen and (max-device-width:414px){.simpread-theme-root,html.simpread-theme-root{font-size:70%!important}sr-rd-mult sr-rd-mult-avatar img{width:30px;height:30px;min-width:30px;min-height:30px}}@media only screen and (max-device-width:320px){.simpread-theme-root,html.simpread-theme-root{font-size:90%!important}sr-rd-content p{margin-bottom:.5em}}}</style><style type="text/css">button[aria-label][data-balloon-pos]{overflow:visible}[aria-label][data-balloon-pos]{position:relative;cursor:pointer}[aria-label][data-balloon-pos]:after{text-indent:0;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;font-weight:400;font-style:normal;text-shadow:none;font-size:12px;background:hsla(0,0%,6%,.95);border-radius:2px;color:#fff;content:attr(aria-label);padding:.5em 1em;white-space:nowrap}[aria-label][data-balloon-pos]:after,[aria-label][data-balloon-pos]:before{opacity:0;pointer-events:none;-webkit-transition:all .18s ease-out .18s;transition:all .18s ease-out .18s;position:absolute;z-index:10}[aria-label][data-balloon-pos]:before{width:0;height:0;border:5px solid transparent;border-top-color:hsla(0,0%,6%,.95);content:""}[aria-label][data-balloon-pos]:hover:after,[aria-label][data-balloon-pos]:hover:before,[aria-label][data-balloon-pos]:not([data-balloon-nofocus]):focus:after,[aria-label][data-balloon-pos]:not([data-balloon-nofocus]):focus:before,[aria-label][data-balloon-pos][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-visible]:before{opacity:1;pointer-events:none}[aria-label][data-balloon-pos].font-awesome:after{font-family:FontAwesome,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif}[aria-label][data-balloon-pos][data-balloon-break]:after{white-space:pre}[aria-label][data-balloon-pos][data-balloon-break][data-balloon-length]:after{white-space:pre-line;word-break:break-word}[aria-label][data-balloon-pos][data-balloon-blunt]:after,[aria-label][data-balloon-pos][data-balloon-blunt]:before{-webkit-transition:none;transition:none}[aria-label][data-balloon-pos][data-balloon-pos=up]:after{margin-bottom:10px}[aria-label][data-balloon-pos][data-balloon-pos=up]:after,[aria-label][data-balloon-pos][data-balloon-pos=up]:before{bottom:100%;left:50%;-webkit-transform:translate(-50%,4px);transform:translate(-50%,4px);-webkit-transform-origin:top;transform-origin:top}[aria-label][data-balloon-pos][data-balloon-pos=up]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=up]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=up][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=up][data-balloon-visible]:before{-webkit-transform:translate(-50%);transform:translate(-50%)}[aria-label][data-balloon-pos][data-balloon-pos=up-left]:after{bottom:100%;left:0;margin-bottom:10px;-webkit-transform:translateY(4px);transform:translateY(4px);-webkit-transform-origin:top;transform-origin:top}[aria-label][data-balloon-pos][data-balloon-pos=up-left]:before{bottom:100%;left:5px;-webkit-transform:translateY(4px);transform:translateY(4px);-webkit-transform-origin:top;transform-origin:top}[aria-label][data-balloon-pos][data-balloon-pos=up-left]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=up-left]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=up-left][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=up-left][data-balloon-visible]:before{-webkit-transform:translate(0);transform:translate(0)}[aria-label][data-balloon-pos][data-balloon-pos=up-right]:after{bottom:100%;right:0;margin-bottom:10px;-webkit-transform:translateY(4px);transform:translateY(4px);-webkit-transform-origin:top;transform-origin:top}[aria-label][data-balloon-pos][data-balloon-pos=up-right]:before{bottom:100%;right:5px;-webkit-transform:translateY(4px);transform:translateY(4px);-webkit-transform-origin:top;transform-origin:top}[aria-label][data-balloon-pos][data-balloon-pos=up-right]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=up-right]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=up-right][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=up-right][data-balloon-visible]:before{-webkit-transform:translate(0);transform:translate(0)}[aria-label][data-balloon-pos][data-balloon-pos=down]:after{left:50%;margin-top:10px;top:100%;-webkit-transform:translate(-50%,-4px);transform:translate(-50%,-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down]:before{width:0;height:0;border:5px solid transparent;border-bottom-color:hsla(0,0%,6%,.95);left:50%;top:100%;-webkit-transform:translate(-50%,-4px);transform:translate(-50%,-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=down]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=down][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=down][data-balloon-visible]:before{-webkit-transform:translate(-50%);transform:translate(-50%)}[aria-label][data-balloon-pos][data-balloon-pos=down-left]:after{left:0;margin-top:10px;top:100%;-webkit-transform:translateY(-4px);transform:translateY(-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down-left]:before{width:0;height:0;border:5px solid transparent;border-bottom-color:hsla(0,0%,6%,.95);left:5px;top:100%;-webkit-transform:translateY(-4px);transform:translateY(-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down-left]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=down-left]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=down-left][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=down-left][data-balloon-visible]:before{-webkit-transform:translate(0);transform:translate(0)}[aria-label][data-balloon-pos][data-balloon-pos=down-right]:after{right:0;margin-top:10px;top:100%;-webkit-transform:translateY(-4px);transform:translateY(-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down-right]:before{width:0;height:0;border:5px solid transparent;border-bottom-color:hsla(0,0%,6%,.95);right:5px;top:100%;-webkit-transform:translateY(-4px);transform:translateY(-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down-right]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=down-right]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=down-right][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=down-right][data-balloon-visible]:before{-webkit-transform:translate(0);transform:translate(0)}[aria-label][data-balloon-pos][data-balloon-pos=left]:after{margin-right:10px;right:100%;top:50%;-webkit-transform:translate(4px,-50%);transform:translate(4px,-50%)}[aria-label][data-balloon-pos][data-balloon-pos=left]:before{width:0;height:0;border:5px solid transparent;border-left-color:hsla(0,0%,6%,.95);right:100%;top:50%;-webkit-transform:translate(4px,-50%);transform:translate(4px,-50%)}[aria-label][data-balloon-pos][data-balloon-pos=left]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=left]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=left][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=left][data-balloon-visible]:before{-webkit-transform:translateY(-50%);transform:translateY(-50%)}[aria-label][data-balloon-pos][data-balloon-pos=right]:after{left:100%;margin-left:10px;top:50%;-webkit-transform:translate(-4px,-50%);transform:translate(-4px,-50%)}[aria-label][data-balloon-pos][data-balloon-pos=right]:before{width:0;height:0;border:5px solid transparent;border-right-color:hsla(0,0%,6%,.95);left:100%;top:50%;-webkit-transform:translate(-4px,-50%);transform:translate(-4px,-50%)}[aria-label][data-balloon-pos][data-balloon-pos=right]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=right]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=right][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=right][data-balloon-visible]:before{-webkit-transform:translateY(-50%);transform:translateY(-50%)}[aria-label][data-balloon-pos][data-balloon-length=small]:after{white-space:normal;width:80px}[aria-label][data-balloon-pos][data-balloon-length=medium]:after{white-space:normal;width:150px}[aria-label][data-balloon-pos][data-balloon-length=large]:after{white-space:normal;width:260px}[aria-label][data-balloon-pos][data-balloon-length=xlarge]:after{white-space:normal;width:380px}@media screen and (max-width:768px){[aria-label][data-balloon-pos][data-balloon-length=xlarge]:after{white-space:normal;width:90vw}}[aria-label][data-balloon-pos]:before{display:none}[aria-label][data-balloon-pos]:after{box-shadow:0 0 10px rgba(0,0,0,.3);border-radius:5px;font-weight:700;font-size:10px}[aria-label][data-balloon-pos][data-balloon-pos=up]:after{line-height:21px}[aria-label][data-balloon-pos][data-balloon-order=downleft]:after{left:120%}[aria-label][data-balloon-pos][data-balloon-order=downright]:after{right:-22px}[aria-label][data-balloon-pos][data-balloon-order=upright]:after{left:10%}</style><style type="text/css">/*!
 * Waves v0.7.5
 * http://fian.my.id/Waves
 * 
 * Copyright 2014-2016 Alfiana E. Sibuea and other contributors
 * Released under the MIT license
 * https://github.com/fians/Waves/blob/master/LICENSE
 */.md-waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent}.md-waves-effect .md-waves-ripple{position:absolute;border-radius:50%;width:100px;height:100px;margin-top:-50px;margin-left:-50px;opacity:0;background:rgba(0,0,0,.2);background:-webkit-radial-gradient(rgba(0,0,0,.2) 0,rgba(0,0,0,.3) 40%,rgba(0,0,0,.4) 50%,rgba(0,0,0,.5) 60%,hsla(0,0%,100%,0) 70%);background:radial-gradient(rgba(0,0,0,.2) 0,rgba(0,0,0,.3) 40%,rgba(0,0,0,.4) 50%,rgba(0,0,0,.5) 60%,hsla(0,0%,100%,0) 70%);-webkit-transition:all .5s ease-out;transition:all .5s ease-out;-webkit-transition-property:-webkit-transform,opacity;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transform:scale(0) translate(0);transform:scale(0) translate(0);pointer-events:none}.md-waves-effect.md-waves-light .md-waves-ripple{background:hsla(0,0%,100%,.4);background:-webkit-radial-gradient(hsla(0,0%,100%,.2) 0,hsla(0,0%,100%,.3) 40%,hsla(0,0%,100%,.4) 50%,hsla(0,0%,100%,.5) 60%,hsla(0,0%,100%,0) 70%);background:radial-gradient(hsla(0,0%,100%,.2) 0,hsla(0,0%,100%,.3) 40%,hsla(0,0%,100%,.4) 50%,hsla(0,0%,100%,.5) 60%,hsla(0,0%,100%,0) 70%)}.md-waves-effect.md-waves-classic .md-waves-ripple{background:rgba(0,0,0,.2)}.md-waves-effect.md-waves-classic.md-waves-light .md-waves-ripple{background:hsla(0,0%,100%,.4)}.md-waves-notransition{-webkit-transition:none!important;transition:none!important}.md-waves-button,.md-waves-circle{-webkit-transform:translateZ(0);transform:translateZ(0);-webkit-mask-image:-webkit-radial-gradient(circle,#fff 100%,#000 0)}.md-waves-button,.md-waves-button-input,.md-waves-button:hover,.md-waves-button:visited{white-space:nowrap;vertical-align:middle;cursor:pointer;border:none;outline:none;color:inherit;background-color:transparent;font-size:1em;line-height:1em;text-align:center;text-decoration:none;z-index:1}.md-waves-button{padding:.85em 1.1em;border-radius:.2em}.md-waves-button-input{margin:0;padding:.85em 1.1em}.md-waves-input-wrapper{border-radius:.2em;vertical-align:bottom}.md-waves-input-wrapper.md-waves-button{padding:0}.md-waves-input-wrapper .md-waves-button-input{position:relative;top:0;left:0;z-index:1}.md-waves-circle{text-align:center;width:2.5em;height:2.5em;line-height:2.5em;border-radius:50%}.md-waves-float{-webkit-mask-image:none;box-shadow:0 1px 1.5px 1px rgba(0,0,0,.12);-webkit-transition:all .3s;transition:all .3s}.md-waves-float:active{box-shadow:0 8px 20px 1px rgba(0,0,0,.3)}.md-waves-block{display:block}</style><style type="text/css">.simpread-font{font:300 16px/1.8 -apple-system,PingFang SC,Microsoft Yahei,Lantinghei SC,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#333;text-rendering:optimizelegibility;-webkit-text-size-adjust:100%;-webkit-font-smoothing:antialiased}.simpread-hidden{display:none}.simpread-read-root{display:-webkit-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;margin:0;top:-1000px;left:0;width:100%;z-index:2147483646;overflow-x:hidden;opacity:0;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}.simpread-read-root-show{top:0}.simpread-read-root-hide{top:1000px}sr-read{display:-webkit-flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-flow:column nowrap;flex-flow:column;margin:20px 20%;min-width:400px;min-height:400px;text-align:center}read-process{position:fixed;top:0;left:0;height:3px;width:100%;background-color:#64b5f6;-webkit-transition:width 2s;transition:width 2s;z-index:20000}sr-rd-content-error{display:block;position:relative;margin:0;margin-bottom:30px;padding:25px;background-color:rgba(0,0,0,.05)}sr-rd-footer{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;font-size:14px}sr-rd-footer,sr-rd-footer-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}sr-rd-footer-group{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-rd-footer-line{width:100%;border-top:1px solid #e0e0e0}sr-rd-footer-text{min-width:150px}sr-rd-footer-copywrite{margin:10px 0 0;color:inherit}sr-rd-footer-copywrite abbr{-webkit-font-feature-settings:normal;font-feature-settings:normal;font-variant:normal;text-decoration:none}sr-rd-footer-copywrite .second{margin:10px 0}sr-rd-footer-copywrite .third a:hover{border:none!important}sr-rd-footer-copywrite .third a:first-child{margin-right:50px}sr-rd-footer-copywrite .sr-icon{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:33px;height:33px;opacity:.8;-webkit-transition:opacity .5s ease;transition:opacity .5s ease;cursor:pointer}sr-rd-footer-copywrite .sr-icon:hover{opacity:1}sr-rd-footer-copywrite a,sr-rd-footer-copywrite a:link,sr-rd-footer-copywrite a:visited{margin:0;padding:0;color:inherit;background-color:transparent;font-size:inherit!important;line-height:normal;text-decoration:none;vertical-align:baseline;vertical-align:initial;border:none!important;box-sizing:border-box}sr-rd-footer-copywrite a:focus,sr-rd-footer-copywrite a:hover,sr-rd-footer a:active{color:inherit;text-decoration:none;border-bottom:1px dotted!important}.simpread-blocks{text-decoration:none!important}.simpread-blocks *{margin:0}.simpread-blocks a{padding:0;text-decoration:none!important}.simpread-blocks img{margin:0;padding:0;border:0;background:transparent;box-shadow:none}.simpread-focus-root{display:block;position:fixed;top:0;left:0;right:0;bottom:0;background-color:hsla(0,0%,92%,.9);z-index:2147483645;opacity:0;-webkit-transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms;transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms}.simpread-focus-highlight{position:relative;box-shadow:0 0 0 20px #fff;background-color:#fff;overflow:visible;z-index:2147483646}.sr-controlbar-bg sr-rd-crlbar,.sr-controlbar-bg sr-rd-crlbar fab{z-index:2147483647}sr-rd-crlbar.controlbar{position:fixed;right:0;bottom:0;width:100px;height:100%;opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}sr-rd-crlbar.controlbar:hover{opacity:1}sr-rd-crlbar fap *{box-sizing:border-box}@media (max-height:620px){fab{zoom:.8}}@media (max-height:783px){dialog-gp dialog-content{max-height:580px}dialog-gp dialog-footer{border-top:1px solid #e0e0e0}}.simpread-highlight-selector{cursor:pointer!important}.simpread-highlight-controlbar,.simpread-highlight-selector{background-color:#fafafa!important;outline:3px dashed #1976d2!important;opacity:.8!important;-webkit-transition:opacity .5s ease!important;transition:opacity .5s ease!important}.simpread-highlight-controlbar{position:relative!important}simpread-highlight,sr-snapshot-ctlbar{position:fixed;top:0;left:0;right:0;padding:15px;height:50px;background-color:rgba(50,50,50,.9);box-shadow:0 2px 5px rgba(0,0,0,.26);box-sizing:border-box;z-index:2147483640}simpread-highlight,sr-highlight-ctl,sr-snapshot-ctlbar{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-highlight-ctl{margin:0 5px;width:50px;height:20px;color:#fff;background-color:#1976d2;border-radius:4px;box-shadow:0 3px 1px -2px rgba(0,0,0,.2),0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12);cursor:pointer}toc-bg{position:fixed;left:0;top:0;width:50px;height:200px;font-size:medium}toc-bg:hover{z-index:3}.toc-bg-hidden{opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}.toc-bg-hidden:hover{opacity:1;z-index:3}.toc-bg-hidden:hover toc{width:180px}toc *{all:unset}toc{position:fixed;left:0;top:100px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;padding:10px;width:0;max-width:200px;max-height:500px;overflow-x:hidden;overflow-y:hidden;cursor:pointer;border:1px solid hsla(0,0%,62%,.22);-webkit-transition:width .5s;transition:width .5s}toc:hover{overflow-y:auto}toc.mini:hover{width:200px!important}toc::-webkit-scrollbar{width:3px}toc::-webkit-scrollbar-thumb{border-radius:10px;background-color:hsla(36,2%,54%,.5)}toc outline{position:relative;display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;overflow:hidden;text-overflow:ellipsis;padding:2px 0;min-height:21px;line-height:21px;text-align:left}toc outline a,toc outline a:active,toc outline a:focus,toc outline a:visited{display:block;width:100%;color:inherit;font-size:11px;text-decoration:none!important;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}toc outline a:hover{font-weight:700!important}toc outline a.toc-outline-theme-dark,toc outline a.toc-outline-theme-night{color:#fff!important}.toc-level-h1{padding-left:5px}.toc-level-h2{padding-left:15px}.toc-level-h3{padding-left:25px}.toc-level-h4{padding-left:35px}.toc-outline-active{border-left:2px solid #f44336}toc outline active{position:absolute;left:0;top:0;bottom:0;padding:0 0 0 3px;border-left:2px solid #e8e8e8}sr-kbd{background:-webkit-gradient(linear,0 0,0 100%,from(#fff785),to(#ffc542));border:1px solid #e3be23;-o-border-image:none;border-image:none;-o-border-image:initial;border-image:initial;position:absolute;left:0;padding:1px 3px 0;font-size:11px!important;font-weight:700;box-shadow:0 3px 7px 0 rgba(0,0,0,.3);overflow:hidden;border-radius:3px}.sr-kbd-a{position:relative}kbd-mapping{position:fixed;left:5px;bottom:5px;-ms-flex-flow:row;flex-flow:row;width:250px;height:500px;background-color:#fff;border:1px solid hsla(0,0%,62%,.22);box-shadow:0 2px 5px rgba(0,0,0,.26);border-radius:3px}kbd-mapping,kbd-maps{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}kbd-maps{margin:40px 0 20px;width:100%;overflow-x:auto}kbd-maps::-webkit-scrollbar-thumb{background-clip:padding-box;border-radius:10px;border:2px solid transparent;background-color:rgba(85,85,85,.55)}kbd-maps::-webkit-scrollbar{width:10px;-webkit-transition:width .7s cubic-bezier(.4,0,.2,1);transition:width .7s cubic-bezier(.4,0,.2,1)}kbd-mapping kbd-map-title{position:absolute;margin:5px 0;width:100%;font-size:14px;font-weight:700}kbd-maps-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}kbd-maps-title{margin:5px 0;padding-left:53px;font-size:12px;font-weight:700}kbd-map kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fafbfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}kbd-map kbd-name{display:inline-block;text-align:right;width:50px}kbd-map kbd-desc{padding-left:3px}sharecard-bg{position:fixed;top:0;left:0;width:100%;height:100%;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(0,0,0,.4);z-index:2147483647}sharecard{max-width:450px;background-color:#64b5f6}sharecard,sharecard-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-head{margin:25px;color:#fff;border-radius:10px;box-shadow:0 2px 6px 0 rgba(0,0,0,.2),0 25px 50px 0 rgba(0,0,0,.15)}sharecard-card{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-card,sharecard-top{display:-webkit-box;display:-ms-flexbox;display:flex}sharecard-top{-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:65px;background-color:#fff;color:#878787;font-size:25px;font-weight:500;border-top-left-radius:10px;border-top-right-radius:10px}sharecard-top span.logos{display:block;width:48px;height:48px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAMAAABg3Am1AAABU1BMVEUAAAAnNJMnNZI3Q5onNJInNJMnNJInNJMnNJI8SJ0tOZY/S55EUKAoNJI6RpwoNJNIU6InNJInNJImNJI7SJwmNJJ2fLUiMJFKVaNCTJ9faK1HUaJOWKVSXaUnNJNYY6pye7cmM5JXYKhwebMjMI8mL4719fW9vb0oNZP/UlLz8/QqN5TAwMAnNJPv7+/Pz8/q6+/p6enNzc3Kysry8vMsOJXc3env7/LU1uXo29vR0dHOzs7ExMTwjo73bW37XV3Aj1TCYELl5u3n5+fW2Obn6O7f4OrZ2+g0QJkxPpgvO5bh4uvS1OTP0ePCwsJQW6ZLVqTs7fHd3d3V1dXqv79VX6lET6A1TIxXUIBSgHxWQnpelHf+WVnopkXqbC7j5Ozi4uLDw8NGUaFATJ9SgH3r6+vGyd7BxNva2trX19ejqM2gpczHx8dze7Zha67Z2dlTgH1aXQeSAAAAJnRSTlMA6ff+497Y8NL+/fv49P379sqab/BeOiX06tzVy8m/tKqpalA7G6oKj0EAAAJlSURBVEjHndNXWxpBFIDhcS2ICRLAkt4Dx4WhLk0E6R0MYoIISrWX5P9f5cwSIRC2+T1czMV5n2FnZwn2eWONUqCAv3H2Uf5Ra1hx4+0WEXtDQW0fCPYJ1EffEfIV4CSROAE4jsePoTFsNmTJF/IeIHF2lgCIn57GodlqDWXBK7IwBYatVlMWFAildPKX7I3m74Z9fsCiQChoimoFQAz04Ad2gH1n9fv9n9hgMNDr9euLWD6fLxQKxaLfb7dTSlahbFVdEPwIQtrAihZQgyKCtCagbQe3xh0QFMgy5MR11+ewYY5/qlZ7vT2xu93ULKjbFLpiUxnIIwjgKmVTLDUFXMrAi2NJWCRLIthTBo4xyOLKpwyqU6CuDCI41hFBCVdOhyLw4FgJ1skCAiyl9BSHbCorgo6VJXTru5hrVCQS8Yr5xLzX59YJSFpVFwD9U0BGC3hGdFpATgRupTGe9R9I1b1ePBvXKDyvq/O/44LT4/E4BUbSCAwj8Evq6HlnOBprx6JhJz8Gktc7xeaP9ndY+0coQvCccFBD4JW60UIY50ciLOAODAQRVOeCHm4Q3Xks6uRDY+CQ+AR4T2wMYh6+jMCIQOp78CFoj0H7EQgIuhI3dGaHCrwgADwCPjJvA372GRigCJg49FUdk3D87pq3zp4SA5zc1Zh9DxfwkpjgUg5Mv+lbeE3McC8Lpu7SA3wk2xzcqL2tN5DfIsQC8HB7UamUy6FQOpTO5QKBQDZbKnWSyUzGjdWCwaDA8+7Le4BNgm3qQGWchYh9s5hNq6wVbBlbwhZYOp3OYOA4zmgEypnM2zj8ByIdedKrH8vDAAAAAElFTkSuQmCC");zoom:.8}sharecard-content{padding:15px;max-height:500px;font-size:20px;text-align:justify;background-color:#2196f3;overflow-x:hidden;overflow-y:auto}sharecard-via{padding:10px;font-size:10px;background-color:#2196f3}sharecard-footer{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:100px;background-color:#fff;color:#878787;font-size:15px;font-weight:500;border-bottom-left-radius:10px;border-bottom-right-radius:10px}sharecard-footer,sharecard-footer div{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sharecard-footer span.qrcode{display:block;width:100px;height:100px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,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")}sharecard-control{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:0 19px;height:80px;background-color:#fff}simpread-snapshot{width:100%;height:100%;cursor:move;z-index:2147483645}simpread-snapshot,sr-mask{position:fixed;left:0;top:0}sr-mask{background-color:rgba(0,0,0,.1)}.simpread-feedback,.simpread-urlscheme{position:fixed;right:20px;bottom:20px;z-index:2147483646}simpread-feedback,simpread-urlscheme{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:20px 20px 0;width:500px;color:rgba(51,51,51,.87);background-color:#fff;border-radius:3px;box-shadow:0 0 2px rgba(0,0,0,.12),0 2px 2px rgba(0,0,0,.26);overflow:hidden;-webkit-transform-origin:bottom;transform-origin:bottom;-webkit-transition:all .6s ease;transition:all .6s ease}simpread-feedback *,simpread-urlscheme *{font-size:12px!important;box-sizing:border-box}simpread-feedback.active,simpread-urlscheme.active{-webkit-animation-name:srFadeInUp;animation-name:srFadeInUp;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback.hide,simpread-urlscheme.hide{-webkit-animation-name:srFadeInDown;animation-name:srFadeInDown;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback sr-fb-label,simpread-urlscheme sr-urls-label{width:100%}simpread-feedback sr-fb-head,simpread-urlscheme sr-urls-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin-bottom:5px;width:100%}simpread-feedback sr-fb-content,simpread-urlscheme sr-urls-content{margin-bottom:5px;width:100%}simpread-feedback sr-urls-footer,simpread-urlscheme sr-urls-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-fb-a,simpread-urlscheme sr-urls-a{color:#2163f7;cursor:pointer}simpread-feedback text-field-state,simpread-urlscheme text-field-state{border-top:none rgba(34,101,247,.8)!important;border-left:none rgba(34,101,247,.8)!important;border-right:none rgba(34,101,247,.8)!important;border-bottom:2px solid rgba(34,101,247,.8)!important}simpread-feedback switch,simpread-urlscheme switch{margin-top:0!important}@-webkit-keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@-webkit-keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}@keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}simpread-feedback sr-fb-head{font-weight:700}simpread-feedback sr-fb-content{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column}simpread-feedback sr-fb-content,simpread-feedback sr-fb-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}simpread-feedback sr-fb-footer{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-close{position:absolute;right:20px;cursor:pointer;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s;z-index:200}simpread-feedback sr-close:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin-top:10px}simpread-feedback sr-stars i{margin-right:10px;cursor:pointer}simpread-feedback sr-stars i svg{-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}simpread-feedback sr-stars i svg:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars i.active svg{-webkit-transform:rotate(0) scale(1);transform:rotate(0) scale(1)}simpread-feedback sr-emojis{display:block;height:100px;overflow:hidden}simpread-feedback sr-emoji{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-transition:.3s;transition:.3s}simpread-feedback sr-emoji>svg{margin:15px 0;width:70px;height:70px;-ms-flex-negative:0;flex-shrink:0}simpread-feedback sr-stars-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin:10px 0 20px}</style><style type="text/css">sr-opt-focus,sr-opt-read{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column}sr-opt-focus,sr-opt-gp,sr-opt-read{display:-webkit-flex;-webkit-box-direction:normal;width:100%}sr-opt-gp{position:relative;-webkit-box-orient:horizontal;-ms-flex-flow:row nowrap;flex-flow:row;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;margin-bottom:25px;font-size:15px}sr-opt-gp input,sr-opt-gp textarea{font-family:Inconsolata,Operator Mono,Consolas,Andale Mono WT,Andale Mono,Lucida Console,Lucida Sans Typewriter,DejaVu Sans Mono,Bitstream Vera Sans Mono,Liberation Mono,Nimbus Mono L,Courier New,Courier,monospace!important}sr-opt-gp sr-opt-label{display:block;position:absolute;margin:-8px 0 0;font-size:14px;font-weight:700;color:rgba(0,137,123,.8);-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;pointer-events:none;-webkit-transform:scale(.75) translateY(-8px);transform:scale(.75) translateY(-8px);-webkit-transform-origin:left top 0;transform-origin:left top 0}sr-opt-themes{display:-webkit-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-flow:row nowrap;flex-flow:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;width:100%;margin:8px 0 17px;padding:0}sr-opt-theme{width:40px;height:20px;cursor:pointer;list-style:none;border-radius:3px;border:1px solid #212121;box-sizing:border-box;opacity:1;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms}sr-opt-theme:hover{-webkit-transform:translateY(-1px);transform:translateY(-1px);box-shadow:0 5px 10px rgba(0,0,0,.2)}sr-opt-theme:not(:first-child){margin-left:5px}sr-opt-theme[sr-type=active]{box-shadow:0 5px 10px rgba(0,0,0,.2);border:none}</style><style type="text/css">notify-gp{font:300 14px -apple-system,PingFang SC,Microsoft Yahei,Lantinghei SC,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;text-rendering:optimizelegibility;-webkit-text-size-adjust:100%;-webkit-font-smoothing:antialiased;display:-webkit-flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-flow:column nowrap;flex-flow:column;-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end;position:fixed;top:0;right:0;margin:0 15px 0 0;padding:0;text-transform:none;pointer-events:none}notify-gp notify{display:-webkit-flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0;margin-top:15px;padding:14px 24px;min-width:288px;max-width:568px;min-height:48px;color:hsla(0,0%,100%,.9);background-color:#000;box-sizing:border-box;border-radius:4px;pointer-events:auto;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;opacity:0;-webkit-transform:scaleY(0);transform:scaleY(0);-webkit-transform-origin:left top 0;transform-origin:left top 0;-webkit-transition:all .45s cubic-bezier(.23,1,.32,1) 0ms,opacity 1s cubic-bezier(.23,1,.32,1) 0ms;transition:all .45s cubic-bezier(.23,1,.32,1) 0ms,opacity 1s cubic-bezier(.23,1,.32,1) 0ms;box-shadow:0 3px 5px -1px rgba(0,0,0,.2),0 6px 10px 0 rgba(0,0,0,.14),0 1px 18px 0 rgba(0,0,0,.12)}notify-gp notify-title{font-size:13px;font-weight:700}notify-gp notify-content{display:block;font-size:14px;font-weight:400;text-align:left;overflow:hidden}notify-gp notify-content a,notify-gp notify-content a:active,notify-gp notify-content a:link,notify-gp notify-content a:visited{margin:inherit;padding-bottom:5px;color:#fff;font-size:inherit;text-decoration:none;-webkit-transition:color .5s;transition:color .5s}notify-gp notify-content a:hover{margin:0;margin:initial;padding:0;padding:initial;color:inherit;font-size:inherit;text-decoration:none}notify-gp notify-i{display:none;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 10px 0 0;width:24px;height:24px;background-position:50%;background-repeat:no-repeat}notify-gp notify-action,notify-gp notify-cancel{display:none;margin:0 8px;max-width:80px;min-width:56px;height:36px;line-height:34px;color:#bb86fc;font-weight:500;font-size:inherit;text-transform:uppercase;text-align:center;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;cursor:pointer}notify-gp .notify-error notify-action,notify-gp .notify-error notify-cancel,notify-gp .notify-success notify-action,notify-gp .notify-success notify-cancel,notify-gp .notify-warning notify-action,notify-gp .notify-warning notify-cancel{color:#fff}notify-gp notify-action:active,notify-gp notify-cancel:active{border-radius:4px;background-color:rgba(98,0,238,.3)}notify-gp notify-cancel{margin:0}notify-gp notify-a{display:block;position:absolute;top:5px;right:5px;cursor:pointer}notify-gp notify-exit{display:none;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-left:5px;width:36px;height:36px;min-width:36px;min-height:36px;background-color:transparent;border-radius:50%;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;cursor:pointer}notify-gp notify-exit:hover{background-color:hsla(0,0%,100%,.4)}notify-gp notify-exit:active{background-color:hsla(0,0%,100%,.2)}notify-gp notify-a notify-span{display:block;width:16px;height:16px;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAABDklEQVQ4T6VT0VFCQQzcrQA7ECoRK1AqEDugA6ECsQPsADvgVSAlaAlWEGdvkjchczI45Osud9nc7m2IEmY2BfAEYA5A6xsARwAHAB8ktR6DeWNmKwAvXlSxY78i+RabEcDM9gAe/qoq+T3JhXINwDu/Xlgc1zYk13TOn+XZA4C7AvgN4LbkZgJYO+84O5C8N7Odi6n8O8llh+ZGAD3uO5LPDgIvzoDRbBDAV+dputBAXKNecQM5B9CefQlAj0JwVmdLdGSwHI1CFXEgOS8ihia1WRNRdpU9JwlatpWVc0gr3c0xu95IAfdPK2uoHkcrJxANkzTJdPKTf3ROchvJk2n0LxNPfV9vnDVEJ+P8C6jMhLeGEqMKAAAAAElFTkSuQmCC);opacity:.9}notify-gp notify-i.holdon{display:block;margin:0 0 0 24px;width:20px;height:20px;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAQAAAAngNWGAAAATUlEQVR4AWMYSuB/4P+V/1lRRFiBIoEYCoGC//+vAypFKFsHFFkJV4AsAVGKzsOjFFUZHqUElCGUwpRRrpCw1YQ9Qzh4SA5wwlE4hAAAiFGQefYhNJkAAAAASUVORK5CYII=);cursor:pointer}notify-gp .notify-show{opacity:1;-webkit-transform:scaleY(1)!important;transform:scaleY(1)!important}notify-gp .notify-hide{-webkit-animation-name:fadeOutUp;animation-name:fadeOutUp;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}notify-gp .notify-success{background-color:#4caf50}notify-gp .notify-warning{background-color:#ffa000}notify-gp .notify-error{background-color:#ef5350}notify-gp .notify-info{background-color:#1976d2}notify-gp .notify-modal{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-flow:column nowrap;flex-flow:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;height:auto;max-height:200px;box-shadow:0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12),0 3px 1px -2px rgba(0,0,0,.2)}notify-gp .notify-modal .notify-modal-content{margin-top:5px;font-size:13px;white-space:normal}notify-gp .notify-modal .notify-modal-content a{margin:0;padding:0;color:inherit;font-size:inherit;text-decoration:underline;cursor:pointer}notify-gp .notify-modal .notify-modal-content a:active,notify-gp .notify-modal .notify-modal-content a:focus,notify-gp .notify-modal .notify-modal-content a:hover,notify-gp .notify-modal .notify-modal-content a:visited{color:inherit}notify-gp .notify-snackbar{position:fixed;bottom:0;left:50%;margin-bottom:5px;-webkit-transform-origin:left bottom 0;transform-origin:left bottom 0}.notify-position-lt-corner{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;margin:0 0 0 15px;left:0;right:auto}.notify-position-lb-corner{margin:0 0 15px 15px;right:auto;left:0}.notify-position-lb-corner,.notify-position-rb-corner{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-flow:column-reverse wrap-reverse;flex-flow:column-reverse wrap-reverse;top:auto;bottom:0}.notify-position-rb-corner{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;margin:0 15px 15px 0;left:auto;right:0}@-webkit-keyframes fadeOutUp{0%{opacity:1}to{margin-top:0;padding:0;height:0;min-height:0;opacity:0;-webkit-transform:scaleY(0);transform:scaleY(0)}}@keyframes fadeOutUp{0%{opacity:1}to{margin-top:0;padding:0;height:0;min-height:0;opacity:0;-webkit-transform:scaleY(0);transform:scaleY(0)}}@media (pointer:coarse){notify-gp{top:auto;bottom:0;left:0;margin:0 10px 10px}notify-gp notify{width:100%;max-width:600px}notify-gp .notify-hide,notify-gp .notify-show{-webkit-transform-origin:bottom!important;transform-origin:bottom!important}notify-gp .notify-snackbar{position:static}}</style><style type="text/css">:root{--sr-annote-color-0:#b4d9fb;--sr-annote-color-1:#ffeb3b;--sr-annote-color-2:#80deea;--sr-annote-color-3:#85d1f6;--sr-annote-color-4:#8cd842;--sr-annote-color-5:#ffb7da}[sr-annote-bg-color]{color:inherit}[sr-annote-bg-color][data-color-type="0"]{background-color:var(--sr-annote-color-0)}[sr-annote-bg-color][data-color-type="1"]{background-color:var(--sr-annote-color-1)}[sr-annote-bg-color][data-color-type="2"]{background-color:var(--sr-annote-color-2)}[sr-annote-bg-color][data-color-type="3"]{background-color:var(--sr-annote-color-3)}[sr-annote-bg-color][data-color-type="4"]{background-color:var(--sr-annote-color-4)}[sr-annote-bg-color][data-color-type="5"]{background-color:var(--sr-annote-color-5)}[sr-annote-bb-color][data-color-type="1"]{border-bottom-color:var(--sr-annote-color-1)}[sr-annote-bb-color][data-color-type="2"]{border-bottom-color:var(--sr-annote-color-2)}[sr-annote-bb-color][data-color-type="3"]{border-bottom-color:var(--sr-annote-color-3)}[sr-annote-bb-color][data-color-type="4"]{border-bottom-color:var(--sr-annote-color-4)}[sr-annote-bb-color][data-color-type="5"]{border-bottom-color:var(--sr-annote-color-5)}[sr-annote-bl-color][data-color-type="1"]{border-left:5px solid var(--sr-annote-color-1)}[sr-annote-bl-color][data-color-type="2"]{border-left:5px solid var(--sr-annote-color-2)}[sr-annote-bl-color][data-color-type="3"]{border-left:5px solid var(--sr-annote-color-3)}[sr-annote-bl-color][data-color-type="4"]{border-left:5px solid var(--sr-annote-color-4)}[sr-annote-bl-color][data-color-type="5"]{border-left:5px solid var(--sr-annote-color-5)}[data-color-style="1"]{background-color:transparent!important;background-repeat:no-repeat;background-size:100% 100%}[data-color-style="1"][data-color-type="1"]{background-color:transparent!important;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaCAMAAAB//6mtAAAAPFBMVEVHcEz/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zsmGdgxAAAAE3RSTlMAJJGnQ3Bh/ANS6An0u9o1FMp/ufrp4AAAAgZJREFUSMe1Vou2oyAM5BEIbxD+/183gFq99bZ7dnVOe6oWZwgJGRj7DcaY+et5BqtVFcA4eLraoLUFgJJLHzvBvuM00nPIzA4qazXkDuKka0VPIHNeiuee/RXO+qaAFjGFlpQQMi7JORcQ24Z+hRhcWpYYo5SikiQpkuBnldLXQwm5hANT53pxz9v1eZIdpEFCsmpLgXF+tVKGg640ZZrmDxacaNcIzqVE3EIoC7yYnjHuLxQ45cvquDP/RvgRiG4R2uYewlXKvXbo2r8x71NyFI4EZmZaj2vEShFJpnYHQopSAT/lgHEhZY2L+1/yVzAooJyyoEGEgLeEMFWC4K9NwHiVuFXlPSLNWbZ2g/61IuKN9AMyrwEUK5a7yQlLnUnwyr1v1luKCZsaq69kwwcCaC3ZWUZZPsFOkxaUA2pt8pHZE6KyhmnXbi3O1/xDSLkXqM8QH1FYwI+26nV1TwTQljy3ma8inSLYWiN+bp74ZX16hc4mnWkb7P0Jv9FejEB8+wsXlTfvLGIbthVvFN3jAbR0l34WUodzw6avJtIp1M6vya61EuTB1EqT3JW5jYfXiWw6Y626eOP7p5Th4FbrfqiZR4PxBjq5rv/BD8jplR12bcYBw44k4KCVtR9Q/Kszni/2W+MLh66n4WjLwz0LX5vqtDnqH4nitD8M/P10ZQ44q634A66hf2zVV84fAAAAAElFTkSuQmCC)}[data-color-style="1"][data-color-type="2"]{background-color:transparent!important;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaCAMAAAB//6mtAAAANlBMVEVHcEyA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3urNaunbAAAAEXRSTlMAe+8uaFLb/AQCG0ANkralydBaCTIAAAIBSURBVEjHtVaJduMwCJRA6ED3///sIstn6qbd13hy+NlxRjBYDEp9h+B9WI6JMRIZnSnEyKQPGKKIjOwnwoD6CXLPuHmeeI5RkREi+dKEEeMKuRKFnJPAq98gvKyf0ORiXbc619KaBXDy7r2Pzwq5Ym0TlFqz5BMROb1fxi96CGeDlcPNz855wji1NddaSpOFcl5kMxHvFvEoKtdi4WDbONbjLdxMokgKelWMGZP/olpgGrKWGfA7yrdw0EQpRMlh1Py1CMnYDv/P7MZri8lJQiXTRnnmV4lzG2X9C9bMQVTTkf2FH6voWJvtf8Whb76Um41GDeD6J+BG9Ttk3B8irziXz5AftbFmrcI4JMrNfXqFGnd5su2fZR8omnhJQMOlNp+Ekaap0NT+CHu3cT5GWPsT/M6Np0haW30ketlu1VBQBvpD+gAAygZWjFgeWaFQSmMbJKPhiQL0FtWyzbzO9ss+d66/dQHnfrAJ+U0rv1qNdOn93ndi3dI692Jy86o1cWt0Sb/0QCh5uHw0ptp+xyktfwXAfRitac17l1hQYbbAYpBng+JYzxHZ4SQ1C0zyczwIw8GJ5siRx2ywNWNopPzJymaps4ljJghjIvKKlr0nBlXEcLXYLX8/nYR9lBJDphEvnWaZZQwKjNvZDE6hUFfCqyvNAevis/7A+s+rFf8Do0ByAiavdJkAAAAASUVORK5CYII=)}[data-color-style="1"][data-color-type="3"]{background-color:transparent!important;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaCAMAAAB//6mtAAAANlBMVEVHcEyF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0fYOABKzAAAAEXRSTlMA3e8uaFJ7/AQCGz8Nt5KlzvXM15AAAAIFSURBVEjHtVaNesIgDARCCA0/gfd/2YXSWuvUbd/sfWqlpQdcIBdjXmGJcVmvmSEh+lBxSYkxHPCICRg4TiwD5idon9F5NiKnZNArkf4EhARpg95JSs5ZEc1vsDyMn8HXYqnbUKWUZp0j/fTex3eD3rG2tVaKSNX1JADO74eJqx7K2dzGQSdKbR8YTStVxgSabbWusvkEzwaJoCpLsY4mHU2unbK/AI1FjCUUCZtizJDjN9UWxiFrIdon2f8E2jUrqhSArmHE/HHXZG+7+yPzvW5zCGtLxT2u9/wmc20jrP/CDJnKVkLieOIHERlR6P8FTZ31Xz2Fm32A4Bz1T2AopnpXuG2iaLiWz5Afalm/RWFcMtZGnx5B0k2e2vqH6RUlIK8LCG6Pzcfhdfsb8NIvYe8W5zYC6VfwE41dpKlNLpm9HjfxuBjv+kX6OOdAD7BhgHLJCAVzHscg++CuCEBvyazHLIZqnyTH/tYFiH6wCX0WTNysRrP0re87sZ7SEj2Y3ETzaU90ORw5cAan1OHyyXux/RmnpvwNzj2fRmsh8C1LrJB1M6kj+cQzQTHKQUxKpjleqsLnOMuDZTg44iw5qtYGWhrMN1xDE+/qiRnqqtx5q4iiwfXskZt+q3bLr6uTm3ENQ8YxX7yrZdYyaGHYW3NyJim1IJxdaRZYp2InHtjePFvxF2YVcjg/uCn9AAAAAElFTkSuQmCC)}[data-color-style="1"][data-color-type="4"]{background-color:transparent!important;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaCAMAAAB//6mtAAAAMFBMVEWM2EJHcEyM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKKz/w5AAAAEHRSTlP/AANRaHvzGy3WqT4Nk77kx+qVTgAAAb9JREFUSMe1VtlyxDAIwwYDvv//b4udY5Nus9NOE71kJocA4SDAXcEb5rWIZiQKFX3OguEFQswqKv6FNxr4iXd/0UvOzsgRRwTUrHmF3clGLsXg3QfAifjwoCjV1hl6qCm12HkAvoF7jzG21lKqVk9WlfK5gkUP42wb2RvpGTHVkYBl0GqdslHW8kMAr0jBMj3y8W9CjCJmCWFVTFTLQQvYxMaB9Lu0r6NxW5Qq+xmBozzUocM/YQWliltfj012RWpsEe4Ax2aSiT/2wKk1y7rQ4T5Uk+pVgVDQwHwbO3fgqnuTvZPa4FbYP0R+CTC0Knh3AEPKawVCNcL9aAFlBPCB4SkQmPpK6SH6jmVKpE8FGKfIRttT9JwIPdBj+jN3dR6cTb8GT0RpOLwIXKHQH6kgZjeHnQ/v/wDf8R8Htww7rxT/wMhXN88PIuWyGk4J3w9vqsPlM1GKVyN/Qe98Na1DkNXRhCbSTILtZC1j3MYHpuMnnc3dzYBrpc0R/XDw6YRjqalzNdheR+d3y5yvW6urcZd9I8I9meG3ZrdyvZ3sT4pYxJEv7rsMrItQ0X17WT7Jw5VQT7Tvy5U/4RRsxRfv9RDt9/UjOgAAAABJRU5ErkJggg==)}[data-color-style="1"][data-color-type="5"]{background-color:transparent!important;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaCAMAAAB//6mtAAAAOVBMVEVHcEz/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r20adnAAAAEnRSTlMAVQ062Hwb/ARpK/Olk7XoScQnLSnCAAAB6ElEQVRIx7VW2Y7DIAzkMBCDOf//Y9cQmiZt2q12m1EegoTGF/ZYiFdYGONHWV+kRBOL0NryzwQalEV76+1yh/gAu4uLLVoURMkG0EivvZ6QsnRyqxgfse48XqE0xgSuZRMpVcg5MFw7wIUMALXWRBRnPOoXK5wPLZmz5snh5nfj3KGfgSJR6h7UGEfipNZnRhZf0FCCcOTaKNs53AiCQ0hkypox6716rgUnmwspU2d6y/geIdQeApsQzxVfhEJo+S/Mu5SFnCFFeSvrkd9G6GX9JzpBgJpMsccQPBEZqtD+bWHLb9yVexEWjTdPL/HvRkJuIfp7Ewgb03fI77nKeGvY3lgyVvdtC6RnABYjtO+ydyQj7QgAw6E23wT2/Hikdgl7g6LmC21X8DvXXxGPNrrEe263hNzSPf/X5CeEPNqA53O6xEKSalRAoclXFIArPLvMRDgZju+e7UFyXl1oRixTanhKb3ffJeuU1rlHkVsfKG6DTpmHGZhTl1cWICRoZ5xdiAdYqM/dgGqM3abEAIVV/xLqOcZtoTtxgBwAEkUG3hRx6QrO68VYOSLvBnzJTRekeNAzLnVkbnVbMIQcvedYn7restzaD7aTxVotu79lv8uMPUj57bDOV82qRNIfaM+Wq+WXresHSSV3cD8ocu8AAAAASUVORK5CYII=)}[data-color-style="2"]{background-color:transparent!important}[data-color-style="2"][data-color-type="1"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,var(--sr-annote-color-1) 0)}[data-color-style="2"][data-color-type="2"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,var(--sr-annote-color-2) 0)}[data-color-style="2"][data-color-type="3"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,var(--sr-annote-color-3) 0)}[data-color-style="2"][data-color-type="4"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,var(--sr-annote-color-4) 0)}[data-color-style="2"][data-color-type="5"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,var(--sr-annote-color-5) 0)}[data-color-style="3"]{position:relative;background-color:transparent!important}[data-color-style="3"]:after{content:"";position:absolute;left:0;bottom:25px;height:8px;width:58px;border-radius:4px;opacity:.8;transition:all .3s}[data-color-style="3"][data-color-type="1"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 85%,var(--sr-annote-color-1) 0)}[data-color-style="3"][data-color-type="2"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 85%,var(--sr-annote-color-2) 0)}[data-color-style="3"][data-color-type="3"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 85%,var(--sr-annote-color-3) 0)}[data-color-style="3"][data-color-type="4"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 85%,var(--sr-annote-color-4) 0)}[data-color-style="3"][data-color-type="5"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 85%,var(--sr-annote-color-5) 0)}sr-annote-note{position:relative;bottom:-5px;padding:0 4px;color:#fff;background-color:#333;font-weight:700;font-style:normal;font-family:arial,helvetica,clean,sans-serif;border-radius:5px;opacity:.8;cursor:pointer}sr-annote-note:after{content:"N"}pre.sr-annote+sr-annote-note{bottom:25px;right:25px}sr-annote-note:hover{opacity:1}sr-annote-note sr-annote-note-tip{position:absolute;left:22px;top:0;padding:.5em 1em;max-width:400px;color:#fff;background:#101010;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;font-weight:400;font-style:normal;text-shadow:none;font-size:12px;text-indent:0;white-space:pre;z-index:10;border-radius:5px;box-shadow:0 0 10px rgba(0,0,0,.3);opacity:0;overflow:auto;pointer-events:none;z-index:20000;transition:all .18s ease-out .18s}sr-annote-note:hover sr-annote-note-tip{opacity:1;pointer-events:auto}sr-annote-note sr-annote-note-tip{overflow:hidden}sr-annote-note sr-annote-note-tip:hover{overflow:overlay}sr-annote-note sr-annote-note-tip::-webkit-scrollbar-track{background-color:transparent}sr-annote-note sr-annote-note-tip::-webkit-scrollbar{width:12px}sr-annote-note sr-annote-note-tip::-webkit-scrollbar-thumb{background-clip:padding-box;padding-top:80px;background-color:#ddd;border:3px solid transparent;border-radius:8px}</style><style type="text/css">.sr-annote-hideall{background-color:transparent!important;pointer-events:none}sr-annote-trigger{position:fixed!important;bottom:52px;right:32px;display:-webkit-box!important;display:-ms-flexbox!important;display:flex!important;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 0 15px;padding:0;width:40px!important;height:40px!important;line-height:40px!important;color:#fff;background-color:rgba(245,82,70,.8);border-radius:50%;cursor:pointer;box-shadow:0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12),0 3px 1px -2px rgba(0,0,0,.2);-webkit-transition:all .5s ease-in-out 0ms;transition:all .5s ease-in-out 0ms;overflow:visible!important;overflow:initial!important}sr-annote-trigger.open{right:95px}sr-annote-trigger.off{background-color:#bdbdbd}sr-annote-trigger sr-i{display:-webkit-box!important;display:-ms-flexbox!important;display:flex!important;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;border-radius:50%}sr-annote{padding:6px 0;background-color:transparent;font-size:inherit;cursor:pointer}.sr-annote[data-type=code],.sr-annote[data-type=img]{border-bottom-width:5px;border-bottom-style:solid}sr-annote[data-color-type="0"]{padding:7px 0}sr-annote-floating{position:fixed;color:#fff;background:hsla(0,0%,6%,.95);font-weight:700;border-radius:5px;box-shadow:0 0 10px rgba(0,0,0,.3);opacity:0;-webkit-animation-delay:.2s;animation-delay:.2s;-webkit-animation-duration:.5s;animation-duration:.5s;-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:sr-annote-slideInUp;animation-name:sr-annote-slideInUp;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s;z-index:2000}sr-annote-floating.hidden{-webkit-animation-duration:.5s;animation-duration:.5s;-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:sr-annote-slideInDown;animation-name:sr-annote-slideInDown;pointer-events:none}.sr-annote-floatingbar-hiden{display:none}@-webkit-keyframes sr-annote-slideInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0);visibility:visible}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@keyframes sr-annote-slideInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0);visibility:visible}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@-webkit-keyframes sr-annote-slideInDown{0%{opacity:1;visibility:visible}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}@keyframes sr-annote-slideInDown{0%{opacity:1;visibility:visible}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}sr-annote-floatingbar{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin:5px}sr-annote-floatingbar,sr-annote-floatingbar sr-anote-fb-item{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-annote-floatingbar sr-anote-fb-item{margin-right:5px;cursor:pointer}sr-annote-floatingbar sr-anote-fb-item:last-child{margin-right:0}sr-annote-floatingbar sr-anote-fb-item{width:20px;height:20px;border-radius:50%;box-sizing:border-box}sr-annote-floatingbar sr-anote-fb-item[type=copy],sr-annote-floatingbar sr-anote-fb-item[type=export],sr-annote-floatingbar sr-anote-fb-item[type=note],sr-annote-floatingbar sr-anote-fb-item[type=remove],sr-annote-floatingbar sr-anote-fb-item[type=style],sr-annote-floatingbar sr-anote-fb-item[type=tag]{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#fff;border-radius:50%}sr-annote-floatingbar sr-anote-fb-item[type=style]{background-color:#f73859!important}sr-annote-floatingbar sr-anote-fb-item[type=export]{background-color:#cc0e74}sr-annote-floatingbar sr-anote-fb-item[type=copy]{background-color:#a78674}sr-annote-floatingbar sr-anote-fb-item[type=remove]{background-color:#f44336}sr-annote-floatingbar sr-anote-fb-item[remove=confirm]{-webkit-transform:rotate(270deg);transform:rotate(270deg);-webkit-transition:all .2s ease-in-out 0s;transition:all .2s ease-in-out 0s}sr-annote-sidebar-bg{position:fixed;top:0;right:0;bottom:180px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;width:256px;font-size:22.4px;font-size:1.4rem;font-weight:500;opacity:0;-webkit-transform:translateX(256px);transform:translateX(256px);-webkit-transition:all .45s cubic-bezier(.23,1,.32,1) 0ms;transition:all .45s cubic-bezier(.23,1,.32,1) 0ms}sr-annote-sidebar-bg.mini{pointer-events:none}sr-annote-sidebar-bg:hover{z-index:2147483647}sr-annote-sidebar-bg.open{opacity:1;-webkit-transform:translateX(0);transform:translateX(0)}sr-annote-sidebar{margin:3px 4px 0;padding-left:20px;height:100%;overflow-x:hidden}sr-annote-sidebar.mini{pointer-events:none}sr-annote-sidebar *{box-sizing:border-box}sr-annote-sidebar{overflow-y:hidden}sr-annote-sidebar:hover{overflow-y:overlay}sr-annote-sidebar::-webkit-scrollbar-track{background-color:transparent}sr-annote-sidebar::-webkit-scrollbar{width:12px}sr-annote-sidebar::-webkit-scrollbar-thumb{padding-top:80px;background-clip:padding-box;background-color:#ddd;border:3px solid transparent;border-radius:8px;border-radius:10px;border:6px solid transparent;background-color:rgba(85,85,85,.55)}sr-annote-sidebar::-webkit-scrollbar{width:0;-webkit-transition:width .7s cubic-bezier(.4,0,.2,1);transition:width .7s cubic-bezier(.4,0,.2,1)}sr-annote-sidebar:hover::-webkit-scrollbar{width:16px}sr-annote-sidebar[srcoll=on]:hover sr-annote-sidebar-card[type=option].mini{-webkit-transform:translateX(-16px);transform:translateX(-16px)}sr-annote-sidebar[srcoll=on]:hover sr-annote-sidebar-card.off{-webkit-transform:translateX(190px);transform:translateX(190px)}sr-annote-sidebar-cards{display:block}sr-annote-sidebar-card{position:relative;display:block!important;margin:12px;color:rgba(51,51,51,.87);background-color:#fff;border-radius:4px;box-shadow:0 2px 5px rgba(0,0,0,.08);-webkit-transition:all .45s cubic-bezier(.23,1,.32,1) 0ms;transition:all .45s cubic-bezier(.23,1,.32,1) 0ms;pointer-events:auto}sr-annote-sidebar-card:hover{box-shadow:0 10px 20px 0 rgba(168,182,191,.6);-webkit-transform:translateY(-1px);transform:translateY(-1px)}sr-annote-sidebar-card:last-child{margin-bottom:30px}sr-annote-sidebar-card.off{display:block;-webkit-transform:translateX(205px);transform:translateX(205px);-webkit-transition:all .25s ease-out;transition:all .25s ease-out}sr-annote-sidebar-card.off:hover{-webkit-transform:translateX(120px)!important;transform:translateX(120px)!important}sr-annote-sidebar-card.hide{display:block;-webkit-transform:translateX(256px);transform:translateX(256px);-webkit-transition:all .25s ease-out;transition:all .25s ease-out}sr-annote-sidebar-card-anchor{position:absolute;left:0;top:0;width:90%;height:100%}sr-annote-sidebar-card-action{position:absolute;top:10px;right:3px;display:block;width:12px;height:12px;line-height:12px;-webkit-transition:all .25s ease-out;transition:all .25s ease-out;cursor:pointer;z-index:20000}sr-annote-sidebar-card-action.open{-webkit-transform:rotate(90deg);transform:rotate(90deg)}sr-annote-sidebar-card[mode=mini]{overflow:hidden}sr-annote-sidebar-card[mode=mini] sr-annote-sidebar-preview{display:block;padding:6px 12px 5px 10px;height:32px;color:#fff;font-size:13px;font-weight:400;white-space:nowrap;text-align:left;text-overflow:ellipsis;text-shadow:1px 1px 3px rgba(0,0,0,.3);-webkit-transition:all .25s ease-out;transition:all .25s ease-out;overflow:hidden}sr-annote-sidebar-card[mode=mini][type=img] sr-annote-sidebar-preview{text-align:center}sr-annote-sidebar-card[mode=mini] sr-annote-sidebar-detail{padding:0 15px;height:0}sr-annote-sidebar-card[mode=mini] sr-annote-sidebar-note,sr-annote-sidebar-card[mode=mini] sr-annote-sidebar-toolbars{display:none}sr-annote-sidebar-card[mode=mini] pre{margin:0!important;padding:0!important;white-space:nowrap;text-overflow:ellipsis;overflow:hidden}sr-annote-sidebar-card[mode=normal] sr-annote-sidebar-preview{display:none}sr-annote-sidebar-card[mode=normal] sr-annote-sidebar-detail{padding:15px;height:auto}sr-annote-sidebar-card[data-color-type="0"]{display:none!important}sr-annote-sidebar-card pre{margin:0!important;padding:0!important;background-color:transparent!important;max-height:200px;font-size:10px;overflow:hidden}sr-annote-sidebar-card input,sr-annote-sidebar-card textarea{font-size:12px!important}sr-annote-sidebar-card img{margin:0;padding:0;max-height:100px;max-width:80%;background:#fff;border:0;border-radius:6px;box-shadow:0 20px 20px -10px rgba(0,0,0,.1)}sr-annote-sidebar-detail{display:block;padding:15px;width:100%;color:#fff;font-size:10px;text-align:justify;border-top-left-radius:4px;border-top-right-radius:4px;-webkit-transition:all .25s ease-out;transition:all .25s ease-out}sr-annote-sidebar-card[type=img] sr-annote-sidebar-detail{text-align:center}sr-annote-sidebar-tags{-ms-flex-wrap:wrap;flex-wrap:wrap;margin-top:15px}sr-annote-sidebar-tag,sr-annote-sidebar-tags{display:-webkit-box;display:-ms-flexbox;display:flex}sr-annote-sidebar-tag{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;padding:4px 12px;color:rgba(0,0,0,.87);background-color:#fff;height:22px;font-size:14px;font-size:.875rem;font-weight:400;white-space:nowrap;border-radius:16px;outline:none;cursor:pointer;overflow:hidden;-webkit-transition:all .2s ease-in-out 0s;transition:all .2s ease-in-out 0s;-webkit-transform-origin:left;transform-origin:left;-webkit-transform:scale(.8);transform:scale(.8)}sr-annote-sidebar-note{display:block;padding:16px;width:100%;background-color:#fff;text-align:left}sr-annote-sidebar-toolbars{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;padding-right:10px;height:32px;background-color:#fff;border-bottom-left-radius:4px;border-bottom-right-radius:4px}sr-annote-sidebar-toolbar,sr-annote-sidebar-toolbars{-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-annote-sidebar-toolbar{display:-webkit-box!important;display:-ms-flexbox!important;display:flex!important;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin-left:5px;width:20px;height:20px;line-height:20px;border-radius:50%;overflow:visible!important;overflow:initial!important}sr-annote-sidebar-toolbar[remove=confirm]{-webkit-transform:rotate(270deg);transform:rotate(270deg);-webkit-transition:all .2s ease-in-out 0s;transition:all .2s ease-in-out 0s}sr-annote-sidebar-toolbar[remove=confirm] svg path{fill:#f44336}sr-annote-sidebar-card[type=unread]{background-color:#cb63e6}sr-annote-sidebar-card[type=unread] sr-annote-sidebar-detail.title{padding:0;text-align:left;font-size:15px}sr-annote-sidebar-card[type=unread] sr-annote-sidebar-detail.desc{padding:0 0 0 10px;border-left:1px outset #fff;border-top-left-radius:0;border-top-right-radius:0}sr-annote-sidebar-card[type=unread][mode=mini] sr-annote-sidebar-preview{font-size:13px;text-shadow:1px 1px 3px rgba(0,0,0,.3)}sr-annote-sidebar-card[type=option]{height:32px;background-color:transparent;box-shadow:none;overflow:visible;overflow:initial}sr-annote-sidebar-card[type=option]:hover{-webkit-transform:translateY(0);transform:translateY(0)}sr-annote-sidebar-card[type=option].mini{margin-right:0}sr-annote-sidebar-card[type=option] sr-annote-sidebar-card-action,sr-annote-sidebar-card[type=option] sr-annote-sidebar-card-anchor{display:none}sr-annote-sidebar-card[type=option] sr-annote-sidebar-preview{background-color:transparent}sr-annote-sidebar-options{position:absolute;top:0;right:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;padding-right:5px;height:100%;background-color:#3a3a3a;border-left:10px outset #222;border-radius:4px}sr-annote-sidebar-option,sr-annote-sidebar-options{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-annote-sidebar-option{display:-webkit-box!important;display:-ms-flexbox!important;display:flex!important;margin-left:5px;width:20px;height:20px;border-radius:50%;-webkit-transition:all .25s ease-out;transition:all .25s ease-out;cursor:pointer;overflow:visible!important;overflow:initial!important}sr-annote-sidebar-option[off=true],sr-annote-sidebar-option[side=false]:not(:first-child){width:0;margin-left:0}sr-annote-sidebar-option[side=false]:first-child{margin-right:5px}sr-annote-sidebar-option[off=true]~sr-annote-sidebar-option:last-child svg{-webkit-transform:rotate(180deg);transform:rotate(180deg)}sr-annote-sidebar-option[type=drag][state=on] svg path,sr-annote-sidebar-option[type=export][state=on] svg path,sr-annote-sidebar-option[type=goon][state=on] svg path,sr-annote-sidebar-option[type=save][state=on] svg path{fill:#8cd842}sr-annote-sidebar-option[type=collapse][state=on] svg{-webkit-transform:rotate(90deg);transform:rotate(90deg)}sr-annote-sidebar-option[lock=true] svg path{fill:#f55246!important}sr-annote-sidebar-card[type=option]:hover~sr-annote-sidebar-card[type=unread]{z-index:-1}</style><style type="text/css">@-webkit-keyframes fadeInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@keyframes fadeInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@-webkit-keyframes fadeOutDown{0%{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}@keyframes fadeOutDown{0%{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}.sr-alertgp{position:fixed;left:0;top:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;background-color:rgba(51,51,51,.8);z-index:2147483647}.sr-alertgp .alert{min-width:400px;min-height:400px;width:650px;background-color:#fff;border-radius:4px;box-shadow:0 14px 45px rgba(0,0,0,.247059);-webkit-animation-name:fadeInUp;animation-name:fadeInUp;-webkit-animation-duration:.8s;animation-duration:.8s;-webkit-animation-fill-mode:both;animation-fill-mode:both}.sr-alertgp .alert,.sr-alertgp .alert .loading{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.sr-alertgp .alert .loading{position:relative;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:100%;height:35%;background-color:transparent}.alert .loading .progress{display:block;margin:10px auto;max-width:80%;max-height:250px;width:20%}.alert .loading .progress .percentage{fill:#666;font-family:sans-serif;font-size:.5em;text-anchor:middle}.alert .loading .progress .circle-bg{fill:none;stroke:#eee;stroke-width:3.8}.alert .loading .progress .circle{fill:none;stroke-width:2.8;stroke-linecap:round;stroke:#1dba90;-webkit-transition:all .2s ease-in-out;transition:all .2s ease-in-out}@-webkit-keyframes scaleAnimation{0%{opacity:0;-webkit-transform:scale(1.5);transform:scale(1.5)}to{opacity:1;-webkit-transform:scale(1);transform:scale(1)}}@keyframes scaleAnimation{0%{opacity:0;-webkit-transform:scale(1.5);transform:scale(1.5)}to{opacity:1;-webkit-transform:scale(1);transform:scale(1)}}@-webkit-keyframes fadeOut{0%{opacity:1}to{opacity:0}}@keyframes fadeOut{0%{opacity:1}to{opacity:0}}@-webkit-keyframes fadeIn{0%{opacity:0}to{opacity:1}}@keyframes fadeIn{0%{opacity:0}to{opacity:1}}.sr-alertgp .alert .close{position:absolute;top:0;right:0;z-index:2}.sr-alertgp .alert .close:hover{-webkit-transform:rotate(270deg);transform:rotate(270deg);-webkit-transition:all .25s ease-out;transition:all .25s ease-out}.sr-alertgp .alert .sr-alert-icon img{max-width:650px;width:100%;-webkit-transform:scale(.7);transform:scale(.7);-webkit-transition:all .5s ease-out;transition:all .5s ease-out}.sr-alertgp .alert .actionbar{position:relative;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:20px;width:80%;height:50px;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) 0ms;transition:all 1s cubic-bezier(.23,1,.32,1) 0ms}.sr-alertgp .alert .actionbar,.sr-alertgp .alert.notification{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center}.sr-alertgp .alert.notification{width:500px;min-height:350px}.sr-alertgp .alert.notification[data-state=siren]{background-image:-webkit-radial-gradient(circle farthest-corner at 10% 20%,#cd212a 0,#ec5f05 90%);background-image:radial-gradient(circle farthest-corner at 10% 20%,#cd212a 0,#ec5f05 90%)}.sr-alertgp .alert.notification[data-state=lock]{background-image:-webkit-radial-gradient(circle farthest-corner at 10% 20%,#8451a1 0,rgba(132,81,161,.83) 90%);background-image:radial-gradient(circle farthest-corner at 10% 20%,#8451a1 0,rgba(132,81,161,.83) 90%)}.sr-alertgp .alert.notification[data-state=warning]{background-image:-webkit-linear-gradient(left,#f2709c,#ff9472);background-image:linear-gradient(90deg,#f2709c,#ff9472)}.sr-alertgp .alert.notification[data-state=bug]{background-image:-webkit-linear-gradient(bottom,#ad5389,#3c1053);background-image:linear-gradient(0deg,#ad5389,#3c1053)}.sr-alertgp .alert.notification[data-state=safe],.sr-alertgp .alert.notification[data-state=server]{background-color:#8ec5fc;background-image:-webkit-linear-gradient(28deg,#8ec5fc,#e0c3fc);background-image:linear-gradient(62deg,#8ec5fc,#e0c3fc)}.sr-alertgp .alert.notification .sr-alert-icon{position:relative;width:100%}.sr-alertgp .alert.notification .loading .progress{padding:5px;background-color:#fff;border-radius:50%}.sr-alertgp .alert.notification .loading .progress .circle-bg{stroke:transparent}.sr-alertgp .alert.notification .loading .progress .circle{stroke-width:1}.sr-alertgp .alert.notification[data-state=siren] .loading .progress .circle{stroke:#cd212a}.sr-alertgp .alert.notification[data-state=lock] .loading .progress .circle{stroke:#8451a1}.sr-alertgp .alert.notification[data-state=warning] .loading .progress .circle{stroke:#f2709c}.sr-alertgp .alert.notification[data-state=bug] .loading .progress .circle{stroke:#ad5389}.sr-alertgp .alert.notification[data-state=safe] .loading .progress .circle,.sr-alertgp .alert.notification[data-state=server] .loading .progress .circle{stroke:#8ec5fc}.sr-alertgp .alert.notification .content{padding:10px 70px;width:100%;color:#fff;text-align:center;font-size:28.8px;font-size:1.8rem;box-sizing:border-box}.sr-alertgp .alert.notification .flag{position:absolute;left:0;top:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;z-index:100000}.sr-alertgp .alert.notification .flag img{width:50px}.sr-alertgp .alert.notification[data-state=lock] .flag img{width:30px}.sr-alertgp .alert.notification .flag img.swing{-webkit-transform-origin:center;transform-origin:center;-webkit-animation-name:swing;animation-name:swing;-webkit-animation-duration:1s;animation-duration:1s}.sr-alertgp .alert.notification .actionbar{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-pack:distribute;justify-content:space-around;margin:0}.sr-alertgp .alert.notification .return{padding:5px 32px;color:#333;background-color:#fff;font-size:15px;font-weight:700;border-radius:56px}.sr-alertgp .alert.notification .return:hover{box-shadow:0 3px 1px -2px rgba(0,0,0,.2),0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12)}@-webkit-keyframes swing{20%{-webkit-transform:rotate(15deg);transform:rotate(15deg)}40%{-webkit-transform:rotate(-10deg);transform:rotate(-10deg)}60%{-webkit-transform:rotate(5deg);transform:rotate(5deg)}80%{-webkit-transform:rotate(-5deg);transform:rotate(-5deg)}to{-webkit-transform:rotate(0deg);transform:rotate(0deg)}}@keyframes swing{20%{-webkit-transform:rotate(15deg);transform:rotate(15deg)}40%{-webkit-transform:rotate(-10deg);transform:rotate(-10deg)}60%{-webkit-transform:rotate(5deg);transform:rotate(5deg)}80%{-webkit-transform:rotate(-5deg);transform:rotate(-5deg)}to{-webkit-transform:rotate(0deg);transform:rotate(0deg)}}</style><style type="text/css">@-webkit-keyframes fadeInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@keyframes fadeInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@-webkit-keyframes fadeOutDown{0%{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}@keyframes fadeOutDown{0%{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}sr-promo-bg{position:fixed;right:15px;bottom:15px;z-index:2147483646}sr-promo,sr-promo-notice{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:10px;color:rgba(51,51,51,.87);background-color:#fff;border-radius:4px;box-shadow:0 12px 18px -6px rgba(0,0,0,.3);overflow:hidden;-webkit-transform-origin:bottom;transform-origin:bottom;-webkit-transition:all .6s ease;transition:all .6s ease;-webkit-animation-name:fadeInUp;animation-name:fadeInUp;-webkit-animation-duration:.5s;animation-duration:.5s;-webkit-animation-fill-mode:both;animation-fill-mode:both}sr-promo img{width:220px;cursor:pointer}sr-promo-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;padding:10px 0 0;width:100%}sr-promo-a{padding:5px 10px;color:#fff;font-size:12px;font-weight:700;border-radius:2px;box-shadow:0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12),0 3px 1px -2px rgba(0,0,0,.2);cursor:pointer}sr-promo-a.later{background-color:#2196f3}sr-promo-a.cancel{background-color:#757575}sr-promo-tip{font-size:12px;padding:5px 10px;border-radius:2px}sr-promo-notice{position:absolute;top:10px;left:10px;right:10px;height:293px;padding-bottom:0;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;font-size:12px;border-radius:0;box-shadow:none;overflow-y:hidden}sr-promo-notice:hover{overflow-y:overlay}sr-promo-notice-title{font-weight:700;text-align:center;margin-bottom:5px;width:100%;font-size:14px}sr-promo-notice-content{margin-top:5px}</style><style type="text/css">dialog-gp .carousel,welcome .carousel{position:relative;width:100%;height:400px;-webkit-perspective:500px;perspective:500px;-webkit-transform-style:preserve-3d;transform-style:preserve-3d;-webkit-transform-origin:0 50%;transform-origin:0 50%;overflow:hidden}dialog-gp .carousel.carousel-slider,welcome .carousel.carousel-slider{top:0;left:0;height:100%}dialog-gp .carousel.carousel-slider .carousel-item,welcome .carousel.carousel-slider .carousel-item{position:absolute;top:0;left:0;width:100%;height:100%;min-height:400px}dialog-gp .carousel .carousel-item,welcome .carousel .carousel-item{display:none;position:absolute;top:0;left:0;width:200px;height:200px}dialog-gp .carousel .carousel-item>img,welcome .carousel .carousel-item>img{width:100%}dialog-gp .carousel .indicators,welcome .carousel .indicators{position:absolute;margin:0;padding:0;left:0;right:0;bottom:0;text-align:center}dialog-gp .carousel .indicators .indicator-item,welcome .carousel .indicators .indicator-item{display:inline-block;position:relative;margin:14px 4px;height:10px;width:10px;background-color:#e0e0e0;-webkit-transition:background-color .3s;transition:background-color .3s;border-radius:50%;cursor:pointer}dialog-gp .carousel .indicators .indicator-item.active,welcome .carousel .indicators .indicator-item.active{background-color:#4caf50}dialog-gp .carousel .carousel-item:not(.active) .materialboxed,dialog-gp .carousel.scrolling .carousel-item .materialboxed,welcome .carousel .carousel-item:not(.active) .materialboxed,welcome .carousel.scrolling .carousel-item .materialboxed{pointer-events:none}</style><style type="text/css">.simpread-upgrade-root *{box-sizing:border-box}.simpread-upgrade-root{-webkit-transition:all .25s ease-out;transition:all .25s ease-out}.simpread-upgrade-root.open{background-color:rgba(51,51,51,.8)}.simpread-upgrade-root dialog-gp{position:relative}.simpread-upgrade-root dialog-gp .close{position:fixed;top:0;right:0;z-index:2}.simpread-upgrade-root dialog-gp .close:hover{-webkit-transform:rotate(270deg);transform:rotate(270deg);-webkit-transition:all .25s ease-out;transition:all .25s ease-out}.simpread-upgrade-root dialog-content{padding-bottom:80px!important;overflow-y:hidden}.simpread-upgrade-root dialog-content:hover{overflow-y:overlay}.simpread-upgrade-root dialog-content::-webkit-scrollbar-track{background-color:transparent}.simpread-upgrade-root dialog-content::-webkit-scrollbar{width:12px}.simpread-upgrade-root dialog-content::-webkit-scrollbar-thumb{background-clip:padding-box;padding-top:80px;background-color:#ddd;border:3px solid transparent;border-radius:8px}.simpread-upgrade-root .floating{position:absolute;left:0;right:0;bottom:0;height:80px;overflow-y:hidden}.simpread-upgrade-root .floating,.simpread-upgrade-root .floating .billing{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.simpread-upgrade-root .floating .billing{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:1px 40px;color:#fff;background-color:#4dbb7c;font-size:15px;font-weight:400;opacity:0;border-radius:30px;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.simpread-upgrade-root .floating .billing.open{-webkit-animation-name:fadeInUp;animation-name:fadeInUp;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-fill-mode:both;animation-fill-mode:both}.simpread-upgrade-root .floating .billing .sales{font-size:12px}.simpread-upgrade-root .floating .billing .rate{margin:0 5px}.simpread-upgrade-root .floating .billing .price{margin-left:2px;margin-right:5px}.upgrade{position:relative;color:rgba(51,51,51,.87);font-family:Hiragino Sans GB,Microsoft Yahei;text-shadow:none}.upgrade .head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.upgrade .head img{margin-bottom:5px;width:60px;border-radius:9px;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.upgrade .head .title{margin:10px 0;font-weight:700;font-size:15px}.upgrade .head .desc{width:70%;text-align:center;font-size:13px}.upgrade .features{position:relative;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-pack:distribute;justify-content:space-around;-webkit-transition:height .2s cubic-bezier(.23,1,.32,1) 0ms;transition:height .2s cubic-bezier(.23,1,.32,1) 0ms}.upgrade .features.init{height:100px}.upgrade .features.init .base,.upgrade .features.init .pro{opacity:0}.upgrade .loading{position:absolute;left:0;top:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;background-color:#fff;z-index:1}.upgrade .loading span{width:50px;height:50px;opacity:.87}.features.error{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:inherit;font-size:14px}.features.error img{margin:10px 0;width:300px}.upgrade .base,.upgrade .pro{margin:20px 20px 13px;width:100%;text-align:center;opacity:1;-webkit-transition:opacity .2s cubic-bezier(.23,1,.32,1) 0ms;transition:opacity .2s cubic-bezier(.23,1,.32,1) 0ms}.upgrade .pricecard{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;border-radius:4px;border:1px solid #eef1f4;position:relative}.upgrade .pro .pricecard{border:2px solid #4dbb7c;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.upgrade .pricecard .mode{margin:20px 10px 10px;font-size:18px;font-weight:700}.upgrade .pricecard .sales{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;height:92px;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center}.upgrade .pricecard .discountrate{position:absolute;top:-12px;left:0;right:0}.upgrade .pricecard .discountrate .rate{padding:3px 10px;color:#fff;background-color:#4dbb7c;font-weight:700;font-size:13px;border-radius:10px}.upgrade .pricecard .desc{font-size:30px}.upgrade .pricecard .desc del{font-size:15px;font-weight:700;text-decoration:line-through}.upgrade .pricecard .price{position:relative;color:#4dbb7c;font-size:30px;font-weight:700}.upgrade .pricecard .message{position:relative;font-size:11px;font-weight:400;background-image:-webkit-linear-gradient(top,hsla(0,0%,100%,0) 50%,#ffeb3b 0);background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,#ffeb3b 0)}.upgrade .pricecard .countdown{margin-top:5px}.upgrade .pricecard .billing{position:relative;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:30px 20%;padding:5px;color:#333;background-color:#e2e2e2;font-size:15px;font-style:normal;font-weight:700;border-radius:4px;cursor:pointer}.upgrade .pricecard .billing,.upgrade .pricecard .billing i{-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms}.upgrade .pricecard .billing i{height:27px;line-height:22px;margin-left:5px}.upgrade .pricecard .billing:hover{box-shadow:0 3px 1px -2px rgba(0,0,0,.2),0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12)}.upgrade .pricecard .billing:hover i{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.upgrade .pricecard .billing:hover .dropdown-price{opacity:1;-webkit-transform:scale(1);transform:scale(1)}.upgrade .pricecard .billing .dropdown-price{position:absolute;left:-41px;top:38px;-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:270px;color:rgba(51,51,51,.87);font-size:12px;text-shadow:none;box-sizing:border-box;border-radius:4px;box-shadow:0 8px 10px 1px rgba(0,0,0,.14),0 3px 14px 2px rgba(0,0,0,.12),0 5px 5px -3px rgba(0,0,0,.2);opacity:0;-webkit-transform:scaleY(0);transform:scaleY(0);-webkit-transform-origin:left top 0;transform-origin:left top 0;-webkit-transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms,-webkit-transform .45s cubic-bezier(.23,1,.32,1) 0ms;transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms,-webkit-transform .45s cubic-bezier(.23,1,.32,1) 0ms;transition:transform .45s cubic-bezier(.23,1,.32,1) 0ms,opacity 1s cubic-bezier(.23,1,.32,1) 0ms;transition:transform .45s cubic-bezier(.23,1,.32,1) 0ms,opacity 1s cubic-bezier(.23,1,.32,1) 0ms,-webkit-transform .45s cubic-bezier(.23,1,.32,1) 0ms;z-index:1}.upgrade .pricecard .billing .dropdown-price,.upgrade .pricecard .billing .dropdown-price .store{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#fff}.upgrade .pricecard .billing .dropdown-price .store{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;padding:8px 24px 8px 16px;width:100%;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) 0ms;transition:all 1s cubic-bezier(.23,1,.32,1) 0ms;cursor:pointer}.upgrade .pricecard .billing .dropdown-price .store:hover{background-color:#eee}.upgrade .pricecard .billing .dropdown-price .store:hover i{-webkit-transform:rotate(270deg) translateY(7px);transform:rotate(270deg) translateY(7px)}.upgrade .pricecard .billing .dropdown-price .store .names{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;width:135px}.upgrade .pricecard .billing .dropdown-price .store .des{width:100%;color:rgba(51,51,51,.56);text-align:left;font-size:10px;-webkit-transform:scale(.8) translateX(-17px);transform:scale(.8) translateX(-17px)}.upgrade .pricecard .billing .dropdown-price .store .num{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;width:46px}.upgrade .pricecard .billing .dropdown-price .store .tips{-webkit-transform:scale(.8);transform:scale(.8);color:#4dbb7c}.upgrade .pricecard .billing .dropdown-price .store i{-webkit-transform:rotate(270deg);transform:rotate(270deg)}.upgrade .base[data-enable=false] .pricecard,.upgrade .pro[data-enable=true] .pricecard{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;height:210px}.upgrade .base[data-enable=false] .pricecard .mode,.upgrade .pro[data-enable=true] .pricecard .mode{font-size:25px}.upgrade .base[data-enable=false] .pricecard .billing,.upgrade .base[data-enable=false] .pricecard .sales,.upgrade .pro[data-enable=true] .pricecard .billing .dropdown-price,.upgrade .pro[data-enable=true] .pricecard .billing i,.upgrade .pro[data-enable=true] .pricecard .discountrate,.upgrade .pro[data-enable=true] .pricecard .sales{display:none}.upgrade .pro[data-enable=true] .pricecard .billing{position:absolute;top:-28px;left:0;right:0;display:inherit;margin:10px 20%;border-radius:30px}.upgrade .pro .billing{color:#fff;background-color:#4dbb7c}.upgrade .features.diff{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;margin-top:20px}.upgrade .features.diff,.upgrade .features .feature{-webkit-box-direction:normal;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.upgrade .features .feature{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:unset;-ms-flex-pack:unset;justify-content:unset;margin:14px 20px 0;font-size:15px}.upgrade .features .feature.empty{height:27px}.upgrade .features .icon{margin-right:10px;width:15px}.upgrade .features .label{width:120px;font-size:15px;text-align:left}.upgrade .features a{color:inherit;cursor:auto}.upgrade .features a.active{padding-bottom:5px;border-bottom:1px dotted;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;cursor:pointer}.upgrade .features a.active:hover{color:#4285f4}.upgrade .features .label .remark{margin-left:5px;padding:2px 5px;background-color:#ffeb3b;font-size:12px;font-weight:400;border-radius:4px}.upgrade .features .label .remark.roadmap{background-color:#e2e2e2}.upgrade .ticket{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;font-size:13px}.upgrade .ticket .message{width:70%;text-align:center}.upgrade .ticket .line{margin:7px 0 0;width:100%;height:1px;background-image:-webkit-linear-gradient(.1deg,rgba(255,18,18,0) -2.8%,#e2e2e2 50.8%,rgba(0,159,8,0) 107.9%);background-image:linear-gradient(89.9deg,rgba(255,18,18,0) -2.8%,#e2e2e2 50.8%,rgba(0,159,8,0) 107.9%)}.upgrade .ticket .notice{margin:20px 20%;padding:5px 20px;color:#333;background-color:#e2e2e2;font-size:15px;font-weight:400;border-radius:4px}.upgrade .ticket .content{margin:0 0 13px;width:80%}.upgrade .ticket .content li{margin-bottom:6px}.upgrade .ticket .content li:last-child{margin-bottom:0}.upgrade .ticket .last{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-pack:distribute;justify-content:space-around;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:20px 0;width:100%}.upgrade .carousels{margin:20px 20px 13px}.upgrade .carousel.carousel-slider{height:420px;border-radius:4px;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.upgrade .carousels setion{position:relative}.upgrade .carousels setion img{margin-top:-82px;width:100%}.upgrade .carousels .descr{position:absolute;left:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:80px;width:100%;padding-bottom:10px;background-color:#fff;font-size:17px}.simpread-upgrade-root.mini dialog-gp{border-radius:15px!important}.simpread-upgrade-root.mini dialog-content{padding:0!important;width:650px!important}.simpread-upgrade-root.mini dialog-gp .close{position:absolute}.simpread-upgrade-root.mini .upgrade .carousels{margin:0}.simpread-upgrade-root.mini .upgrade .carousels .descr{padding-bottom:70px;height:130px}.simpread-upgrade-root.mini .upgrade .carousel.carousel-slider{height:450px}.simpread-upgrade-root.mini .floating .billing{margin-bottom:30px;min-height:40px}.simpread-upgrade-root.mini footer{position:absolute;top:199px;left:-60px;right:-60px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;-webkit-box-align:center;-ms-flex-align:center;align-items:center}</style><style type="text/css">.simpread-tipsalert-root dialog-gp{position:absolute}.simpread-tipsalert-root dialog-gp .close{position:absolute;top:0;right:0;z-index:2}.simpread-tipsalert-root dialog-content{padding:0!important;width:650px!important}.simpread-tipsalert-root .details{position:relative;color:rgba(51,51,51,.87);font-family:Hiragino Sans GB,Microsoft Yahei;text-shadow:none}.simpread-tipsalert-root .details .carousel.carousel-slider{height:450px;border-radius:4px;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.simpread-tipsalert-root .details .carousels setion sr-div-center{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;height:321px;box-sizing:border-box}.simpread-tipsalert-root .details .carousels setion sr-div-center.hidden{display:none}.simpread-tipsalert-root .details .carousels setion sr-div-center img{margin-top:20px!important;height:321px;width:auto!important}.simpread-tipsalert-root .details .carousels setion sr-div-center sr-video{position:absolute;left:0;right:0;width:100%;height:321px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;z-index:2}.simpread-tipsalert-root .details .carousels setion .tipsimg.error{width:0!important;height:0!important}.simpread-tipsalert-root .details .carousels setion .tipsimg:after{content:"\F1C5" " Sorry, the image below is broken :(";font-family:Font Awesome\ 5 Free;font-weight:900;position:absolute;top:0;left:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:650px;height:100%;color:#646464;background-color:#fff;z-index:2}.simpread-tipsalert-root .details .carousels setion sr-div-center sr-video+img{opacity:.5}.simpread-tipsalert-root .details .carousels setion img{margin-top:-82px;width:100%}.simpread-tipsalert-root .details .carousels setion video{height:321px}.simpread-tipsalert-root .details .carousels setion video.active{display:block}.simpread-tipsalert-root .details .carousels .descr{position:absolute;left:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:80px;width:100%;padding-bottom:10px;background-color:#fff;font-size:14px}.simpread-tipsalert-root .details .carousels .descr b{margin:0 3px}.simpread-tipsalert-root .details .carousels .descr.large{font-size:17px}.simpread-tipsalert-root .floating{position:absolute;left:0;right:0;bottom:14px;height:80px;overflow-y:hidden}.simpread-tipsalert-root .floating,.simpread-tipsalert-root .floating .docs{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.simpread-tipsalert-root .floating .docs{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:1px 40px;height:30px;color:#fff;background-color:#4dbb7c;font-size:15px;font-weight:400;border-radius:30px;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.simpread-tipsalert-root footer{position:absolute;top:199px;left:-60px;right:-60px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;-webkit-box-align:center;-ms-flex-align:center;align-items:center}</style><style type="text/css">sr-annote-popup{display:block;width:480px}sr-annote-popup-gp{position:relative;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin-bottom:25px}sr-annote-popup-label{color:rgba(0,137,123,.8);font-size:14px;font-weight:700;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;pointer-events:none;-webkit-transition:all .45s cubic-bezier(.23,1,.32,1) 0ms;transition:all .45s cubic-bezier(.23,1,.32,1) 0ms;-webkit-transform:scale(.75) translateY(-8px);transform:scale(.75) translateY(-8px);-webkit-transform-origin:left top 0;transform-origin:left top 0}sr-annote-popup-desc{position:relative;padding:10px 30px;font-size:15px;text-align:justify;color:#555;line-height:1.6;border-bottom:1px solid #e0e0e0}sr-annote-popup-desc:before{content:"\201C";position:absolute;left:-5px;top:-24px;color:rgba(0,137,123,.8);font-family:Arial;font-size:4em}sr-annote-popup-desc sr-annote{background-color:transparent!important}sr-annote-popup-gp[type=img]{-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-annote-popup-gp ol,sr-annote-popup-gp ul{margin:0 0 1.2em;margin-left:1.3em;padding:0;list-style:disc}sr-annote-popup-gp ol li,sr-annote-popup-gp ul li{margin:0 0 1.2em;font-size:15px;list-style:disc}sr-annote-popup-gp a{padding:0 5px;vertical-align:baseline;vertical-align:initial}sr-annote-popup-gp a,sr-annote-popup-gp a:link{color:#463f5c;font-size:inherit;font-weight:inherit;text-decoration:underline;border:none}sr-annote-popup-gp a:hover{background:transparent}sr-annote-popup-gp img{margin:0;padding:0;max-width:50%;height:auto;background:#fff;border:0;border-radius:6px;box-shadow:0 20px 20px -10px rgba(0,0,0,.1)}sr-annote-popup-gp pre{padding:10px!important;background-color:transparent!important;border-radius:6px!important;box-shadow:0 20px 20px -10px rgba(0,0,0,.1);overflow-x:auto}sr-annote-popup auto-complete list-view{max-height:150px!important}sr-annote-popup list-view::-webkit-scrollbar-thumb{background-clip:padding-box;border-radius:10px;border:2px solid transparent;background-color:rgba(85,85,85,.55)}sr-annote-popup list-view::-webkit-scrollbar{width:10px;-webkit-transition:width .7s cubic-bezier(.4,0,.2,1);transition:width .7s cubic-bezier(.4,0,.2,1)}sr-annote-popup-gp sr-annote-floatingbar{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:left;-ms-flex-pack:left;justify-content:left}sr-annote-popup-gp sr-annote-floatingbar sr-anote-fb-item[data-color-type]{display:-webkit-box;display:-ms-flexbox;display:flex;width:30px;height:30px;border-radius:50%}sr-annote-popup-gp sr-annote-floatingbar sr-anote-fb-item[data-color-type],sr-annote-popup-gp sr-anote-item{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-right:20px}sr-annote-popup-gp sr-anote-item{display:-webkit-box!important;display:-ms-flexbox!important;display:flex!important;padding:0 5px;font-size:13px;font-weight:400;-webkit-transition:all .5s ease-in-out 0ms;transition:all .5s ease-in-out 0ms}sr-annote-popup-gp sr-anote-lock{position:absolute;left:0;top:-3px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;background-color:hsla(0,0%,100%,.8);z-index:1}sr-annote-popup-gp sr-anote-lock svg{margin-top:3px;cursor:pointer}</style><style type="text/css">.gu-mirror{position:fixed!important;margin:0!important;z-index:9999!important;opacity:.8;-ms-filter:"progid:DXImageTransform.Microsoft.Alpha(Opacity=80)";filter:alpha(opacity=80)}.gu-hide{display:none!important}.gu-unselectable{-webkit-user-select:none!important;-moz-user-select:none!important;-ms-user-select:none!important;user-select:none!important}.gu-transit{opacity:.2;-ms-filter:"progid:DXImageTransform.Microsoft.Alpha(Opacity=20)";filter:alpha(opacity=20)}</style><style type="text/css">sr-search{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;padding:10px;border:1px solid #dfe1e5;border-radius:8px}sr-search,sr-search sr-search-header{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal;margin-bottom:10px}sr-search sr-search-header{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-search sr-search-header img{margin-right:10px;width:22px}sr-search sr-search-header sr-search-span{font-weight:700}sr-search-unreader-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;border-bottom:1px solid #dfe1e5;margin-bottom:5px}sr-search-content{max-height:666px;overflow-x:hidden;overflow-y:auto}sr-search-unreader-title{font-weight:700;font-size:15px;margin-bottom:5px}sr-search-unreader-create{margin-bottom:5px;color:#70757a}sr-search-unreader-desc{margin-bottom:5px}sr-search-unreader-tags{margin-bottom:5px;color:#70757a;font-size:11px;font-style:italic}sr-search-unreader-tag{margin-right:5px}sr-search-paging{width:100%;margin:10px}sr-search-paging,sr-search-paging sr-search-more{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-search-paging sr-search-more{width:36px;height:36px;box-shadow:0 0 0 1px rgba(0,0,0,.04),0 4px 8px 0 rgba(0,0,0,.2);border-radius:50%;opacity:.9;cursor:pointer}sr-search-paging sr-search-more:hover{opacity:1}sr-search-paging sr-search-more.disable{cursor:no-drop}sr-search-paging sr-search-more svg{width:24px;height:24px;fill:#757575}sr-search-info{text-align:center}</style><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/index-8d5ab195e774e3fc0705.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/login-9489325c3dba66c15b36.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/about-21ad854b65cdc28a5879.js"><link as="script" rel="prefetch" href="./ICLR 2021 Conference _ OpenReview_files/group-11f7765f7db1e915182b.js.下载"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/venues-41b2f7835ab776406336.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/contact-c0f4dc3837847c913ba0.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/faq-6d1c8a0d3e1b4eea5c70.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/legal/terms-1acd9435966a971c48c8.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/legal/privacy-a259b7f57265e0f174e8.js"><link as="fetch" rel="prefetch" href="https://openreview.net/_next/data/v1.0.9-1-gf539684/faq.json"><style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style id="MJX-CHTML-styles">
mjx-container[jax="CHTML"] {
  line-height: 0;
}

mjx-container [space="1"] {
  margin-left: .111em;
}

mjx-container [space="2"] {
  margin-left: .167em;
}

mjx-container [space="3"] {
  margin-left: .222em;
}

mjx-container [space="4"] {
  margin-left: .278em;
}

mjx-container [space="5"] {
  margin-left: .333em;
}

mjx-container [rspace="1"] {
  margin-right: .111em;
}

mjx-container [rspace="2"] {
  margin-right: .167em;
}

mjx-container [rspace="3"] {
  margin-right: .222em;
}

mjx-container [rspace="4"] {
  margin-right: .278em;
}

mjx-container [rspace="5"] {
  margin-right: .333em;
}

mjx-container [size="s"] {
  font-size: 70.7%;
}

mjx-container [size="ss"] {
  font-size: 50%;
}

mjx-container [size="Tn"] {
  font-size: 60%;
}

mjx-container [size="sm"] {
  font-size: 85%;
}

mjx-container [size="lg"] {
  font-size: 120%;
}

mjx-container [size="Lg"] {
  font-size: 144%;
}

mjx-container [size="LG"] {
  font-size: 173%;
}

mjx-container [size="hg"] {
  font-size: 207%;
}

mjx-container [size="HG"] {
  font-size: 249%;
}

mjx-container [width="full"] {
  width: 100%;
}

mjx-box {
  display: inline-block;
}

mjx-block {
  display: block;
}

mjx-itable {
  display: inline-table;
}

mjx-row {
  display: table-row;
}

mjx-row > * {
  display: table-cell;
}

mjx-mtext {
  display: inline-block;
  text-align: left;
}

mjx-mstyle {
  display: inline-block;
}

mjx-merror {
  display: inline-block;
  color: red;
  background-color: yellow;
}

mjx-mphantom {
  visibility: hidden;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-math {
  display: inline-block;
  text-align: left;
  line-height: 0;
  text-indent: 0;
  font-style: normal;
  font-weight: normal;
  font-size: 100%;
  font-size-adjust: none;
  letter-spacing: normal;
  word-wrap: normal;
  word-spacing: normal;
  white-space: nowrap;
  direction: ltr;
  padding: 1px 0;
}

mjx-container[jax="CHTML"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="CHTML"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="CHTML"][display="true"] mjx-math {
  padding: 0;
}

mjx-container[jax="CHTML"][justify="left"] {
  text-align: left;
}

mjx-container[jax="CHTML"][justify="right"] {
  text-align: right;
}

mjx-mi {
  display: inline-block;
  text-align: left;
}

mjx-mo {
  display: inline-block;
  text-align: left;
}

mjx-stretchy-h {
  display: inline-table;
  width: 100%;
}

mjx-stretchy-h > * {
  display: table-cell;
  width: 0;
}

mjx-stretchy-h > * > mjx-c {
  display: inline-block;
  transform: scalex(1.0000001);
}

mjx-stretchy-h > * > mjx-c::before {
  display: inline-block;
  padding: .001em 0;
  width: initial;
}

mjx-stretchy-h > mjx-ext {
  overflow: hidden;
  width: 100%;
}

mjx-stretchy-h > mjx-ext > mjx-c::before {
  transform: scalex(500);
}

mjx-stretchy-h > mjx-ext > mjx-c {
  width: 0;
}

mjx-stretchy-h > mjx-beg > mjx-c {
  margin-right: -.1em;
}

mjx-stretchy-h > mjx-end > mjx-c {
  margin-left: -.1em;
}

mjx-stretchy-v {
  display: inline-block;
}

mjx-stretchy-v > * {
  display: block;
}

mjx-stretchy-v > mjx-beg {
  height: 0;
}

mjx-stretchy-v > mjx-end > mjx-c {
  display: block;
}

mjx-stretchy-v > * > mjx-c {
  transform: scaley(1.0000001);
  transform-origin: left center;
  overflow: hidden;
}

mjx-stretchy-v > mjx-ext {
  display: block;
  height: 100%;
  box-sizing: border-box;
  border: 0px solid transparent;
  overflow: hidden;
}

mjx-stretchy-v > mjx-ext > mjx-c::before {
  width: initial;
}

mjx-stretchy-v > mjx-ext > mjx-c {
  transform: scaleY(500) translateY(.1em);
  overflow: visible;
}

mjx-mark {
  display: inline-block;
  height: 0px;
}

mjx-mn {
  display: inline-block;
  text-align: left;
}

mjx-msub {
  display: inline-block;
  text-align: left;
}

mjx-msup {
  display: inline-block;
  text-align: left;
}

mjx-munder {
  display: inline-block;
  text-align: left;
}

mjx-over {
  text-align: left;
}

mjx-munder:not([limits="false"]) {
  display: inline-table;
}

mjx-munder > mjx-row {
  text-align: left;
}

mjx-under {
  padding-bottom: .1em;
}

mjx-mover {
  display: inline-block;
  text-align: left;
}

mjx-mover:not([limits="false"]) {
  padding-top: .1em;
}

mjx-mover:not([limits="false"]) > * {
  display: block;
  text-align: left;
}

mjx-TeXAtom {
  display: inline-block;
  text-align: left;
}

mjx-c {
  display: inline-block;
}

mjx-utext {
  display: inline-block;
  padding: .75em 0 .2em 0;
}

mjx-c::before {
  display: block;
  width: 0;
}

.MJX-TEX {
  font-family: MJXZERO, MJXTEX;
}

.TEX-B {
  font-family: MJXZERO, MJXTEX-B;
}

.TEX-I {
  font-family: MJXZERO, MJXTEX-I;
}

.TEX-MI {
  font-family: MJXZERO, MJXTEX-MI;
}

.TEX-BI {
  font-family: MJXZERO, MJXTEX-BI;
}

.TEX-S1 {
  font-family: MJXZERO, MJXTEX-S1;
}

.TEX-S2 {
  font-family: MJXZERO, MJXTEX-S2;
}

.TEX-S3 {
  font-family: MJXZERO, MJXTEX-S3;
}

.TEX-S4 {
  font-family: MJXZERO, MJXTEX-S4;
}

.TEX-A {
  font-family: MJXZERO, MJXTEX-A;
}

.TEX-C {
  font-family: MJXZERO, MJXTEX-C;
}

.TEX-CB {
  font-family: MJXZERO, MJXTEX-CB;
}

.TEX-FR {
  font-family: MJXZERO, MJXTEX-FR;
}

.TEX-FRB {
  font-family: MJXZERO, MJXTEX-FRB;
}

.TEX-SS {
  font-family: MJXZERO, MJXTEX-SS;
}

.TEX-SSB {
  font-family: MJXZERO, MJXTEX-SSB;
}

.TEX-SSI {
  font-family: MJXZERO, MJXTEX-SSI;
}

.TEX-SC {
  font-family: MJXZERO, MJXTEX-SC;
}

.TEX-T {
  font-family: MJXZERO, MJXTEX-T;
}

.TEX-V {
  font-family: MJXZERO, MJXTEX-V;
}

.TEX-VB {
  font-family: MJXZERO, MJXTEX-VB;
}

mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c {
  font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important;
}

@font-face /* 0 */ {
  font-family: MJXZERO;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff");
}

@font-face /* 1 */ {
  font-family: MJXTEX;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff");
}

@font-face /* 2 */ {
  font-family: MJXTEX-B;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff");
}

@font-face /* 3 */ {
  font-family: MJXTEX-I;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff");
}

@font-face /* 4 */ {
  font-family: MJXTEX-MI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff");
}

@font-face /* 5 */ {
  font-family: MJXTEX-BI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff");
}

@font-face /* 6 */ {
  font-family: MJXTEX-S1;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff");
}

@font-face /* 7 */ {
  font-family: MJXTEX-S2;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff");
}

@font-face /* 8 */ {
  font-family: MJXTEX-S3;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff");
}

@font-face /* 9 */ {
  font-family: MJXTEX-S4;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff");
}

@font-face /* 10 */ {
  font-family: MJXTEX-A;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff");
}

@font-face /* 11 */ {
  font-family: MJXTEX-C;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff");
}

@font-face /* 12 */ {
  font-family: MJXTEX-CB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff");
}

@font-face /* 13 */ {
  font-family: MJXTEX-FR;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff");
}

@font-face /* 14 */ {
  font-family: MJXTEX-FRB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff");
}

@font-face /* 15 */ {
  font-family: MJXTEX-SS;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff");
}

@font-face /* 16 */ {
  font-family: MJXTEX-SSB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff");
}

@font-face /* 17 */ {
  font-family: MJXTEX-SSI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff");
}

@font-face /* 18 */ {
  font-family: MJXTEX-SC;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff");
}

@font-face /* 19 */ {
  font-family: MJXTEX-T;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff");
}

@font-face /* 20 */ {
  font-family: MJXTEX-V;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff");
}

@font-face /* 21 */ {
  font-family: MJXTEX-VB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff");
}

mjx-c.mjx-c25::before {
  padding: 0.75em 0.833em 0.056em 0;
  content: "%";
}

mjx-c.mjx-c28::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: "(";
}

mjx-c.mjx-c29::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: ")";
}

mjx-c.mjx-c2E::before {
  padding: 0.12em 0.278em 0 0;
  content: ".";
}

mjx-c.mjx-c2F::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "/";
}

mjx-c.mjx-c30::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "0";
}

mjx-c.mjx-c31::before {
  padding: 0.666em 0.5em 0 0;
  content: "1";
}

mjx-c.mjx-c32::before {
  padding: 0.666em 0.5em 0 0;
  content: "2";
}

mjx-c.mjx-c34::before {
  padding: 0.677em 0.5em 0 0;
  content: "4";
}

mjx-c.mjx-c35::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "5";
}

mjx-c.mjx-c36::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "6";
}

mjx-c.mjx-c37::before {
  padding: 0.676em 0.5em 0.022em 0;
  content: "7";
}

mjx-c.mjx-c39::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "9";
}

mjx-c.mjx-c3C::before {
  padding: 0.54em 0.778em 0.04em 0;
  content: "<";
}

mjx-c.mjx-c61::before {
  padding: 0.448em 0.5em 0.011em 0;
  content: "a";
}

mjx-c.mjx-c64::before {
  padding: 0.694em 0.556em 0.011em 0;
  content: "d";
}

mjx-c.mjx-c74::before {
  padding: 0.615em 0.389em 0.01em 0;
  content: "t";
}

mjx-c.mjx-c7B::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "{";
}

mjx-c.mjx-c7D::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "}";
}

mjx-c.mjx-cB1::before {
  padding: 0.666em 0.778em 0 0;
  content: "\B1";
}

mjx-c.mjx-cD7::before {
  padding: 0.491em 0.778em 0 0;
  content: "\D7";
}

mjx-c.mjx-c3A9::before {
  padding: 0.704em 0.722em 0 0;
  content: "\3A9";
}

mjx-c.mjx-c2113::before {
  padding: 0.705em 0.417em 0.02em 0;
  content: "\2113";
}

mjx-c.mjx-c211D.TEX-A::before {
  padding: 0.683em 0.722em 0 0;
  content: "R";
}

mjx-c.mjx-c2203::before {
  padding: 0.694em 0.556em 0 0;
  content: "\2203";
}

mjx-c.mjx-c2223::before {
  padding: 0.75em 0.278em 0.249em 0;
  content: "\2223";
}

mjx-c.mjx-c2227::before {
  padding: 0.598em 0.667em 0.022em 0;
  content: "\2227";
}

mjx-c.mjx-c2228::before {
  padding: 0.598em 0.667em 0.022em 0;
  content: "\2228";
}

mjx-c.mjx-c1D431.TEX-B::before {
  padding: 0.444em 0.607em 0 0;
  content: "x";
}

mjx-c.mjx-c1D440.TEX-I::before {
  padding: 0.683em 1.051em 0 0;
  content: "M";
}

[noIC] mjx-c.mjx-c1D440.TEX-I:last-child::before {
  padding-right: 0.97em;
}

mjx-c.mjx-c1D441.TEX-I::before {
  padding: 0.683em 0.888em 0 0;
  content: "N";
}

[noIC] mjx-c.mjx-c1D441.TEX-I:last-child::before {
  padding-right: 0.803em;
}

mjx-c.mjx-c1D442.TEX-I::before {
  padding: 0.704em 0.763em 0.022em 0;
  content: "O";
}

mjx-c.mjx-c1D451.TEX-I::before {
  padding: 0.694em 0.52em 0.01em 0;
  content: "d";
}

mjx-c.mjx-c1D45D.TEX-I::before {
  padding: 0.442em 0.503em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c1D466.TEX-I::before {
  padding: 0.442em 0.49em 0.205em 0;
  content: "y";
}

mjx-c.mjx-c1D700.TEX-I::before {
  padding: 0.452em 0.466em 0.022em 0;
  content: "\3B5";
}

mjx-c.mjx-c1D703.TEX-I::before {
  padding: 0.705em 0.469em 0.01em 0;
  content: "\3B8";
}
</style></head><body><div id="__next"><nav class="navbar navbar-inverse navbar-fixed-top" role="navigation"><div class="container"><div class="navbar-header"><button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand home push-link" href="https://openreview.net/"><strong>OpenReview</strong>.net</a></div><div id="navbar" class="navbar-collapse collapse"><form class="navbar-form navbar-left profile-search" role="search"><div class="form-group has-feedback"><input type="text" name="term" class="form-control" value="" placeholder="Search OpenReview..." autocomplete="off"><span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span></div><input type="hidden" name="group" value="all"><input type="hidden" name="content" value="all"><input type="hidden" name="source" value="all"><input type="hidden" name="sort" value="cdate:desc"></form><ul class="nav navbar-nav navbar-right"><li id="user-menu"><a href="https://openreview.net/login">Login</a></li></ul></div></div></nav><div id="or-banner" class="banner" style=""><div class="container"><div class="row"><div class="col-xs-12"><a title="Venue Homepage" href="https://openreview.net/group?id=ICLR.cc/2021"><img class="icon" src="./ICLR 2021 Conference _ OpenReview_files/arrow_left.svg" alt="back arrow">Go to <strong>ICLR 2021</strong> homepage</a></div></div></div></div><div id="flash-message-container" class="alert alert-danger fixed-overlay" role="alert" style="display:none"><div class="container"><div class="row"><div class="col-xs-12"><div class="alert-content"><button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button></div></div></div></div></div><div class="container"><div class="row"><div class="col-xs-12"><main id="content" class="group  "><div id="group-container"><div id="header" class="venue-header" style="display: block;"><h1>International Conference on Learning Representations</h1>
<h3>ICLR 2021</h3>

  <h4>
      <span class="venue-location">
        <span class="glyphicon glyphicon-globe" aria-hidden="true"></span> Vienna, Austria
      </span>
      <span class="venue-date">
        <span class="glyphicon glyphicon-calendar" aria-hidden="true"></span> May 04 2021
      </span>
      <span class="venue-website">
        <span class="glyphicon glyphicon-new-window" aria-hidden="true"></span> <a href="https://iclr.cc/" title="International Conference on Learning Representations Homepage" target="_blank">https://iclr.cc/</a>
      </span>
      <span class="venue-contact">
        <span class="glyphicon glyphicon-envelope" aria-hidden="true"></span> <a href="mailto:iclr2021programchairs@googlegroups.com" target="_blank">iclr2021programchairs@googlegroups.com</a>
      </span>
  </h4>

<div class="description">
    <p class="dark">Please see the venue website for more information.<br> </p><p class="dark"><strong>Abstract Submission End:</strong> Sep 28 2020 03:00PM UTC-0</p><p class="dark"><strong>Paper Submission End:</strong> Oct 2 2020 03:00PM UTC-0</p>
  <p></p>
</div>
</div><div id="invitation"></div><div id="notes">
<div class="tabs-container " style=""><ul class="nav nav-tabs" role="tablist">
    <li role="presentation" style="display: none;">
      <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#your-consoles" aria-controls="your-consoles" role="tab" data-toggle="tab" data-tab-index="0" data-modify-history="true">
        Your Consoles
      </a>
    </li>
    <li role="presentation" class="active">
      <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#oral-presentations" aria-controls="oral-presentations" role="tab" data-toggle="tab" data-tab-index="1" data-modify-history="true" aria-expanded="true">
        Oral Presentations
      </a>
    </li>
    <li role="presentation">
      <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#spotlight-presentations" aria-controls="spotlight-presentations" role="tab" data-toggle="tab" data-tab-index="2" data-modify-history="true">
        Spotlight Presentations
      </a>
    </li>
    <li role="presentation">
      <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#poster-presentations" aria-controls="poster-presentations" role="tab" data-toggle="tab" data-tab-index="3" data-modify-history="true">
        Poster Presentations
      </a>
    </li>
    <li role="presentation">
      <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#withdrawn-rejected-submissions" aria-controls="withdrawn-rejected-submissions" role="tab" data-toggle="tab" data-tab-index="4" data-modify-history="true">
        Withdrawn/Rejected Submissions
      </a>
    </li>
</ul>

<div class="tab-content">
    <div role="tabpanel" class="tab-pane fade  " id="your-consoles">
      
    </div>
    <div role="tabpanel" class="tab-pane fade   active in" id="oral-presentations">
      
    <ul class="list-unstyled submissions-list">
    <li class="note " data-id="nIAxjsniDzg" data-number="581">
      <h4>
        <a href="https://openreview.net/forum?id=nIAxjsniDzg">
            What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study
        </a>
      
        
          <a href="https://openreview.net/pdf?id=nIAxjsniDzg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Marcin_Andrychowicz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marcin_Andrychowicz1">Marcin Andrychowicz</a>, <a href="https://openreview.net/profile?id=~Anton_Raichuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anton_Raichuk1">Anton Raichuk</a>, <a href="https://openreview.net/profile?email=stanczyk%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="stanczyk@google.com">Piotr Stańczyk</a>, <a href="https://openreview.net/profile?email=eorsini%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="eorsini@google.com">Manu Orsini</a>, <a href="https://openreview.net/profile?email=sertan%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sertan@google.com">Sertan Girgin</a>, <a href="https://openreview.net/profile?id=~Rapha%C3%ABl_Marinier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Raphaël_Marinier1">Raphaël Marinier</a>, <a href="https://openreview.net/profile?id=~Leonard_Hussenot1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leonard_Hussenot1">Leonard Hussenot</a>, <a href="https://openreview.net/profile?id=~Matthieu_Geist1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthieu_Geist1">Matthieu Geist</a>, <a href="https://openreview.net/profile?id=~Olivier_Pietquin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Olivier_Pietquin1">Olivier Pietquin</a>, <a href="https://openreview.net/profile?id=~Marcin_Michalski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marcin_Michalski1">Marcin Michalski</a>, <a href="https://openreview.net/profile?id=~Sylvain_Gelly1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sylvain_Gelly1">Sylvain Gelly</a>, <a href="https://openreview.net/profile?id=~Olivier_Bachem1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Olivier_Bachem1">Olivier Bachem</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#nIAxjsniDzg-details-280" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="nIAxjsniDzg-details-280"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement learning, continuous control</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In recent years, reinforcement learning (RL) has been successfully applied to many different continuous control tasks. While RL algorithms are often conceptually simple, their state-of-the-art implementations take numerous low- and high-level design decisions that strongly affect the performance of the resulting agents. Those choices are usually not extensively discussed in the literature, leading to discrepancy between published descriptions of algorithms and their implementations. This makes it hard to attribute progress in RL and slows down overall progress [Engstrom'20]. As a step towards filling that gap, we implement &gt;50 such ``"choices" in a unified on-policy deep actor-critic framework, allowing us to investigate their impact in a large-scale empirical study. We train over 250'000 agents in five continuous control environments of different complexity and provide insights and practical recommendations for the training of on-policy deep actor-critic RL agents.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We conduct a large-scale empirical study that provides insights and practical recommendations for the training of on-policy deep actor-critic RL agents.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=nIAxjsniDzg&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="rC8sJ4i6kaH" data-number="2250">
      <h4>
        <a href="https://openreview.net/forum?id=rC8sJ4i6kaH">
            Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data
        </a>
      
        
          <a href="https://openreview.net/pdf?id=rC8sJ4i6kaH" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Colin_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Colin_Wei1">Colin Wei</a>, <a href="https://openreview.net/profile?email=kshen6%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kshen6@stanford.edu">Kendrick Shen</a>, <a href="https://openreview.net/profile?id=~Yining_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yining_Chen1">Yining Chen</a>, <a href="https://openreview.net/profile?id=~Tengyu_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tengyu_Ma1">Tengyu Ma</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#rC8sJ4i6kaH-details-129" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rC8sJ4i6kaH-details-129"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning theory, domain adaptation theory, unsupervised learning theory, semi-supervised learning theory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Self-training algorithms, which train a model to fit pseudolabels predicted by another previously-learned model, have been very successful for learning with unlabeled data using neural networks. However, the current theoretical understanding of self-training only applies to linear models. This work provides a unified theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervised learning. At the core of our analysis is a simple but realistic “expansion” assumption, which states that a low-probability subset of the data must expand to a neighborhood with large probability relative to the subset. We also assume that neighborhoods of examples in different classes have minimal overlap. We prove that under these assumptions, the minimizers of population objectives based on self-training and input-consistency regularization will achieve high accuracy with respect to ground-truth labels. By using off-the-shelf generalization bounds, we immediately convert this result to sample complexity guarantees for neural nets that are polynomial in the margin and Lipschitzness. Our results help explain the empirical successes of recently proposed self-training algorithms which use input consistency regularization.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper provides accuracy guarantees for self-training with deep networks on polynomial unlabeled samples for semi-supervised learning, unsupervised domain adaptation, and unsupervised learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="rALA0Xo6yNJ" data-number="1331">
      <h4>
        <a href="https://openreview.net/forum?id=rALA0Xo6yNJ">
            Learning to Reach Goals via Iterated Supervised Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=rALA0Xo6yNJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Dibya_Ghosh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dibya_Ghosh1">Dibya Ghosh</a>, <a href="https://openreview.net/profile?id=~Abhishek_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abhishek_Gupta1">Abhishek Gupta</a>, <a href="https://openreview.net/profile?id=~Ashwin_Reddy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ashwin_Reddy1">Ashwin Reddy</a>, <a href="https://openreview.net/profile?id=~Justin_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Justin_Fu1">Justin Fu</a>, <a href="https://openreview.net/profile?id=~Coline_Manon_Devin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Coline_Manon_Devin1">Coline Manon Devin</a>, <a href="https://openreview.net/profile?id=~Benjamin_Eysenbach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Eysenbach1">Benjamin Eysenbach</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 24 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#rALA0Xo6yNJ-details-137" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rALA0Xo6yNJ-details-137"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">goal reaching, reinforcement learning, behavior cloning, goal-conditioned RL</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Current reinforcement learning (RL) algorithms can be brittle and difficult to use, especially when learning goal-reaching behaviors from sparse rewards. Although supervised imitation learning provides a simple and stable alternative, it requires access to demonstrations from a human supervisor. In this paper, we study RL algorithms that use imitation learning to acquire goal reaching policies from scratch, without the need for expert demonstrations or a value function. In lieu of demonstrations, we leverage the property that any trajectory is a successful demonstration for reaching the final state in that same trajectory. We propose a simple algorithm in which an agent continually relabels and imitates the trajectories it generates to progressively learn goal-reaching behaviors from scratch. Each iteration, the agent collects new trajectories using the latest policy, and maximizes the likelihood of the actions along these trajectories under the goal that was actually reached, so as to improve the policy. We formally show that this iterated supervised learning procedure optimizes a bound on the RL objective, derive performance bounds of the learned policy, and empirically demonstrate improved goal-reaching performance and robustness over current RL algorithms in several benchmark tasks. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present GCSL, a simple RL method that uses supervised learning to learn goal-reaching policies.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="m5Qsh0kBQG" data-number="2611">
      <h4>
        <a href="https://openreview.net/forum?id=m5Qsh0kBQG">
            Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients
        </a>
      
        
          <a href="https://openreview.net/pdf?id=m5Qsh0kBQG" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Brenden_K_Petersen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brenden_K_Petersen1">Brenden K Petersen</a>, <a href="https://openreview.net/profile?email=landajuelala1%40llnl.gov" class="profile-link" data-toggle="tooltip" data-placement="top" title="landajuelala1@llnl.gov">Mikel Landajuela Larma</a>, <a href="https://openreview.net/profile?id=~Terrell_N._Mundhenk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Terrell_N._Mundhenk1">Terrell N. Mundhenk</a>, <a href="https://openreview.net/profile?email=santiago10%40llnl.gov" class="profile-link" data-toggle="tooltip" data-placement="top" title="santiago10@llnl.gov">Claudio Prata Santiago</a>, <a href="https://openreview.net/profile?id=~Soo_Kyung_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soo_Kyung_Kim1">Soo Kyung Kim</a>, <a href="https://openreview.net/profile?email=kim102%40llnl.gov" class="profile-link" data-toggle="tooltip" data-placement="top" title="kim102@llnl.gov">Joanne Taery Kim</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#m5Qsh0kBQG-details-339" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="m5Qsh0kBQG-details-339"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">symbolic regression, reinforcement learning, automated machine learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Discovering the underlying mathematical expressions describing a dataset is a core challenge for artificial intelligence. This is the problem of symbolic regression. Despite recent advances in training neural networks to solve complex tasks, deep learning approaches to symbolic regression are underexplored. We propose a framework that leverages deep learning for symbolic regression via a simple idea: use a large model to search the space of small models. Specifically, we use a recurrent neural network to emit a distribution over tractable mathematical expressions and employ a novel risk-seeking policy gradient to train the network to generate better-fitting expressions. Our algorithm outperforms several baseline methods (including Eureqa, the gold standard for symbolic regression) in its ability to exactly recover symbolic expressions on a series of benchmark problems, both with and without added noise. More broadly, our contributions include a framework that can be applied to optimize hierarchical, variable-length objects under a black-box performance metric, with the ability to incorporate constraints in situ, and a risk-seeking policy gradient formulation that optimizes for best-case performance instead of expected performance.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A deep learning approach to symbolic regression, in which an autoregressive RNN emits a distribution over expressions that is optimized using a risk-seeking policy gradient.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="PULSD5qI2N1" data-number="1820">
      <h4>
        <a href="https://openreview.net/forum?id=PULSD5qI2N1">
            Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime
        </a>
      
        
          <a href="https://openreview.net/pdf?id=PULSD5qI2N1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Atsushi_Nitanda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Atsushi_Nitanda1">Atsushi Nitanda</a>, <a href="https://openreview.net/profile?id=~Taiji_Suzuki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Taiji_Suzuki1">Taiji Suzuki</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#PULSD5qI2N1-details-425" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PULSD5qI2N1-details-425"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">stochastic gradient descent, two-layer neural network, over-parameterization, neural tangent kernel</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We analyze the convergence of the averaged stochastic gradient descent for overparameterized two-layer neural networks for regression problems. It was recently found that a neural tangent kernel (NTK) plays an important role in showing the global convergence of gradient-based methods under the NTK regime, where the learning dynamics for overparameterized neural networks can be almost characterized by that for the associated reproducing kernel Hilbert space (RKHS). However, there is still room for a convergence rate analysis in the NTK regime. In this study, we show that the averaged stochastic gradient descent can achieve the minimax optimal convergence rate, with the global convergence guarantee, by exploiting the complexities of the target function and the RKHS associated with the NTK. Moreover, we show that the target function specified by the NTK of a ReLU network can be learned at the optimal convergence rate through a smooth approximation of a ReLU network under certain conditions.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This is the first paper to overcome technical challenges of achieving the optimal convergence rate under the NTK regime.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="JWOiYxMG92s" data-number="294">
      <h4>
        <a href="https://openreview.net/forum?id=JWOiYxMG92s">
            Free Lunch for Few-shot Learning:  Distribution Calibration
        </a>
      
        
          <a href="https://openreview.net/pdf?id=JWOiYxMG92s" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Shuo_Yang5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuo_Yang5">Shuo Yang</a>, <a href="https://openreview.net/profile?id=~Lu_Liu7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lu_Liu7">Lu Liu</a>, <a href="https://openreview.net/profile?id=~Min_Xu5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Min_Xu5">Min Xu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#JWOiYxMG92s-details-511" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JWOiYxMG92s-details-511"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">few-shot learning, image classification, distribution estimation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Learning from a limited number of samples is challenging since the learned model can easily become overfitted based on the biased distribution formed by only a few training examples. In this paper, we calibrate the distribution of these few-sample classes by transferring statistics from the classes with sufficient examples. Then an adequate number of examples can be sampled from the calibrated distribution to expand the inputs to the classifier. We assume every dimension in the feature representation follows a Gaussian distribution so that the mean and the variance of the distribution can borrow from that of similar classes whose statistics are better estimated with an adequate number of samples. Our method can be built on top of off-the-shelf pretrained feature extractors and classification models without extra parameters. We show that a simple logistic regression classifier trained using the features sampled from our calibrated distribution can outperform the state-of-the-art accuracy on three datasets (~5% improvement on miniImageNet compared to the next best). The visualization of these generated features demonstrates that our calibrated distribution is an accurate estimation. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">The code is available at: https://github.com/ShuoYang-1998/Few_Shot_Distribution_Calibration</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="HajQFbx_yB" data-number="362">
      <h4>
        <a href="https://openreview.net/forum?id=HajQFbx_yB">
            Scalable Learning and MAP Inference for Nonsymmetric Determinantal Point Processes
        </a>
      
        
          <a href="https://openreview.net/pdf?id=HajQFbx_yB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mike_Gartrell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mike_Gartrell1">Mike Gartrell</a>, <a href="https://openreview.net/profile?id=~Insu_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Insu_Han1">Insu Han</a>, <a href="https://openreview.net/profile?id=~Elvis_Dohmatob1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Elvis_Dohmatob1">Elvis Dohmatob</a>, <a href="https://openreview.net/profile?id=~Jennifer_Gillenwater1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jennifer_Gillenwater1">Jennifer Gillenwater</a>, <a href="https://openreview.net/profile?id=~Victor-Emmanuel_Brunel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Victor-Emmanuel_Brunel1">Victor-Emmanuel Brunel</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#HajQFbx_yB-details-950" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HajQFbx_yB-details-950"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">determinantal point processes, unsupervised learning, representation learning, submodular optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Determinantal point processes (DPPs) have attracted significant attention in machine learning for their ability to model subsets drawn from a large item collection. Recent work shows that nonsymmetric DPP (NDPP) kernels have significant advantages over symmetric kernels in terms of modeling power and predictive performance. However, for an item collection of size <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="0" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math></mjx-assistive-mml></mjx-container>, existing NDPP learning and inference algorithms require memory quadratic in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="1" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math></mjx-assistive-mml></mjx-container> and runtime cubic (for learning) or quadratic (for inference) in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="2" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math></mjx-assistive-mml></mjx-container>, making them impractical for many typical subset selection tasks. In this work, we develop a learning algorithm with space and time requirements linear in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="3" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math></mjx-assistive-mml></mjx-container> by introducing a new NDPP kernel decomposition. We also derive a linear-complexity NDPP maximum a posteriori (MAP) inference algorithm that applies not only to our new kernel but also to that of prior work. Through evaluation on real-world datasets, we show that our algorithms scale significantly better, and can match the predictive performance of prior work.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose scalable learning and maximum a posteriori (MAP) inference algorithms for nonsymmetric determinantal point processes (DPPs).</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=HajQFbx_yB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xpx9zj7CUlY" data-number="2076">
      <h4>
        <a href="https://openreview.net/forum?id=xpx9zj7CUlY">
            Randomized Automatic Differentiation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xpx9zj7CUlY" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Deniz_Oktay2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deniz_Oktay2">Deniz Oktay</a>, <a href="https://openreview.net/profile?email=mcgreivy%40princeton.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mcgreivy@princeton.edu">Nick McGreivy</a>, <a href="https://openreview.net/profile?email=jaduol%40princeton.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jaduol@princeton.edu">Joshua Aduol</a>, <a href="https://openreview.net/profile?id=~Alex_Beatson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_Beatson1">Alex Beatson</a>, <a href="https://openreview.net/profile?id=~Ryan_P_Adams1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ryan_P_Adams1">Ryan P Adams</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 14 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xpx9zj7CUlY-details-911" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xpx9zj7CUlY-details-911"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">automatic differentiation, autodiff, backprop, deep learning, pdes, stochastic optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The successes of deep learning, variational inference, and many other fields have been aided by specialized implementations of reverse-mode automatic differentiation (AD) to compute gradients of mega-dimensional objectives. The AD techniques underlying these tools were designed to compute exact gradients to numerical precision, but modern machine learning models are almost always trained with stochastic gradient descent. Why spend computation and memory on exact (minibatch) gradients only to use them for stochastic optimization? We develop a general framework and approach for randomized automatic differentiation (RAD), which can allow unbiased gradient estimates to be computed with reduced memory in return for variance. We examine limitations of the general approach, and argue that we must leverage problem specific structure to realize benefits. We develop RAD techniques for a variety of simple neural network architectures, and show that for a fixed memory budget, RAD converges in fewer iterations than using a small batch size for feedforward networks, and in a similar number for recurrent networks. We also show that RAD can be applied to scientific computing, and use it to develop a low-memory stochastic gradient method for optimizing the control parameters of a linear reaction-diffusion PDE representing a fission reactor.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We develop a general framework and approach for randomized automatic differentiation (RAD), which can allow unbiased gradient estimates to be computed with reduced memory in return for variance.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=xpx9zj7CUlY&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="UuchYL8wSZo" data-number="949">
      <h4>
        <a href="https://openreview.net/forum?id=UuchYL8wSZo">
            Learning Generalizable Visual Representations via Interactive Gameplay
        </a>
      
        
          <a href="https://openreview.net/pdf?id=UuchYL8wSZo" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Luca_Weihs1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luca_Weihs1">Luca Weihs</a>, <a href="https://openreview.net/profile?id=~Aniruddha_Kembhavi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aniruddha_Kembhavi1">Aniruddha Kembhavi</a>, <a href="https://openreview.net/profile?id=~Kiana_Ehsani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kiana_Ehsani1">Kiana Ehsani</a>, <a href="https://openreview.net/profile?id=~Sarah_M_Pratt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sarah_M_Pratt1">Sarah M Pratt</a>, <a href="https://openreview.net/profile?email=winsonh%40allenai.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="winsonh@allenai.org">Winson Han</a>, <a href="https://openreview.net/profile?email=alvaroh%40allenai.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="alvaroh@allenai.org">Alvaro Herrasti</a>, <a href="https://openreview.net/profile?id=~Eric_Kolve1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eric_Kolve1">Eric Kolve</a>, <a href="https://openreview.net/profile?email=dustins%40allenai.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="dustins@allenai.org">Dustin Schwenk</a>, <a href="https://openreview.net/profile?id=~Roozbeh_Mottaghi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roozbeh_Mottaghi1">Roozbeh Mottaghi</a>, <a href="https://openreview.net/profile?id=~Ali_Farhadi3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ali_Farhadi3">Ali Farhadi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#UuchYL8wSZo-details-572" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UuchYL8wSZo-details-572"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">representation learning, deep reinforcement learning, computer vision</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A growing body of research suggests that embodied gameplay, prevalent not just in human cultures but across a variety of animal species including turtles and ravens, is critical in developing the neural flexibility for creative problem solving, decision making, and socialization. Comparatively little is known regarding the impact of embodied gameplay upon artificial agents. While recent work has produced agents proficient in abstract games, these environments are far removed the real world and thus these agents can provide little insight into the advantages of embodied play. Hiding games, such as hide-and-seek, played universally, provide a rich ground for studying the impact of embodied gameplay on representation learning in the context of perspective taking, secret keeping, and false belief understanding. Here we are the first to show that embodied adversarial reinforcement learning agents playing Cache, a variant of hide-and-seek, in a high fidelity, interactive, environment, learn generalizable representations of their observations encoding information such as object permanence, free space, and containment. Moving closer to biologically motivated learning strategies, our agents' representations, enhanced by intentionality and memory, are developed through interaction and play. These results serve as a model for studying how facets of vision develop through interaction, provide an experimental framework for assessing what is learned by artificial agents, and demonstrates the value of moving from large, static, datasets towards experiential, interactive, representation learning.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show the representation learned through interaction and gameplay generalizes better compared to passive and static representation learning methods.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="KvyxFqZS_D" data-number="2523">
      <h4>
        <a href="https://openreview.net/forum?id=KvyxFqZS_D">
            Global Convergence of Three-layer Neural Networks in the Mean Field Regime
        </a>
      
        
          <a href="https://openreview.net/pdf?id=KvyxFqZS_D" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=huypham%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="huypham@stanford.edu">Huy Tuan Pham</a>, <a href="https://openreview.net/profile?id=~Phan-Minh_Nguyen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Phan-Minh_Nguyen1">Phan-Minh Nguyen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#KvyxFqZS_D-details-359" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KvyxFqZS_D-details-359"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning theory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In the mean field regime, neural networks are appropriately scaled so that as the width tends to infinity, the learning dynamics tends to a nonlinear and nontrivial dynamical limit, known as the mean field limit. This lends a way to study large-width neural networks via analyzing the mean field limit. Recent works have successfully applied such analysis to two-layer networks and provided global convergence guarantees. The extension to multilayer ones however has been a highly challenging puzzle, and little is known about the optimization efficiency in the mean field regime when there are more than two layers.
      
      In this work, we prove a global convergence result for unregularized feedforward three-layer networks in the mean field regime. We first develop a rigorous framework to establish the mean field limit of three-layer networks under stochastic gradient descent training. To that end, we propose the idea of a neuronal embedding, which comprises of a fixed probability space that encapsulates neural networks of arbitrary sizes. The identified mean field limit is then used to prove a global convergence guarantee under suitable regularity and convergence mode assumptions, which – unlike previous works on two-layer networks – does not rely critically on convexity. Underlying the result is a universal approximation property, natural of neural networks, which importantly is shown to hold at any finite training time (not necessarily at convergence) via an algebraic topology argument.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a rigorous framework for three-layer neural networks in the mean field regime and prove a global convergence guarantee.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Mk6PZtgAgfq" data-number="1166">
      <h4>
        <a href="https://openreview.net/forum?id=Mk6PZtgAgfq">
            Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Mk6PZtgAgfq" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Max_B_Paulus1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Max_B_Paulus1">Max B Paulus</a>, <a href="https://openreview.net/profile?id=~Chris_J._Maddison1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chris_J._Maddison1">Chris J. Maddison</a>, <a href="https://openreview.net/profile?id=~Andreas_Krause1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_Krause1">Andreas Krause</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Mk6PZtgAgfq-details-603" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Mk6PZtgAgfq-details-603"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">gumbel, softmax, gumbel-softmax, straight-through, straightthrough, rao, rao-blackwell</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We reduce the variance of the straight-through Gumbel-Softmax estimator to improve its performance. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Mk6PZtgAgfq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Ua6zuk0WRH" data-number="656">
      <h4>
        <a href="https://openreview.net/forum?id=Ua6zuk0WRH">
            Rethinking Attention with Performers
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Ua6zuk0WRH" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Krzysztof_Marcin_Choromanski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Krzysztof_Marcin_Choromanski1">Krzysztof Marcin Choromanski</a>, <a href="https://openreview.net/profile?email=vl304%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="vl304@cam.ac.uk">Valerii Likhosherstov</a>, <a href="https://openreview.net/profile?id=~David_Dohan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Dohan1">David Dohan</a>, <a href="https://openreview.net/profile?id=~Xingyou_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xingyou_Song1">Xingyou Song</a>, <a href="https://openreview.net/profile?id=~Andreea_Gane1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreea_Gane1">Andreea Gane</a>, <a href="https://openreview.net/profile?id=~Tamas_Sarlos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tamas_Sarlos1">Tamas Sarlos</a>, <a href="https://openreview.net/profile?email=phawkins%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="phawkins@google.com">Peter Hawkins</a>, <a href="https://openreview.net/profile?id=~Jared_Quincy_Davis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jared_Quincy_Davis1">Jared Quincy Davis</a>, <a href="https://openreview.net/profile?id=~Afroz_Mohiuddin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Afroz_Mohiuddin1">Afroz Mohiuddin</a>, <a href="https://openreview.net/profile?id=~Lukasz_Kaiser1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lukasz_Kaiser1">Lukasz Kaiser</a>, <a href="https://openreview.net/profile?id=~David_Benjamin_Belanger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Benjamin_Belanger1">David Benjamin Belanger</a>, <a href="https://openreview.net/profile?id=~Lucy_J_Colwell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lucy_J_Colwell1">Lucy J Colwell</a>, <a href="https://openreview.net/profile?id=~Adrian_Weller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adrian_Weller1">Adrian Weller</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Ua6zuk0WRH-details-436" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ua6zuk0WRH-details-436"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">performer, transformer, attention, softmax, approximation, linear, bert, bidirectional, unidirectional, orthogonal, random, features, FAVOR, kernel, generalized, sparsity, reformer, linformer, protein, trembl, uniprot</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can also be used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low  estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce Performers, linear full-rank-attention Transformers via provable random feature approximation methods, without relying on sparsity or low-rankness.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="XSLF1XFq5h" data-number="1848">
      <h4>
        <a href="https://openreview.net/forum?id=XSLF1XFq5h">
            Getting a CLUE: A  Method for Explaining Uncertainty Estimates
        </a>
      
        
          <a href="https://openreview.net/pdf?id=XSLF1XFq5h" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Javier_Antoran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Javier_Antoran1">Javier Antoran</a>, <a href="https://openreview.net/profile?id=~Umang_Bhatt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Umang_Bhatt1">Umang Bhatt</a>, <a href="https://openreview.net/profile?id=~Tameem_Adel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tameem_Adel1">Tameem Adel</a>, <a href="https://openreview.net/profile?id=~Adrian_Weller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adrian_Weller1">Adrian Weller</a>, <a href="https://openreview.net/profile?id=~Jos%C3%A9_Miguel_Hern%C3%A1ndez-Lobato1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~José_Miguel_Hernández-Lobato1">José Miguel Hernández-Lobato</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#XSLF1XFq5h-details-991" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XSLF1XFq5h-details-991"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">interpretability, uncertainty, explainability</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input's prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a method to help explain uncertainties of any differentiable probabilistic model by perturbing input features.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tW4QEInpni" data-number="2648">
      <h4>
        <a href="https://openreview.net/forum?id=tW4QEInpni">
            When Do Curricula Work?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tW4QEInpni" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xiaoxia_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoxia_Wu1">Xiaoxia Wu</a>, <a href="https://openreview.net/profile?id=~Ethan_Dyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ethan_Dyer1">Ethan Dyer</a>, <a href="https://openreview.net/profile?id=~Behnam_Neyshabur1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Behnam_Neyshabur1">Behnam Neyshabur</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 02 Apr 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tW4QEInpni-details-331" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tW4QEInpni-details-331"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Curriculum Learning, Understanding Deep Learning, Empirical Investigation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Inspired by human learning, researchers have proposed ordering examples during training based on their difficulty. Both curriculum learning, exposing a network to easier examples early in training, and anti-curriculum learning, showing the most difficult examples first, have been suggested as improvements to the standard i.i.d. training. In this work, we set out to investigate the relative benefits of ordered learning. We first investigate the implicit curricula resulting from architectural and optimization bias and find that samples are learned in a highly consistent order. Next, to quantify the benefit of explicit curricula, we conduct extensive experiments over thousands of orderings spanning three kinds of learning: curriculum, anti-curriculum, and random-curriculum -- in which the size of the training dataset is dynamically increased over time, but the examples are randomly ordered. We find that for standard benchmark datasets, curricula have only marginal benefits, and that randomly ordered samples perform as well or better than curricula and anti-curricula, suggesting that any benefit is entirely due to the dynamic training set size. Inspired by common use cases of curriculum learning in practice, we investigate the role of limited training time budget and noisy data in the success of curriculum learning. Our experiments demonstrate that curriculum, but not anti-curriculum or random ordering can indeed improve the performance either with limited training time budget or in the existence of noisy data.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We conduct extensive experiments over thousands of orderings to investigate the effectiveness of  three kinds of learning: curriculum, anti-curriculum, and random-curriculum.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="B7v4QMR6Z9w" data-number="1241">
      <h4>
        <a href="https://openreview.net/forum?id=B7v4QMR6Z9w">
            Federated Learning Based on Dynamic Regularization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=B7v4QMR6Z9w" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Durmus_Alp_Emre_Acar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Durmus_Alp_Emre_Acar1">Durmus Alp Emre Acar</a>, <a href="https://openreview.net/profile?id=~Yue_Zhao12" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yue_Zhao12">Yue Zhao</a>, <a href="https://openreview.net/profile?id=~Ramon_Matas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ramon_Matas1">Ramon Matas</a>, <a href="https://openreview.net/profile?id=~Matthew_Mattina1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthew_Mattina1">Matthew Mattina</a>, <a href="https://openreview.net/profile?id=~Paul_Whatmough1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Whatmough1">Paul Whatmough</a>, <a href="https://openreview.net/profile?id=~Venkatesh_Saligrama1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Venkatesh_Saligrama1">Venkatesh Saligrama</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#B7v4QMR6Z9w-details-826" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="B7v4QMR6Z9w-details-826"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Federated Learning, Deep Neural Networks, Distributed Optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a novel federated learning method for distributively training neural network models, where the server orchestrates cooperation between a subset of randomly chosen devices in each round. We view Federated Learning problem primarily from a communication perspective and allow more device level computations to save transmission costs. We point out a fundamental dilemma, in that the minima of the local-device level empirical loss are inconsistent with those of the global empirical loss. Different from recent prior works, that either attempt inexact minimization or utilize devices for parallelizing gradient computation, we propose a dynamic regularizer for each device at each round, so that in the limit the global and device solutions are aligned. We demonstrate both through empirical results on real and synthetic data as well as analytical results that our scheme leads to efficient training, in both convex and non-convex settings, while being fully agnostic to device heterogeneity and robust to large number of devices, partial participation and unbalanced data.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present, FedDyn, a novel dynamic regularization method for Federated Learning where the risk objective for each device is dynamically updated to ensure the device optima is asymptotically consistent with stationary points of the global loss.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="iAX0l6Cz8ub" data-number="332">
      <h4>
        <a href="https://openreview.net/forum?id=iAX0l6Cz8ub">
            Geometry-aware Instance-reweighted Adversarial Training
        </a>
      
        
          <a href="https://openreview.net/pdf?id=iAX0l6Cz8ub" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jingfeng_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jingfeng_Zhang1">Jingfeng Zhang</a>, <a href="https://openreview.net/profile?id=~Jianing_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianing_Zhu2">Jianing Zhu</a>, <a href="https://openreview.net/profile?id=~Gang_Niu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gang_Niu1">Gang Niu</a>, <a href="https://openreview.net/profile?id=~Bo_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Han1">Bo Han</a>, <a href="https://openreview.net/profile?id=~Masashi_Sugiyama1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Masashi_Sugiyama1">Masashi Sugiyama</a>, <a href="https://openreview.net/profile?id=~Mohan_Kankanhalli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohan_Kankanhalli1">Mohan Kankanhalli</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#iAX0l6Cz8ub-details-349" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iAX0l6Cz8ub-details-349"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Adversarial robustness</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In adversarial machine learning, there was a common belief that robustness and accuracy hurt each other. The belief was challenged by recent studies where we can maintain the robustness and improve the accuracy. However, the other direction, whether we can keep the accuracy and improve the robustness, is conceptually and practically more interesting, since robust accuracy should be lower than standard accuracy for any model. In this paper, we show this direction is also promising. Firstly, we find even over-parameterized deep networks may still have insufficient model capacity, because adversarial training has an overwhelming smoothing effect. Secondly, given limited model capacity, we argue adversarial data should have unequal importance: geometrically speaking, a natural data point closer to/farther from the class boundary is less/more robust, and the corresponding adversarial data point should be assigned with larger/smaller weight. Finally, to implement the idea, we propose geometry-aware instance-reweighted adversarial training, where the weights are based on how difficult it is to attack a natural data point. Experiments show that our proposal boosts the robustness of standard adversarial training; combining two directions, we improve both robustness and accuracy of standard adversarial training.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper has proposed a novel adversarial training method, i.e., geometry-aware instance-reweighted adversarial training (GAIRAT), which sheds new lights on improving the adversarial training.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=iAX0l6Cz8ub&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="gvxJzw8kW4b" data-number="3032">
      <h4>
        <a href="https://openreview.net/forum?id=gvxJzw8kW4b">
            Co-Mixup: Saliency Guided Joint Mixup with Supermodular Diversity
        </a>
      
        
          <a href="https://openreview.net/pdf?id=gvxJzw8kW4b" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~JangHyun_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~JangHyun_Kim1">JangHyun Kim</a>, <a href="https://openreview.net/profile?id=~Wonho_Choo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wonho_Choo1">Wonho Choo</a>, <a href="https://openreview.net/profile?id=~Hosan_Jeong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hosan_Jeong1">Hosan Jeong</a>, <a href="https://openreview.net/profile?id=~Hyun_Oh_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hyun_Oh_Song1">Hyun Oh Song</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 05 Feb 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#gvxJzw8kW4b-details-877" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gvxJzw8kW4b-details-877"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Data Augmentation, Deep Learning, Supervised Learning, Discrete Optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While deep neural networks show great performance on fitting to the training distribution, improving the networks' generalization performance to the test distribution and robustness to the sensitivity to input perturbations still remain as a challenge. Although a number of mixup based augmentation strategies have been proposed to partially address them, it remains unclear as to how to best utilize the supervisory signal within each input data for mixup from the optimization perspective. We propose a new perspective on batch mixup and formulate the optimal construction of a batch of mixup data maximizing the data saliency measure of each individual mixup data and encouraging the supermodular diversity among the constructed mixup data. This leads to a novel discrete optimization problem minimizing the difference between submodular functions. We also propose an efficient modular approximation based iterative submodular minimization algorithm for efficient mixup computation per each minibatch suitable for minibatch based neural network training. Our experiments show the proposed method achieves the state of the art generalization, calibration, and weakly supervised localization results compared to other mixup methods. The source code is available at https://github.com/snu-mllab/Co-Mixup.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a new perspective on joint mixup augmentation and formulate the optimal construction of a batch of mixup data.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=gvxJzw8kW4b&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="DktZb97_Fx" data-number="2251">
      <h4>
        <a href="https://openreview.net/forum?id=DktZb97_Fx">
            SenSeI: Sensitive Set Invariance for Enforcing Individual Fairness
        </a>
      
        
          <a href="https://openreview.net/pdf?id=DktZb97_Fx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mikhail_Yurochkin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mikhail_Yurochkin1">Mikhail Yurochkin</a>, <a href="https://openreview.net/profile?id=~Yuekai_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuekai_Sun1">Yuekai Sun</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#DktZb97_Fx-details-298" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DktZb97_Fx-details-298"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Algorithmic fairness, invariance</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we cast fair machine learning as invariant machine learning. We first formulate a version of individual fairness that enforces invariance on certain sensitive sets. We then design a transport-based regularizer that enforces this version of individual fairness and develop an algorithm to minimize the regularizer efficiently. Our theoretical results guarantee the proposed approach trains certifiably fair ML models. Finally, in the experimental studies we demonstrate improved fairness metrics in comparison to several recent fair training procedures on three ML tasks that are susceptible to algorithmic bias.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a new invariance-enforcing regularizer for training individually fair ML systems.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=DktZb97_Fx&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="rsf1z-JSj87" data-number="3246">
      <h4>
        <a href="https://openreview.net/forum?id=rsf1z-JSj87">
            End-to-end Adversarial Text-to-Speech
        </a>
      
        
          <a href="https://openreview.net/pdf?id=rsf1z-JSj87" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jeff_Donahue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jeff_Donahue1">Jeff Donahue</a>, <a href="https://openreview.net/profile?id=~Sander_Dieleman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sander_Dieleman1">Sander Dieleman</a>, <a href="https://openreview.net/profile?id=~Mikolaj_Binkowski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mikolaj_Binkowski1">Mikolaj Binkowski</a>, <a href="https://openreview.net/profile?id=~Erich_Elsen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Erich_Elsen1">Erich Elsen</a>, <a href="https://openreview.net/profile?id=~Karen_Simonyan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Karen_Simonyan1">Karen Simonyan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#rsf1z-JSj87-details-725" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rsf1z-JSj87-details-725"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">text-to-speech, speech synthesis, adversarial, GAN, end-to-end, feed-forward, generative model</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Modern text-to-speech synthesis pipelines typically involve multiple processing stages, each of which is designed or learnt independently from the rest. In this work, we take on the challenging task of learning to synthesise speech from normalised text or phonemes in an end-to-end manner, resulting in models which operate directly on character or phoneme input sequences and produce raw speech audio outputs. Our proposed generator is feed-forward and thus efficient for both training and inference, using a differentiable alignment scheme based on token length prediction. It learns to produce high fidelity audio through a combination of adversarial feedback and prediction losses constraining the generated audio to roughly match the ground truth in terms of its total duration and mel-spectrogram. To allow the model to capture temporal variation in the generated audio, we employ soft dynamic time warping in the spectrogram-based prediction loss. The resulting model achieves a mean opinion score exceeding 4 on a 5 point scale, which is comparable to the state-of-the-art models relying on multi-stage training and additional supervision.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Efficient, adversarially trained feed-forward text-to-speech model producing high-quality speech, learnt end-to-end in a single stage.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="mSAKhLYLSsl" data-number="420">
      <h4>
        <a href="https://openreview.net/forum?id=mSAKhLYLSsl">
            Dataset Condensation with Gradient Matching
        </a>
      
        
          <a href="https://openreview.net/pdf?id=mSAKhLYLSsl" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Bo_Zhao4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Zhao4">Bo Zhao</a>, <a href="https://openreview.net/profile?id=~Konda_Reddy_Mopuri3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Konda_Reddy_Mopuri3">Konda Reddy Mopuri</a>, <a href="https://openreview.net/profile?id=~Hakan_Bilen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hakan_Bilen1">Hakan Bilen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 08 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#mSAKhLYLSsl-details-447" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mSAKhLYLSsl-details-447"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">dataset condensation, data-efficient learning, image generation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">As the state-of-the-art machine learning methods in many fields rely on larger datasets, storing datasets and training models on them become significantly more expensive. This paper proposes a training set synthesis technique for data-efficient learning, called Dataset Condensation, that learns to condense large dataset into a small set of informative synthetic samples for training deep neural networks from scratch. We formulate this goal as a gradient matching problem between the gradients of deep neural network weights that are trained on the original and our synthetic data. We rigorously evaluate its performance in several computer vision benchmarks and demonstrate that it significantly outperforms the state-of-the-art methods. Finally we explore the use of our method in continual learning and neural architecture search and report promising gains when limited memory and computations are available.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper proposes a training set synthesis technique that learns to produce a small set of informative samples for training deep neural networks from scratch in a small fraction of computational cost while achieving as close results as possible.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="PKubaeJkw3" data-number="2508">
      <h4>
        <a href="https://openreview.net/forum?id=PKubaeJkw3">
            Rethinking Architecture Selection in Differentiable NAS
        </a>
      
        
          <a href="https://openreview.net/pdf?id=PKubaeJkw3" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ruochen_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruochen_Wang2">Ruochen Wang</a>, <a href="https://openreview.net/profile?id=~Minhao_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minhao_Cheng1">Minhao Cheng</a>, <a href="https://openreview.net/profile?id=~Xiangning_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiangning_Chen1">Xiangning Chen</a>, <a href="https://openreview.net/profile?id=~Xiaocheng_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaocheng_Tang1">Xiaocheng Tang</a>, <a href="https://openreview.net/profile?id=~Cho-Jui_Hsieh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cho-Jui_Hsieh1">Cho-Jui Hsieh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 04 Apr 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#PKubaeJkw3-details-539" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PKubaeJkw3-details-539"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Differentiable Neural Architecture Search is one of the most popular Neural Architecture Search (NAS) methods for its search efficiency and simplicity, accomplished by jointly optimizing the model weight and architecture parameters in a weight-sharing supernet via gradient-based algorithms. At the end of the search phase, the operations with the largest architecture parameters will be selected to form the final architecture, with the implicit assumption that the values of architecture parameters reflect the operation strength. While much has been discussed about the supernet's optimization, the architecture selection process has received little attention. We provide empirical and theoretical analysis to show that the magnitude of architecture parameters does not necessarily indicate how much the operation contributes to the supernet's performance. We propose an alternative perturbation-based architecture selection that directly measures each operation's influence on the supernet. We re-evaluate several differentiable NAS methods with the proposed architecture selection and find that it is able to extract significantly improved architectures from the underlying supernets consistently. Furthermore, we find that several failure modes of DARTS can be greatly alleviated with the proposed selection method, indicating that much of the poor generalization observed in DARTS can be attributed to the failure of magnitude-based architecture selection rather than entirely the optimization of its supernet.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=PKubaeJkw3&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="jWkw45-9AbL" data-number="2042">
      <h4>
        <a href="https://openreview.net/forum?id=jWkw45-9AbL">
            A Distributional Approach to Controlled Text Generation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=jWkw45-9AbL" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Muhammad_Khalifa2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Muhammad_Khalifa2">Muhammad Khalifa</a>, <a href="https://openreview.net/profile?id=~Hady_Elsahar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hady_Elsahar2">Hady Elsahar</a>, <a href="https://openreview.net/profile?id=~Marc_Dymetman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marc_Dymetman1">Marc Dymetman</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#jWkw45-9AbL-details-903" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jWkw45-9AbL-details-903"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Controlled NLG, Pretrained Language Models, Bias in Language Models, Energy-Based Models, Information Geometry, Exponential Families</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a  Distributional  Approach for addressing  Controlled  Text  Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both “pointwise’” and “distributional” constraints over the target LM — to our knowledge, the first model with such generality —while minimizing KL divergence from the initial LM distribution.  The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient.  We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM.  We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models.  Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence.
      Code available at https://github.com/naver/gdc</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a novel approach to Controlled NLG, relying on Constraints over Distributions, Information Geometry, and Sampling from Energy-Based Models.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="QIRlze3I6hX" data-number="1056">
      <h4>
        <a href="https://openreview.net/forum?id=QIRlze3I6hX">
            Learning Cross-Domain Correspondence for Control with Dynamics Cycle-Consistency
        </a>
      
        
          <a href="https://openreview.net/pdf?id=QIRlze3I6hX" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Qiang_Zhang5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qiang_Zhang5">Qiang Zhang</a>, <a href="https://openreview.net/profile?id=~Tete_Xiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tete_Xiao1">Tete Xiao</a>, <a href="https://openreview.net/profile?id=~Alexei_A_Efros1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexei_A_Efros1">Alexei A Efros</a>, <a href="https://openreview.net/profile?id=~Lerrel_Pinto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lerrel_Pinto1">Lerrel Pinto</a>, <a href="https://openreview.net/profile?id=~Xiaolong_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaolong_Wang3">Xiaolong Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#QIRlze3I6hX-details-435" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QIRlze3I6hX-details-435"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">self-supervised learning, robotics</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">At the heart of many robotics problems is the challenge of learning correspondences across domains. For instance, imitation learning requires obtaining correspondence between humans and robots; sim-to-real requires correspondence between physics simulators and real hardware; transfer learning requires correspondences between different robot environments. In this paper, we propose to learn correspondence across such domains emphasizing on differing modalities (vision and internal state), physics parameters (mass and friction), and morphologies (number of limbs). Importantly, correspondences are learned using unpaired and randomly collected data from the two domains. We propose dynamics cycles that align dynamic robotic behavior across two domains using a cycle consistency constraint. Once this correspondence is found, we can directly transfer the policy trained on one domain to the other, without needing any additional fine-tuning on the second domain. We perform experiments across a variety of problem domains, both in simulation and on real robots. Our framework is able to align uncalibrated monocular video of a real robot arm to dynamic state-action trajectories of a simulated arm without paired data. Video demonstrations of our results are available at: https://sites.google.com/view/cycledynamics .</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We learn correspondence across domains with different modalities, physics parameters, and morphologies for control tasks (in both simulation and real robot) with Dynamics Cycle-Consistency. </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=QIRlze3I6hX&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="0-uUGPbIjD" data-number="702">
      <h4>
        <a href="https://openreview.net/forum?id=0-uUGPbIjD">
            Human-Level Performance in No-Press Diplomacy via Equilibrium Search
        </a>
      
        
          <a href="https://openreview.net/pdf?id=0-uUGPbIjD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jonathan_Gray2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Gray2">Jonathan Gray</a>, <a href="https://openreview.net/profile?id=~Adam_Lerer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adam_Lerer1">Adam Lerer</a>, <a href="https://openreview.net/profile?id=~Anton_Bakhtin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anton_Bakhtin1">Anton Bakhtin</a>, <a href="https://openreview.net/profile?id=~Noam_Brown2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noam_Brown2">Noam Brown</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#0-uUGPbIjD-details-200" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0-uUGPbIjD-details-200"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">multi-agent systems, regret minimization, no-regret learning, game theory, reinforcement learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Prior AI breakthroughs in complex games have focused on either the purely adversarial or purely cooperative settings. In contrast, Diplomacy is a game of shifting alliances that involves both cooperation and competition. For this reason, Diplomacy has proven to be a formidable research challenge. In this paper we describe an agent for the no-press variant of Diplomacy that combines supervised learning on human data with one-step lookahead search via regret minimization. Regret minimization techniques have been behind previous AI successes in adversarial games, most notably poker, but have not previously been shown to be successful in large-scale games involving cooperation. We show that our agent greatly exceeds the performance of past no-press Diplomacy bots, is unexploitable by expert humans, and ranks in the top 2% of human players when playing anonymous games on a popular Diplomacy website.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present an agent that approximates a one-step equilibrium in no-press Diplomacy using no-regret learning and show that it exceeds human-level performance</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=0-uUGPbIjD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Ysuv-WOFeKR" data-number="2407">
      <h4>
        <a href="https://openreview.net/forum?id=Ysuv-WOFeKR">
            Parrot: Data-Driven Behavioral Priors for Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Ysuv-WOFeKR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Avi_Singh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Avi_Singh1">Avi Singh</a>, <a href="https://openreview.net/profile?id=~Huihan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huihan_Liu1">Huihan Liu</a>, <a href="https://openreview.net/profile?id=~Gaoyue_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gaoyue_Zhou1">Gaoyue Zhou</a>, <a href="https://openreview.net/profile?id=~Albert_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Albert_Yu1">Albert Yu</a>, <a href="https://openreview.net/profile?id=~Nicholas_Rhinehart1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_Rhinehart1">Nicholas Rhinehart</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Ysuv-WOFeKR-details-157" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ysuv-WOFeKR-details-157"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, imitation learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Reinforcement learning provides a general framework for flexible decision making and control, but requires extensive data collection for each new task that an agent needs to learn. In other machine learning fields, such as natural language processing or computer vision, pre-training on large, previously collected datasets to bootstrap learning for new tasks has emerged as a powerful paradigm to reduce data requirements when learning a new task. In this paper, we ask the following question: how can we enable similarly useful pre-training for RL agents? We propose a method for pre-training behavioral priors that can capture complex input-output relationships observed in successful trials from a wide range of previously seen tasks, and we show how this learned prior can be used for rapidly learning new tasks without impeding the RL agent's ability to try out novel behaviors. We demonstrate the effectiveness of our approach in challenging robotic manipulation domains involving image observations and sparse reward functions, where our method outperforms prior works by a substantial margin. Additional materials can be found on our project website: https://sites.google.com/view/parrot-rl</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a method for pre-training a prior for reinforcement learning using data from a diverse range of tasks, and use this prior to speed up learning of new tasks. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-2FCwDKRREu" data-number="1770">
      <h4>
        <a href="https://openreview.net/forum?id=-2FCwDKRREu">
            Learning Invariant Representations for Reinforcement Learning without Reconstruction
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-2FCwDKRREu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Amy_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amy_Zhang1">Amy Zhang</a>, <a href="https://openreview.net/profile?id=~Rowan_Thomas_McAllister1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rowan_Thomas_McAllister1">Rowan Thomas McAllister</a>, <a href="https://openreview.net/profile?id=~Roberto_Calandra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roberto_Calandra1">Roberto Calandra</a>, <a href="https://openreview.net/profile?id=~Yarin_Gal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yarin_Gal1">Yarin Gal</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-2FCwDKRREu-details-548" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-2FCwDKRREu-details-548"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">rich observations, bisimulation metrics, representation learning, state abstractions</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="FGqiDsBUKL0" data-number="140">
      <h4>
        <a href="https://openreview.net/forum?id=FGqiDsBUKL0">
            Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=FGqiDsBUKL0" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xingang_Pan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xingang_Pan1">Xingang Pan</a>, <a href="https://openreview.net/profile?id=~Bo_Dai2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Dai2">Bo Dai</a>, <a href="https://openreview.net/profile?id=~Ziwei_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziwei_Liu1">Ziwei Liu</a>, <a href="https://openreview.net/profile?id=~Chen_Change_Loy2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chen_Change_Loy2">Chen Change Loy</a>, <a href="https://openreview.net/profile?id=~Ping_Luo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ping_Luo2">Ping Luo</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#FGqiDsBUKL0-details-51" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FGqiDsBUKL0-details-51"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Generative Adversarial Network, 3D Reconstruction</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Natural images are projections of 3D objects on a 2D image plane. While state-of-the-art 2D generative models like GANs show unprecedented quality in modeling the natural image manifold, it is unclear whether they implicitly capture the underlying 3D object structures. And if so, how could we exploit such knowledge to recover the 3D shapes of objects in the images? To answer these questions, in this work, we present the first attempt to directly mine 3D geometric cues from an off-the-shelf 2D GAN that is trained on RGB images only. Through our investigation, we found that such a pre-trained GAN indeed contains rich 3D knowledge and thus can be used to recover 3D shape from a single 2D image in an unsupervised manner. The core of our framework is an iterative strategy that explores and exploits diverse viewpoint and lighting variations in the GAN image manifold. The framework does not require 2D keypoint or 3D annotations, or strong assumptions on object shapes (e.g. shapes are symmetric), yet it successfully recovers 3D shapes with high precision for human faces, cats, cars, and buildings. The recovered 3D shapes immediately allow high-quality image editing like relighting and object rotation. We quantitatively demonstrate the effectiveness of our approach compared to previous methods in both 3D shape reconstruction and face rotation. Our code is available at https://github.com/XingangPan/GAN2Shape.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Unsupervised 3D Shape Reconstruction from 2D Image GANs</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=FGqiDsBUKL0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="RmB-88r9dL" data-number="2143">
      <h4>
        <a href="https://openreview.net/forum?id=RmB-88r9dL">
            VCNet and Functional Targeted Regularization For Learning Causal Effects of Continuous Treatments
        </a>
      
        
          <a href="https://openreview.net/pdf?id=RmB-88r9dL" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=lizhen%40uchicago.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="lizhen@uchicago.edu">Lizhen Nie</a>, <a href="https://openreview.net/profile?id=~Mao_Ye11" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mao_Ye11">Mao Ye</a>, <a href="https://openreview.net/profile?id=~qiang_liu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~qiang_liu4">qiang liu</a>, <a href="https://openreview.net/profile?email=nicolae%40galton.uchicago.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="nicolae@galton.uchicago.edu">Dan Nicolae</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 14 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#RmB-88r9dL-details-151" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RmB-88r9dL-details-151"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">causal inference, continuous treatment effect, doubly robustness</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Motivated by the rising abundance of observational data with continuous treatments, we investigate the problem of estimating the average dose-response curve (ADRF). Available parametric methods are limited in their model space, and previous attempts in leveraging neural network to enhance model expressiveness relied on partitioning continuous treatment into blocks and using separate heads for each block; this however produces in practice discontinuous ADRFs. Therefore, the question of how to adapt the structure and training of neural network to estimate ADRFs remains open. This paper makes two important contributions. First, we propose a novel varying coefficient neural network (VCNet) that improves model expressiveness while preserving continuity of the estimated ADRF. Second, to improve finite sample performance, we generalize targeted regularization to obtain a doubly robust estimator of the whole ADRF curve.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a varying coefficient network and a functional targeted regularization for estimating continuous treatment.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="dYeAHXnpWJ4" data-number="1633">
      <h4>
        <a href="https://openreview.net/forum?id=dYeAHXnpWJ4">
            Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability
        </a>
      
        
          <a href="https://openreview.net/pdf?id=dYeAHXnpWJ4" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Suraj_Srinivas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Suraj_Srinivas1">Suraj Srinivas</a>, <a href="https://openreview.net/profile?email=francois.fleuret%40unige.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="francois.fleuret@unige.ch">Francois Fleuret</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#dYeAHXnpWJ4-details-175" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dYeAHXnpWJ4-details-175"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Interpretability, saliency maps, score-matching</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Current methods for the interpretability of discriminative deep neural networks commonly rely on the model's input-gradients, i.e., the gradients of the output logits w.r.t. the inputs. The common assumption is that these input-gradients contain information regarding <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="4" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2223"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D431 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mi>y</mi><mo>∣</mo><mrow><mi mathvariant="bold">x</mi></mrow><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, the model's discriminative capabilities, thus justifying their use for interpretability. However, in this work, we show that these input-gradients can be arbitrarily manipulated as a consequence of the shift-invariance of softmax without changing the discriminative function. This leaves an open question: given that input-gradients can be arbitrary, why are they highly structured and explanatory in standard models?
      
      In this work, we re-interpret the logits of standard softmax-based classifiers as unnormalized log-densities of the data distribution and show that input-gradients can be viewed as gradients of a class-conditional generative model <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="5" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D431 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2223"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mrow><mi mathvariant="bold">x</mi></mrow><mo>∣</mo><mi>y</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> implicit in the discriminative model. This leads us to hypothesize that the highly structured and explanatory nature of input-gradients may be due to the alignment of this class-conditional model <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="6" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D431 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2223"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>p</mi><mrow><mi>θ</mi></mrow></msub><mo stretchy="false">(</mo><mrow><mi mathvariant="bold">x</mi></mrow><mo>∣</mo><mi>y</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> with that of the ground truth data distribution <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="7" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c64"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c61"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D431 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2223"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>p</mi><mrow><mtext>data</mtext></mrow></msub><mo stretchy="false">(</mo><mrow><mi mathvariant="bold">x</mi></mrow><mo>∣</mo><mi>y</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>. We test this hypothesis by studying the effect of density alignment on gradient explanations. To achieve this density alignment, we use an algorithm called score-matching, and propose novel approximations to this algorithm to enable training large-scale models.
      
      Our experiments show that improving the alignment of the implicit density model with the data distribution enhances gradient structure and explanatory power while reducing this alignment has the opposite effect. This also leads us to conjecture that unintended density alignment in standard neural network training may explain the highly structured nature of input-gradients observed in practice. Overall, our finding that input-gradients capture information regarding an implicit generative model implies that we need to re-think their use for interpreting discriminative models.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Input-gradients in discriminative neural net models capture information regarding an implicit density model, rather than that of the underlying discriminative model which it is intended to explain.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="uAX8q61EVRu" data-number="437">
      <h4>
        <a href="https://openreview.net/forum?id=uAX8q61EVRu">
            Neural Synthesis of Binaural Speech From Mono Audio
        </a>
      
        
          <a href="https://openreview.net/pdf?id=uAX8q61EVRu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alexander_Richard1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Richard1">Alexander Richard</a>, <a href="https://openreview.net/profile?id=~Dejan_Markovic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dejan_Markovic1">Dejan Markovic</a>, <a href="https://openreview.net/profile?id=~Israel_D._Gebru1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Israel_D._Gebru1">Israel D. Gebru</a>, <a href="https://openreview.net/profile?id=~Steven_Krenn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_Krenn1">Steven Krenn</a>, <a href="https://openreview.net/profile?id=~Gladstone_Alexander_Butler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gladstone_Alexander_Butler1">Gladstone Alexander Butler</a>, <a href="https://openreview.net/profile?id=~Fernando_Torre1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fernando_Torre1">Fernando Torre</a>, <a href="https://openreview.net/profile?id=~Yaser_Sheikh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yaser_Sheikh1">Yaser Sheikh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#uAX8q61EVRu-details-152" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uAX8q61EVRu-details-152"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">binaural audio, sound spatialization, neural sound synthesis, binaural speech, speech processing, speech generation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present a neural rendering approach for binaural sound synthesis that can produce realistic and spatially accurate binaural sound in realtime. The network takes, as input, a single-channel audio source and synthesizes, as output, two-channel binaural sound, conditioned on the relative position and orientation of the listener with respect to the source. We investigate deficiencies of the l2-loss on raw waveforms in a theoretical analysis and introduce an improved loss that overcomes these limitations. In an empirical evaluation, we establish that our approach is the first to generate spatially accurate waveform outputs (as measured by real recordings) and outperforms existing approaches by a considerable margin, both quantitatively and in a perceptual study. Dataset and code are available online.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an end-to-end approach to neural binaural sound synthesis that for the first time outperforms DSP-based methods in a qualitative evaluation and in a perceptual study.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=uAX8q61EVRu&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="a-xFK8Ymz5J" data-number="1087">
      <h4>
        <a href="https://openreview.net/forum?id=a-xFK8Ymz5J">
            DiffWave: A Versatile Diffusion Model for Audio Synthesis
        </a>
      
        
          <a href="https://openreview.net/pdf?id=a-xFK8Ymz5J" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=z4kong%40eng.ucsd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="z4kong@eng.ucsd.edu">Zhifeng Kong</a>, <a href="https://openreview.net/profile?id=~Wei_Ping1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Ping1">Wei Ping</a>, <a href="https://openreview.net/profile?id=~Jiaji_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiaji_Huang1">Jiaji Huang</a>, <a href="https://openreview.net/profile?email=kexinzhao%40baidu.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="kexinzhao@baidu.com">Kexin Zhao</a>, <a href="https://openreview.net/profile?id=~Bryan_Catanzaro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bryan_Catanzaro1">Bryan Catanzaro</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#a-xFK8Ymz5J-details-896" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="a-xFK8Ymz5J-details-896"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">diffusion probabilistic models, audio synthesis, speech synthesis, generative models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">DiffWave is a versatile diffusion probabilistic model for waveform generation, which matches the state-of-the-art neural vocoder in terms of quality and can generate abundant realistic voices in time-domain without any conditional information.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="YicbFdNTTy" data-number="1909">
      <h4>
        <a href="https://openreview.net/forum?id=YicbFdNTTy">
            An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
        </a>
      
        
          <a href="https://openreview.net/pdf?id=YicbFdNTTy" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alexey_Dosovitskiy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexey_Dosovitskiy1">Alexey Dosovitskiy</a>, <a href="https://openreview.net/profile?id=~Lucas_Beyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lucas_Beyer1">Lucas Beyer</a>, <a href="https://openreview.net/profile?id=~Alexander_Kolesnikov2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Kolesnikov2">Alexander Kolesnikov</a>, <a href="https://openreview.net/profile?id=~Dirk_Weissenborn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dirk_Weissenborn1">Dirk Weissenborn</a>, <a href="https://openreview.net/profile?id=~Xiaohua_Zhai2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaohua_Zhai2">Xiaohua Zhai</a>, <a href="https://openreview.net/profile?id=~Thomas_Unterthiner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Unterthiner1">Thomas Unterthiner</a>, <a href="https://openreview.net/profile?id=~Mostafa_Dehghani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mostafa_Dehghani1">Mostafa Dehghani</a>, <a href="https://openreview.net/profile?id=~Matthias_Minderer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthias_Minderer1">Matthias Minderer</a>, <a href="https://openreview.net/profile?id=~Georg_Heigold1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Georg_Heigold1">Georg Heigold</a>, <a href="https://openreview.net/profile?id=~Sylvain_Gelly1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sylvain_Gelly1">Sylvain Gelly</a>, <a href="https://openreview.net/profile?id=~Jakob_Uszkoreit1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jakob_Uszkoreit1">Jakob Uszkoreit</a>, <a href="https://openreview.net/profile?id=~Neil_Houlsby1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neil_Houlsby1">Neil Houlsby</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>19 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#YicbFdNTTy-details-830" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YicbFdNTTy-details-830"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">computer vision, image recognition, self-attention, transformer, large-scale training</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Transformers applied directly to image patches and pre-trained on large datasets work really well on image classification.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="RGJbergVIoO" data-number="3693">
      <h4>
        <a href="https://openreview.net/forum?id=RGJbergVIoO">
            On the mapping between Hopfield networks and Restricted Boltzmann Machines
        </a>
      
        
          <a href="https://openreview.net/pdf?id=RGJbergVIoO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Matthew_Smart1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthew_Smart1">Matthew Smart</a>, <a href="https://openreview.net/profile?email=zilmana%40physics.utoronto.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="zilmana@physics.utoronto.ca">Anton Zilman</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 06 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#RGJbergVIoO-details-723" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RGJbergVIoO-details-723"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Hopfield Networks, Restricted Boltzmann Machines, Statistical Physics</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two important models at the interface of statistical physics, machine learning, and neuroscience. Recently, there has been interest in the relationship between HNs and RBMs, due to their similarity under the statistical mechanics formalism. An exact mapping between HNs and RBMs has been previously noted for the special case of orthogonal (“uncorrelated”) encoded patterns. We present here an exact mapping in the case of correlated pattern HNs, which are more broadly applicable to existing datasets. Specifically, we show that any HN with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="8" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></mjx-assistive-mml></mjx-container> binary variables and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="9" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>&lt;</mo><mi>N</mi></math></mjx-assistive-mml></mjx-container> potentially correlated binary patterns can be transformed into an RBM with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="10" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></mjx-assistive-mml></mjx-container> binary visible variables and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="11" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></mjx-assistive-mml></mjx-container> gaussian hidden variables. We outline the conditions under which the reverse mapping exists, and conduct experiments on the MNIST dataset which suggest the mapping provides a useful initialization to the RBM weights. We discuss extensions, the potential importance of this correspondence for the training of RBMs, and for understanding the performance of feature extraction methods which utilize RBMs.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Hopfield networks with correlated patterns can be mapped to Restricted Boltzmann Machines with orthogonal weights. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="cPZOyoDloxl" data-number="1385">
      <h4>
        <a href="https://openreview.net/forum?id=cPZOyoDloxl">
            SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments
        </a>
      
        
          <a href="https://openreview.net/pdf?id=cPZOyoDloxl" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Glen_Berseth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Glen_Berseth1">Glen Berseth</a>, <a href="https://openreview.net/profile?email=dangengdg%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="dangengdg@berkeley.edu">Daniel Geng</a>, <a href="https://openreview.net/profile?id=~Coline_Manon_Devin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Coline_Manon_Devin1">Coline Manon Devin</a>, <a href="https://openreview.net/profile?id=~Nicholas_Rhinehart1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_Rhinehart1">Nicholas Rhinehart</a>, <a href="https://openreview.net/profile?id=~Chelsea_Finn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chelsea_Finn1">Chelsea Finn</a>, <a href="https://openreview.net/profile?id=~Dinesh_Jayaraman2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dinesh_Jayaraman2">Dinesh Jayaraman</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#cPZOyoDloxl-details-611" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="cPZOyoDloxl-details-611"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Every living organism struggles against disruptive environmental forces to carve out and maintain an orderly niche. We propose that such a struggle to achieve and preserve order might offer a principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing reinforcement learning (SMiRL). SMiRL alternates between learning a density model to evaluate the surprise of a stimulus, and improving the policy to seek more predictable stimuli. The policy seeks out stable and repeatable situations that counteract the environment's prevailing sources of entropy. This might include avoiding other hostile agents, or finding a stable, balanced pose for a bipedal robot in the face of disturbance forces. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls, and navigate to escape enemies in a maze without any task-specific reward supervision. We further show that SMiRL can be used together with standard task rewards to accelerate reward-driven learning.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Using Bayesian surprise as an unsupervised intrinsic reward function to learn complex behaviors in unstable environments.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="0XXpJ4OtjW" data-number="2502">
      <h4>
        <a href="https://openreview.net/forum?id=0XXpJ4OtjW">
            Evolving Reinforcement Learning Algorithms
        </a>
      
        
          <a href="https://openreview.net/pdf?id=0XXpJ4OtjW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~John_D_Co-Reyes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_D_Co-Reyes1">John D Co-Reyes</a>, <a href="https://openreview.net/profile?id=~Yingjie_Miao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yingjie_Miao1">Yingjie Miao</a>, <a href="https://openreview.net/profile?id=~Daiyi_Peng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daiyi_Peng1">Daiyi Peng</a>, <a href="https://openreview.net/profile?email=ereal%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ereal@google.com">Esteban Real</a>, <a href="https://openreview.net/profile?id=~Quoc_V_Le1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Quoc_V_Le1">Quoc V Le</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>, <a href="https://openreview.net/profile?id=~Honglak_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Honglak_Lee2">Honglak Lee</a>, <a href="https://openreview.net/profile?id=~Aleksandra_Faust1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aleksandra_Faust1">Aleksandra Faust</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#0XXpJ4OtjW-details-682" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0XXpJ4OtjW-details-682"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, evolutionary algorithms, meta-learning, genetic programming</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a method for meta-learning reinforcement learning algorithms by searching over the space of computational graphs which compute the loss function for a value-based model-free RL agent to optimize. The learned algorithms are domain-agnostic and can generalize to new environments not seen during training. Our method can both learn from scratch and bootstrap off known existing algorithms, like DQN, enabling interpretable modifications which improve performance. Learning from scratch on simple classical control and gridworld tasks, our method rediscovers the temporal-difference (TD) algorithm. Bootstrapped from DQN, we highlight two learned algorithms which obtain good generalization performance over other classical control tasks, gridworld type tasks, and Atari games. The analysis of the learned algorithm behavior shows resemblance to recently proposed RL algorithms that address overestimation in value-based methods.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We meta-learn RL algorithms by evolving computational graphs which compute the loss function for a value-based model-free RL agent to optimize.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=0XXpJ4OtjW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="wb3wxCObbRT" data-number="2209">
      <h4>
        <a href="https://openreview.net/forum?id=wb3wxCObbRT">
            Growing Efficient Deep Networks by Structured Continuous Sparsification
        </a>
      
        
          <a href="https://openreview.net/pdf?id=wb3wxCObbRT" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xin_Yuan5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xin_Yuan5">Xin Yuan</a>, <a href="https://openreview.net/profile?id=~Pedro_Henrique_Pamplona_Savarese1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pedro_Henrique_Pamplona_Savarese1">Pedro Henrique Pamplona Savarese</a>, <a href="https://openreview.net/profile?id=~Michael_Maire1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Maire1">Michael Maire</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#wb3wxCObbRT-details-148" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wb3wxCObbRT-details-148"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning, computer vision, network pruning, neural architecture search</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives.  Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters.  By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training.  For example, we achieve <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="12" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c39"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c37"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>49.7</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> inference FLOPs and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="13" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>47.4</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="14" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>75.2</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> top-1 validation accuracy --- all without any dedicated fine-tuning stage.  Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, and recurrent networks for language modeling, demonstrate that we both train faster and produce more efficient networks than competing architecture pruning or search methods.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an efficient training method that dynamically grows and prunes neural network architectures.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="gZ9hCDWe6ke" data-number="1041">
      <h4>
        <a href="https://openreview.net/forum?id=gZ9hCDWe6ke">
            Deformable DETR: Deformable Transformers for End-to-End Object Detection
        </a>
      
        
          <a href="https://openreview.net/pdf?id=gZ9hCDWe6ke" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xizhou_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xizhou_Zhu1">Xizhou Zhu</a>, <a href="https://openreview.net/profile?id=~Weijie_Su2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weijie_Su2">Weijie Su</a>, <a href="https://openreview.net/profile?email=luotto%40sensetime.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="luotto@sensetime.com">Lewei Lu</a>, <a href="https://openreview.net/profile?email=binli%40ustc.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="binli@ustc.edu.cn">Bin Li</a>, <a href="https://openreview.net/profile?id=~Xiaogang_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaogang_Wang2">Xiaogang Wang</a>, <a href="https://openreview.net/profile?id=~Jifeng_Dai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jifeng_Dai1">Jifeng Dai</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#gZ9hCDWe6ke-details-678" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gZ9hCDWe6ke-details-678"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Efficient Attention Mechanism, Deformation Modeling, Multi-scale Representation, End-to-End Object Detection</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="15" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></mjx-assistive-mml></mjx-container> less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Deformable DETR is an efficient and fast-converging end-to-end object detector. It mitigates the high complexity and slow convergence issues of DETR via a novel sampling-based efficient attention mechanism. </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=gZ9hCDWe6ke&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="NzTU59SYbNq" data-number="2114">
      <h4>
        <a href="https://openreview.net/forum?id=NzTU59SYbNq">
            EigenGame: PCA as a Nash Equilibrium
        </a>
      
        
          <a href="https://openreview.net/pdf?id=NzTU59SYbNq" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ian_Gemp1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ian_Gemp1">Ian Gemp</a>, <a href="https://openreview.net/profile?id=~Brian_McWilliams2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brian_McWilliams2">Brian McWilliams</a>, <a href="https://openreview.net/profile?id=~Claire_Vernade1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Claire_Vernade1">Claire Vernade</a>, <a href="https://openreview.net/profile?id=~Thore_Graepel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thore_Graepel1">Thore Graepel</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#NzTU59SYbNq-details-250" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NzTU59SYbNq-details-250"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">pca, principal components analysis, nash, games, eigendecomposition, svd, singular value decomposition</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present a novel view on principal components analysis as a competitive game in which each approximate eigenvector is controlled by a player whose goal is to maximize their own utility function. We analyze the properties of this PCA game and the behavior of its gradient based updates. The resulting algorithm---which combines elements from Oja's rule with a  generalized Gram-Schmidt orthogonalization---is naturally decentralized and hence parallelizable through message passing. We demonstrate the scalability of the algorithm with experiments on large image datasets and neural network activations. We discuss how this new view of PCA as a differentiable game can lead to further algorithmic developments and insights.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We formulate the solution to PCA as the Nash of a suitable game with accompanying algorithm that we demonstrate on a 200TB dataset.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=NzTU59SYbNq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="kmG8vRXTFv" data-number="761">
      <h4>
        <a href="https://openreview.net/forum?id=kmG8vRXTFv">
            Augmenting Physical Models with Deep Networks for Complex Dynamics Forecasting
        </a>
      
        
          <a href="https://openreview.net/pdf?id=kmG8vRXTFv" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yuan_Yin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuan_Yin1">Yuan Yin</a>, <a href="https://openreview.net/profile?id=~Vincent_LE_GUEN1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vincent_LE_GUEN1">Vincent LE GUEN</a>, <a href="https://openreview.net/profile?id=~J%C3%A9r%C3%A9mie_DONA2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jérémie_DONA2">Jérémie DONA</a>, <a href="https://openreview.net/profile?id=~Emmanuel_de_Bezenac1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emmanuel_de_Bezenac1">Emmanuel de Bezenac</a>, <a href="https://openreview.net/profile?id=~Ibrahim_Ayed1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ibrahim_Ayed1">Ibrahim Ayed</a>, <a href="https://openreview.net/profile?id=~Nicolas_THOME2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicolas_THOME2">Nicolas THOME</a>, <a href="https://openreview.net/profile?id=~patrick_gallinari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~patrick_gallinari1">patrick gallinari</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#kmG8vRXTFv-details-848" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kmG8vRXTFv-details-848"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">spatio-temporal forecasting, deep learning, physics, differential equations, hybrid systems</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Forecasting complex dynamical phenomena in settings where only partial knowledge of their dynamics is available is a prevalent problem across various scientific fields. While purely data-driven approaches are arguably insufficient in this context, standard physical modeling based approaches tend to be over-simplistic, inducing non-negligible errors. In this work, we introduce the APHYNITY framework, a principled approach for augmenting incomplete physical dynamics described by differential equations with deep data-driven models. It consists in decomposing the dynamics into two components: a physical component accounting for the dynamics for which we have some prior knowledge, and a data-driven component accounting for errors of the physical model. The learning problem is carefully formulated such that the physical model explains as much of the data as possible, while the data-driven component only describes information that cannot be captured by the physical model, no more, no less. This not only provides the existence and uniqueness for this decomposition, but also ensures interpretability and benefits generalization. Experiments made on three important use cases, each representative of a different family of phenomena, i.e. reaction-diffusion equations, wave equations and the non-linear damped pendulum, show that APHYNITY can efficiently leverage approximate physical models to accurately forecast the evolution of the system and correctly identify relevant physical parameters.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a new principled framework for combining physical models with deep data-driven networks, for which we provide theoretical decomposition guarantees.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Mos9F9kDwkz" data-number="3496">
      <h4>
        <a href="https://openreview.net/forum?id=Mos9F9kDwkz">
            Complex Query Answering with Neural Link Predictors
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Mos9F9kDwkz" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=erik.arakelyan.18%40alumni.ucl.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="erik.arakelyan.18@alumni.ucl.ac.uk">Erik Arakelyan</a>, <a href="https://openreview.net/profile?email=dfdazac%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dfdazac@gmail.com">Daniel Daza</a>, <a href="https://openreview.net/profile?id=~Pasquale_Minervini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pasquale_Minervini1">Pasquale Minervini</a>, <a href="https://openreview.net/profile?id=~Michael_Cochez2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Cochez2">Michael Cochez</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Mos9F9kDwkz-details-717" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Mos9F9kDwkz-details-717"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">neural link prediction, complex query answering</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Neural link predictors are immensely useful for identifying missing edges in large scale Knowledge Graphs. However, it is still not clear how to use these models for answering more complex queries that arise in a number of domains, such as queries using logical conjunctions (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="16" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2227"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∧</mo></math></mjx-assistive-mml></mjx-container>), disjunctions (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="17" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2228"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∨</mo></math></mjx-assistive-mml></mjx-container>) and existential quantifiers (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="18" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-n"><mjx-c class="mjx-c2203"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">∃</mi></math></mjx-assistive-mml></mjx-container>), while accounting for missing edges. In this work, we propose a framework for efficiently answering complex queries on incomplete Knowledge Graphs. We translate each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor. We then analyse two solutions to the optimisation problem, including gradient-based and combinatorial search. In our experiments, the proposed approach produces more accurate results than state-of-the-art methods --- black-box neural models trained on millions of generated queries --- without the need of training on a large and diverse set of complex queries. Using orders of magnitude less training data, we obtain relative improvements ranging from 8% up to 40% in Hits@3 across different knowledge graphs containing factual information. Finally, we demonstrate that it is possible to explain the outcome of our model in terms of the intermediate solutions identified for each of the complex query atoms. All our source code and datasets are available online, at https://github.com/uclnlp/cqd.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show how to answer complex queries by answering their sub-queries via neural link predictors, aggregating results via t-norms and t-conorms, and identifying the optimal variable substitutions by solving an optimisation problem.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="EbIDjBynYJ8" data-number="218">
      <h4>
        <a href="https://openreview.net/forum?id=EbIDjBynYJ8">
            Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding
        </a>
      
        
          <a href="https://openreview.net/pdf?id=EbIDjBynYJ8" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~David_A._Klindt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_A._Klindt1">David A. Klindt</a>, <a href="https://openreview.net/profile?id=~Lukas_Schott2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lukas_Schott2">Lukas Schott</a>, <a href="https://openreview.net/profile?id=~Yash_Sharma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yash_Sharma1">Yash Sharma</a>, <a href="https://openreview.net/profile?id=~Ivan_Ustyuzhaninov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ivan_Ustyuzhaninov1">Ivan Ustyuzhaninov</a>, <a href="https://openreview.net/profile?id=~Wieland_Brendel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wieland_Brendel1">Wieland Brendel</a>, <a href="https://openreview.net/profile?id=~Matthias_Bethge1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthias_Bethge1">Matthias Bethge</a>, <a href="https://openreview.net/profile?email=dpaiton%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dpaiton@gmail.com">Dylan Paiton</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#EbIDjBynYJ8-details-962" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EbIDjBynYJ8-details-962"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">disentanglement, independent component analysis, natural scene statistics</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Disentangling the underlying generative factors from complex data has so far been limited to carefully constructed scenarios. We propose a path towards natural data by first showing that the statistics of natural data provide enough structure to enable disentanglement, both theoretically and empirically. Specifically, we provide evidence that objects in natural movies undergo transitions that are typically small in magnitude with occasional large jumps, which is characteristic of a temporally sparse distribution. To address this finding we provide a novel proof that relies on a sparse prior on temporally adjacent observations to recover the true latent variables up to permutations and sign flips, directly providing a stronger result than previous work. We show that equipping practical estimation methods with our prior often surpasses the current state-of-the-art on several established benchmark datasets without any impractical assumptions, such as knowledge of the number of changing generative factors. Furthermore, we contribute two new benchmarks, Natural Sprites and KITTI Masks, which integrate the measured natural dynamics to enable disentanglement evaluation with more realistic datasets. We leverage these benchmarks to test our theory, demonstrating improved performance. We also identify non-obvious challenges for current methods in scaling to more natural domains. Taken together our work addresses key issues in disentanglement research for moving towards more natural settings. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Our work addresses key issues in disentanglement research for moving towards more natural settings. </span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="O3Y56aqpChA" data-number="1260">
      <h4>
        <a href="https://openreview.net/forum?id=O3Y56aqpChA">
            Self-training For Few-shot Transfer Across Extreme Task Differences
        </a>
      
        
          <a href="https://openreview.net/pdf?id=O3Y56aqpChA" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Cheng_Perng_Phoo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cheng_Perng_Phoo1">Cheng Perng Phoo</a>, <a href="https://openreview.net/profile?id=~Bharath_Hariharan3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bharath_Hariharan3">Bharath Hariharan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#O3Y56aqpChA-details-805" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="O3Y56aqpChA-details-805"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">few-shot learning, self-training, cross-domain few-shot learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Most few-shot learning techniques are pre-trained on a large, labeled “base dataset”. In problem domains where such large labeled datasets are not available for pre-training (e.g., X-ray, satellite images), one must resort to pre-training in a different “source” problem domain (e.g., ImageNet), which can be very different from the desired target task. Traditional few-shot and transfer learning techniques fail in the presence of such extreme differences between the source and target tasks. In this paper, we present a simple and effective solution to tackle this extreme domain gap: self-training a source domain representation on unlabeled data from the target domain. We show that this improves one-shot performance on the target domain by 2.9 points on average on the challenging BSCD-FSL benchmark consisting of datasets from multiple domains.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Self-training a source domain classifier on unlabeled data from the target domain improves cross-domain few-shot transfer. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="PxTIG12RRHS" data-number="2561">
      <h4>
        <a href="https://openreview.net/forum?id=PxTIG12RRHS">
            Score-Based Generative Modeling through Stochastic Differential Equations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=PxTIG12RRHS" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yang_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Song1">Yang Song</a>, <a href="https://openreview.net/profile?id=~Jascha_Sohl-Dickstein2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jascha_Sohl-Dickstein2">Jascha Sohl-Dickstein</a>, <a href="https://openreview.net/profile?id=~Diederik_P_Kingma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Diederik_P_Kingma1">Diederik P Kingma</a>, <a href="https://openreview.net/profile?id=~Abhishek_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abhishek_Kumar1">Abhishek Kumar</a>, <a href="https://openreview.net/profile?id=~Stefano_Ermon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefano_Ermon1">Stefano Ermon</a>, <a href="https://openreview.net/profile?id=~Ben_Poole1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ben_Poole1">Ben Poole</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Feb 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#PxTIG12RRHS-details-429" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PxTIG12RRHS-details-429"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">generative models, score-based generative models, stochastic differential equations, score matching, diffusion</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. 
      Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="19" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1024</mn><mo>×</mo><mn>1024</mn></math></mjx-assistive-mml></mjx-container> images for the first time from a score-based generative model.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A general framework for training and sampling from score-based models that unifies and generalizes previous methods, allows likelihood computation, and enables controllable generation.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Wj4ODo0uyCF" data-number="3265">
      <h4>
        <a href="https://openreview.net/forum?id=Wj4ODo0uyCF">
            Share or Not? Learning to Schedule Language-Specific Capacity for Multilingual Translation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Wj4ODo0uyCF" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Biao_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Biao_Zhang2">Biao Zhang</a>, <a href="https://openreview.net/profile?id=~Ankur_Bapna1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ankur_Bapna1">Ankur Bapna</a>, <a href="https://openreview.net/profile?id=~Rico_Sennrich1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rico_Sennrich1">Rico Sennrich</a>, <a href="https://openreview.net/profile?id=~Orhan_Firat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Orhan_Firat1">Orhan Firat</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Wj4ODo0uyCF-details-503" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Wj4ODo0uyCF-details-503"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">language-specific modeling, conditional computation, multilingual translation, multilingual transformer</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Using a mix of shared and language-specific (LS) parameters has shown promise in multilingual neural machine  translation (MNMT), but the question of when and where LS capacity matters most is still under-studied. We offer such a study by proposing conditional language-specific routing (CLSR). CLSR employs hard binary gates conditioned on token representations to dynamically select LS or shared paths. By manipulating these gates, it can schedule LS capacity across sub-layers in MNMT subject to the guidance of translation signals and budget constraints. Moreover, CLSR can easily scale up to massively multilingual settings. Experiments with Transformer on OPUS-100 and WMT datasets show that: 1) MNMT is sensitive to both the amount and the position of LS modeling: distributing 10%-30% LS computation to the top and/or bottom encoder/decoder layers delivers the best performance; and 2) one-to-many translation benefits more from CLSR compared to many-to-one translation, particularly with unbalanced training data. Our study further verifies the trade-off between the shared capacity and LS capacity for multilingual translation. We corroborate our analysis by confirming the soundness of our findings as foundation of our improved multilingual Transformers. Source code and models are available at https://github.com/googleinterns/cct-m4.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We investigate and improve parameter-sharing strategies in multilingual Transformers by utilizing conditional computation.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="yWkP7JuHX1" data-number="2477">
      <h4>
        <a href="https://openreview.net/forum?id=yWkP7JuHX1">
            Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering
        </a>
      
        
          <a href="https://openreview.net/pdf?id=yWkP7JuHX1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yuxuan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuxuan_Zhang1">Yuxuan Zhang</a>, <a href="https://openreview.net/profile?id=~Wenzheng_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenzheng_Chen1">Wenzheng Chen</a>, <a href="https://openreview.net/profile?id=~Huan_Ling1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huan_Ling1">Huan Ling</a>, <a href="https://openreview.net/profile?id=~Jun_Gao3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jun_Gao3">Jun Gao</a>, <a href="https://openreview.net/profile?id=~Yinan_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yinan_Zhang2">Yinan Zhang</a>, <a href="https://openreview.net/profile?id=~Antonio_Torralba1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Antonio_Torralba1">Antonio Torralba</a>, <a href="https://openreview.net/profile?id=~Sanja_Fidler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanja_Fidler1">Sanja Fidler</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#yWkP7JuHX1-details-307" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="yWkP7JuHX1-details-307"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Differentiable rendering, inverse graphics, GANs</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Differentiable rendering has paved the way to training neural networks to perform “inverse graphics” tasks such as predicting 3D geometry from monocular photographs. To train high performing models, most of the current approaches rely on multi-view imagery which are not readily available in practice.  Recent Generative Adversarial Networks (GANs) that synthesize images, in contrast, seem to acquire 3D knowledge implicitly during training: object viewpoints can be manipulated by simply manipulating the latent codes. However, these latent codes often lack further physical interpretation and thus GANs cannot easily be inverted to perform explicit 3D reasoning. In this paper, we aim to extract and disentangle 3D knowledge learned by generative models by utilizing differentiable renderers. Key to our approach is to exploit GANs as a multi-view data generator to train an inverse graphics network using an off-the-shelf differentiable renderer, and the trained inverse graphics network as a teacher to disentangle the GAN's latent code into interpretable 3D properties. The entire architecture is trained iteratively using cycle consistency losses. We show that our approach significantly outperforms state-of-the-art inverse graphics networks trained on existing datasets, both quantitatively and via user studies. We further showcase the disentangled GAN as a controllable 3D “neural renderer", complementing traditional graphics renderers.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We marry generative models with differentiable rendering to extract and disentangle 3D knowledge learned implicitly by generative image synthesis models</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=yWkP7JuHX1&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="UH-cmocLJC" data-number="700">
      <h4>
        <a href="https://openreview.net/forum?id=UH-cmocLJC">
            How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=UH-cmocLJC" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Keyulu_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Keyulu_Xu1">Keyulu Xu</a>, <a href="https://openreview.net/profile?id=~Mozhi_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mozhi_Zhang1">Mozhi Zhang</a>, <a href="https://openreview.net/profile?id=~Jingling_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jingling_Li1">Jingling Li</a>, <a href="https://openreview.net/profile?id=~Simon_Shaolei_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Shaolei_Du1">Simon Shaolei Du</a>, <a href="https://openreview.net/profile?id=~Ken-Ichi_Kawarabayashi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ken-Ichi_Kawarabayashi1">Ken-Ichi Kawarabayashi</a>, <a href="https://openreview.net/profile?id=~Stefanie_Jegelka3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefanie_Jegelka3">Stefanie Jegelka</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#UH-cmocLJC-details-331" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UH-cmocLJC-details-331"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">extrapolation, deep learning, out-of-distribution, graph neural networks, deep learning theory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study how neural networks trained by gradient descent  extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), do not extrapolate well in certain simple tasks, Graph Neural Networks (GNNs) -- structured networks with MLP modules -- have shown some success in more complex tasks.  Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most nonlinear functions. But, they can provably learn a linear target function when the training distribution is sufficiently diverse. Second, in connection to analyzing the successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-specific non-linearities in the architecture or features. Our theoretical analysis builds on a connection of over-parameterized networks to the neural tangent kernel.  Empirically, our theory holds across different training settings.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We study how neural networks trained by gradient descent extrapolate.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Ud3DSz72nYR" data-number="3005">
      <h4>
        <a href="https://openreview.net/forum?id=Ud3DSz72nYR">
            Contrastive Explanations for Reinforcement Learning via Embedded Self Predictions
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Ud3DSz72nYR" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhengxian_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhengxian_Lin1">Zhengxian Lin</a>, <a href="https://openreview.net/profile?id=~Kin-Ho_Lam1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kin-Ho_Lam1">Kin-Ho Lam</a>, <a href="https://openreview.net/profile?id=~Alan_Fern1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alan_Fern1">Alan Fern</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Ud3DSz72nYR-details-393" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ud3DSz72nYR-details-393"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Explainable AI, Deep Reinforcement Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We investigate a deep reinforcement learning (RL) architecture that supports explaining why a learned agent prefers one action over another. The key idea is to learn action-values that are directly represented via human-understandable properties of expected futures. This is realized via the embedded self-prediction (ESP) model, which learns said properties in terms of human provided features. Action preferences can then be explained by contrasting the future properties predicted for each action. To address cases where there are a large number of features, we develop a novel method for computing minimal sufficient explanations from an ESP. Our case studies in three domains, including a complex strategy game, show that ESP models can be effectively learned and support insightful explanations. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduced the embedded self-prediction (ESP) model for producing meaningful and sound contrastive explanations for RL agents.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ud3DSz72nYR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="rJA5Pz7lHKb" data-number="2584">
      <h4>
        <a href="https://openreview.net/forum?id=rJA5Pz7lHKb">
            Improved Autoregressive Modeling with Distribution Smoothing
        </a>
      
        
          <a href="https://openreview.net/pdf?id=rJA5Pz7lHKb" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Chenlin_Meng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chenlin_Meng1">Chenlin Meng</a>, <a href="https://openreview.net/profile?id=~Jiaming_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiaming_Song1">Jiaming Song</a>, <a href="https://openreview.net/profile?id=~Yang_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Song1">Yang Song</a>, <a href="https://openreview.net/profile?id=~Shengjia_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shengjia_Zhao1">Shengjia Zhao</a>, <a href="https://openreview.net/profile?id=~Stefano_Ermon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefano_Ermon1">Stefano Ermon</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 21 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#rJA5Pz7lHKb-details-414" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rJA5Pz7lHKb-details-414"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">generative models, autoregressive models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While autoregressive models excel at image compression, their sample quality is often lacking. Although not realistic, generated images often have high likelihood according to the model, resembling the case of adversarial examples. Inspired by a successful adversarial defense method, we incorporate randomized smoothing into autoregressive generative modeling. We first model a smoothed version of the data distribution, and then reverse the smoothing process to recover the original data distribution. This procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world image datasets while obtaining competitive likelihoods on synthetic datasets.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="wWK7yXkULyh" data-number="2434">
      <h4>
        <a href="https://openreview.net/forum?id=wWK7yXkULyh">
            MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training
        </a>
      
        
          <a href="https://openreview.net/pdf?id=wWK7yXkULyh" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Beidi_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Beidi_Chen1">Beidi Chen</a>, <a href="https://openreview.net/profile?id=~Zichang_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zichang_Liu1">Zichang Liu</a>, <a href="https://openreview.net/profile?id=~Binghui_Peng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Binghui_Peng1">Binghui Peng</a>, <a href="https://openreview.net/profile?id=~Zhaozhuo_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhaozhuo_Xu1">Zhaozhuo Xu</a>, <a href="https://openreview.net/profile?id=~Jonathan_Lingjie_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Lingjie_Li1">Jonathan Lingjie Li</a>, <a href="https://openreview.net/profile?id=~Tri_Dao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tri_Dao1">Tri Dao</a>, <a href="https://openreview.net/profile?id=~Zhao_Song3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhao_Song3">Zhao Song</a>, <a href="https://openreview.net/profile?id=~Anshumali_Shrivastava1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anshumali_Shrivastava1">Anshumali Shrivastava</a>, <a href="https://openreview.net/profile?id=~Christopher_Re1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Re1">Christopher Re</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#wWK7yXkULyh-details-193" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wWK7yXkULyh-details-193"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Large-scale Deep Learning, Large-scale Machine Learning, Efficient Training, Randomized Algorithms</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent advances by practitioners in the deep learning community have breathed new life into Locality Sensitive Hashing (LSH), using it to reduce memory and time bottlenecks in neural network (NN) training. However, while LSH has sub-linear guarantees for approximate near-neighbor search in theory, it is known to have inefficient query time in practice due to its use of random hash functions. Moreover, when model parameters are changing, LSH suffers from update overhead. This work is motivated by an observation that model parameters evolve slowly, such that the changes do not always require an LSH update to maintain performance. This phenomenon points to the potential for a reduction in update time and allows for a modified learnable version of data-dependent LSH to improve query time at a low cost. We use the above insights to build MONGOOSE, an end-to-end LSH framework for efficient NN training. In particular, MONGOOSE is equipped with a scheduling algorithm to adaptively perform LSH updates with provable guarantees and learnable hash functions to improve query efficiency. Empirically, we validate MONGOOSE on large-scale deep learning models for recommendation systems and language modeling. We find that it achieves up to 8% better accuracy compared to previous LSH approaches, with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="20" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>6.5</mn><mo>×</mo></math></mjx-assistive-mml></mjx-container> speed-up and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="21" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c36"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>6</mn><mo>×</mo></math></mjx-assistive-mml></mjx-container> reduction in memory usage.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose MONGOOSE,  a learnable LSH framework for efficient neural network training.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=wWK7yXkULyh&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="3AOj0RCNC2" data-number="857">
      <h4>
        <a href="https://openreview.net/forum?id=3AOj0RCNC2">
            Gradient Projection Memory for Continual Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=3AOj0RCNC2" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Gobinda_Saha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gobinda_Saha1">Gobinda Saha</a>, <a href="https://openreview.net/profile?id=~Isha_Garg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Isha_Garg1">Isha Garg</a>, <a href="https://openreview.net/profile?id=~Kaushik_Roy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaushik_Roy1">Kaushik Roy</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#3AOj0RCNC2-details-637" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3AOj0RCNC2-details-637"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Continual Learning, Representation Learning, Computer Vision, Deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The ability to learn continually without forgetting the past tasks is a desired attribute for artificial learning systems. Existing approaches to enable such learning in artificial neural networks usually rely on network growth, importance based weight update or replay of old data from the memory. In contrast, we propose a novel approach where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for the past tasks. We find the bases of these subspaces by analyzing network representations (activations) after learning each task with Singular Value Decomposition (SVD) in a single shot manner and store them in the memory as Gradient Projection Memory (GPM). With qualitative and quantitative analyses, we show that such orthogonal gradient descent induces minimum to no interference with the past tasks, thereby mitigates forgetting. We evaluate our algorithm on diverse image classification datasets with short and long sequences of tasks and report better or on-par performance compared to the state-of-the-art approaches. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">To avoid catastrophic forgetting in continual learning, we propose a novel approach where a neural network learns new tasks by taking gradient steps in the orthogonal direction to the gradient subspaces deemed important for past tasks.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="uCY5MuAxcxU" data-number="2860">
      <h4>
        <a href="https://openreview.net/forum?id=uCY5MuAxcxU">
            Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=uCY5MuAxcxU" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhiyuan_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiyuan_Li2">Zhiyuan Li</a>, <a href="https://openreview.net/profile?id=~Yi_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Zhang1">Yi Zhang</a>, <a href="https://openreview.net/profile?id=~Sanjeev_Arora1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanjeev_Arora1">Sanjeev Arora</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#uCY5MuAxcxU-details-187" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uCY5MuAxcxU-details-187"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">sample complexity separation, equivariance, convolutional neural networks, fully-connected</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Convolutional neural networks often dominate fully-connected counterparts in generalization performance, especially on image classification tasks. This is often explained in terms of \textquotedblleft better inductive bias.\textquotedblright\  However, this has not been made mathematically rigorous, and the hurdle is that the sufficiently wide fully-connected net can always simulate the convolutional net. Thus the training algorithm plays a role. The current work describes a natural task on which a provable sample complexity gap can be shown, for standard training algorithms. We construct a single natural distribution on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="22" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.41em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c7B"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-cB1"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c7D"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>d</mi></msup><mo>×</mo><mo fence="false" stretchy="false">{</mo><mo>±</mo><mn>1</mn><mo fence="false" stretchy="false">}</mo></math></mjx-assistive-mml></mjx-container> on which any orthogonal-invariant algorithm (i.e. fully-connected networks trained with most gradient-based methods from gaussian initialization) requires <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="23" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A9"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> samples to generalize while <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="24" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> samples suffice for convolutional architectures. Furthermore, we demonstrate a single target function, learning which on all possible distributions leads to an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="25" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> vs <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="26" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A9"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D700 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><msup><mi>d</mi><mn>2</mn></msup><mrow><mo>/</mo></mrow><mi>ε</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> gap. The proof relies on the fact that SGD on fully-connected network is orthogonal equivariant. Similar results are achieved for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="27" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> regression and adaptive training algorithms, e.g. Adam and AdaGrad, which are only permutation equivariant.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We construct a single natural distribution on which any  fully-connected networks trained with SGD requires \Omega(d^2) samples to generalize while O (1) samples suffices for convolutional architectures.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Pd_oMxH8IlF" data-number="2705">
      <h4>
        <a href="https://openreview.net/forum?id=Pd_oMxH8IlF">
            Iterated learning for emergent systematicity in VQA
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Pd_oMxH8IlF" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ankit_Vani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ankit_Vani1">Ankit Vani</a>, <a href="https://openreview.net/profile?id=~Max_Schwarzer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Max_Schwarzer1">Max Schwarzer</a>, <a href="https://openreview.net/profile?id=~Yuchen_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuchen_Lu1">Yuchen Lu</a>, <a href="https://openreview.net/profile?email=eeshandhekane%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="eeshandhekane@gmail.com">Eeshan Dhekane</a>, <a href="https://openreview.net/profile?id=~Aaron_Courville3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aaron_Courville3">Aaron Courville</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Pd_oMxH8IlF-details-496" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Pd_oMxH8IlF-details-496"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">iterated learning, cultural transmission, neural module network, clevr, shapes, vqa, visual question answering, systematic generalization, compositionality</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We use iterated learning to encourage the emergence of structure in the generated programs for neural module networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="F3s69XzWOia" data-number="1126">
      <h4>
        <a href="https://openreview.net/forum?id=F3s69XzWOia">
            Coupled Oscillatory Recurrent Neural Network (coRNN): An accurate and (gradient) stable architecture for learning long time dependencies
        </a>
      
        
          <a href="https://openreview.net/pdf?id=F3s69XzWOia" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~T._Konstantin_Rusch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~T._Konstantin_Rusch1">T. Konstantin Rusch</a>, <a href="https://openreview.net/profile?id=~Siddhartha_Mishra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siddhartha_Mishra1">Siddhartha Mishra</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Oral</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#F3s69XzWOia-details-689" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="F3s69XzWOia-details-689"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">RNNs, Oscillators, Gradient stability, Long-term dependencies</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Circuits of biological neurons, such as in the functional parts of the brain can be modeled as networks of coupled oscillators. Inspired by the ability of these systems to express a rich set of outputs while keeping (gradients of) state variables bounded, we propose a novel architecture for recurrent neural networks. Our proposed RNN is based on a time-discretization of a system of second-order ordinary differential equations, modeling networks of controlled nonlinear oscillators. We prove precise bounds on the gradients of the hidden states, leading to the mitigation of the exploding and vanishing gradient problem for this RNN. Experiments show that the proposed RNN is comparable in performance to the state of the art on a variety of benchmarks, demonstrating the potential of this architecture to provide stable and accurate RNNs for processing complex sequential data.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A biologically motivated and discretized ODE based RNN for learning long-term dependencies, with rigorous bounds mitigating the exploding and vanishing gradient problem.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=F3s69XzWOia&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
</ul>
</div>
    <div role="tabpanel" class="tab-pane fade  " id="spotlight-presentations">
      
    </div>
    <div role="tabpanel" class="tab-pane fade  " id="poster-presentations">
      
    </div>
    <div role="tabpanel" class="tab-pane fade  " id="withdrawn-rejected-submissions">
      
    </div>
</div>
</div></div></div></main></div></div></div><footer class="sitemap"><div class="container"><div class="row hidden-xs"><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/about">About OpenReview</a></li><li><a href="https://openreview.net/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="https://openreview.net/venues">All Venues</a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/contact">Contact</a></li><li><a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/faq">Frequently Asked Questions</a></li><li><a href="https://openreview.net/legal/terms">Terms of Service</a></li><li><a href="https://openreview.net/legal/privacy">Privacy Policy</a></li></ul></div></div><div class="row visible-xs-block"><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://openreview.net/about">About OpenReview</a></li><li><a href="https://openreview.net/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="https://openreview.net/venues">All Venues</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://openreview.net/faq">Frequently Asked Questions</a></li><li><a href="https://openreview.net/contact">Contact</a></li><li><a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li><li><a href="https://openreview.net/legal/terms">Terms of Service</a></li><li><a href="https://openreview.net/legal/privacy">Privacy Policy</a></li></ul></div></div></div></footer><footer class="sponsor"><div class="container"><div class="row"><div class="col-sm-10 col-sm-offset-1"><p class="text-center">OpenReview is created by the<!-- --> <a href="http://www.iesl.cs.umass.edu/" target="_blank" rel="noopener noreferrer">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.</p></div></div></div></footer><div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog"><div class="modal-dialog "><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button><h3 class="modal-title">Send Feedback</h3></div><div class="modal-body"><p>Enter your feedback below and we'll get back to you as soon as possible.</p><form><div class="form-group"><input type="email" name="from" class="form-control" placeholder="Email" required=""></div><div class="form-group"><input type="text" name="subject" class="form-control" placeholder="Subject"></div><div class="form-group"><textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea></div></form></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button><button type="button" class="btn btn-primary">Send</button></div></div></div></div><div id="bibtex-modal" class="modal fade" tabindex="-1" role="dialog"><div class="modal-dialog "><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button><h3 class="modal-title">BibTeX Record</h3></div><div class="modal-body"><pre class="bibtex-content"></pre><em class="instructions">Click anywhere on the box above to highlight complete record</em></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Done</button></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"groupId":"ICLR.cc/2021/Conference","webfieldCode":"// Webfield Code for ICLR.cc/2021/Conference\nwindow.user = {\"id\":\"guest_1617528810720\",\"isGuest\":true};\n$(function() {\n  var args = {\"id\":\"ICLR.cc/2021/Conference\"};\n  var group = {\"id\":\"ICLR.cc/2021/Conference\",\"cdate\":1587911460292,\"ddate\":null,\"tcdate\":1587911460292,\"tmdate\":1611607761057,\"tddate\":null,\"signatures\":[\"ICLR.cc/2021/Conference\"],\"signatories\":[\"ICLR.cc/2021/Conference\"],\"readers\":[\"everyone\"],\"nonreaders\":[],\"writers\":[\"ICLR.cc/2021/Conference\"],\"members\":[\"ICLR.cc/2021/Conference/Program_Chairs\",\"OpenReview.net/Support\"],\"details\":{\"writable\":false}};\n  var document = null;\n  var window = null;\n  var model = {\n    tokenPayload: function() {\n      return { user: user }\n    }\n  };\n\n  $('#group-container').empty();\n  \n\n  // ------------------------------------\n// Venue homepage template\n//\n// This webfield displays the conference header (#header), the submit button (#invitation),\n// and a tabbed interface for viewing various types of notes.\n// ------------------------------------\n\n// Constants\nvar PARENT_GROUP_ID = 'ICLR.cc/2021';\nvar CONFERENCE_ID = 'ICLR.cc/2021/Conference';\nvar BLIND_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Blind_Submission';\nvar WITHDRAWN_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Withdrawn_Submission';\nvar DESK_REJECTED_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Desk_Rejected_Submission';\nvar DECISION_INVITATION_REGEX = 'ICLR.cc/2021/Conference/Paper.*/-/Decision';\nvar DECISION_HEADING_MAP = {\n  \"Accept (Oral)\": \"Oral Presentations\", \n  \"Accept (Spotlight)\": \"Spotlight Presentations\", \n  \"Accept (Poster)\": \"Poster Presentations\", \n  \"Reject\": \"Withdrawn/Rejected Submissions\"\n  \n};\nvar PAGE_SIZE = 25;\n\nvar HEADER = {\"title\": \"International Conference on Learning Representations\", \"subtitle\": \"ICLR 2021\", \"location\": \"Vienna, Austria\", \"date\": \"May 04 2021\", \"website\": \"https://iclr.cc/\", \"instructions\": \"\u003cp class='dark'\u003ePlease see the venue website for more information.\u003cbr\u003e \u003cp class='dark'\u003e\u003cstrong\u003eAbstract Submission End:\u003c/strong\u003e Sep 28 2020 03:00PM UTC-0\u003c/p\u003e\u003cp class='dark'\u003e\u003cstrong\u003ePaper Submission End:\u003c/strong\u003e Oct 2 2020 03:00PM UTC-0\u003c/p\u003e\", \"deadline\": \"\", \"contact\": \"iclr2021programchairs@googlegroups.com\", \"reviewers_name\": \"Reviewers\", \"area_chairs_name\": \"Area_Chairs\", \"reviewers_id\": \"ICLR.cc/2021/Conference/Reviewers\", \"authors_id\": \"ICLR.cc/2021/Conference/Authors\", \"program_chairs_id\": \"ICLR.cc/2021/Conference/Program_Chairs\", \"area_chairs_id\": \"ICLR.cc/2021/Conference/Area_Chairs\", \"submission_id\": \"ICLR.cc/2021/Conference/-/Submission\", \"blind_submission_id\": \"ICLR.cc/2021/Conference/-/Blind_Submission\", \"withdrawn_submission_id\": \"ICLR.cc/2021/Conference/-/Withdrawn_Submission\", \"desk_rejected_submission_id\": \"ICLR.cc/2021/Conference/-/Desk_Rejected_Submission\", \"public\": true};\n\nvar paperDisplayOptions = {\n  pdfLink: true,\n  replyCount: true,\n  showContents: true\n};\n\nvar sections = [];\n\n// Main is the entry point to the webfield code and runs everything\nfunction main() {\n  if (args \u0026\u0026 args.referrer) {\n    OpenBanner.referrerLink(args.referrer);\n  } else if (PARENT_GROUP_ID.length){\n    OpenBanner.venueHomepageLink(PARENT_GROUP_ID);\n  }\n  Webfield.ui.setup('#group-container', CONFERENCE_ID);  // required\n\n  renderConferenceHeader();\n\n  renderConferenceTabs();\n\n  load().then(renderContent).then(Webfield.ui.done);\n}\n\n// Load makes all the API calls needed to get the data to render the page\nfunction load() {\n  var notesP = Webfield.getAll('/notes', {\n    invitation: BLIND_SUBMISSION_ID,\n    details: 'replyCount,invitation,original'\n  });\n\n  var withdrawnNotesP = WITHDRAWN_SUBMISSION_ID ? Webfield.getAll('/notes', {\n    invitation: WITHDRAWN_SUBMISSION_ID,\n    details: 'replyCount,invitation,original'\n  }) : $.Deferred().resolve([]);\n\n  var deskRejectedNotesP = DESK_REJECTED_SUBMISSION_ID ? Webfield.getAll('/notes', {\n    invitation: DESK_REJECTED_SUBMISSION_ID,\n    details: 'replyCount,invitation,original'\n  }) : $.Deferred().resolve([]);\n\n  var decisionNotesP = Webfield.getAll('/notes', {\n    invitation: DECISION_INVITATION_REGEX,\n  });\n\n  var userGroupsP;\n  if (!user || _.startsWith(user.id, 'guest_')) {\n    userGroupsP = $.Deferred().resolve([]);\n  } else {\n    userGroupsP = Webfield.getAll('/groups', {\n      regex: CONFERENCE_ID + '/.*',\n      member: user.id,\n      web: true\n    }).then(function(groups) {\n      if (!groups || !groups.length) {\n        return [];\n      }\n\n      return groups.map(function(g) { return g.id; });\n    });\n  }\n\n  return $.when(notesP, decisionNotesP, withdrawnNotesP, deskRejectedNotesP, userGroupsP);\n}\n\nfunction renderConferenceHeader() {\n  Webfield.ui.venueHeader(HEADER);\n  Webfield.ui.spinner('#notes', { inline: true });\n}\n\nfunction getElementId(decision) {\n  if (!decision) return decision;\n  return decision.replace(/\\W/g, '-')\n    .replace('(', '')\n    .replace(')', '')\n    .toLowerCase();\n}\n\nfunction renderConferenceTabs() {\n  sections.push({\n    heading: 'Your Consoles',\n    id: 'your-consoles',\n  });\n  var tabNames = new Set();\n  for (var decision in DECISION_HEADING_MAP) {\n    tabNames.add(DECISION_HEADING_MAP[decision]);\n  }\n  var tabArray = Array.from(tabNames);\n  tabArray.forEach(function(tabName) {\n    sections.push({\n      heading: tabName,\n      id: getElementId(tabName)\n    });\n  })\n\n  Webfield.ui.tabPanel(sections, {\n    container: '#notes',\n    hidden: true\n  });\n}\n\nfunction createConsoleLinks(allGroups) {\n  var uniqueGroups = _.sortedUniq(allGroups.sort());\n\n  return uniqueGroups.map(function(group) {\n    var groupName = group.split('/').pop();\n    if (groupName.slice(-1) === 's') {\n      groupName = groupName.slice(0, -1);\n    }\n\n    return [\n      '\u003cli class=\"note invitation-link\"\u003e',\n        '\u003ca href=\"/group?id=' + group + '\"\u003e' + groupName.replace(/_/g, ' ') + ' Console\u003c/a\u003e',\n      '\u003c/li\u003e'\n    ].join('');\n  });\n}\n\nfunction groupNotesByDecision(notes, decisionNotes, withdrawnNotes, deskRejectedNotes) {\n  // Categorize notes into buckets defined by DECISION_HEADING_MAP\n  var notesDict = _.keyBy(notes, 'id');\n\n  var papersByDecision = {};\n  for (var decision in DECISION_HEADING_MAP) {\n    papersByDecision[getElementId(DECISION_HEADING_MAP[decision])] = [];\n  }\n\n  decisionNotes.forEach(function(d) {\n    var tabName = DECISION_HEADING_MAP[d.content.decision];\n    if (tabName) {\n      var decisionKey = getElementId(tabName);\n      if (notesDict[d.forum] \u0026\u0026 papersByDecision[decisionKey]) {\n        papersByDecision[decisionKey].push(notesDict[d.forum]);\n      }\n    }\n\n  });\n\n  if (papersByDecision['withdrawn-rejected-submissions']) {\n    papersByDecision['withdrawn-rejected-submissions'] = papersByDecision['withdrawn-rejected-submissions'].concat(withdrawnNotes.concat(deskRejectedNotes));\n  }\n  return papersByDecision;\n}\n\nfunction renderContent(notes, decisionNotes, withdrawnNotes, deskRejectedNotes, userGroups) {\n  var papersByDecision = groupNotesByDecision(notes, decisionNotes, withdrawnNotes, deskRejectedNotes);\n\n  // Your Consoles Tab\n  if (userGroups \u0026\u0026 userGroups.length) {\n    var consoleLinks = createConsoleLinks(userGroups);\n    $('#your-consoles').html('\u003cul class=\"list-unstyled submissions-list\"\u003e' +\n      consoleLinks.join('\\n') + '\u003c/ul\u003e');\n\n    $('.tabs-container a[href=\"#your-consoles\"]').parent().show();\n  } else {\n    $('.tabs-container a[href=\"#your-consoles\"]').parent().hide();\n  }\n\n  // Register event handlers\n  $('#group-container').on('shown.bs.tab', 'ul.nav-tabs li a', function(e) {\n    var containerSelector = $(e.target).attr('href');\n    var containerId = containerSelector.substring(1);\n    if (!papersByDecision.hasOwnProperty(containerId) || !$(containerSelector).length) {\n      return;\n    }\n\n    setTimeout(function() {\n      Webfield.ui.searchResults(\n        papersByDecision[containerId],\n        Object.assign({}, paperDisplayOptions, { showTags: false, container: containerSelector })\n      );\n    }, 150);\n  });\n\n  $('#group-container').on('hidden.bs.tab', 'ul.nav-tabs li a', function(e) {\n    var containerSelector = $(e.target).attr('href');\n    var containerId = containerSelector.substring(1);\n    if (!papersByDecision.hasOwnProperty(containerId) || !$(containerSelector).length) {\n      return;\n    }\n\n    Webfield.ui.spinner(containerSelector, { inline: true });\n  });\n\n  $('#notes \u003e .spinner-container').remove();\n  $('.tabs-container').show();\n}\n\n// Go!\nmain();\n\n});\n//# sourceURL=webfieldCode.js","query":{"id":"ICLR.cc/2021/Conference"}}},"page":"/group","query":{"id":"ICLR.cc/2021/Conference"},"buildId":"v1.0.9-1-gf539684","isFallback":false,"gip":true,"head":[["meta",{"charSet":"utf-8"}],["meta",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["meta",{"property":"og:image","content":"https://openreview.net/images/openreview_logo_512.png"}],["meta",{"property":"og:type","content":"website"}],["meta",{"property":"og:site_name","content":"OpenReview"}],["meta",{"name":"twitter:card","content":"summary"}],["meta",{"name":"twitter:site","content":"@openreviewnet"}],["title",{"children":"ICLR 2021 Conference | OpenReview"}],["meta",{"name":"description","content":"Welcome to the OpenReview homepage for ICLR 2021 Conference"}],["meta",{"property":"og:title","content":"ICLR 2021 Conference"}],["meta",{"property":"og:description","content":"Welcome to the OpenReview homepage for ICLR 2021 Conference"}]]}</script><script nomodule="" src="./ICLR 2021 Conference _ OpenReview_files/polyfills-f9ac226678fc858257de.js.下载"></script><script src="./ICLR 2021 Conference _ OpenReview_files/main-6b56d8b97b908e519b08.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/webpack-d7b2fb72fb7257504a38.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/framework.b11cd6ab3c62dae3dfb8.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/29107295.c7a36c5cb4964dc936e4.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/b637e9a5.d9354c88856ddbaf0be2.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/42e8bd60.2605cce5b4c2063fa3d3.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/55e0983c962a44019f922501f82fb92be7dc6038.b1fe41cb81db81e91237.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/181a3372a136420bacd8143b1a96dfd629357ddc.7ec9ffd88886a5000e00.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/a070395ba8e9275b849fcd4310593532831ac4b7.8744cf13ed507c350a25.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/1220f4ffb4828c6d46a13072dc033abc74519993.3d3dd7c19e661b4a6bfa.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/styles.c669c7da917090bc8543.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/_app-0232e1e8985093b3faa8.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/group-11f7765f7db1e915182b.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/_buildManifest.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/_ssgManifest.js.下载" async=""></script><script> </script><script>// Webfield Code for ICLR.cc/2021/Conference
window.user = {"id":"guest_1617528810720","isGuest":true};
$(function() {
  var args = {"id":"ICLR.cc/2021/Conference"};
  var group = {"id":"ICLR.cc/2021/Conference","cdate":1587911460292,"ddate":null,"tcdate":1587911460292,"tmdate":1611607761057,"tddate":null,"signatures":["ICLR.cc/2021/Conference"],"signatories":["ICLR.cc/2021/Conference"],"readers":["everyone"],"nonreaders":[],"writers":["ICLR.cc/2021/Conference"],"members":["ICLR.cc/2021/Conference/Program_Chairs","OpenReview.net/Support"],"details":{"writable":false}};
  var document = null;
  var window = null;
  var model = {
    tokenPayload: function() {
      return { user: user }
    }
  };

  $('#group-container').empty();
  

  // ------------------------------------
// Venue homepage template
//
// This webfield displays the conference header (#header), the submit button (#invitation),
// and a tabbed interface for viewing various types of notes.
// ------------------------------------

// Constants
var PARENT_GROUP_ID = 'ICLR.cc/2021';
var CONFERENCE_ID = 'ICLR.cc/2021/Conference';
var BLIND_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Blind_Submission';
var WITHDRAWN_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Withdrawn_Submission';
var DESK_REJECTED_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Desk_Rejected_Submission';
var DECISION_INVITATION_REGEX = 'ICLR.cc/2021/Conference/Paper.*/-/Decision';
var DECISION_HEADING_MAP = {
  "Accept (Oral)": "Oral Presentations", 
  "Accept (Spotlight)": "Spotlight Presentations", 
  "Accept (Poster)": "Poster Presentations", 
  "Reject": "Withdrawn/Rejected Submissions"
  
};
var PAGE_SIZE = 25;

var HEADER = {"title": "International Conference on Learning Representations", "subtitle": "ICLR 2021", "location": "Vienna, Austria", "date": "May 04 2021", "website": "https://iclr.cc/", "instructions": "<p class='dark'>Please see the venue website for more information.<br> <p class='dark'><strong>Abstract Submission End:</strong> Sep 28 2020 03:00PM UTC-0</p><p class='dark'><strong>Paper Submission End:</strong> Oct 2 2020 03:00PM UTC-0</p>", "deadline": "", "contact": "iclr2021programchairs@googlegroups.com", "reviewers_name": "Reviewers", "area_chairs_name": "Area_Chairs", "reviewers_id": "ICLR.cc/2021/Conference/Reviewers", "authors_id": "ICLR.cc/2021/Conference/Authors", "program_chairs_id": "ICLR.cc/2021/Conference/Program_Chairs", "area_chairs_id": "ICLR.cc/2021/Conference/Area_Chairs", "submission_id": "ICLR.cc/2021/Conference/-/Submission", "blind_submission_id": "ICLR.cc/2021/Conference/-/Blind_Submission", "withdrawn_submission_id": "ICLR.cc/2021/Conference/-/Withdrawn_Submission", "desk_rejected_submission_id": "ICLR.cc/2021/Conference/-/Desk_Rejected_Submission", "public": true};

var paperDisplayOptions = {
  pdfLink: true,
  replyCount: true,
  showContents: true
};

var sections = [];

// Main is the entry point to the webfield code and runs everything
function main() {
  if (args && args.referrer) {
    OpenBanner.referrerLink(args.referrer);
  } else if (PARENT_GROUP_ID.length){
    OpenBanner.venueHomepageLink(PARENT_GROUP_ID);
  }
  Webfield.ui.setup('#group-container', CONFERENCE_ID);  // required

  renderConferenceHeader();

  renderConferenceTabs();

  load().then(renderContent).then(Webfield.ui.done);
}

// Load makes all the API calls needed to get the data to render the page
function load() {
  var notesP = Webfield.getAll('/notes', {
    invitation: BLIND_SUBMISSION_ID,
    details: 'replyCount,invitation,original'
  });

  var withdrawnNotesP = WITHDRAWN_SUBMISSION_ID ? Webfield.getAll('/notes', {
    invitation: WITHDRAWN_SUBMISSION_ID,
    details: 'replyCount,invitation,original'
  }) : $.Deferred().resolve([]);

  var deskRejectedNotesP = DESK_REJECTED_SUBMISSION_ID ? Webfield.getAll('/notes', {
    invitation: DESK_REJECTED_SUBMISSION_ID,
    details: 'replyCount,invitation,original'
  }) : $.Deferred().resolve([]);

  var decisionNotesP = Webfield.getAll('/notes', {
    invitation: DECISION_INVITATION_REGEX,
  });

  var userGroupsP;
  if (!user || _.startsWith(user.id, 'guest_')) {
    userGroupsP = $.Deferred().resolve([]);
  } else {
    userGroupsP = Webfield.getAll('/groups', {
      regex: CONFERENCE_ID + '/.*',
      member: user.id,
      web: true
    }).then(function(groups) {
      if (!groups || !groups.length) {
        return [];
      }

      return groups.map(function(g) { return g.id; });
    });
  }

  return $.when(notesP, decisionNotesP, withdrawnNotesP, deskRejectedNotesP, userGroupsP);
}

function renderConferenceHeader() {
  Webfield.ui.venueHeader(HEADER);
  Webfield.ui.spinner('#notes', { inline: true });
}

function getElementId(decision) {
  if (!decision) return decision;
  return decision.replace(/\W/g, '-')
    .replace('(', '')
    .replace(')', '')
    .toLowerCase();
}

function renderConferenceTabs() {
  sections.push({
    heading: 'Your Consoles',
    id: 'your-consoles',
  });
  var tabNames = new Set();
  for (var decision in DECISION_HEADING_MAP) {
    tabNames.add(DECISION_HEADING_MAP[decision]);
  }
  var tabArray = Array.from(tabNames);
  tabArray.forEach(function(tabName) {
    sections.push({
      heading: tabName,
      id: getElementId(tabName)
    });
  })

  Webfield.ui.tabPanel(sections, {
    container: '#notes',
    hidden: true
  });
}

function createConsoleLinks(allGroups) {
  var uniqueGroups = _.sortedUniq(allGroups.sort());

  return uniqueGroups.map(function(group) {
    var groupName = group.split('/').pop();
    if (groupName.slice(-1) === 's') {
      groupName = groupName.slice(0, -1);
    }

    return [
      '<li class="note invitation-link">',
        '<a href="/group?id=' + group + '">' + groupName.replace(/_/g, ' ') + ' Console</a>',
      '</li>'
    ].join('');
  });
}

function groupNotesByDecision(notes, decisionNotes, withdrawnNotes, deskRejectedNotes) {
  // Categorize notes into buckets defined by DECISION_HEADING_MAP
  var notesDict = _.keyBy(notes, 'id');

  var papersByDecision = {};
  for (var decision in DECISION_HEADING_MAP) {
    papersByDecision[getElementId(DECISION_HEADING_MAP[decision])] = [];
  }

  decisionNotes.forEach(function(d) {
    var tabName = DECISION_HEADING_MAP[d.content.decision];
    if (tabName) {
      var decisionKey = getElementId(tabName);
      if (notesDict[d.forum] && papersByDecision[decisionKey]) {
        papersByDecision[decisionKey].push(notesDict[d.forum]);
      }
    }

  });

  if (papersByDecision['withdrawn-rejected-submissions']) {
    papersByDecision['withdrawn-rejected-submissions'] = papersByDecision['withdrawn-rejected-submissions'].concat(withdrawnNotes.concat(deskRejectedNotes));
  }
  return papersByDecision;
}

function renderContent(notes, decisionNotes, withdrawnNotes, deskRejectedNotes, userGroups) {
  var papersByDecision = groupNotesByDecision(notes, decisionNotes, withdrawnNotes, deskRejectedNotes);

  // Your Consoles Tab
  if (userGroups && userGroups.length) {
    var consoleLinks = createConsoleLinks(userGroups);
    $('#your-consoles').html('<ul class="list-unstyled submissions-list">' +
      consoleLinks.join('\n') + '</ul>');

    $('.tabs-container a[href="#your-consoles"]').parent().show();
  } else {
    $('.tabs-container a[href="#your-consoles"]').parent().hide();
  }

  // Register event handlers
  $('#group-container').on('shown.bs.tab', 'ul.nav-tabs li a', function(e) {
    var containerSelector = $(e.target).attr('href');
    var containerId = containerSelector.substring(1);
    if (!papersByDecision.hasOwnProperty(containerId) || !$(containerSelector).length) {
      return;
    }

    setTimeout(function() {
      Webfield.ui.searchResults(
        papersByDecision[containerId],
        Object.assign({}, paperDisplayOptions, { showTags: false, container: containerSelector })
      );
    }, 150);
  });

  $('#group-container').on('hidden.bs.tab', 'ul.nav-tabs li a', function(e) {
    var containerSelector = $(e.target).attr('href');
    var containerId = containerSelector.substring(1);
    if (!papersByDecision.hasOwnProperty(containerId) || !$(containerSelector).length) {
      return;
    }

    Webfield.ui.spinner(containerSelector, { inline: true });
  });

  $('#notes > .spinner-container').remove();
  $('.tabs-container').show();
}

// Go!
main();

});
//# sourceURL=webfieldCode.js</script></body></html>
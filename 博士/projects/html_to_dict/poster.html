<!DOCTYPE html>
<!-- saved from url=(0076)https://openreview.net/group?id=ICLR.cc/2021/Conference#poster-presentations -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin="true"><link rel="stylesheet" href="./poster_files/css"><link rel="stylesheet" href="./poster_files/bootstrap.min.css"><link rel="icon" href="https://openreview.net/favicon.ico"><script type="text/javascript" async="" src="./poster_files/analytics.js.下载"></script><script async="" src="./poster_files/js"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag() { dataLayer.push(arguments); }
            gtag('js', new Date());
            gtag('config', 'UA-108703919-1', {
              page_path: window.location.pathname + window.location.search,
              transport_type: 'beacon'
            });</script><meta name="viewport" content="width=device-width, initial-scale=1"><meta property="og:image" content="https://openreview.net/images/openreview_logo_512.png"><meta property="og:type" content="website"><meta property="og:site_name" content="OpenReview"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@openreviewnet"><title>ICLR 2021 Conference | OpenReview</title><meta name="description" content="Welcome to the OpenReview homepage for ICLR 2021 Conference"><meta property="og:title" content="ICLR 2021 Conference"><meta property="og:description" content="Welcome to the OpenReview homepage for ICLR 2021 Conference"><link rel="preload" href="./poster_files/styles.38b7829b.chunk.css" as="style"><link rel="stylesheet" href="./poster_files/styles.38b7829b.chunk.css" data-n-g=""><noscript data-n-css="true"></noscript><link rel="preload" href="./poster_files/main-6b56d8b97b908e519b08.js.下载" as="script"><link rel="preload" href="./poster_files/webpack-d7b2fb72fb7257504a38.js.下载" as="script"><link rel="preload" href="./poster_files/framework.b11cd6ab3c62dae3dfb8.js.下载" as="script"><link rel="preload" href="./poster_files/29107295.c7a36c5cb4964dc936e4.js.下载" as="script"><link rel="preload" href="./poster_files/b637e9a5.d9354c88856ddbaf0be2.js.下载" as="script"><link rel="preload" href="./poster_files/42e8bd60.2605cce5b4c2063fa3d3.js.下载" as="script"><link rel="preload" href="./poster_files/55e0983c962a44019f922501f82fb92be7dc6038.b1fe41cb81db81e91237.js.下载" as="script"><link rel="preload" href="./poster_files/181a3372a136420bacd8143b1a96dfd629357ddc.7ec9ffd88886a5000e00.js.下载" as="script"><link rel="preload" href="./poster_files/a070395ba8e9275b849fcd4310593532831ac4b7.8744cf13ed507c350a25.js.下载" as="script"><link rel="preload" href="./poster_files/1220f4ffb4828c6d46a13072dc033abc74519993.3d3dd7c19e661b4a6bfa.js.下载" as="script"><link rel="preload" href="./poster_files/styles.c669c7da917090bc8543.js.下载" as="script"><link rel="preload" href="./poster_files/_app-0232e1e8985093b3faa8.js.下载" as="script"><link rel="preload" href="./poster_files/group-11f7765f7db1e915182b.js.下载" as="script"><script src="./poster_files/tex-chtml-full.js.下载" async="" crossorigin="anonymous"></script><style type="text/css">.simpread-theme-root{font-size:62.5%!important}sr-rd-content,sr-rd-desc,sr-rd-title{width:100%}sr-rd-title{display:-webkit-box;margin:1em 0 .5em;overflow:hidden;text-overflow:ellipsis;text-rendering:optimizelegibility;-webkit-line-clamp:3;-webkit-box-orient:vertical}sr-rd-content{text-align:left;word-break:break-word}sr-rd-desc{text-align:justify;line-height:2.4;margin:0 0 1.2em;box-sizing:border-box}sr-rd-content{font-size:25.6px;font-size:1.6rem;line-height:1.6}sr-rd-content h1,sr-rd-content h1 *,sr-rd-content h2,sr-rd-content h2 *,sr-rd-content h3,sr-rd-content h3 *,sr-rd-content h4,sr-rd-content h4 *,sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{word-break:break-all}sr-rd-content div,sr-rd-content p{display:block;float:inherit;line-height:1.6;font-size:25.6px;font-size:1.6rem}sr-rd-content div,sr-rd-content p,sr-rd-content pre,sr-rd-content sr-blockquote{margin:0 0 1.2em;word-break:break-word}sr-rd-content a{padding:0 5px;vertical-align:baseline;vertical-align:initial}sr-rd-content a,sr-rd-content a:link{color:inherit;font-size:inherit;font-weight:inherit;border:none}sr-rd-content a:hover{background:transparent}sr-rd-content img{margin:10px;padding:5px;max-width:100%;background:#fff;border:1px solid #bbb;box-shadow:1px 1px 3px #d4d4d4}sr-rd-content figcaption{text-align:center;font-size:14px}sr-rd-content sr-blockquote{display:block;position:relative;padding:15px 25px;text-align:left;line-height:inherit}sr-rd-content sr-blockquote:before{position:absolute}sr-rd-content sr-blockquote *{margin:0;font-size:inherit}sr-rd-content table{width:100%;margin:0 0 1.2em;word-break:keep-all;word-break:normal;overflow:auto;border:none}sr-rd-content table td,sr-rd-content table th{border:none}sr-rd-content ul{margin:0 0 1.2em;margin-left:1.3em;padding:0;list-style:disc}sr-rd-content ol{list-style:decimal;margin:0;padding:0}sr-rd-content ol li,sr-rd-content ul li{font-size:inherit;list-style:disc;margin:0 0 1.2em}sr-rd-content ol li{list-style:decimal;margin-left:1.3em}sr-rd-content ol li *,sr-rd-content ul li *{margin:0;text-align:left;text-align:initial}sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em}sr-rd-content li ul{list-style:circle}sr-rd-content pre{font-family:Consolas,Monaco,Andale Mono,Source Code Pro,Liberation Mono,Courier,monospace;display:block;padding:15px;line-height:1.5;word-break:break-all;word-wrap:break-word;white-space:pre;overflow:auto}sr-rd-content pre,sr-rd-content pre *,sr-rd-content pre div{font-size:17.6px;font-size:1.1rem}sr-rd-content li pre code,sr-rd-content p pre code,sr-rd-content pre{background-color:transparent;border:none}sr-rd-content pre code{margin:0;padding:0}sr-rd-content pre code,sr-rd-content pre code *{font-size:17.6px;font-size:1.1rem}sr-rd-content pre p{margin:0;padding:0;color:inherit;font-size:inherit;line-height:inherit}sr-rd-content li code,sr-rd-content p code{margin:0 4px;padding:2px 4px;font-size:17.6px;font-size:1.1rem}sr-rd-content mark{margin:0 5px;padding:2px;background:#fffdd1;border-bottom:1px solid #ffedce}.sr-rd-content-img{width:90%;height:auto}.sr-rd-content-img-load{width:48px;height:48px;margin:0;padding:0;border-style:none;border-width:0;background-repeat:no-repeat;background-image:url(data:image/gif;base64,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)}.sr-rd-content-center{text-align:center;display:-webkit-box;-webkit-box-align:center;-webkit-box-pack:center;-webkit-box-orient:vertical}.sr-rd-content-center-small{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex}.sr-rd-content-center-small img{margin:0;padding:0;border:0;box-shadow:none}.sr-rd-content-nobeautify{margin:0;padding:0;border:0;box-shadow:0 0 0}sr-rd-mult{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin:0 0 16px;padding:16px 0 24px;width:100%;background-color:#fff;border-radius:4px;box-shadow:0 1px 2px 0 rgba(60,64,67,.3),0 2px 6px 2px rgba(60,64,67,.15)}sr-rd-mult:hover{-webkit-transition:all .45s 0ms;transition:all .45s 0ms;box-shadow:1px 1px 8px rgba(0,0,0,.16)}sr-rd-mult sr-rd-mult-content{padding:0 16px;overflow:auto}sr-rd-mult sr-rd-mult-avatar,sr-rd-mult sr-rd-mult-content{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sr-rd-mult sr-rd-mult-avatar{-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 15px}sr-rd-mult sr-rd-mult-avatar span{display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;max-width:75px;overflow:hidden;text-overflow:ellipsis;text-align:left;font-size:16px;font-size:1rem}sr-rd-mult sr-rd-mult-avatar img{margin-bottom:0;max-width:50px;max-height:50px;width:50px;height:50px;border-radius:50%}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center{margin:0}sr-page{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;width:100%}</style><style type="text/css">sr-rd-theme-github{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{position:relative;margin-top:1em;margin-bottom:1pc;font-weight:700;line-height:1.4;text-align:left;color:#363636}sr-rd-content h1{padding-bottom:.3em;font-size:57.6px;font-size:3.6rem;line-height:1.2}sr-rd-content h2{padding-bottom:.3em;font-size:44.8px;font-size:2.8rem;line-height:1.225}sr-rd-content h3{font-size:38.4px;font-size:2.4rem;line-height:1.43}sr-rd-content h4{font-size:32px;font-size:2rem}sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h6{color:#777}sr-rd-content ol,sr-rd-content ul{list-style-type:disc;padding:0;padding-left:2em}sr-rd-content ol ol,sr-rd-content ul ol{list-style-type:lower-roman}sr-rd-content ol ol ol,sr-rd-content ol ul ol,sr-rd-content ul ol ol,sr-rd-content ul ul ol{list-style-type:lower-alpha}sr-rd-content table{width:100%;overflow:auto;word-break:normal;word-break:keep-all}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{padding:6px 13px;border:1px solid #ddd}sr-rd-content table tr{background-color:#fff;border-top:1px solid #ccc}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{border-left:4px solid #ddd}.simpread-theme-root{background-color:#fff;color:#333}sr-rd-title{font-family:PT Sans,SF UI Display,\.PingFang SC,PingFang SC,Neue Haas Grotesk Text Pro,Arial Nova,Segoe UI,Microsoft YaHei,Microsoft JhengHei,Helvetica Neue,Source Han Sans SC,Noto Sans CJK SC,Source Han Sans CN,Noto Sans SC,Source Han Sans TC,Noto Sans CJK TC,Hiragino Sans GB,sans-serif;font-size:54.4px;font-size:3.4rem;font-weight:700;line-height:1.3}sr-rd-desc{position:relative;margin:0;margin-bottom:30px;padding:25px;padding-left:56px;font-size:28.8px;font-size:1.8rem;color:#777;background-color:rgba(0,0,0,.05);box-sizing:border-box}sr-rd-desc:before{content:"\201C";position:absolute;top:-28px;left:16px;font-size:80px;font-family:Arial;color:rgba(0,0,0,.15)}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#363636;font-weight:400;line-height:1.8}sr-rd-content b *,sr-rd-content strong,sr-rd-content strong * sr-rd-content b{-webkit-animation:none 0s ease 0s 1 normal none running;animation:none 0s ease 0s 1 normal none running;-webkit-backface-visibility:visible;backface-visibility:visible;background:transparent none repeat 0 0/auto auto padding-box border-box scroll;border:medium none currentColor;border-collapse:separate;-o-border-image:none;border-image:none;border-radius:0;border-spacing:0;bottom:auto;box-shadow:none;box-sizing:content-box;caption-side:top;clear:none;clip:auto;color:#000;-webkit-columns:auto;-moz-columns:auto;columns:auto;-webkit-column-count:auto;-moz-column-count:auto;column-count:auto;-webkit-column-fill:balance;-moz-column-fill:balance;column-fill:balance;-webkit-column-gap:normal;-moz-column-gap:normal;column-gap:normal;-webkit-column-rule:medium none currentColor;-moz-column-rule:medium none currentColor;column-rule:medium none currentColor;-webkit-column-span:1;-moz-column-span:1;column-span:1;-webkit-column-width:auto;-moz-column-width:auto;column-width:auto;content:normal;counter-increment:none;counter-reset:none;cursor:auto;direction:ltr;display:inline;empty-cells:show;float:none;font-family:serif;font-size:medium;font-style:normal;font-variant:normal;font-weight:400;font-stretch:normal;line-height:normal;height:auto;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;left:auto;letter-spacing:normal;list-style:disc outside none;margin:0;max-height:none;max-width:none;min-height:0;min-width:0;opacity:1;orphans:2;outline:medium none invert;overflow:visible;overflow-x:visible;overflow-y:visible;padding:0;page-break-after:auto;page-break-before:auto;page-break-inside:auto;-webkit-perspective:none;perspective:none;-webkit-perspective-origin:50% 50%;perspective-origin:50% 50%;position:static;right:auto;-moz-tab-size:8;-o-tab-size:8;tab-size:8;table-layout:auto;text-align:left;text-align-last:auto;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;top:auto;-webkit-transform:none;transform:none;-webkit-transform-origin:50% 50% 0;transform-origin:50% 50% 0;-webkit-transform-style:flat;transform-style:flat;-webkit-transition:none 0s ease 0s;transition:none 0s ease 0s;unicode-bidi:normal;vertical-align:baseline;visibility:visible;white-space:normal;widows:2;width:auto;word-spacing:normal;z-index:auto;all:initial}sr-rd-content a,sr-rd-content a:link{color:#4183c4;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#4183c4;text-decoration:underline}sr-rd-content pre{background-color:#f7f7f7;border-radius:3px}sr-rd-content li code,sr-rd-content p code{background-color:rgba(0,0,0,.04);border-radius:3px}.simpread-multi-root{background:#f8f9fa}</style><style type="text/css">sr-rd-theme-newsprint{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-weight:700}sr-rd-content h1{font-size:48px;font-size:3rem;line-height:1.6em;margin-top:2em}sr-rd-content h2,sr-rd-content h3{font-size:32px;font-size:2rem;line-height:1.15;margin-top:2.285714em;margin-bottom:1.15em}sr-rd-content h3{font-weight:400}sr-rd-content h4{font-size:28.8px;font-size:1.8rem;margin-top:2.67em}sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h1{border-bottom:1px solid;margin-bottom:1.875em;padding-bottom:.8125em}sr-rd-content ol,sr-rd-content ul{margin:0 0 1.5em 1.5em}sr-rd-content ol li{list-style-type:decimal;list-style-position:outside}sr-rd-content ul li{list-style-type:disc;list-style-position:outside}sr-rd-content table{width:100%;margin-bottom:1.5em;font-size:25.6px;font-size:1.6rem}sr-rd-content thead th,tfoot th{padding:.25em .25em .25em .4em;text-transform:uppercase}sr-rd-content th{text-align:left}sr-rd-content td{vertical-align:top;padding:.25em .25em .25em .4em}sr-rd-content thead{background-color:#dadada}sr-rd-content tr:nth-child(2n){background:#e8e7e7}sr-rd-content sr-blockquote{padding:10px 15px;border-left-style:solid;border-left-width:10px;border-color:#d6dbdf;background:none repeat scroll 0 0 rgba(102,128,153,.05);text-align:left}sr-rd-content sr-blockquote:before{content:""}.simpread-multi-root,.simpread-theme-root{background-color:#f3f2ee;color:#2c3e50}sr-rd-title{font-family:PingFang SC,Hiragino Sans GB,Microsoft Yahei,WenQuanYi Micro Hei,sans-serif;line-height:1.5;font-weight:500;font-size:48px;font-size:3rem;color:#07b;border-bottom:1px solid;margin-bottom:1.875em;padding-bottom:.8125em}sr-rd-desc{color:rgba(102,128,153,.6);background-color:rgba(102,128,153,.075);border-radius:4px;margin-bottom:1em;padding:15px;font-size:32px;font-size:2rem;line-height:1.5;text-align:center}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{line-height:1.8;color:#2c3e50}sr-rd-content a,sr-rd-content a:link{color:#08c;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#5ba4e5}sr-rd-content li code,sr-rd-content p code,sr-rd-content pre{background-color:#dadada}sr-rd-mult{background-color:rgba(102,128,153,.075)}</style><style type="text/css">sr-rd-theme-gothic{display:none}sr-rd-content h1{line-height:64px;line-height:4rem;margin:64px 0 28px;margin:4rem 0 1.75rem;padding:20px 30px}sr-rd-content h1,sr-rd-content h2{font-weight:400;text-align:center;text-transform:uppercase}sr-rd-content h2{line-height:48px;line-height:3rem;margin:48px 0 31px;margin:3rem 0 1.9375rem;padding:0 30px}sr-rd-content h3,sr-rd-content h4,sr-rd-content h5{font-weight:400}sr-rd-content h6{font-weight:700}sr-rd-content h1{font-size:57.6px;font-size:3.6rem}sr-rd-content h2{font-size:51.2px;font-size:3.2rem}sr-rd-content h3{font-size:40px;font-size:2.5rem}sr-rd-content h4{font-size:35.2px;font-size:2.2rem}sr-rd-content h5{font-size:30.4px;font-size:1.9rem}sr-rd-content h6{font-size:27.2px;font-size:1.7rem}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{margin-top:1.2em;margin-bottom:.6em;color:#111}sr-rd-content ol,sr-rd-content ul{list-style-type:disc;margin-left:3em}sr-rd-content ol ol,sr-rd-content ul ol{list-style-type:lower-roman}sr-rd-content ol ol ol,sr-rd-content ol ul ol,sr-rd-content ul ol ol,sr-rd-content ul ul ol{list-style-type:lower-alpha}sr-rd-content table{margin-bottom:20px}sr-rd-content table td,sr-rd-content table th{padding:8px;line-height:20px;line-height:1.25rem;vertical-align:top;border-top:1px solid #ddd}sr-rd-content table th{font-weight:700}sr-rd-content table thead th{vertical-align:bottom}sr-rd-content table caption+thead tr:first-child td,sr-rd-content table caption+thead tr:first-child th,sr-rd-content table colgroup+thead tr:first-child td,sr-rd-content table colgroup+thead tr:first-child th,sr-rd-content table thead:first-child tr:first-child td,sr-rd-content table thead:first-child tr:first-child th{border-top:0}sr-rd-content table tbody+tbody{border-top:2px solid #ddd}sr-rd-content sr-blockquote{margin:0 0 17.777px;margin:0 0 1.11111rem;padding:8px 17.777px 0 16.888px;padding:.5rem 1.11111rem 0 1.05556rem;border-left:1px solid gray}sr-rd-content sr-blockquote,sr-rd-content sr-blockquote p{line-height:2;color:#6f6f6f}.simpread-multi-root,.simpread-theme-root{background:#fcfcfc;color:#333}sr-rd-title{font-weight:400;line-height:64px;line-height:4rem;text-align:center;text-transform:uppercase;color:#111;font-size:51.2px;font-size:3.2rem}sr-rd-desc{margin:0 0 17.777px;margin:0 0 1.11111rem;padding:8px 17.777px 0 16.888px;padding:.5rem 1.11111rem 0 1.05556rem;font-size:32px;font-size:2rem;line-height:2;color:#6f6f6f;border-left:1px solid gray}sr-rd-content{font-weight:400;color:#333}sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#333}sr-rd-content a,sr-rd-content a:link{color:#900;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#900;text-decoration:underline}sr-rd-content li code,sr-rd-content p code,sr-rd-content pre{background-color:transparent;border:1px solid #ccc}sr-rd-mult{background-color:#f2f2f2}</style><style type="text/css">sr-rd-theme-engwrite{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{margin:20px 0 10px;padding:0;font-weight:500;-webkit-font-smoothing:antialiased}sr-rd-content h1{font-weight:300;text-align:center;font-size:44.8px;font-size:2.8rem;color:#933d3f}sr-rd-content h2{font-size:38.4px;font-size:2.4rem;border-bottom:1px solid #ccc;color:#000}sr-rd-content h3{font-size:28.8px;font-size:1.8rem}sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h6{color:#777}sr-rd-content ol,sr-rd-content ul{padding-left:30px}sr-rd-content ol li>:first-child,sr-rd-content ol li ol:first-of-type,sr-rd-content ol li ul:first-of-type,sr-rd-content ul li>:first-child,sr-rd-content ul li ol:first-of-type,sr-rd-content ul li ul:first-of-type{margin-top:0}sr-rd-content ol ol,sr-rd-content ol ul,sr-rd-content ul ol,sr-rd-content ul ul{margin-bottom:0}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{border:1px solid #ccc;padding:6px 13px}sr-rd-content table tr{border-top:1px solid #ccc;background-color:#fff}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{text-align:left;border-top:1px dotted #cdc7bc;border-bottom:1px dotted #cdc7bc;background-color:#f8edda;color:#777}sr-blockquote>:first-child{margin-top:0}sr-blockquote>:last-child{margin-bottom:0}.simpread-multi-root,.simpread-theme-root{background-color:#fcf5ed;color:#333}sr-rd-title{font-weight:300;text-align:center;font-size:44.8px;font-size:2.8rem;color:#933d3f}sr-rd-desc{padding:10px;background-color:#f8edda;color:#777;font-size:32px;font-size:2rem;text-align:center;border-top:1px dotted #cdc7bc;border-bottom:1px dotted #cdc7bc}sr-rd-content{padding:20px 0;margin:0 auto}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#333;line-height:1.8}sr-rd-content a,sr-rd-content a:link{color:#ae3737;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{text-decoration:underline}sr-rd-content pre{background-color:transparent;border:1px solid #ccc;border-radius:3px}sr-rd-content li code,sr-rd-content p code{border:1px solid #eaeaea;background-color:#f4ece3;border-radius:3px}sr-rd-mult{background-color:#f8edda}</style><style type="text/css">sr-rd-theme-octopress{display:none}sr-rd-content h1{font-size:56.32px;font-size:3.52rem;line-height:30.72px;line-height:1.92rem}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{text-rendering:optimizelegibility;margin-bottom:20.8px;margin-bottom:1.3rem;font-weight:700}sr-rd-content h2{font-size:38.4px;font-size:2.4rem}sr-rd-content h3{font-size:33.28px;font-size:2.08rem}sr-rd-content h4{font-size:28.8px;font-size:1.8rem}sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h1,sr-rd-content h2{padding-top:27.2px;padding-top:1.7rem;padding-bottom:19.2px;padding-bottom:1.2rem;background:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAABCAYAAACsXeyTAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAFUlEQVQIHWNIS0sr/v//PwMMDzY+ADqMahlW4J91AAAAAElFTkSuQmCC") 0 100% repeat-x}sr-rd-content h2{padding-top:20.8px;padding-top:1.3rem;padding-bottom:0}sr-rd-content ul{list-style-type:disc}sr-rd-content ul ul{list-style-type:circle;margin-bottom:0}sr-rd-content ul ul ul{list-style-type:square;margin-bottom:0}sr-rd-content ol{list-style-type:decimal}sr-rd-content ol ol{list-style-type:lower-alpha;margin-bottom:0}sr-rd-content ol ol ol{list-style-type:lower-roman;margin-bottom:0}sr-rd-content ol,sr-rd-content ol ol,sr-rd-content ol ul,sr-rd-content ul,sr-rd-content ul ol,sr-rd-content ul ul{margin-left:1.3em}sr-rd-content ol ol,sr-rd-content ol ul,sr-rd-content ul ol,sr-rd-content ul ul{margin-bottom:0}sr-rd-content table{width:100%;overflow:auto;word-break:normal;word-break:keep-all}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{padding:6px 13px;border:1px solid #ddd}sr-rd-content table tr{background-color:#fff;border-top:1px solid #ccc}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{font-style:italic;font-size:inherit;line-height:2;padding-left:1em;border-left:4px solid hsla(0,0%,67%,.5)}.simpread-multi-root,.simpread-theme-root{background:#f8f8f8 url("data:image/png;base64,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") 0 0;color:#333}sr-rd-title{font-size:56.32px;font-size:3.52rem;line-height:64px;line-height:4rem;font-weight:700;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAABCAYAAACsXeyTAAAACXBIW…sTAAALEwEAmpwYAAAAFUlEQVQIHWNIS0sr/v//PwMMDzY+ADqMahlW4J91AAAAAElFTkSuQmCC) 0 100% repeat-x}sr-rd-desc{font-style:italic;font-size:30.72px;font-size:1.92rem;line-height:2;padding-left:1em;border-left:4px solid hsla(0,0%,67%,.5)}sr-rd-content{margin:0 auto;padding:1em 0}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{line-height:2;color:#333}sr-rd-content a,sr-rd-content a:link{color:#1863a1;text-decoration:underline}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#0181eb;text-decoration:underline}sr-rd-content pre{color:#586e75;background-color:#fdf6e3;border-radius:.4em;border:1px solid #e7dec3}sr-rd-content li code,sr-rd-content p code{color:#555;background-color:transparent;border:1px solid #ddd}sr-rd-mult{background-color:#ededed}</style><style type="text/css">sr-rd-theme-pixyii{display:none}sr-rd-content h1,sr-rd-content h1 *,sr-rd-content h2,sr-rd-content h2 *,sr-rd-content h3,sr-rd-content h3 *,sr-rd-content h4,sr-rd-content h4 *,sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{color:inherit;font-weight:900;line-height:1.2;margin:1em 0}sr-rd-content h1,sr-rd-content h1 *{font-size:62.72px;font-size:3.92rem}sr-rd-content h2,sr-rd-content h2 *{font-size:58.24px;font-size:3.64rem}sr-rd-content h3,sr-rd-content h3 *{font-size:36.4px;font-size:2.275rem}sr-rd-content h4,sr-rd-content h4 *{font-size:29.12px;font-size:1.82rem}sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{font-size:25.168px;font-size:1.573rem}sr-rd-content ol,sr-rd-content ul{font-size:28px;font-size:1.75rem;line-height:24px;line-height:1.5rem}sr-rd-content li{font-size:25.2px;font-size:1.575rem;line-height:1.8;margin:0;position:relative}sr-rd-content table{width:100%;font-size:25.2px;font-size:1.575rem}sr-rd-content table>tbody>tr>td,sr-rd-content table>tbody>tr>th,sr-rd-content table>tfoot>tr>td,sr-rd-content table>tfoot>tr>th,sr-rd-content table>thead>tr>td,sr-rd-content table>thead>tr>th{padding:12px;line-height:1.2;vertical-align:top;border-top:1px solid #333}sr-rd-content table>thead>tr>th{vertical-align:bottom;border-bottom:2px solid #333}sr-rd-content table>caption+thead>tr:first-child>td,sr-rd-content table>caption+thead>tr:first-child>th,sr-rd-content table>colgroup+thead>tr:first-child>td,sr-rd-content table>colgroup+thead>tr:first-child>th,sr-rd-content table>thead:first-child>tr:first-child>td,sr-rd-content table>thead:first-child>tr:first-child>th{border-top:0}sr-rd-content table>tbody+tbody{border-top:2px solid #333}sr-rd-content sr-blockquote{margin:16px 0;margin:1rem 0;padding:1.33em;font-style:italic;border-left:5px solid #7a7a7a;color:#555}.simpread-theme-root{background-color:#fff;color:#555}sr-rd-title{font-family:PingFang SC,Hiragino Sans GB,Microsoft Yahei,WenQuanYi Micro Hei,sans-serif;font-size:67.2px;font-size:4.2rem;font-weight:900;line-height:1.2}sr-rd-desc{margin:16px 0;margin:1rem 0;padding:1.33em;font-style:italic;font-size:32px;font-size:2rem;line-height:2;border-left:5px solid #7a7a7a;color:#555}sr-rd-content{font-size:33.6px;font-size:2.1rem;line-height:1.8;font-weight:400;color:#555}sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#555;font-size:28px;font-size:1.75rem;line-height:1.8;font-weight:300}sr-rd-content b,sr-rd-content b *,sr-rd-content strong,sr-rd-content strong *{font-weight:700}sr-rd-content a,sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover,sr-rd-content a:link{color:#463f5c;text-decoration:underline}sr-rd-content sr-blockquote code{font-size:inherit}sr-rd-content pre{border:1px solid #7a7a7a}sr-rd-content li code,sr-rd-content p code,sr-rd-content pre{color:#7a7a7a;background-color:transparent}.simpread-multi-root{background:#f8f9fa}</style><style type="text/css">sr-rd-theme-monospace{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-weight:700;color:#6363ac}sr-rd-content h1{font-size:35.2px;font-size:2.2rem}sr-rd-content h2{font-size:32px;font-size:2rem}sr-rd-content h3{font-size:28.8px;font-size:1.8rem}sr-rd-content h4{font-size:25.6px;font-size:1.6rem}sr-rd-content h5{font-size:22.4px;font-size:1.4rem}sr-rd-content h6{font-size:20.8px;font-size:1.3rem}sr-rd-content strong{color:#b5302e}sr-rd-content em{font-style:italic;color:#400469}sr-rd-content ol,sr-rd-content ul{list-style-type:none}sr-rd-content ol li,sr-rd-content ul li{margin:0}sr-rd-content table{line-height:25.6px;line-height:1.6rem;border-collapse:collapse;border-spacing:0;empty-cells:show;border:1px solid #cbcbcb}sr-rd-content thead{background-color:#e0e0e0;color:#000;text-align:left;vertical-align:bottom}sr-rd-content td,sr-rd-content th{border-left:1px solid #cbcbcb;border-width:0 0 0 1px;margin:0;overflow:visible;padding:.5em 1em}sr-rd-content sr-blockquote{background-color:hsla(0,0%,50%,.05);border-top-right-radius:5px;border-bottom-right-radius:5px;border-left:8px solid #979797;line-height:2}sr-rd-content sr-blockquote *{line-height:inherit}.simpread-theme-root{color:#333;background:#fff}sr-rd-title{font-size:44.8px;font-size:2.8rem;line-height:1.2;font-weight:700;color:#6363ac}sr-rd-desc{padding:10px;background-color:hsla(0,0%,50%,.05);font-size:28.8px;font-size:1.8rem;text-align:center;border-top-right-radius:5px;border-bottom-right-radius:5px;border-left:8px solid #979797}sr-rd-content{color:#333}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{line-height:1.7}sr-rd-content a,sr-rd-content a:link{color:#005dad;text-decoration:underline}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#fff;background-color:#2a6496;text-decoration:none}sr-rd-content pre{color:#e9eded;background-color:#263238}sr-rd-content li code,sr-rd-content p code{color:#949415;background-color:transparent}.simpread-multi-root{background:#f8f9fa}</style><style type="text/css">sr-rd-theme-night{display:none}sr-rd-content h1{margin-top:2em}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{color:#dedede;font-weight:400;clear:both;-ms-word-wrap:break-word;word-wrap:break-word;margin:0;padding:0}sr-rd-content h1{font-size:57.6px;font-size:3.6rem;line-height:64px;line-height:4rem;margin-bottom:38.4px;margin-bottom:2.4rem;letter-spacing:-1.5px}sr-rd-content h2{font-size:38.4px;font-size:2.4rem;line-height:48px;line-height:3rem}sr-rd-content h2,sr-rd-content h3{margin-bottom:38.4px;margin-bottom:2.4rem;letter-spacing:-1px}sr-rd-content h3{font-size:28.8px;font-size:1.8rem;line-height:38.4px;line-height:2.4rem}sr-rd-content h4{font-size:25.6px;font-size:1.6rem;line-height:35.2px;line-height:2.2rem;margin-bottom:38.4px;margin-bottom:2.4rem}sr-rd-content h5{font-size:16px;font-size:1rem;line-height:20px;line-height:1.25rem;margin-bottom:24px;margin-bottom:1.5rem}sr-rd-content h6{font-size:25.6px;font-size:1.6rem;line-height:25.6px;line-height:1.6rem;margin-bottom:12px;margin-bottom:.75rem;font-weight:700}sr-rd-content ol,sr-rd-content ul{padding:0 0 0 30px;padding:0 0 0 1.875rem}sr-rd-content ul{list-style:square}sr-rd-content ol{list-style:decimal}sr-rd-content ol ol,sr-rd-content ol ul,sr-rd-content ul ol,sr-rd-content ul ul{margin:0}sr-rd-content li div{padding-top:0}sr-rd-content li,sr-rd-content li p{margin:0;position:relative}sr-rd-content table{margin-top:0;margin-bottom:24px;margin-bottom:1.5rem;border-collapse:collapse;border-spacing:0;page-break-inside:auto;text-align:left}sr-rd-content table a{color:#dedede}sr-rd-content thead{display:table-header-group}sr-rd-content table td,sr-rd-content table th{border:1px solid #474d54}sr-rd-content sr-blockquote{margin:0 0 30px 30px;margin:0 0 1.875rem 1.875rem;border-left:2px solid #474d54;padding-left:30px;margin-top:35px;line-height:2}.simpread-multi-root,.simpread-theme-root{background:#363b40;color:#b8bfc6}sr-rd-title{color:#dedede;font-size:50.4px;font-size:3.15rem;line-height:56px;line-height:3.5rem;letter-spacing:-1.5px}sr-rd-desc{margin:35px;margin-left:0;padding-left:30px;padding-left:1.875rem;font-size:32px;font-size:2rem;line-height:2;border-left:2px solid #474d54}sr-rd-content,sr-rd-desc{color:#b8bfc6}sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#b8bfc6;margin-top:0;line-height:2}sr-rd-content a,sr-rd-content a:link{color:#e0e0e0;text-decoration:underline;-webkit-transition:all .2s ease-in-out;transition:all .2s ease-in-out}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#fff}sr-rd-content pre{background-color:transparent;border:1px solid}sr-rd-content li code,sr-rd-content p code{background:rgba(0,0,0,.05)}sr-rd-mult{background-color:#2d3034}panel{background-color:#2e2e2e!important}panel panel-tab span{color:#fff}panel sr-opt-gp sr-opt-label{color:rgba(108,255,240,.8);font-weight:400}panel text-field-float{color:rgba(108,255,240,.8)!important;font-weight:400!important}panel list-field{background-color:transparent!important}panel list-field:hover list-field-name{color:#fff!important;font-weight:700}panel input,panel list-field-name{color:hsla(35,10%,76%,.87)!important}panel list-view{background-color:#2e2e2e!important}panel-tabs{border-bottom-color:#393d40!important}sr-annote{color:#333}</style><style type="text/css">sr-rd-theme-dark{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-weight:700;color:#dbdbfd}sr-rd-content h1{font-size:48px;font-size:3rem}sr-rd-content h2{font-size:44.8px;font-size:2.8rem}sr-rd-content h3{font-size:40px;font-size:2.5rem}sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{color:#549ad8}sr-rd-content h5{font-size:32px;font-size:2rem}sr-rd-content h6{font-size:28.8px;font-size:1.8rem}sr-rd-content strong{color:#ffffc5}sr-rd-content em{color:#c885f5}sr-rd-content table{width:100%;line-height:25.6px;line-height:1.6rem;border-collapse:collapse;border-spacing:0;empty-cells:show;border:1px solid #cbcbcb}sr-rd-content thead{background-color:#263238;color:#f5f5f5;text-align:left;vertical-align:bottom}sr-rd-content table td,sr-rd-content table th{border-left:1px solid #cbcbcb;border-width:0 0 0 1px;margin:0;overflow:visible;padding:.5em 1em}sr-rd-content sr-blockquote{background-color:hsla(0,0%,50%,.05);border-top-right-radius:5px;border-bottom-right-radius:5px;border-left:8px solid #979797;color:#ebebeb}.simpread-multi-root,.simpread-theme-root{color:#ebebeb;background:#222}sr-rd-title{padding-bottom:.3em;font-size:44.8px;font-size:2.8rem;font-weight:700;line-height:1.2;color:#dbdbfd;border-bottom:1px solid #eee}sr-rd-desc{margin:20px;margin-left:0;padding:5px 20px;font-size:28.8px;font-size:1.8rem;background-color:hsla(0,0%,50%,.05);color:#ebebeb;border-top-right-radius:5px;border-bottom-right-radius:5px;border-left:8px solid #979797}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{line-height:1.7;color:#ebebeb}sr-rd-content a,sr-rd-content a:link{color:#8ac9ff;text-decoration:underline}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{background-color:#2a6496;color:#fff;text-decoration:none}sr-rd-content pre{color:#e9eded;background-color:#263238}sr-rd-content li code,sr-rd-content p code{color:#caca16;background-color:transparent}sr-rd-mult{background-color:hsla(0,0%,50%,.1)}panel{background-color:#222!important}panel panel-tab span{color:#fff}panel sr-opt-gp sr-opt-label{color:rgba(108,255,240,.8);font-weight:400}panel text-field-float{color:rgba(108,255,240,.8)!important;font-weight:400!important}panel list-field{background-color:transparent!important}panel list-field:hover list-field-name{color:#fff!important;font-weight:700}panel input,panel list-field-name{color:hsla(35,10%,76%,.87)!important}panel list-view{background-color:#222!important}panel text-field input{color:rgba(108,255,240,.8)!important}panel-tabs{border-bottom-color:#393d40!important}sr-annote{color:#333}</style><style type="text/css">sr-rd-theme-mail{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{position:relative;margin-top:1em;margin-bottom:1pc;font-weight:700;line-height:1.4;text-align:left;color:#363636}sr-rd-content h1{padding-bottom:.3em;font-size:36p;line-height:1.2}sr-rd-content h2{padding-bottom:.3em;font-size:28p;line-height:1.225}sr-rd-content h3{font-size:24p;line-height:1.43}sr-rd-content h4{font-size:2p}sr-rd-content h5{font-size:16px}sr-rd-content h6{font-size:16px;color:#777}sr-rd-content ol,sr-rd-content ul{list-style-type:disc;padding:0;padding-left:2em}sr-rd-content ol ol,sr-rd-content ul ol{list-style-type:lower-roman}sr-rd-content ol ol ol,sr-rd-content ol ul ol,sr-rd-content ul ol ol,sr-rd-content ul ul ol{list-style-type:lower-alpha}sr-rd-content table{width:100%;overflow:auto;word-break:normal;word-break:keep-all}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{padding:6px 13px;border:1px solid #ddd}sr-rd-content table tr{background-color:#fff;border-top:1px solid #ccc}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{border-left:4px solid #ddd}.simpread-theme-root{background-color:#fff;color:#333}.sr-header{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:left;-ms-flex-pack:left;justify-content:left;width:100%;margin:10px 0;height:41px;border-bottom:1px solid #e0e0e0;padding-bottom:10px}.sr-header,.sr-header a{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#195bf7}.sr-header a{display:-webkit-box;display:-ms-flexbox;display:flex;text-decoration:none}.sr-header .sr-name{height:41px;line-height:41px;font-size:20px;font-weight:700;text-decoration:none}.sr-header .sr-logo{display:block;width:41px;height:41px;background-repeat:no-repeat;background-position:50%;background-image:url(https://simpread-1254315611.file.myqcloud.com/favicon/favicon-32x32.png);margin-right:5px}.sr-header .sr-slogan{height:41px;line-height:44px;font-weight:700;font-size:15px}.sr-rd-footer{font-size:14px;text-align:center;color:#363636}.sr-rd-footer-group{display:-webkit-box;display:-ms-flexbox;display:flex;height:20px}.sr-rd-footer-line{width:100%;border-top:1px solid #e0e0e0}.sr-rd-footer-text{min-width:150px;line-height:0;text-align:center}.sr-rd-footer-copywrite{margin:10px 0 0;color:#363636}.sr-rd-footer-copywrite abbr{-webkit-font-feature-settings:normal;font-feature-settings:normal;font-variant:normal;text-decoration:none}.sr-rd-footer-copywrite .second{margin:10px 0}.sr-rd-footer-copywrite .third a:hover{border:none!important}.sr-rd-footer-copywrite .third a:first-child{margin-right:50px}.sr-rd-footer-copywrite .sr-icon{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:33px;height:33px;opacity:.8;-webkit-transition:opacity .5s ease;transition:opacity .5s ease;cursor:pointer}.sr-rd-footer-copywrite .sr-icon:hover{opacity:1}.sr-rd-footer-copywrite a,.sr-rd-footer-copywrite a:link,.sr-rd-footer-copywrite a:visited{margin:0;padding:0;color:inherit;background-color:transparent;font-size:inherit!important;line-height:normal;text-decoration:none;vertical-align:baseline;vertical-align:initial;border:none!important;box-sizing:border-box}.sr-rd-footer-copywrite a:focus,.sr-rd-footer-copywrite a:hover,.sr-rd-footer a:active{color:inherit;text-decoration:none;border-bottom:1px dotted!important}.sr-rd-content-desc{margin:0;padding:0 0 0 1em;color:#363636;line-height:2;font-size:18px;border-left:4px solid hsla(0,0%,67%,.5)}sr-rd-content{font-size:16px;line-height:1.6}sr-rd-content h1,sr-rd-content h1 *,sr-rd-content h2,sr-rd-content h2 *,sr-rd-content h3,sr-rd-content h3 *,sr-rd-content h4,sr-rd-content h4 *,sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{word-break:break-all}sr-rd-content div,sr-rd-content p{display:block;float:inherit;line-height:1.6;font-size:16px}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#363636;font-weight:400;line-height:1.8}sr-rd-content strong,sr-rd-content strong *{-webkit-animation:none 0s ease 0s 1 normal none running;animation:none 0s ease 0s 1 normal none running;-webkit-backface-visibility:visible;backface-visibility:visible;background:transparent none repeat 0 0/auto auto padding-box border-box scroll;border:medium none currentColor;border-collapse:separate;-o-border-image:none;border-image:none;border-radius:0;border-spacing:0;bottom:auto;box-shadow:none;box-sizing:content-box;caption-side:top;clear:none;clip:auto;color:#000;-webkit-columns:auto;-moz-columns:auto;columns:auto;-webkit-column-count:auto;-moz-column-count:auto;column-count:auto;-webkit-column-fill:balance;-moz-column-fill:balance;column-fill:balance;-webkit-column-gap:normal;-moz-column-gap:normal;column-gap:normal;-webkit-column-rule:medium none currentColor;-moz-column-rule:medium none currentColor;column-rule:medium none currentColor;-webkit-column-span:1;-moz-column-span:1;column-span:1;-webkit-column-width:auto;-moz-column-width:auto;column-width:auto;content:normal;counter-increment:none;counter-reset:none;cursor:auto;direction:ltr;display:inline;empty-cells:show;float:none;font-family:serif;font-size:medium;font-style:normal;font-variant:normal;font-weight:400;font-stretch:normal;line-height:normal;height:auto;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;left:auto;letter-spacing:normal;list-style:disc outside none;margin:0;max-height:none;max-width:none;min-height:0;min-width:0;opacity:1;orphans:2;outline:medium none invert;overflow:visible;overflow-x:visible;overflow-y:visible;padding:0;page-break-after:auto;page-break-before:auto;page-break-inside:auto;-webkit-perspective:none;perspective:none;-webkit-perspective-origin:50% 50%;perspective-origin:50% 50%;position:static;right:auto;-moz-tab-size:8;-o-tab-size:8;tab-size:8;table-layout:auto;text-align:left;text-align-last:auto;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;top:auto;-webkit-transform:none;transform:none;-webkit-transform-origin:50% 50% 0;transform-origin:50% 50% 0;-webkit-transform-style:flat;transform-style:flat;-webkit-transition:none 0s ease 0s;transition:none 0s ease 0s;unicode-bidi:normal;vertical-align:baseline;visibility:visible;white-space:normal;widows:2;width:auto;word-spacing:normal;z-index:auto;all:initial}sr-rd-content a,sr-rd-content a:link{color:#4183c4;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#4183c4;text-decoration:underline}sr-rd-content figure{margin:0;padding:0}sr-rd-content img{display:inline-block;padding:0;height:auto;line-height:100%;max-width:50%;text-decoration:none;vertical-align:text-bottom;border-radius:10px;outline:none}sr-rd-content pre{background-color:#f7f7f7;border-radius:3px}sr-rd-content pre *{font-size:1.1px}sr-rd-content li code,sr-rd-content p code{background-color:rgba(0,0,0,.04);border-radius:3px}.simpread-multi-root{background:#f8f9fa}.sr-rd-mult{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin:0 0 16px;padding:16px 0 24px;width:100%;background-color:#fff;border-bottom:1px solid #e0e0e0}.sr-rd-mult .sr-rd-mult-content{padding:0 16px;overflow:auto}.sr-rd-mult .sr-rd-mult-avatar{margin:0 15px}.sr-rd-mult .sr-rd-mult-avatar span{display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;max-width:75px;overflow:hidden;text-overflow:ellipsis;text-align:left;font-size:16px;font-size:1rem}.sr-rd-mult .sr-rd-mult-avatar img{margin-bottom:0;max-width:50px;max-height:50px;width:50px;height:50px;border-radius:50%}.sr-rd-mult .sr-rd-mult-avatar .sr-rd-content-center{margin:0}sr-rd-content.embed *{font-size:medium}sr-rd-content.embed img{max-width:100%}sr-rd-content.embed a,sr-rd-content.embed a:hover{color:inherit;font-size:medium}sr-rd-content.embed a:hover{background-color:inherit}sr-rd-content.embed .MathJax_Processed,sr-rd-content.embed math{display:none}sr-rd-content.embed pre{color:#000;color:initial;background-color:transparent}sr-rd-content.embed pre,sr-rd-content.embed pre *{font-size:13px!important}</style><style type="text/css">@media (pointer:coarse){sr-read{margin:20px 5%!important;min-width:0!important;max-width:90%!important}sr-rd-title{margin-top:0;font-size:2.7rem}sr-rd-content sr-blockquote,sr-rd-desc{margin:10 0!important;padding:0 0 0 10px!important;width:90%;font-size:1.8rem;font-style:normal;line-height:1.7;text-align:justify}sr-rd-content{font-size:1.75rem;font-weight:300}sr-rd-content figure{margin:0;padding:0;text-align:center}sr-rd-content a,sr-rd-content a:link,sr-rd-content li code,sr-rd-content p code{font-size:inherit}sr-rd-footer{margin-top:20px}sr-blockquote,sr-blockquote *{margin:5px!important;padding:5px!important}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6,sr-rd-title{font-family:PingFang SC,Verdana,Helvetica Neue,Microsoft Yahei,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#000;font-weight:100;line-height:1.35}sr-rd-content-h1,sr-rd-content-h2,sr-rd-content-h3,sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{margin-top:1.2em;margin-bottom:.6em;line-height:1.35}sr-rd-content-h1,sr-rd-content h1{font-size:1.8em}sr-rd-content-h2,sr-rd-content h2{font-size:1.6em}sr-rd-content-h3,sr-rd-content h3{font-size:1.4em}sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-size:1.2em}sr-rd-content-ul,sr-rd-content ul{margin-left:1.3em!important;list-style:disc}sr-rd-content-ol,sr-rd-content ol{list-style:decimal;margin-left:1.9em!important}sr-rd-content-ol ol,sr-rd-content-ol ul,sr-rd-content-ul ol,sr-rd-content-ul ul,sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em!important}sr-rd-content img{margin:0;padding:0;border:0;max-width:100%!important;height:auto;box-shadow:0 20px 20px -10px rgba(0,0,0,.1)}sr-rd-mult{min-width:0;background-color:#fff;box-shadow:0 1px 6px rgba(32,33,36,.28);border-radius:8px}sr-rd-mult sr-rd-mult-avatar div{margin:0}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center-small{margin:7px 0!important}sr-rd-mult sr-rd-mult-avatar span{display:block}sr-rd-mult sr-rd-mult-content{padding-left:0}@media only screen and (max-device-width:1024px){.simpread-theme-root,html.simpread-theme-root{font-size:80%!important}sr-rd-mult sr-rd-mult-avatar img{width:50px;height:50px;min-width:50px;min-height:50px}}@media only screen and (max-device-width:414px){.simpread-theme-root,html.simpread-theme-root{font-size:70%!important}sr-rd-mult sr-rd-mult-avatar img{width:30px;height:30px;min-width:30px;min-height:30px}}@media only screen and (max-device-width:320px){.simpread-theme-root,html.simpread-theme-root{font-size:90%!important}sr-rd-content p{margin-bottom:.5em}}}</style><style type="text/css">button[aria-label][data-balloon-pos]{overflow:visible}[aria-label][data-balloon-pos]{position:relative;cursor:pointer}[aria-label][data-balloon-pos]:after{text-indent:0;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;font-weight:400;font-style:normal;text-shadow:none;font-size:12px;background:hsla(0,0%,6%,.95);border-radius:2px;color:#fff;content:attr(aria-label);padding:.5em 1em;white-space:nowrap}[aria-label][data-balloon-pos]:after,[aria-label][data-balloon-pos]:before{opacity:0;pointer-events:none;-webkit-transition:all .18s ease-out .18s;transition:all .18s ease-out .18s;position:absolute;z-index:10}[aria-label][data-balloon-pos]:before{width:0;height:0;border:5px solid transparent;border-top-color:hsla(0,0%,6%,.95);content:""}[aria-label][data-balloon-pos]:hover:after,[aria-label][data-balloon-pos]:hover:before,[aria-label][data-balloon-pos]:not([data-balloon-nofocus]):focus:after,[aria-label][data-balloon-pos]:not([data-balloon-nofocus]):focus:before,[aria-label][data-balloon-pos][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-visible]:before{opacity:1;pointer-events:none}[aria-label][data-balloon-pos].font-awesome:after{font-family:FontAwesome,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif}[aria-label][data-balloon-pos][data-balloon-break]:after{white-space:pre}[aria-label][data-balloon-pos][data-balloon-break][data-balloon-length]:after{white-space:pre-line;word-break:break-word}[aria-label][data-balloon-pos][data-balloon-blunt]:after,[aria-label][data-balloon-pos][data-balloon-blunt]:before{-webkit-transition:none;transition:none}[aria-label][data-balloon-pos][data-balloon-pos=up]:after{margin-bottom:10px}[aria-label][data-balloon-pos][data-balloon-pos=up]:after,[aria-label][data-balloon-pos][data-balloon-pos=up]:before{bottom:100%;left:50%;-webkit-transform:translate(-50%,4px);transform:translate(-50%,4px);-webkit-transform-origin:top;transform-origin:top}[aria-label][data-balloon-pos][data-balloon-pos=up]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=up]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=up][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=up][data-balloon-visible]:before{-webkit-transform:translate(-50%);transform:translate(-50%)}[aria-label][data-balloon-pos][data-balloon-pos=up-left]:after{bottom:100%;left:0;margin-bottom:10px;-webkit-transform:translateY(4px);transform:translateY(4px);-webkit-transform-origin:top;transform-origin:top}[aria-label][data-balloon-pos][data-balloon-pos=up-left]:before{bottom:100%;left:5px;-webkit-transform:translateY(4px);transform:translateY(4px);-webkit-transform-origin:top;transform-origin:top}[aria-label][data-balloon-pos][data-balloon-pos=up-left]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=up-left]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=up-left][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=up-left][data-balloon-visible]:before{-webkit-transform:translate(0);transform:translate(0)}[aria-label][data-balloon-pos][data-balloon-pos=up-right]:after{bottom:100%;right:0;margin-bottom:10px;-webkit-transform:translateY(4px);transform:translateY(4px);-webkit-transform-origin:top;transform-origin:top}[aria-label][data-balloon-pos][data-balloon-pos=up-right]:before{bottom:100%;right:5px;-webkit-transform:translateY(4px);transform:translateY(4px);-webkit-transform-origin:top;transform-origin:top}[aria-label][data-balloon-pos][data-balloon-pos=up-right]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=up-right]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=up-right][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=up-right][data-balloon-visible]:before{-webkit-transform:translate(0);transform:translate(0)}[aria-label][data-balloon-pos][data-balloon-pos=down]:after{left:50%;margin-top:10px;top:100%;-webkit-transform:translate(-50%,-4px);transform:translate(-50%,-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down]:before{width:0;height:0;border:5px solid transparent;border-bottom-color:hsla(0,0%,6%,.95);left:50%;top:100%;-webkit-transform:translate(-50%,-4px);transform:translate(-50%,-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=down]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=down][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=down][data-balloon-visible]:before{-webkit-transform:translate(-50%);transform:translate(-50%)}[aria-label][data-balloon-pos][data-balloon-pos=down-left]:after{left:0;margin-top:10px;top:100%;-webkit-transform:translateY(-4px);transform:translateY(-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down-left]:before{width:0;height:0;border:5px solid transparent;border-bottom-color:hsla(0,0%,6%,.95);left:5px;top:100%;-webkit-transform:translateY(-4px);transform:translateY(-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down-left]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=down-left]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=down-left][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=down-left][data-balloon-visible]:before{-webkit-transform:translate(0);transform:translate(0)}[aria-label][data-balloon-pos][data-balloon-pos=down-right]:after{right:0;margin-top:10px;top:100%;-webkit-transform:translateY(-4px);transform:translateY(-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down-right]:before{width:0;height:0;border:5px solid transparent;border-bottom-color:hsla(0,0%,6%,.95);right:5px;top:100%;-webkit-transform:translateY(-4px);transform:translateY(-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down-right]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=down-right]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=down-right][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=down-right][data-balloon-visible]:before{-webkit-transform:translate(0);transform:translate(0)}[aria-label][data-balloon-pos][data-balloon-pos=left]:after{margin-right:10px;right:100%;top:50%;-webkit-transform:translate(4px,-50%);transform:translate(4px,-50%)}[aria-label][data-balloon-pos][data-balloon-pos=left]:before{width:0;height:0;border:5px solid transparent;border-left-color:hsla(0,0%,6%,.95);right:100%;top:50%;-webkit-transform:translate(4px,-50%);transform:translate(4px,-50%)}[aria-label][data-balloon-pos][data-balloon-pos=left]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=left]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=left][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=left][data-balloon-visible]:before{-webkit-transform:translateY(-50%);transform:translateY(-50%)}[aria-label][data-balloon-pos][data-balloon-pos=right]:after{left:100%;margin-left:10px;top:50%;-webkit-transform:translate(-4px,-50%);transform:translate(-4px,-50%)}[aria-label][data-balloon-pos][data-balloon-pos=right]:before{width:0;height:0;border:5px solid transparent;border-right-color:hsla(0,0%,6%,.95);left:100%;top:50%;-webkit-transform:translate(-4px,-50%);transform:translate(-4px,-50%)}[aria-label][data-balloon-pos][data-balloon-pos=right]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=right]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=right][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=right][data-balloon-visible]:before{-webkit-transform:translateY(-50%);transform:translateY(-50%)}[aria-label][data-balloon-pos][data-balloon-length=small]:after{white-space:normal;width:80px}[aria-label][data-balloon-pos][data-balloon-length=medium]:after{white-space:normal;width:150px}[aria-label][data-balloon-pos][data-balloon-length=large]:after{white-space:normal;width:260px}[aria-label][data-balloon-pos][data-balloon-length=xlarge]:after{white-space:normal;width:380px}@media screen and (max-width:768px){[aria-label][data-balloon-pos][data-balloon-length=xlarge]:after{white-space:normal;width:90vw}}[aria-label][data-balloon-pos]:before{display:none}[aria-label][data-balloon-pos]:after{box-shadow:0 0 10px rgba(0,0,0,.3);border-radius:5px;font-weight:700;font-size:10px}[aria-label][data-balloon-pos][data-balloon-pos=up]:after{line-height:21px}[aria-label][data-balloon-pos][data-balloon-order=downleft]:after{left:120%}[aria-label][data-balloon-pos][data-balloon-order=downright]:after{right:-22px}[aria-label][data-balloon-pos][data-balloon-order=upright]:after{left:10%}</style><style type="text/css">/*!
 * Waves v0.7.5
 * http://fian.my.id/Waves
 * 
 * Copyright 2014-2016 Alfiana E. Sibuea and other contributors
 * Released under the MIT license
 * https://github.com/fians/Waves/blob/master/LICENSE
 */.md-waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent}.md-waves-effect .md-waves-ripple{position:absolute;border-radius:50%;width:100px;height:100px;margin-top:-50px;margin-left:-50px;opacity:0;background:rgba(0,0,0,.2);background:-webkit-radial-gradient(rgba(0,0,0,.2) 0,rgba(0,0,0,.3) 40%,rgba(0,0,0,.4) 50%,rgba(0,0,0,.5) 60%,hsla(0,0%,100%,0) 70%);background:radial-gradient(rgba(0,0,0,.2) 0,rgba(0,0,0,.3) 40%,rgba(0,0,0,.4) 50%,rgba(0,0,0,.5) 60%,hsla(0,0%,100%,0) 70%);-webkit-transition:all .5s ease-out;transition:all .5s ease-out;-webkit-transition-property:-webkit-transform,opacity;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transform:scale(0) translate(0);transform:scale(0) translate(0);pointer-events:none}.md-waves-effect.md-waves-light .md-waves-ripple{background:hsla(0,0%,100%,.4);background:-webkit-radial-gradient(hsla(0,0%,100%,.2) 0,hsla(0,0%,100%,.3) 40%,hsla(0,0%,100%,.4) 50%,hsla(0,0%,100%,.5) 60%,hsla(0,0%,100%,0) 70%);background:radial-gradient(hsla(0,0%,100%,.2) 0,hsla(0,0%,100%,.3) 40%,hsla(0,0%,100%,.4) 50%,hsla(0,0%,100%,.5) 60%,hsla(0,0%,100%,0) 70%)}.md-waves-effect.md-waves-classic .md-waves-ripple{background:rgba(0,0,0,.2)}.md-waves-effect.md-waves-classic.md-waves-light .md-waves-ripple{background:hsla(0,0%,100%,.4)}.md-waves-notransition{-webkit-transition:none!important;transition:none!important}.md-waves-button,.md-waves-circle{-webkit-transform:translateZ(0);transform:translateZ(0);-webkit-mask-image:-webkit-radial-gradient(circle,#fff 100%,#000 0)}.md-waves-button,.md-waves-button-input,.md-waves-button:hover,.md-waves-button:visited{white-space:nowrap;vertical-align:middle;cursor:pointer;border:none;outline:none;color:inherit;background-color:transparent;font-size:1em;line-height:1em;text-align:center;text-decoration:none;z-index:1}.md-waves-button{padding:.85em 1.1em;border-radius:.2em}.md-waves-button-input{margin:0;padding:.85em 1.1em}.md-waves-input-wrapper{border-radius:.2em;vertical-align:bottom}.md-waves-input-wrapper.md-waves-button{padding:0}.md-waves-input-wrapper .md-waves-button-input{position:relative;top:0;left:0;z-index:1}.md-waves-circle{text-align:center;width:2.5em;height:2.5em;line-height:2.5em;border-radius:50%}.md-waves-float{-webkit-mask-image:none;box-shadow:0 1px 1.5px 1px rgba(0,0,0,.12);-webkit-transition:all .3s;transition:all .3s}.md-waves-float:active{box-shadow:0 8px 20px 1px rgba(0,0,0,.3)}.md-waves-block{display:block}</style><style type="text/css">.simpread-font{font:300 16px/1.8 -apple-system,PingFang SC,Microsoft Yahei,Lantinghei SC,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#333;text-rendering:optimizelegibility;-webkit-text-size-adjust:100%;-webkit-font-smoothing:antialiased}.simpread-hidden{display:none}.simpread-read-root{display:-webkit-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;margin:0;top:-1000px;left:0;width:100%;z-index:2147483646;overflow-x:hidden;opacity:0;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}.simpread-read-root-show{top:0}.simpread-read-root-hide{top:1000px}sr-read{display:-webkit-flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-flow:column nowrap;flex-flow:column;margin:20px 20%;min-width:400px;min-height:400px;text-align:center}read-process{position:fixed;top:0;left:0;height:3px;width:100%;background-color:#64b5f6;-webkit-transition:width 2s;transition:width 2s;z-index:20000}sr-rd-content-error{display:block;position:relative;margin:0;margin-bottom:30px;padding:25px;background-color:rgba(0,0,0,.05)}sr-rd-footer{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;font-size:14px}sr-rd-footer,sr-rd-footer-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}sr-rd-footer-group{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-rd-footer-line{width:100%;border-top:1px solid #e0e0e0}sr-rd-footer-text{min-width:150px}sr-rd-footer-copywrite{margin:10px 0 0;color:inherit}sr-rd-footer-copywrite abbr{-webkit-font-feature-settings:normal;font-feature-settings:normal;font-variant:normal;text-decoration:none}sr-rd-footer-copywrite .second{margin:10px 0}sr-rd-footer-copywrite .third a:hover{border:none!important}sr-rd-footer-copywrite .third a:first-child{margin-right:50px}sr-rd-footer-copywrite .sr-icon{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:33px;height:33px;opacity:.8;-webkit-transition:opacity .5s ease;transition:opacity .5s ease;cursor:pointer}sr-rd-footer-copywrite .sr-icon:hover{opacity:1}sr-rd-footer-copywrite a,sr-rd-footer-copywrite a:link,sr-rd-footer-copywrite a:visited{margin:0;padding:0;color:inherit;background-color:transparent;font-size:inherit!important;line-height:normal;text-decoration:none;vertical-align:baseline;vertical-align:initial;border:none!important;box-sizing:border-box}sr-rd-footer-copywrite a:focus,sr-rd-footer-copywrite a:hover,sr-rd-footer a:active{color:inherit;text-decoration:none;border-bottom:1px dotted!important}.simpread-blocks{text-decoration:none!important}.simpread-blocks *{margin:0}.simpread-blocks a{padding:0;text-decoration:none!important}.simpread-blocks img{margin:0;padding:0;border:0;background:transparent;box-shadow:none}.simpread-focus-root{display:block;position:fixed;top:0;left:0;right:0;bottom:0;background-color:hsla(0,0%,92%,.9);z-index:2147483645;opacity:0;-webkit-transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms;transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms}.simpread-focus-highlight{position:relative;box-shadow:0 0 0 20px #fff;background-color:#fff;overflow:visible;z-index:2147483646}.sr-controlbar-bg sr-rd-crlbar,.sr-controlbar-bg sr-rd-crlbar fab{z-index:2147483647}sr-rd-crlbar.controlbar{position:fixed;right:0;bottom:0;width:100px;height:100%;opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}sr-rd-crlbar.controlbar:hover{opacity:1}sr-rd-crlbar fap *{box-sizing:border-box}@media (max-height:620px){fab{zoom:.8}}@media (max-height:783px){dialog-gp dialog-content{max-height:580px}dialog-gp dialog-footer{border-top:1px solid #e0e0e0}}.simpread-highlight-selector{cursor:pointer!important}.simpread-highlight-controlbar,.simpread-highlight-selector{background-color:#fafafa!important;outline:3px dashed #1976d2!important;opacity:.8!important;-webkit-transition:opacity .5s ease!important;transition:opacity .5s ease!important}.simpread-highlight-controlbar{position:relative!important}simpread-highlight,sr-snapshot-ctlbar{position:fixed;top:0;left:0;right:0;padding:15px;height:50px;background-color:rgba(50,50,50,.9);box-shadow:0 2px 5px rgba(0,0,0,.26);box-sizing:border-box;z-index:2147483640}simpread-highlight,sr-highlight-ctl,sr-snapshot-ctlbar{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-highlight-ctl{margin:0 5px;width:50px;height:20px;color:#fff;background-color:#1976d2;border-radius:4px;box-shadow:0 3px 1px -2px rgba(0,0,0,.2),0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12);cursor:pointer}toc-bg{position:fixed;left:0;top:0;width:50px;height:200px;font-size:medium}toc-bg:hover{z-index:3}.toc-bg-hidden{opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}.toc-bg-hidden:hover{opacity:1;z-index:3}.toc-bg-hidden:hover toc{width:180px}toc *{all:unset}toc{position:fixed;left:0;top:100px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;padding:10px;width:0;max-width:200px;max-height:500px;overflow-x:hidden;overflow-y:hidden;cursor:pointer;border:1px solid hsla(0,0%,62%,.22);-webkit-transition:width .5s;transition:width .5s}toc:hover{overflow-y:auto}toc.mini:hover{width:200px!important}toc::-webkit-scrollbar{width:3px}toc::-webkit-scrollbar-thumb{border-radius:10px;background-color:hsla(36,2%,54%,.5)}toc outline{position:relative;display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;overflow:hidden;text-overflow:ellipsis;padding:2px 0;min-height:21px;line-height:21px;text-align:left}toc outline a,toc outline a:active,toc outline a:focus,toc outline a:visited{display:block;width:100%;color:inherit;font-size:11px;text-decoration:none!important;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}toc outline a:hover{font-weight:700!important}toc outline a.toc-outline-theme-dark,toc outline a.toc-outline-theme-night{color:#fff!important}.toc-level-h1{padding-left:5px}.toc-level-h2{padding-left:15px}.toc-level-h3{padding-left:25px}.toc-level-h4{padding-left:35px}.toc-outline-active{border-left:2px solid #f44336}toc outline active{position:absolute;left:0;top:0;bottom:0;padding:0 0 0 3px;border-left:2px solid #e8e8e8}sr-kbd{background:-webkit-gradient(linear,0 0,0 100%,from(#fff785),to(#ffc542));border:1px solid #e3be23;-o-border-image:none;border-image:none;-o-border-image:initial;border-image:initial;position:absolute;left:0;padding:1px 3px 0;font-size:11px!important;font-weight:700;box-shadow:0 3px 7px 0 rgba(0,0,0,.3);overflow:hidden;border-radius:3px}.sr-kbd-a{position:relative}kbd-mapping{position:fixed;left:5px;bottom:5px;-ms-flex-flow:row;flex-flow:row;width:250px;height:500px;background-color:#fff;border:1px solid hsla(0,0%,62%,.22);box-shadow:0 2px 5px rgba(0,0,0,.26);border-radius:3px}kbd-mapping,kbd-maps{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}kbd-maps{margin:40px 0 20px;width:100%;overflow-x:auto}kbd-maps::-webkit-scrollbar-thumb{background-clip:padding-box;border-radius:10px;border:2px solid transparent;background-color:rgba(85,85,85,.55)}kbd-maps::-webkit-scrollbar{width:10px;-webkit-transition:width .7s cubic-bezier(.4,0,.2,1);transition:width .7s cubic-bezier(.4,0,.2,1)}kbd-mapping kbd-map-title{position:absolute;margin:5px 0;width:100%;font-size:14px;font-weight:700}kbd-maps-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}kbd-maps-title{margin:5px 0;padding-left:53px;font-size:12px;font-weight:700}kbd-map kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fafbfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}kbd-map kbd-name{display:inline-block;text-align:right;width:50px}kbd-map kbd-desc{padding-left:3px}sharecard-bg{position:fixed;top:0;left:0;width:100%;height:100%;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(0,0,0,.4);z-index:2147483647}sharecard{max-width:450px;background-color:#64b5f6}sharecard,sharecard-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-head{margin:25px;color:#fff;border-radius:10px;box-shadow:0 2px 6px 0 rgba(0,0,0,.2),0 25px 50px 0 rgba(0,0,0,.15)}sharecard-card{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-card,sharecard-top{display:-webkit-box;display:-ms-flexbox;display:flex}sharecard-top{-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:65px;background-color:#fff;color:#878787;font-size:25px;font-weight:500;border-top-left-radius:10px;border-top-right-radius:10px}sharecard-top span.logos{display:block;width:48px;height:48px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAMAAABg3Am1AAABU1BMVEUAAAAnNJMnNZI3Q5onNJInNJMnNJInNJMnNJI8SJ0tOZY/S55EUKAoNJI6RpwoNJNIU6InNJInNJImNJI7SJwmNJJ2fLUiMJFKVaNCTJ9faK1HUaJOWKVSXaUnNJNYY6pye7cmM5JXYKhwebMjMI8mL4719fW9vb0oNZP/UlLz8/QqN5TAwMAnNJPv7+/Pz8/q6+/p6enNzc3Kysry8vMsOJXc3env7/LU1uXo29vR0dHOzs7ExMTwjo73bW37XV3Aj1TCYELl5u3n5+fW2Obn6O7f4OrZ2+g0QJkxPpgvO5bh4uvS1OTP0ePCwsJQW6ZLVqTs7fHd3d3V1dXqv79VX6lET6A1TIxXUIBSgHxWQnpelHf+WVnopkXqbC7j5Ozi4uLDw8NGUaFATJ9SgH3r6+vGyd7BxNva2trX19ejqM2gpczHx8dze7Zha67Z2dlTgH1aXQeSAAAAJnRSTlMA6ff+497Y8NL+/fv49P379sqab/BeOiX06tzVy8m/tKqpalA7G6oKj0EAAAJlSURBVEjHndNXWxpBFIDhcS2ICRLAkt4Dx4WhLk0E6R0MYoIISrWX5P9f5cwSIRC2+T1czMV5n2FnZwn2eWONUqCAv3H2Uf5Ra1hx4+0WEXtDQW0fCPYJ1EffEfIV4CSROAE4jsePoTFsNmTJF/IeIHF2lgCIn57GodlqDWXBK7IwBYatVlMWFAildPKX7I3m74Z9fsCiQChoimoFQAz04Ad2gH1n9fv9n9hgMNDr9euLWD6fLxQKxaLfb7dTSlahbFVdEPwIQtrAihZQgyKCtCagbQe3xh0QFMgy5MR11+ewYY5/qlZ7vT2xu93ULKjbFLpiUxnIIwjgKmVTLDUFXMrAi2NJWCRLIthTBo4xyOLKpwyqU6CuDCI41hFBCVdOhyLw4FgJ1skCAiyl9BSHbCorgo6VJXTru5hrVCQS8Yr5xLzX59YJSFpVFwD9U0BGC3hGdFpATgRupTGe9R9I1b1ePBvXKDyvq/O/44LT4/E4BUbSCAwj8Evq6HlnOBprx6JhJz8Gktc7xeaP9ndY+0coQvCccFBD4JW60UIY50ciLOAODAQRVOeCHm4Q3Xks6uRDY+CQ+AR4T2wMYh6+jMCIQOp78CFoj0H7EQgIuhI3dGaHCrwgADwCPjJvA372GRigCJg49FUdk3D87pq3zp4SA5zc1Zh9DxfwkpjgUg5Mv+lbeE3McC8Lpu7SA3wk2xzcqL2tN5DfIsQC8HB7UamUy6FQOpTO5QKBQDZbKnWSyUzGjdWCwaDA8+7Le4BNgm3qQGWchYh9s5hNq6wVbBlbwhZYOp3OYOA4zmgEypnM2zj8ByIdedKrH8vDAAAAAElFTkSuQmCC");zoom:.8}sharecard-content{padding:15px;max-height:500px;font-size:20px;text-align:justify;background-color:#2196f3;overflow-x:hidden;overflow-y:auto}sharecard-via{padding:10px;font-size:10px;background-color:#2196f3}sharecard-footer{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:100px;background-color:#fff;color:#878787;font-size:15px;font-weight:500;border-bottom-left-radius:10px;border-bottom-right-radius:10px}sharecard-footer,sharecard-footer div{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sharecard-footer span.qrcode{display:block;width:100px;height:100px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,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")}sharecard-control{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:0 19px;height:80px;background-color:#fff}simpread-snapshot{width:100%;height:100%;cursor:move;z-index:2147483645}simpread-snapshot,sr-mask{position:fixed;left:0;top:0}sr-mask{background-color:rgba(0,0,0,.1)}.simpread-feedback,.simpread-urlscheme{position:fixed;right:20px;bottom:20px;z-index:2147483646}simpread-feedback,simpread-urlscheme{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:20px 20px 0;width:500px;color:rgba(51,51,51,.87);background-color:#fff;border-radius:3px;box-shadow:0 0 2px rgba(0,0,0,.12),0 2px 2px rgba(0,0,0,.26);overflow:hidden;-webkit-transform-origin:bottom;transform-origin:bottom;-webkit-transition:all .6s ease;transition:all .6s ease}simpread-feedback *,simpread-urlscheme *{font-size:12px!important;box-sizing:border-box}simpread-feedback.active,simpread-urlscheme.active{-webkit-animation-name:srFadeInUp;animation-name:srFadeInUp;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback.hide,simpread-urlscheme.hide{-webkit-animation-name:srFadeInDown;animation-name:srFadeInDown;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback sr-fb-label,simpread-urlscheme sr-urls-label{width:100%}simpread-feedback sr-fb-head,simpread-urlscheme sr-urls-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin-bottom:5px;width:100%}simpread-feedback sr-fb-content,simpread-urlscheme sr-urls-content{margin-bottom:5px;width:100%}simpread-feedback sr-urls-footer,simpread-urlscheme sr-urls-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-fb-a,simpread-urlscheme sr-urls-a{color:#2163f7;cursor:pointer}simpread-feedback text-field-state,simpread-urlscheme text-field-state{border-top:none rgba(34,101,247,.8)!important;border-left:none rgba(34,101,247,.8)!important;border-right:none rgba(34,101,247,.8)!important;border-bottom:2px solid rgba(34,101,247,.8)!important}simpread-feedback switch,simpread-urlscheme switch{margin-top:0!important}@-webkit-keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@-webkit-keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}@keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}simpread-feedback sr-fb-head{font-weight:700}simpread-feedback sr-fb-content{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column}simpread-feedback sr-fb-content,simpread-feedback sr-fb-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}simpread-feedback sr-fb-footer{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-close{position:absolute;right:20px;cursor:pointer;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s;z-index:200}simpread-feedback sr-close:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin-top:10px}simpread-feedback sr-stars i{margin-right:10px;cursor:pointer}simpread-feedback sr-stars i svg{-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}simpread-feedback sr-stars i svg:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars i.active svg{-webkit-transform:rotate(0) scale(1);transform:rotate(0) scale(1)}simpread-feedback sr-emojis{display:block;height:100px;overflow:hidden}simpread-feedback sr-emoji{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-transition:.3s;transition:.3s}simpread-feedback sr-emoji>svg{margin:15px 0;width:70px;height:70px;-ms-flex-negative:0;flex-shrink:0}simpread-feedback sr-stars-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin:10px 0 20px}</style><style type="text/css">sr-opt-focus,sr-opt-read{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column}sr-opt-focus,sr-opt-gp,sr-opt-read{display:-webkit-flex;-webkit-box-direction:normal;width:100%}sr-opt-gp{position:relative;-webkit-box-orient:horizontal;-ms-flex-flow:row nowrap;flex-flow:row;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;margin-bottom:25px;font-size:15px}sr-opt-gp input,sr-opt-gp textarea{font-family:Inconsolata,Operator Mono,Consolas,Andale Mono WT,Andale Mono,Lucida Console,Lucida Sans Typewriter,DejaVu Sans Mono,Bitstream Vera Sans Mono,Liberation Mono,Nimbus Mono L,Courier New,Courier,monospace!important}sr-opt-gp sr-opt-label{display:block;position:absolute;margin:-8px 0 0;font-size:14px;font-weight:700;color:rgba(0,137,123,.8);-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;pointer-events:none;-webkit-transform:scale(.75) translateY(-8px);transform:scale(.75) translateY(-8px);-webkit-transform-origin:left top 0;transform-origin:left top 0}sr-opt-themes{display:-webkit-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-flow:row nowrap;flex-flow:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;width:100%;margin:8px 0 17px;padding:0}sr-opt-theme{width:40px;height:20px;cursor:pointer;list-style:none;border-radius:3px;border:1px solid #212121;box-sizing:border-box;opacity:1;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms}sr-opt-theme:hover{-webkit-transform:translateY(-1px);transform:translateY(-1px);box-shadow:0 5px 10px rgba(0,0,0,.2)}sr-opt-theme:not(:first-child){margin-left:5px}sr-opt-theme[sr-type=active]{box-shadow:0 5px 10px rgba(0,0,0,.2);border:none}</style><style type="text/css">notify-gp{font:300 14px -apple-system,PingFang SC,Microsoft Yahei,Lantinghei SC,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;text-rendering:optimizelegibility;-webkit-text-size-adjust:100%;-webkit-font-smoothing:antialiased;display:-webkit-flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-flow:column nowrap;flex-flow:column;-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end;position:fixed;top:0;right:0;margin:0 15px 0 0;padding:0;text-transform:none;pointer-events:none}notify-gp notify{display:-webkit-flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0;margin-top:15px;padding:14px 24px;min-width:288px;max-width:568px;min-height:48px;color:hsla(0,0%,100%,.9);background-color:#000;box-sizing:border-box;border-radius:4px;pointer-events:auto;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;opacity:0;-webkit-transform:scaleY(0);transform:scaleY(0);-webkit-transform-origin:left top 0;transform-origin:left top 0;-webkit-transition:all .45s cubic-bezier(.23,1,.32,1) 0ms,opacity 1s cubic-bezier(.23,1,.32,1) 0ms;transition:all .45s cubic-bezier(.23,1,.32,1) 0ms,opacity 1s cubic-bezier(.23,1,.32,1) 0ms;box-shadow:0 3px 5px -1px rgba(0,0,0,.2),0 6px 10px 0 rgba(0,0,0,.14),0 1px 18px 0 rgba(0,0,0,.12)}notify-gp notify-title{font-size:13px;font-weight:700}notify-gp notify-content{display:block;font-size:14px;font-weight:400;text-align:left;overflow:hidden}notify-gp notify-content a,notify-gp notify-content a:active,notify-gp notify-content a:link,notify-gp notify-content a:visited{margin:inherit;padding-bottom:5px;color:#fff;font-size:inherit;text-decoration:none;-webkit-transition:color .5s;transition:color .5s}notify-gp notify-content a:hover{margin:0;margin:initial;padding:0;padding:initial;color:inherit;font-size:inherit;text-decoration:none}notify-gp notify-i{display:none;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 10px 0 0;width:24px;height:24px;background-position:50%;background-repeat:no-repeat}notify-gp notify-action,notify-gp notify-cancel{display:none;margin:0 8px;max-width:80px;min-width:56px;height:36px;line-height:34px;color:#bb86fc;font-weight:500;font-size:inherit;text-transform:uppercase;text-align:center;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;cursor:pointer}notify-gp .notify-error notify-action,notify-gp .notify-error notify-cancel,notify-gp .notify-success notify-action,notify-gp .notify-success notify-cancel,notify-gp .notify-warning notify-action,notify-gp .notify-warning notify-cancel{color:#fff}notify-gp notify-action:active,notify-gp notify-cancel:active{border-radius:4px;background-color:rgba(98,0,238,.3)}notify-gp notify-cancel{margin:0}notify-gp notify-a{display:block;position:absolute;top:5px;right:5px;cursor:pointer}notify-gp notify-exit{display:none;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-left:5px;width:36px;height:36px;min-width:36px;min-height:36px;background-color:transparent;border-radius:50%;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;cursor:pointer}notify-gp notify-exit:hover{background-color:hsla(0,0%,100%,.4)}notify-gp notify-exit:active{background-color:hsla(0,0%,100%,.2)}notify-gp notify-a notify-span{display:block;width:16px;height:16px;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAABDklEQVQ4T6VT0VFCQQzcrQA7ECoRK1AqEDugA6ECsQPsADvgVSAlaAlWEGdvkjchczI45Osud9nc7m2IEmY2BfAEYA5A6xsARwAHAB8ktR6DeWNmKwAvXlSxY78i+RabEcDM9gAe/qoq+T3JhXINwDu/Xlgc1zYk13TOn+XZA4C7AvgN4LbkZgJYO+84O5C8N7Odi6n8O8llh+ZGAD3uO5LPDgIvzoDRbBDAV+dputBAXKNecQM5B9CefQlAj0JwVmdLdGSwHI1CFXEgOS8ihia1WRNRdpU9JwlatpWVc0gr3c0xu95IAfdPK2uoHkcrJxANkzTJdPKTf3ROchvJk2n0LxNPfV9vnDVEJ+P8C6jMhLeGEqMKAAAAAElFTkSuQmCC);opacity:.9}notify-gp notify-i.holdon{display:block;margin:0 0 0 24px;width:20px;height:20px;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAQAAAAngNWGAAAATUlEQVR4AWMYSuB/4P+V/1lRRFiBIoEYCoGC//+vAypFKFsHFFkJV4AsAVGKzsOjFFUZHqUElCGUwpRRrpCw1YQ9Qzh4SA5wwlE4hAAAiFGQefYhNJkAAAAASUVORK5CYII=);cursor:pointer}notify-gp .notify-show{opacity:1;-webkit-transform:scaleY(1)!important;transform:scaleY(1)!important}notify-gp .notify-hide{-webkit-animation-name:fadeOutUp;animation-name:fadeOutUp;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}notify-gp .notify-success{background-color:#4caf50}notify-gp .notify-warning{background-color:#ffa000}notify-gp .notify-error{background-color:#ef5350}notify-gp .notify-info{background-color:#1976d2}notify-gp .notify-modal{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-flow:column nowrap;flex-flow:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;height:auto;max-height:200px;box-shadow:0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12),0 3px 1px -2px rgba(0,0,0,.2)}notify-gp .notify-modal .notify-modal-content{margin-top:5px;font-size:13px;white-space:normal}notify-gp .notify-modal .notify-modal-content a{margin:0;padding:0;color:inherit;font-size:inherit;text-decoration:underline;cursor:pointer}notify-gp .notify-modal .notify-modal-content a:active,notify-gp .notify-modal .notify-modal-content a:focus,notify-gp .notify-modal .notify-modal-content a:hover,notify-gp .notify-modal .notify-modal-content a:visited{color:inherit}notify-gp .notify-snackbar{position:fixed;bottom:0;left:50%;margin-bottom:5px;-webkit-transform-origin:left bottom 0;transform-origin:left bottom 0}.notify-position-lt-corner{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;margin:0 0 0 15px;left:0;right:auto}.notify-position-lb-corner{margin:0 0 15px 15px;right:auto;left:0}.notify-position-lb-corner,.notify-position-rb-corner{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-flow:column-reverse wrap-reverse;flex-flow:column-reverse wrap-reverse;top:auto;bottom:0}.notify-position-rb-corner{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;margin:0 15px 15px 0;left:auto;right:0}@-webkit-keyframes fadeOutUp{0%{opacity:1}to{margin-top:0;padding:0;height:0;min-height:0;opacity:0;-webkit-transform:scaleY(0);transform:scaleY(0)}}@keyframes fadeOutUp{0%{opacity:1}to{margin-top:0;padding:0;height:0;min-height:0;opacity:0;-webkit-transform:scaleY(0);transform:scaleY(0)}}@media (pointer:coarse){notify-gp{top:auto;bottom:0;left:0;margin:0 10px 10px}notify-gp notify{width:100%;max-width:600px}notify-gp .notify-hide,notify-gp .notify-show{-webkit-transform-origin:bottom!important;transform-origin:bottom!important}notify-gp .notify-snackbar{position:static}}</style><style type="text/css">:root{--sr-annote-color-0:#b4d9fb;--sr-annote-color-1:#ffeb3b;--sr-annote-color-2:#80deea;--sr-annote-color-3:#85d1f6;--sr-annote-color-4:#8cd842;--sr-annote-color-5:#ffb7da}[sr-annote-bg-color]{color:inherit}[sr-annote-bg-color][data-color-type="0"]{background-color:var(--sr-annote-color-0)}[sr-annote-bg-color][data-color-type="1"]{background-color:var(--sr-annote-color-1)}[sr-annote-bg-color][data-color-type="2"]{background-color:var(--sr-annote-color-2)}[sr-annote-bg-color][data-color-type="3"]{background-color:var(--sr-annote-color-3)}[sr-annote-bg-color][data-color-type="4"]{background-color:var(--sr-annote-color-4)}[sr-annote-bg-color][data-color-type="5"]{background-color:var(--sr-annote-color-5)}[sr-annote-bb-color][data-color-type="1"]{border-bottom-color:var(--sr-annote-color-1)}[sr-annote-bb-color][data-color-type="2"]{border-bottom-color:var(--sr-annote-color-2)}[sr-annote-bb-color][data-color-type="3"]{border-bottom-color:var(--sr-annote-color-3)}[sr-annote-bb-color][data-color-type="4"]{border-bottom-color:var(--sr-annote-color-4)}[sr-annote-bb-color][data-color-type="5"]{border-bottom-color:var(--sr-annote-color-5)}[sr-annote-bl-color][data-color-type="1"]{border-left:5px solid var(--sr-annote-color-1)}[sr-annote-bl-color][data-color-type="2"]{border-left:5px solid var(--sr-annote-color-2)}[sr-annote-bl-color][data-color-type="3"]{border-left:5px solid var(--sr-annote-color-3)}[sr-annote-bl-color][data-color-type="4"]{border-left:5px solid var(--sr-annote-color-4)}[sr-annote-bl-color][data-color-type="5"]{border-left:5px solid var(--sr-annote-color-5)}[data-color-style="1"]{background-color:transparent!important;background-repeat:no-repeat;background-size:100% 100%}[data-color-style="1"][data-color-type="1"]{background-color:transparent!important;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaCAMAAAB//6mtAAAAPFBMVEVHcEz/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zsmGdgxAAAAE3RSTlMAJJGnQ3Bh/ANS6An0u9o1FMp/ufrp4AAAAgZJREFUSMe1Vou2oyAM5BEIbxD+/183gFq99bZ7dnVOe6oWZwgJGRj7DcaY+et5BqtVFcA4eLraoLUFgJJLHzvBvuM00nPIzA4qazXkDuKka0VPIHNeiuee/RXO+qaAFjGFlpQQMi7JORcQ24Z+hRhcWpYYo5SikiQpkuBnldLXQwm5hANT53pxz9v1eZIdpEFCsmpLgXF+tVKGg640ZZrmDxacaNcIzqVE3EIoC7yYnjHuLxQ45cvquDP/RvgRiG4R2uYewlXKvXbo2r8x71NyFI4EZmZaj2vEShFJpnYHQopSAT/lgHEhZY2L+1/yVzAooJyyoEGEgLeEMFWC4K9NwHiVuFXlPSLNWbZ2g/61IuKN9AMyrwEUK5a7yQlLnUnwyr1v1luKCZsaq69kwwcCaC3ZWUZZPsFOkxaUA2pt8pHZE6KyhmnXbi3O1/xDSLkXqM8QH1FYwI+26nV1TwTQljy3ma8inSLYWiN+bp74ZX16hc4mnWkb7P0Jv9FejEB8+wsXlTfvLGIbthVvFN3jAbR0l34WUodzw6avJtIp1M6vya61EuTB1EqT3JW5jYfXiWw6Y626eOP7p5Th4FbrfqiZR4PxBjq5rv/BD8jplR12bcYBw44k4KCVtR9Q/Kszni/2W+MLh66n4WjLwz0LX5vqtDnqH4nitD8M/P10ZQ44q634A66hf2zVV84fAAAAAElFTkSuQmCC)}[data-color-style="1"][data-color-type="2"]{background-color:transparent!important;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaCAMAAAB//6mtAAAANlBMVEVHcEyA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3urNaunbAAAAEXRSTlMAe+8uaFLb/AQCG0ANkralydBaCTIAAAIBSURBVEjHtVaJduMwCJRA6ED3///sIstn6qbd13hy+NlxRjBYDEp9h+B9WI6JMRIZnSnEyKQPGKKIjOwnwoD6CXLPuHmeeI5RkREi+dKEEeMKuRKFnJPAq98gvKyf0ORiXbc619KaBXDy7r2Pzwq5Ym0TlFqz5BMROb1fxi96CGeDlcPNz855wji1NddaSpOFcl5kMxHvFvEoKtdi4WDbONbjLdxMokgKelWMGZP/olpgGrKWGfA7yrdw0EQpRMlh1Py1CMnYDv/P7MZri8lJQiXTRnnmV4lzG2X9C9bMQVTTkf2FH6voWJvtf8Whb76Um41GDeD6J+BG9Ttk3B8irziXz5AftbFmrcI4JMrNfXqFGnd5su2fZR8omnhJQMOlNp+Ekaap0NT+CHu3cT5GWPsT/M6Np0haW30ketlu1VBQBvpD+gAAygZWjFgeWaFQSmMbJKPhiQL0FtWyzbzO9ss+d66/dQHnfrAJ+U0rv1qNdOn93ndi3dI692Jy86o1cWt0Sb/0QCh5uHw0ptp+xyktfwXAfRitac17l1hQYbbAYpBng+JYzxHZ4SQ1C0zyczwIw8GJ5siRx2ywNWNopPzJymaps4ljJghjIvKKlr0nBlXEcLXYLX8/nYR9lBJDphEvnWaZZQwKjNvZDE6hUFfCqyvNAevis/7A+s+rFf8Do0ByAiavdJkAAAAASUVORK5CYII=)}[data-color-style="1"][data-color-type="3"]{background-color:transparent!important;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaCAMAAAB//6mtAAAANlBMVEVHcEyF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0fYOABKzAAAAEXRSTlMA3e8uaFJ7/AQCGz8Nt5KlzvXM15AAAAIFSURBVEjHtVaNesIgDARCCA0/gfd/2YXSWuvUbd/sfWqlpQdcIBdjXmGJcVmvmSEh+lBxSYkxHPCICRg4TiwD5idon9F5NiKnZNArkf4EhARpg95JSs5ZEc1vsDyMn8HXYqnbUKWUZp0j/fTex3eD3rG2tVaKSNX1JADO74eJqx7K2dzGQSdKbR8YTStVxgSabbWusvkEzwaJoCpLsY4mHU2unbK/AI1FjCUUCZtizJDjN9UWxiFrIdon2f8E2jUrqhSArmHE/HHXZG+7+yPzvW5zCGtLxT2u9/wmc20jrP/CDJnKVkLieOIHERlR6P8FTZ31Xz2Fm32A4Bz1T2AopnpXuG2iaLiWz5Afalm/RWFcMtZGnx5B0k2e2vqH6RUlIK8LCG6Pzcfhdfsb8NIvYe8W5zYC6VfwE41dpKlNLpm9HjfxuBjv+kX6OOdAD7BhgHLJCAVzHscg++CuCEBvyazHLIZqnyTH/tYFiH6wCX0WTNysRrP0re87sZ7SEj2Y3ETzaU90ORw5cAan1OHyyXux/RmnpvwNzj2fRmsh8C1LrJB1M6kj+cQzQTHKQUxKpjleqsLnOMuDZTg44iw5qtYGWhrMN1xDE+/qiRnqqtx5q4iiwfXskZt+q3bLr6uTm3ENQ8YxX7yrZdYyaGHYW3NyJim1IJxdaRZYp2InHtjePFvxF2YVcjg/uCn9AAAAAElFTkSuQmCC)}[data-color-style="1"][data-color-type="4"]{background-color:transparent!important;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaCAMAAAB//6mtAAAAMFBMVEWM2EJHcEyM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKKz/w5AAAAEHRSTlP/AANRaHvzGy3WqT4Nk77kx+qVTgAAAb9JREFUSMe1VtlyxDAIwwYDvv//b4udY5Nus9NOE71kJocA4SDAXcEb5rWIZiQKFX3OguEFQswqKv6FNxr4iXd/0UvOzsgRRwTUrHmF3clGLsXg3QfAifjwoCjV1hl6qCm12HkAvoF7jzG21lKqVk9WlfK5gkUP42wb2RvpGTHVkYBl0GqdslHW8kMAr0jBMj3y8W9CjCJmCWFVTFTLQQvYxMaB9Lu0r6NxW5Qq+xmBozzUocM/YQWliltfj012RWpsEe4Ax2aSiT/2wKk1y7rQ4T5Uk+pVgVDQwHwbO3fgqnuTvZPa4FbYP0R+CTC0Knh3AEPKawVCNcL9aAFlBPCB4SkQmPpK6SH6jmVKpE8FGKfIRttT9JwIPdBj+jN3dR6cTb8GT0RpOLwIXKHQH6kgZjeHnQ/v/wDf8R8Htww7rxT/wMhXN88PIuWyGk4J3w9vqsPlM1GKVyN/Qe98Na1DkNXRhCbSTILtZC1j3MYHpuMnnc3dzYBrpc0R/XDw6YRjqalzNdheR+d3y5yvW6urcZd9I8I9meG3ZrdyvZ3sT4pYxJEv7rsMrItQ0X17WT7Jw5VQT7Tvy5U/4RRsxRfv9RDt9/UjOgAAAABJRU5ErkJggg==)}[data-color-style="1"][data-color-type="5"]{background-color:transparent!important;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaCAMAAAB//6mtAAAAOVBMVEVHcEz/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r20adnAAAAEnRSTlMAVQ062Hwb/ARpK/Olk7XoScQnLSnCAAAB6ElEQVRIx7VW2Y7DIAzkMBCDOf//Y9cQmiZt2q12m1EegoTGF/ZYiFdYGONHWV+kRBOL0NryzwQalEV76+1yh/gAu4uLLVoURMkG0EivvZ6QsnRyqxgfse48XqE0xgSuZRMpVcg5MFw7wIUMALXWRBRnPOoXK5wPLZmz5snh5nfj3KGfgSJR6h7UGEfipNZnRhZf0FCCcOTaKNs53AiCQ0hkypox6716rgUnmwspU2d6y/geIdQeApsQzxVfhEJo+S/Mu5SFnCFFeSvrkd9G6GX9JzpBgJpMsccQPBEZqtD+bWHLb9yVexEWjTdPL/HvRkJuIfp7Ewgb03fI77nKeGvY3lgyVvdtC6RnABYjtO+ydyQj7QgAw6E23wT2/Hikdgl7g6LmC21X8DvXXxGPNrrEe263hNzSPf/X5CeEPNqA53O6xEKSalRAoclXFIArPLvMRDgZju+e7UFyXl1oRixTanhKb3ffJeuU1rlHkVsfKG6DTpmHGZhTl1cWICRoZ5xdiAdYqM/dgGqM3abEAIVV/xLqOcZtoTtxgBwAEkUG3hRx6QrO68VYOSLvBnzJTRekeNAzLnVkbnVbMIQcvedYn7restzaD7aTxVotu79lv8uMPUj57bDOV82qRNIfaM+Wq+WXresHSSV3cD8ocu8AAAAASUVORK5CYII=)}[data-color-style="2"]{background-color:transparent!important}[data-color-style="2"][data-color-type="1"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,var(--sr-annote-color-1) 0)}[data-color-style="2"][data-color-type="2"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,var(--sr-annote-color-2) 0)}[data-color-style="2"][data-color-type="3"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,var(--sr-annote-color-3) 0)}[data-color-style="2"][data-color-type="4"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,var(--sr-annote-color-4) 0)}[data-color-style="2"][data-color-type="5"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,var(--sr-annote-color-5) 0)}[data-color-style="3"]{position:relative;background-color:transparent!important}[data-color-style="3"]:after{content:"";position:absolute;left:0;bottom:25px;height:8px;width:58px;border-radius:4px;opacity:.8;transition:all .3s}[data-color-style="3"][data-color-type="1"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 85%,var(--sr-annote-color-1) 0)}[data-color-style="3"][data-color-type="2"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 85%,var(--sr-annote-color-2) 0)}[data-color-style="3"][data-color-type="3"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 85%,var(--sr-annote-color-3) 0)}[data-color-style="3"][data-color-type="4"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 85%,var(--sr-annote-color-4) 0)}[data-color-style="3"][data-color-type="5"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 85%,var(--sr-annote-color-5) 0)}sr-annote-note{position:relative;bottom:-5px;padding:0 4px;color:#fff;background-color:#333;font-weight:700;font-style:normal;font-family:arial,helvetica,clean,sans-serif;border-radius:5px;opacity:.8;cursor:pointer}sr-annote-note:after{content:"N"}pre.sr-annote+sr-annote-note{bottom:25px;right:25px}sr-annote-note:hover{opacity:1}sr-annote-note sr-annote-note-tip{position:absolute;left:22px;top:0;padding:.5em 1em;max-width:400px;color:#fff;background:#101010;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;font-weight:400;font-style:normal;text-shadow:none;font-size:12px;text-indent:0;white-space:pre;z-index:10;border-radius:5px;box-shadow:0 0 10px rgba(0,0,0,.3);opacity:0;overflow:auto;pointer-events:none;z-index:20000;transition:all .18s ease-out .18s}sr-annote-note:hover sr-annote-note-tip{opacity:1;pointer-events:auto}sr-annote-note sr-annote-note-tip{overflow:hidden}sr-annote-note sr-annote-note-tip:hover{overflow:overlay}sr-annote-note sr-annote-note-tip::-webkit-scrollbar-track{background-color:transparent}sr-annote-note sr-annote-note-tip::-webkit-scrollbar{width:12px}sr-annote-note sr-annote-note-tip::-webkit-scrollbar-thumb{background-clip:padding-box;padding-top:80px;background-color:#ddd;border:3px solid transparent;border-radius:8px}</style><style type="text/css">.sr-annote-hideall{background-color:transparent!important;pointer-events:none}sr-annote-trigger{position:fixed!important;bottom:52px;right:32px;display:-webkit-box!important;display:-ms-flexbox!important;display:flex!important;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 0 15px;padding:0;width:40px!important;height:40px!important;line-height:40px!important;color:#fff;background-color:rgba(245,82,70,.8);border-radius:50%;cursor:pointer;box-shadow:0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12),0 3px 1px -2px rgba(0,0,0,.2);-webkit-transition:all .5s ease-in-out 0ms;transition:all .5s ease-in-out 0ms;overflow:visible!important;overflow:initial!important}sr-annote-trigger.open{right:95px}sr-annote-trigger.off{background-color:#bdbdbd}sr-annote-trigger sr-i{display:-webkit-box!important;display:-ms-flexbox!important;display:flex!important;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;border-radius:50%}sr-annote{padding:6px 0;background-color:transparent;font-size:inherit;cursor:pointer}.sr-annote[data-type=code],.sr-annote[data-type=img]{border-bottom-width:5px;border-bottom-style:solid}sr-annote[data-color-type="0"]{padding:7px 0}sr-annote-floating{position:fixed;color:#fff;background:hsla(0,0%,6%,.95);font-weight:700;border-radius:5px;box-shadow:0 0 10px rgba(0,0,0,.3);opacity:0;-webkit-animation-delay:.2s;animation-delay:.2s;-webkit-animation-duration:.5s;animation-duration:.5s;-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:sr-annote-slideInUp;animation-name:sr-annote-slideInUp;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s;z-index:2000}sr-annote-floating.hidden{-webkit-animation-duration:.5s;animation-duration:.5s;-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:sr-annote-slideInDown;animation-name:sr-annote-slideInDown;pointer-events:none}.sr-annote-floatingbar-hiden{display:none}@-webkit-keyframes sr-annote-slideInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0);visibility:visible}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@keyframes sr-annote-slideInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0);visibility:visible}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@-webkit-keyframes sr-annote-slideInDown{0%{opacity:1;visibility:visible}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}@keyframes sr-annote-slideInDown{0%{opacity:1;visibility:visible}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}sr-annote-floatingbar{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin:5px}sr-annote-floatingbar,sr-annote-floatingbar sr-anote-fb-item{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-annote-floatingbar sr-anote-fb-item{margin-right:5px;cursor:pointer}sr-annote-floatingbar sr-anote-fb-item:last-child{margin-right:0}sr-annote-floatingbar sr-anote-fb-item{width:20px;height:20px;border-radius:50%;box-sizing:border-box}sr-annote-floatingbar sr-anote-fb-item[type=copy],sr-annote-floatingbar sr-anote-fb-item[type=export],sr-annote-floatingbar sr-anote-fb-item[type=note],sr-annote-floatingbar sr-anote-fb-item[type=remove],sr-annote-floatingbar sr-anote-fb-item[type=style],sr-annote-floatingbar sr-anote-fb-item[type=tag]{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#fff;border-radius:50%}sr-annote-floatingbar sr-anote-fb-item[type=style]{background-color:#f73859!important}sr-annote-floatingbar sr-anote-fb-item[type=export]{background-color:#cc0e74}sr-annote-floatingbar sr-anote-fb-item[type=copy]{background-color:#a78674}sr-annote-floatingbar sr-anote-fb-item[type=remove]{background-color:#f44336}sr-annote-floatingbar sr-anote-fb-item[remove=confirm]{-webkit-transform:rotate(270deg);transform:rotate(270deg);-webkit-transition:all .2s ease-in-out 0s;transition:all .2s ease-in-out 0s}sr-annote-sidebar-bg{position:fixed;top:0;right:0;bottom:180px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;width:256px;font-size:22.4px;font-size:1.4rem;font-weight:500;opacity:0;-webkit-transform:translateX(256px);transform:translateX(256px);-webkit-transition:all .45s cubic-bezier(.23,1,.32,1) 0ms;transition:all .45s cubic-bezier(.23,1,.32,1) 0ms}sr-annote-sidebar-bg.mini{pointer-events:none}sr-annote-sidebar-bg:hover{z-index:2147483647}sr-annote-sidebar-bg.open{opacity:1;-webkit-transform:translateX(0);transform:translateX(0)}sr-annote-sidebar{margin:3px 4px 0;padding-left:20px;height:100%;overflow-x:hidden}sr-annote-sidebar.mini{pointer-events:none}sr-annote-sidebar *{box-sizing:border-box}sr-annote-sidebar{overflow-y:hidden}sr-annote-sidebar:hover{overflow-y:overlay}sr-annote-sidebar::-webkit-scrollbar-track{background-color:transparent}sr-annote-sidebar::-webkit-scrollbar{width:12px}sr-annote-sidebar::-webkit-scrollbar-thumb{padding-top:80px;background-clip:padding-box;background-color:#ddd;border:3px solid transparent;border-radius:8px;border-radius:10px;border:6px solid transparent;background-color:rgba(85,85,85,.55)}sr-annote-sidebar::-webkit-scrollbar{width:0;-webkit-transition:width .7s cubic-bezier(.4,0,.2,1);transition:width .7s cubic-bezier(.4,0,.2,1)}sr-annote-sidebar:hover::-webkit-scrollbar{width:16px}sr-annote-sidebar[srcoll=on]:hover sr-annote-sidebar-card[type=option].mini{-webkit-transform:translateX(-16px);transform:translateX(-16px)}sr-annote-sidebar[srcoll=on]:hover sr-annote-sidebar-card.off{-webkit-transform:translateX(190px);transform:translateX(190px)}sr-annote-sidebar-cards{display:block}sr-annote-sidebar-card{position:relative;display:block!important;margin:12px;color:rgba(51,51,51,.87);background-color:#fff;border-radius:4px;box-shadow:0 2px 5px rgba(0,0,0,.08);-webkit-transition:all .45s cubic-bezier(.23,1,.32,1) 0ms;transition:all .45s cubic-bezier(.23,1,.32,1) 0ms;pointer-events:auto}sr-annote-sidebar-card:hover{box-shadow:0 10px 20px 0 rgba(168,182,191,.6);-webkit-transform:translateY(-1px);transform:translateY(-1px)}sr-annote-sidebar-card:last-child{margin-bottom:30px}sr-annote-sidebar-card.off{display:block;-webkit-transform:translateX(205px);transform:translateX(205px);-webkit-transition:all .25s ease-out;transition:all .25s ease-out}sr-annote-sidebar-card.off:hover{-webkit-transform:translateX(120px)!important;transform:translateX(120px)!important}sr-annote-sidebar-card.hide{display:block;-webkit-transform:translateX(256px);transform:translateX(256px);-webkit-transition:all .25s ease-out;transition:all .25s ease-out}sr-annote-sidebar-card-anchor{position:absolute;left:0;top:0;width:90%;height:100%}sr-annote-sidebar-card-action{position:absolute;top:10px;right:3px;display:block;width:12px;height:12px;line-height:12px;-webkit-transition:all .25s ease-out;transition:all .25s ease-out;cursor:pointer;z-index:20000}sr-annote-sidebar-card-action.open{-webkit-transform:rotate(90deg);transform:rotate(90deg)}sr-annote-sidebar-card[mode=mini]{overflow:hidden}sr-annote-sidebar-card[mode=mini] sr-annote-sidebar-preview{display:block;padding:6px 12px 5px 10px;height:32px;color:#fff;font-size:13px;font-weight:400;white-space:nowrap;text-align:left;text-overflow:ellipsis;text-shadow:1px 1px 3px rgba(0,0,0,.3);-webkit-transition:all .25s ease-out;transition:all .25s ease-out;overflow:hidden}sr-annote-sidebar-card[mode=mini][type=img] sr-annote-sidebar-preview{text-align:center}sr-annote-sidebar-card[mode=mini] sr-annote-sidebar-detail{padding:0 15px;height:0}sr-annote-sidebar-card[mode=mini] sr-annote-sidebar-note,sr-annote-sidebar-card[mode=mini] sr-annote-sidebar-toolbars{display:none}sr-annote-sidebar-card[mode=mini] pre{margin:0!important;padding:0!important;white-space:nowrap;text-overflow:ellipsis;overflow:hidden}sr-annote-sidebar-card[mode=normal] sr-annote-sidebar-preview{display:none}sr-annote-sidebar-card[mode=normal] sr-annote-sidebar-detail{padding:15px;height:auto}sr-annote-sidebar-card[data-color-type="0"]{display:none!important}sr-annote-sidebar-card pre{margin:0!important;padding:0!important;background-color:transparent!important;max-height:200px;font-size:10px;overflow:hidden}sr-annote-sidebar-card input,sr-annote-sidebar-card textarea{font-size:12px!important}sr-annote-sidebar-card img{margin:0;padding:0;max-height:100px;max-width:80%;background:#fff;border:0;border-radius:6px;box-shadow:0 20px 20px -10px rgba(0,0,0,.1)}sr-annote-sidebar-detail{display:block;padding:15px;width:100%;color:#fff;font-size:10px;text-align:justify;border-top-left-radius:4px;border-top-right-radius:4px;-webkit-transition:all .25s ease-out;transition:all .25s ease-out}sr-annote-sidebar-card[type=img] sr-annote-sidebar-detail{text-align:center}sr-annote-sidebar-tags{-ms-flex-wrap:wrap;flex-wrap:wrap;margin-top:15px}sr-annote-sidebar-tag,sr-annote-sidebar-tags{display:-webkit-box;display:-ms-flexbox;display:flex}sr-annote-sidebar-tag{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;padding:4px 12px;color:rgba(0,0,0,.87);background-color:#fff;height:22px;font-size:14px;font-size:.875rem;font-weight:400;white-space:nowrap;border-radius:16px;outline:none;cursor:pointer;overflow:hidden;-webkit-transition:all .2s ease-in-out 0s;transition:all .2s ease-in-out 0s;-webkit-transform-origin:left;transform-origin:left;-webkit-transform:scale(.8);transform:scale(.8)}sr-annote-sidebar-note{display:block;padding:16px;width:100%;background-color:#fff;text-align:left}sr-annote-sidebar-toolbars{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;padding-right:10px;height:32px;background-color:#fff;border-bottom-left-radius:4px;border-bottom-right-radius:4px}sr-annote-sidebar-toolbar,sr-annote-sidebar-toolbars{-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-annote-sidebar-toolbar{display:-webkit-box!important;display:-ms-flexbox!important;display:flex!important;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin-left:5px;width:20px;height:20px;line-height:20px;border-radius:50%;overflow:visible!important;overflow:initial!important}sr-annote-sidebar-toolbar[remove=confirm]{-webkit-transform:rotate(270deg);transform:rotate(270deg);-webkit-transition:all .2s ease-in-out 0s;transition:all .2s ease-in-out 0s}sr-annote-sidebar-toolbar[remove=confirm] svg path{fill:#f44336}sr-annote-sidebar-card[type=unread]{background-color:#cb63e6}sr-annote-sidebar-card[type=unread] sr-annote-sidebar-detail.title{padding:0;text-align:left;font-size:15px}sr-annote-sidebar-card[type=unread] sr-annote-sidebar-detail.desc{padding:0 0 0 10px;border-left:1px outset #fff;border-top-left-radius:0;border-top-right-radius:0}sr-annote-sidebar-card[type=unread][mode=mini] sr-annote-sidebar-preview{font-size:13px;text-shadow:1px 1px 3px rgba(0,0,0,.3)}sr-annote-sidebar-card[type=option]{height:32px;background-color:transparent;box-shadow:none;overflow:visible;overflow:initial}sr-annote-sidebar-card[type=option]:hover{-webkit-transform:translateY(0);transform:translateY(0)}sr-annote-sidebar-card[type=option].mini{margin-right:0}sr-annote-sidebar-card[type=option] sr-annote-sidebar-card-action,sr-annote-sidebar-card[type=option] sr-annote-sidebar-card-anchor{display:none}sr-annote-sidebar-card[type=option] sr-annote-sidebar-preview{background-color:transparent}sr-annote-sidebar-options{position:absolute;top:0;right:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;padding-right:5px;height:100%;background-color:#3a3a3a;border-left:10px outset #222;border-radius:4px}sr-annote-sidebar-option,sr-annote-sidebar-options{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-annote-sidebar-option{display:-webkit-box!important;display:-ms-flexbox!important;display:flex!important;margin-left:5px;width:20px;height:20px;border-radius:50%;-webkit-transition:all .25s ease-out;transition:all .25s ease-out;cursor:pointer;overflow:visible!important;overflow:initial!important}sr-annote-sidebar-option[off=true],sr-annote-sidebar-option[side=false]:not(:first-child){width:0;margin-left:0}sr-annote-sidebar-option[side=false]:first-child{margin-right:5px}sr-annote-sidebar-option[off=true]~sr-annote-sidebar-option:last-child svg{-webkit-transform:rotate(180deg);transform:rotate(180deg)}sr-annote-sidebar-option[type=drag][state=on] svg path,sr-annote-sidebar-option[type=export][state=on] svg path,sr-annote-sidebar-option[type=goon][state=on] svg path,sr-annote-sidebar-option[type=save][state=on] svg path{fill:#8cd842}sr-annote-sidebar-option[type=collapse][state=on] svg{-webkit-transform:rotate(90deg);transform:rotate(90deg)}sr-annote-sidebar-option[lock=true] svg path{fill:#f55246!important}sr-annote-sidebar-card[type=option]:hover~sr-annote-sidebar-card[type=unread]{z-index:-1}</style><style type="text/css">@-webkit-keyframes fadeInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@keyframes fadeInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@-webkit-keyframes fadeOutDown{0%{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}@keyframes fadeOutDown{0%{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}.sr-alertgp{position:fixed;left:0;top:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;background-color:rgba(51,51,51,.8);z-index:2147483647}.sr-alertgp .alert{min-width:400px;min-height:400px;width:650px;background-color:#fff;border-radius:4px;box-shadow:0 14px 45px rgba(0,0,0,.247059);-webkit-animation-name:fadeInUp;animation-name:fadeInUp;-webkit-animation-duration:.8s;animation-duration:.8s;-webkit-animation-fill-mode:both;animation-fill-mode:both}.sr-alertgp .alert,.sr-alertgp .alert .loading{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.sr-alertgp .alert .loading{position:relative;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:100%;height:35%;background-color:transparent}.alert .loading .progress{display:block;margin:10px auto;max-width:80%;max-height:250px;width:20%}.alert .loading .progress .percentage{fill:#666;font-family:sans-serif;font-size:.5em;text-anchor:middle}.alert .loading .progress .circle-bg{fill:none;stroke:#eee;stroke-width:3.8}.alert .loading .progress .circle{fill:none;stroke-width:2.8;stroke-linecap:round;stroke:#1dba90;-webkit-transition:all .2s ease-in-out;transition:all .2s ease-in-out}@-webkit-keyframes scaleAnimation{0%{opacity:0;-webkit-transform:scale(1.5);transform:scale(1.5)}to{opacity:1;-webkit-transform:scale(1);transform:scale(1)}}@keyframes scaleAnimation{0%{opacity:0;-webkit-transform:scale(1.5);transform:scale(1.5)}to{opacity:1;-webkit-transform:scale(1);transform:scale(1)}}@-webkit-keyframes fadeOut{0%{opacity:1}to{opacity:0}}@keyframes fadeOut{0%{opacity:1}to{opacity:0}}@-webkit-keyframes fadeIn{0%{opacity:0}to{opacity:1}}@keyframes fadeIn{0%{opacity:0}to{opacity:1}}.sr-alertgp .alert .close{position:absolute;top:0;right:0;z-index:2}.sr-alertgp .alert .close:hover{-webkit-transform:rotate(270deg);transform:rotate(270deg);-webkit-transition:all .25s ease-out;transition:all .25s ease-out}.sr-alertgp .alert .sr-alert-icon img{max-width:650px;width:100%;-webkit-transform:scale(.7);transform:scale(.7);-webkit-transition:all .5s ease-out;transition:all .5s ease-out}.sr-alertgp .alert .actionbar{position:relative;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:20px;width:80%;height:50px;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) 0ms;transition:all 1s cubic-bezier(.23,1,.32,1) 0ms}.sr-alertgp .alert .actionbar,.sr-alertgp .alert.notification{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center}.sr-alertgp .alert.notification{width:500px;min-height:350px}.sr-alertgp .alert.notification[data-state=siren]{background-image:-webkit-radial-gradient(circle farthest-corner at 10% 20%,#cd212a 0,#ec5f05 90%);background-image:radial-gradient(circle farthest-corner at 10% 20%,#cd212a 0,#ec5f05 90%)}.sr-alertgp .alert.notification[data-state=lock]{background-image:-webkit-radial-gradient(circle farthest-corner at 10% 20%,#8451a1 0,rgba(132,81,161,.83) 90%);background-image:radial-gradient(circle farthest-corner at 10% 20%,#8451a1 0,rgba(132,81,161,.83) 90%)}.sr-alertgp .alert.notification[data-state=warning]{background-image:-webkit-linear-gradient(left,#f2709c,#ff9472);background-image:linear-gradient(90deg,#f2709c,#ff9472)}.sr-alertgp .alert.notification[data-state=bug]{background-image:-webkit-linear-gradient(bottom,#ad5389,#3c1053);background-image:linear-gradient(0deg,#ad5389,#3c1053)}.sr-alertgp .alert.notification[data-state=safe],.sr-alertgp .alert.notification[data-state=server]{background-color:#8ec5fc;background-image:-webkit-linear-gradient(28deg,#8ec5fc,#e0c3fc);background-image:linear-gradient(62deg,#8ec5fc,#e0c3fc)}.sr-alertgp .alert.notification .sr-alert-icon{position:relative;width:100%}.sr-alertgp .alert.notification .loading .progress{padding:5px;background-color:#fff;border-radius:50%}.sr-alertgp .alert.notification .loading .progress .circle-bg{stroke:transparent}.sr-alertgp .alert.notification .loading .progress .circle{stroke-width:1}.sr-alertgp .alert.notification[data-state=siren] .loading .progress .circle{stroke:#cd212a}.sr-alertgp .alert.notification[data-state=lock] .loading .progress .circle{stroke:#8451a1}.sr-alertgp .alert.notification[data-state=warning] .loading .progress .circle{stroke:#f2709c}.sr-alertgp .alert.notification[data-state=bug] .loading .progress .circle{stroke:#ad5389}.sr-alertgp .alert.notification[data-state=safe] .loading .progress .circle,.sr-alertgp .alert.notification[data-state=server] .loading .progress .circle{stroke:#8ec5fc}.sr-alertgp .alert.notification .content{padding:10px 70px;width:100%;color:#fff;text-align:center;font-size:28.8px;font-size:1.8rem;box-sizing:border-box}.sr-alertgp .alert.notification .flag{position:absolute;left:0;top:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;z-index:100000}.sr-alertgp .alert.notification .flag img{width:50px}.sr-alertgp .alert.notification[data-state=lock] .flag img{width:30px}.sr-alertgp .alert.notification .flag img.swing{-webkit-transform-origin:center;transform-origin:center;-webkit-animation-name:swing;animation-name:swing;-webkit-animation-duration:1s;animation-duration:1s}.sr-alertgp .alert.notification .actionbar{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-pack:distribute;justify-content:space-around;margin:0}.sr-alertgp .alert.notification .return{padding:5px 32px;color:#333;background-color:#fff;font-size:15px;font-weight:700;border-radius:56px}.sr-alertgp .alert.notification .return:hover{box-shadow:0 3px 1px -2px rgba(0,0,0,.2),0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12)}@-webkit-keyframes swing{20%{-webkit-transform:rotate(15deg);transform:rotate(15deg)}40%{-webkit-transform:rotate(-10deg);transform:rotate(-10deg)}60%{-webkit-transform:rotate(5deg);transform:rotate(5deg)}80%{-webkit-transform:rotate(-5deg);transform:rotate(-5deg)}to{-webkit-transform:rotate(0deg);transform:rotate(0deg)}}@keyframes swing{20%{-webkit-transform:rotate(15deg);transform:rotate(15deg)}40%{-webkit-transform:rotate(-10deg);transform:rotate(-10deg)}60%{-webkit-transform:rotate(5deg);transform:rotate(5deg)}80%{-webkit-transform:rotate(-5deg);transform:rotate(-5deg)}to{-webkit-transform:rotate(0deg);transform:rotate(0deg)}}</style><style type="text/css">@-webkit-keyframes fadeInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@keyframes fadeInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@-webkit-keyframes fadeOutDown{0%{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}@keyframes fadeOutDown{0%{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}sr-promo-bg{position:fixed;right:15px;bottom:15px;z-index:2147483646}sr-promo,sr-promo-notice{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:10px;color:rgba(51,51,51,.87);background-color:#fff;border-radius:4px;box-shadow:0 12px 18px -6px rgba(0,0,0,.3);overflow:hidden;-webkit-transform-origin:bottom;transform-origin:bottom;-webkit-transition:all .6s ease;transition:all .6s ease;-webkit-animation-name:fadeInUp;animation-name:fadeInUp;-webkit-animation-duration:.5s;animation-duration:.5s;-webkit-animation-fill-mode:both;animation-fill-mode:both}sr-promo img{width:220px;cursor:pointer}sr-promo-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;padding:10px 0 0;width:100%}sr-promo-a{padding:5px 10px;color:#fff;font-size:12px;font-weight:700;border-radius:2px;box-shadow:0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12),0 3px 1px -2px rgba(0,0,0,.2);cursor:pointer}sr-promo-a.later{background-color:#2196f3}sr-promo-a.cancel{background-color:#757575}sr-promo-tip{font-size:12px;padding:5px 10px;border-radius:2px}sr-promo-notice{position:absolute;top:10px;left:10px;right:10px;height:293px;padding-bottom:0;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;font-size:12px;border-radius:0;box-shadow:none;overflow-y:hidden}sr-promo-notice:hover{overflow-y:overlay}sr-promo-notice-title{font-weight:700;text-align:center;margin-bottom:5px;width:100%;font-size:14px}sr-promo-notice-content{margin-top:5px}</style><style type="text/css">dialog-gp .carousel,welcome .carousel{position:relative;width:100%;height:400px;-webkit-perspective:500px;perspective:500px;-webkit-transform-style:preserve-3d;transform-style:preserve-3d;-webkit-transform-origin:0 50%;transform-origin:0 50%;overflow:hidden}dialog-gp .carousel.carousel-slider,welcome .carousel.carousel-slider{top:0;left:0;height:100%}dialog-gp .carousel.carousel-slider .carousel-item,welcome .carousel.carousel-slider .carousel-item{position:absolute;top:0;left:0;width:100%;height:100%;min-height:400px}dialog-gp .carousel .carousel-item,welcome .carousel .carousel-item{display:none;position:absolute;top:0;left:0;width:200px;height:200px}dialog-gp .carousel .carousel-item>img,welcome .carousel .carousel-item>img{width:100%}dialog-gp .carousel .indicators,welcome .carousel .indicators{position:absolute;margin:0;padding:0;left:0;right:0;bottom:0;text-align:center}dialog-gp .carousel .indicators .indicator-item,welcome .carousel .indicators .indicator-item{display:inline-block;position:relative;margin:14px 4px;height:10px;width:10px;background-color:#e0e0e0;-webkit-transition:background-color .3s;transition:background-color .3s;border-radius:50%;cursor:pointer}dialog-gp .carousel .indicators .indicator-item.active,welcome .carousel .indicators .indicator-item.active{background-color:#4caf50}dialog-gp .carousel .carousel-item:not(.active) .materialboxed,dialog-gp .carousel.scrolling .carousel-item .materialboxed,welcome .carousel .carousel-item:not(.active) .materialboxed,welcome .carousel.scrolling .carousel-item .materialboxed{pointer-events:none}</style><style type="text/css">.simpread-upgrade-root *{box-sizing:border-box}.simpread-upgrade-root{-webkit-transition:all .25s ease-out;transition:all .25s ease-out}.simpread-upgrade-root.open{background-color:rgba(51,51,51,.8)}.simpread-upgrade-root dialog-gp{position:relative}.simpread-upgrade-root dialog-gp .close{position:fixed;top:0;right:0;z-index:2}.simpread-upgrade-root dialog-gp .close:hover{-webkit-transform:rotate(270deg);transform:rotate(270deg);-webkit-transition:all .25s ease-out;transition:all .25s ease-out}.simpread-upgrade-root dialog-content{padding-bottom:80px!important;overflow-y:hidden}.simpread-upgrade-root dialog-content:hover{overflow-y:overlay}.simpread-upgrade-root dialog-content::-webkit-scrollbar-track{background-color:transparent}.simpread-upgrade-root dialog-content::-webkit-scrollbar{width:12px}.simpread-upgrade-root dialog-content::-webkit-scrollbar-thumb{background-clip:padding-box;padding-top:80px;background-color:#ddd;border:3px solid transparent;border-radius:8px}.simpread-upgrade-root .floating{position:absolute;left:0;right:0;bottom:0;height:80px;overflow-y:hidden}.simpread-upgrade-root .floating,.simpread-upgrade-root .floating .billing{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.simpread-upgrade-root .floating .billing{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:1px 40px;color:#fff;background-color:#4dbb7c;font-size:15px;font-weight:400;opacity:0;border-radius:30px;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.simpread-upgrade-root .floating .billing.open{-webkit-animation-name:fadeInUp;animation-name:fadeInUp;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-fill-mode:both;animation-fill-mode:both}.simpread-upgrade-root .floating .billing .sales{font-size:12px}.simpread-upgrade-root .floating .billing .rate{margin:0 5px}.simpread-upgrade-root .floating .billing .price{margin-left:2px;margin-right:5px}.upgrade{position:relative;color:rgba(51,51,51,.87);font-family:Hiragino Sans GB,Microsoft Yahei;text-shadow:none}.upgrade .head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.upgrade .head img{margin-bottom:5px;width:60px;border-radius:9px;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.upgrade .head .title{margin:10px 0;font-weight:700;font-size:15px}.upgrade .head .desc{width:70%;text-align:center;font-size:13px}.upgrade .features{position:relative;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-pack:distribute;justify-content:space-around;-webkit-transition:height .2s cubic-bezier(.23,1,.32,1) 0ms;transition:height .2s cubic-bezier(.23,1,.32,1) 0ms}.upgrade .features.init{height:100px}.upgrade .features.init .base,.upgrade .features.init .pro{opacity:0}.upgrade .loading{position:absolute;left:0;top:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;background-color:#fff;z-index:1}.upgrade .loading span{width:50px;height:50px;opacity:.87}.features.error{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:inherit;font-size:14px}.features.error img{margin:10px 0;width:300px}.upgrade .base,.upgrade .pro{margin:20px 20px 13px;width:100%;text-align:center;opacity:1;-webkit-transition:opacity .2s cubic-bezier(.23,1,.32,1) 0ms;transition:opacity .2s cubic-bezier(.23,1,.32,1) 0ms}.upgrade .pricecard{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;border-radius:4px;border:1px solid #eef1f4;position:relative}.upgrade .pro .pricecard{border:2px solid #4dbb7c;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.upgrade .pricecard .mode{margin:20px 10px 10px;font-size:18px;font-weight:700}.upgrade .pricecard .sales{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;height:92px;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center}.upgrade .pricecard .discountrate{position:absolute;top:-12px;left:0;right:0}.upgrade .pricecard .discountrate .rate{padding:3px 10px;color:#fff;background-color:#4dbb7c;font-weight:700;font-size:13px;border-radius:10px}.upgrade .pricecard .desc{font-size:30px}.upgrade .pricecard .desc del{font-size:15px;font-weight:700;text-decoration:line-through}.upgrade .pricecard .price{position:relative;color:#4dbb7c;font-size:30px;font-weight:700}.upgrade .pricecard .message{position:relative;font-size:11px;font-weight:400;background-image:-webkit-linear-gradient(top,hsla(0,0%,100%,0) 50%,#ffeb3b 0);background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,#ffeb3b 0)}.upgrade .pricecard .countdown{margin-top:5px}.upgrade .pricecard .billing{position:relative;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:30px 20%;padding:5px;color:#333;background-color:#e2e2e2;font-size:15px;font-style:normal;font-weight:700;border-radius:4px;cursor:pointer}.upgrade .pricecard .billing,.upgrade .pricecard .billing i{-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms}.upgrade .pricecard .billing i{height:27px;line-height:22px;margin-left:5px}.upgrade .pricecard .billing:hover{box-shadow:0 3px 1px -2px rgba(0,0,0,.2),0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12)}.upgrade .pricecard .billing:hover i{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.upgrade .pricecard .billing:hover .dropdown-price{opacity:1;-webkit-transform:scale(1);transform:scale(1)}.upgrade .pricecard .billing .dropdown-price{position:absolute;left:-41px;top:38px;-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:270px;color:rgba(51,51,51,.87);font-size:12px;text-shadow:none;box-sizing:border-box;border-radius:4px;box-shadow:0 8px 10px 1px rgba(0,0,0,.14),0 3px 14px 2px rgba(0,0,0,.12),0 5px 5px -3px rgba(0,0,0,.2);opacity:0;-webkit-transform:scaleY(0);transform:scaleY(0);-webkit-transform-origin:left top 0;transform-origin:left top 0;-webkit-transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms,-webkit-transform .45s cubic-bezier(.23,1,.32,1) 0ms;transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms,-webkit-transform .45s cubic-bezier(.23,1,.32,1) 0ms;transition:transform .45s cubic-bezier(.23,1,.32,1) 0ms,opacity 1s cubic-bezier(.23,1,.32,1) 0ms;transition:transform .45s cubic-bezier(.23,1,.32,1) 0ms,opacity 1s cubic-bezier(.23,1,.32,1) 0ms,-webkit-transform .45s cubic-bezier(.23,1,.32,1) 0ms;z-index:1}.upgrade .pricecard .billing .dropdown-price,.upgrade .pricecard .billing .dropdown-price .store{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#fff}.upgrade .pricecard .billing .dropdown-price .store{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;padding:8px 24px 8px 16px;width:100%;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) 0ms;transition:all 1s cubic-bezier(.23,1,.32,1) 0ms;cursor:pointer}.upgrade .pricecard .billing .dropdown-price .store:hover{background-color:#eee}.upgrade .pricecard .billing .dropdown-price .store:hover i{-webkit-transform:rotate(270deg) translateY(7px);transform:rotate(270deg) translateY(7px)}.upgrade .pricecard .billing .dropdown-price .store .names{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;width:135px}.upgrade .pricecard .billing .dropdown-price .store .des{width:100%;color:rgba(51,51,51,.56);text-align:left;font-size:10px;-webkit-transform:scale(.8) translateX(-17px);transform:scale(.8) translateX(-17px)}.upgrade .pricecard .billing .dropdown-price .store .num{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;width:46px}.upgrade .pricecard .billing .dropdown-price .store .tips{-webkit-transform:scale(.8);transform:scale(.8);color:#4dbb7c}.upgrade .pricecard .billing .dropdown-price .store i{-webkit-transform:rotate(270deg);transform:rotate(270deg)}.upgrade .base[data-enable=false] .pricecard,.upgrade .pro[data-enable=true] .pricecard{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;height:210px}.upgrade .base[data-enable=false] .pricecard .mode,.upgrade .pro[data-enable=true] .pricecard .mode{font-size:25px}.upgrade .base[data-enable=false] .pricecard .billing,.upgrade .base[data-enable=false] .pricecard .sales,.upgrade .pro[data-enable=true] .pricecard .billing .dropdown-price,.upgrade .pro[data-enable=true] .pricecard .billing i,.upgrade .pro[data-enable=true] .pricecard .discountrate,.upgrade .pro[data-enable=true] .pricecard .sales{display:none}.upgrade .pro[data-enable=true] .pricecard .billing{position:absolute;top:-28px;left:0;right:0;display:inherit;margin:10px 20%;border-radius:30px}.upgrade .pro .billing{color:#fff;background-color:#4dbb7c}.upgrade .features.diff{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;margin-top:20px}.upgrade .features.diff,.upgrade .features .feature{-webkit-box-direction:normal;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.upgrade .features .feature{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:unset;-ms-flex-pack:unset;justify-content:unset;margin:14px 20px 0;font-size:15px}.upgrade .features .feature.empty{height:27px}.upgrade .features .icon{margin-right:10px;width:15px}.upgrade .features .label{width:120px;font-size:15px;text-align:left}.upgrade .features a{color:inherit;cursor:auto}.upgrade .features a.active{padding-bottom:5px;border-bottom:1px dotted;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;cursor:pointer}.upgrade .features a.active:hover{color:#4285f4}.upgrade .features .label .remark{margin-left:5px;padding:2px 5px;background-color:#ffeb3b;font-size:12px;font-weight:400;border-radius:4px}.upgrade .features .label .remark.roadmap{background-color:#e2e2e2}.upgrade .ticket{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;font-size:13px}.upgrade .ticket .message{width:70%;text-align:center}.upgrade .ticket .line{margin:7px 0 0;width:100%;height:1px;background-image:-webkit-linear-gradient(.1deg,rgba(255,18,18,0) -2.8%,#e2e2e2 50.8%,rgba(0,159,8,0) 107.9%);background-image:linear-gradient(89.9deg,rgba(255,18,18,0) -2.8%,#e2e2e2 50.8%,rgba(0,159,8,0) 107.9%)}.upgrade .ticket .notice{margin:20px 20%;padding:5px 20px;color:#333;background-color:#e2e2e2;font-size:15px;font-weight:400;border-radius:4px}.upgrade .ticket .content{margin:0 0 13px;width:80%}.upgrade .ticket .content li{margin-bottom:6px}.upgrade .ticket .content li:last-child{margin-bottom:0}.upgrade .ticket .last{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-pack:distribute;justify-content:space-around;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:20px 0;width:100%}.upgrade .carousels{margin:20px 20px 13px}.upgrade .carousel.carousel-slider{height:420px;border-radius:4px;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.upgrade .carousels setion{position:relative}.upgrade .carousels setion img{margin-top:-82px;width:100%}.upgrade .carousels .descr{position:absolute;left:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:80px;width:100%;padding-bottom:10px;background-color:#fff;font-size:17px}.simpread-upgrade-root.mini dialog-gp{border-radius:15px!important}.simpread-upgrade-root.mini dialog-content{padding:0!important;width:650px!important}.simpread-upgrade-root.mini dialog-gp .close{position:absolute}.simpread-upgrade-root.mini .upgrade .carousels{margin:0}.simpread-upgrade-root.mini .upgrade .carousels .descr{padding-bottom:70px;height:130px}.simpread-upgrade-root.mini .upgrade .carousel.carousel-slider{height:450px}.simpread-upgrade-root.mini .floating .billing{margin-bottom:30px;min-height:40px}.simpread-upgrade-root.mini footer{position:absolute;top:199px;left:-60px;right:-60px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;-webkit-box-align:center;-ms-flex-align:center;align-items:center}</style><style type="text/css">.simpread-tipsalert-root dialog-gp{position:absolute}.simpread-tipsalert-root dialog-gp .close{position:absolute;top:0;right:0;z-index:2}.simpread-tipsalert-root dialog-content{padding:0!important;width:650px!important}.simpread-tipsalert-root .details{position:relative;color:rgba(51,51,51,.87);font-family:Hiragino Sans GB,Microsoft Yahei;text-shadow:none}.simpread-tipsalert-root .details .carousel.carousel-slider{height:450px;border-radius:4px;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.simpread-tipsalert-root .details .carousels setion sr-div-center{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;height:321px;box-sizing:border-box}.simpread-tipsalert-root .details .carousels setion sr-div-center.hidden{display:none}.simpread-tipsalert-root .details .carousels setion sr-div-center img{margin-top:20px!important;height:321px;width:auto!important}.simpread-tipsalert-root .details .carousels setion sr-div-center sr-video{position:absolute;left:0;right:0;width:100%;height:321px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;z-index:2}.simpread-tipsalert-root .details .carousels setion .tipsimg.error{width:0!important;height:0!important}.simpread-tipsalert-root .details .carousels setion .tipsimg:after{content:"\F1C5" " Sorry, the image below is broken :(";font-family:Font Awesome\ 5 Free;font-weight:900;position:absolute;top:0;left:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:650px;height:100%;color:#646464;background-color:#fff;z-index:2}.simpread-tipsalert-root .details .carousels setion sr-div-center sr-video+img{opacity:.5}.simpread-tipsalert-root .details .carousels setion img{margin-top:-82px;width:100%}.simpread-tipsalert-root .details .carousels setion video{height:321px}.simpread-tipsalert-root .details .carousels setion video.active{display:block}.simpread-tipsalert-root .details .carousels .descr{position:absolute;left:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:80px;width:100%;padding-bottom:10px;background-color:#fff;font-size:14px}.simpread-tipsalert-root .details .carousels .descr b{margin:0 3px}.simpread-tipsalert-root .details .carousels .descr.large{font-size:17px}.simpread-tipsalert-root .floating{position:absolute;left:0;right:0;bottom:14px;height:80px;overflow-y:hidden}.simpread-tipsalert-root .floating,.simpread-tipsalert-root .floating .docs{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.simpread-tipsalert-root .floating .docs{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:1px 40px;height:30px;color:#fff;background-color:#4dbb7c;font-size:15px;font-weight:400;border-radius:30px;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.simpread-tipsalert-root footer{position:absolute;top:199px;left:-60px;right:-60px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;-webkit-box-align:center;-ms-flex-align:center;align-items:center}</style><style type="text/css">sr-annote-popup{display:block;width:480px}sr-annote-popup-gp{position:relative;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin-bottom:25px}sr-annote-popup-label{color:rgba(0,137,123,.8);font-size:14px;font-weight:700;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;pointer-events:none;-webkit-transition:all .45s cubic-bezier(.23,1,.32,1) 0ms;transition:all .45s cubic-bezier(.23,1,.32,1) 0ms;-webkit-transform:scale(.75) translateY(-8px);transform:scale(.75) translateY(-8px);-webkit-transform-origin:left top 0;transform-origin:left top 0}sr-annote-popup-desc{position:relative;padding:10px 30px;font-size:15px;text-align:justify;color:#555;line-height:1.6;border-bottom:1px solid #e0e0e0}sr-annote-popup-desc:before{content:"\201C";position:absolute;left:-5px;top:-24px;color:rgba(0,137,123,.8);font-family:Arial;font-size:4em}sr-annote-popup-desc sr-annote{background-color:transparent!important}sr-annote-popup-gp[type=img]{-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-annote-popup-gp ol,sr-annote-popup-gp ul{margin:0 0 1.2em;margin-left:1.3em;padding:0;list-style:disc}sr-annote-popup-gp ol li,sr-annote-popup-gp ul li{margin:0 0 1.2em;font-size:15px;list-style:disc}sr-annote-popup-gp a{padding:0 5px;vertical-align:baseline;vertical-align:initial}sr-annote-popup-gp a,sr-annote-popup-gp a:link{color:#463f5c;font-size:inherit;font-weight:inherit;text-decoration:underline;border:none}sr-annote-popup-gp a:hover{background:transparent}sr-annote-popup-gp img{margin:0;padding:0;max-width:50%;height:auto;background:#fff;border:0;border-radius:6px;box-shadow:0 20px 20px -10px rgba(0,0,0,.1)}sr-annote-popup-gp pre{padding:10px!important;background-color:transparent!important;border-radius:6px!important;box-shadow:0 20px 20px -10px rgba(0,0,0,.1);overflow-x:auto}sr-annote-popup auto-complete list-view{max-height:150px!important}sr-annote-popup list-view::-webkit-scrollbar-thumb{background-clip:padding-box;border-radius:10px;border:2px solid transparent;background-color:rgba(85,85,85,.55)}sr-annote-popup list-view::-webkit-scrollbar{width:10px;-webkit-transition:width .7s cubic-bezier(.4,0,.2,1);transition:width .7s cubic-bezier(.4,0,.2,1)}sr-annote-popup-gp sr-annote-floatingbar{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:left;-ms-flex-pack:left;justify-content:left}sr-annote-popup-gp sr-annote-floatingbar sr-anote-fb-item[data-color-type]{display:-webkit-box;display:-ms-flexbox;display:flex;width:30px;height:30px;border-radius:50%}sr-annote-popup-gp sr-annote-floatingbar sr-anote-fb-item[data-color-type],sr-annote-popup-gp sr-anote-item{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-right:20px}sr-annote-popup-gp sr-anote-item{display:-webkit-box!important;display:-ms-flexbox!important;display:flex!important;padding:0 5px;font-size:13px;font-weight:400;-webkit-transition:all .5s ease-in-out 0ms;transition:all .5s ease-in-out 0ms}sr-annote-popup-gp sr-anote-lock{position:absolute;left:0;top:-3px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;background-color:hsla(0,0%,100%,.8);z-index:1}sr-annote-popup-gp sr-anote-lock svg{margin-top:3px;cursor:pointer}</style><style type="text/css">.gu-mirror{position:fixed!important;margin:0!important;z-index:9999!important;opacity:.8;-ms-filter:"progid:DXImageTransform.Microsoft.Alpha(Opacity=80)";filter:alpha(opacity=80)}.gu-hide{display:none!important}.gu-unselectable{-webkit-user-select:none!important;-moz-user-select:none!important;-ms-user-select:none!important;user-select:none!important}.gu-transit{opacity:.2;-ms-filter:"progid:DXImageTransform.Microsoft.Alpha(Opacity=20)";filter:alpha(opacity=20)}</style><style type="text/css">sr-search{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;padding:10px;border:1px solid #dfe1e5;border-radius:8px}sr-search,sr-search sr-search-header{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal;margin-bottom:10px}sr-search sr-search-header{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-search sr-search-header img{margin-right:10px;width:22px}sr-search sr-search-header sr-search-span{font-weight:700}sr-search-unreader-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;border-bottom:1px solid #dfe1e5;margin-bottom:5px}sr-search-content{max-height:666px;overflow-x:hidden;overflow-y:auto}sr-search-unreader-title{font-weight:700;font-size:15px;margin-bottom:5px}sr-search-unreader-create{margin-bottom:5px;color:#70757a}sr-search-unreader-desc{margin-bottom:5px}sr-search-unreader-tags{margin-bottom:5px;color:#70757a;font-size:11px;font-style:italic}sr-search-unreader-tag{margin-right:5px}sr-search-paging{width:100%;margin:10px}sr-search-paging,sr-search-paging sr-search-more{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-search-paging sr-search-more{width:36px;height:36px;box-shadow:0 0 0 1px rgba(0,0,0,.04),0 4px 8px 0 rgba(0,0,0,.2);border-radius:50%;opacity:.9;cursor:pointer}sr-search-paging sr-search-more:hover{opacity:1}sr-search-paging sr-search-more.disable{cursor:no-drop}sr-search-paging sr-search-more svg{width:24px;height:24px;fill:#757575}sr-search-info{text-align:center}</style><link as="script" rel="prefetch" href="./poster_files/index-8d5ab195e774e3fc0705.js.下载"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/login-9489325c3dba66c15b36.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/about-21ad854b65cdc28a5879.js"><link as="script" rel="prefetch" href="./poster_files/group-11f7765f7db1e915182b.js.下载"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/venues-41b2f7835ab776406336.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/contact-c0f4dc3837847c913ba0.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/faq-6d1c8a0d3e1b4eea5c70.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/legal/terms-1acd9435966a971c48c8.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/legal/privacy-a259b7f57265e0f174e8.js"><link as="fetch" rel="prefetch" href="https://openreview.net/_next/data/v1.0.9-1-gf539684/faq.json"><style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style id="MJX-CHTML-styles">
mjx-container[jax="CHTML"] {
  line-height: 0;
}

mjx-container [space="1"] {
  margin-left: .111em;
}

mjx-container [space="2"] {
  margin-left: .167em;
}

mjx-container [space="3"] {
  margin-left: .222em;
}

mjx-container [space="4"] {
  margin-left: .278em;
}

mjx-container [space="5"] {
  margin-left: .333em;
}

mjx-container [rspace="1"] {
  margin-right: .111em;
}

mjx-container [rspace="2"] {
  margin-right: .167em;
}

mjx-container [rspace="3"] {
  margin-right: .222em;
}

mjx-container [rspace="4"] {
  margin-right: .278em;
}

mjx-container [rspace="5"] {
  margin-right: .333em;
}

mjx-container [size="s"] {
  font-size: 70.7%;
}

mjx-container [size="ss"] {
  font-size: 50%;
}

mjx-container [size="Tn"] {
  font-size: 60%;
}

mjx-container [size="sm"] {
  font-size: 85%;
}

mjx-container [size="lg"] {
  font-size: 120%;
}

mjx-container [size="Lg"] {
  font-size: 144%;
}

mjx-container [size="LG"] {
  font-size: 173%;
}

mjx-container [size="hg"] {
  font-size: 207%;
}

mjx-container [size="HG"] {
  font-size: 249%;
}

mjx-container [width="full"] {
  width: 100%;
}

mjx-box {
  display: inline-block;
}

mjx-block {
  display: block;
}

mjx-itable {
  display: inline-table;
}

mjx-row {
  display: table-row;
}

mjx-row > * {
  display: table-cell;
}

mjx-mtext {
  display: inline-block;
  text-align: left;
}

mjx-mstyle {
  display: inline-block;
}

mjx-merror {
  display: inline-block;
  color: red;
  background-color: yellow;
}

mjx-mphantom {
  visibility: hidden;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-math {
  display: inline-block;
  text-align: left;
  line-height: 0;
  text-indent: 0;
  font-style: normal;
  font-weight: normal;
  font-size: 100%;
  font-size-adjust: none;
  letter-spacing: normal;
  word-wrap: normal;
  word-spacing: normal;
  white-space: nowrap;
  direction: ltr;
  padding: 1px 0;
}

mjx-container[jax="CHTML"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="CHTML"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="CHTML"][display="true"] mjx-math {
  padding: 0;
}

mjx-container[jax="CHTML"][justify="left"] {
  text-align: left;
}

mjx-container[jax="CHTML"][justify="right"] {
  text-align: right;
}

mjx-mrow {
  display: inline-block;
  text-align: left;
}

mjx-inferredMrow {
  display: inline-block;
  text-align: left;
}

mjx-mi {
  display: inline-block;
  text-align: left;
}

mjx-mo {
  display: inline-block;
  text-align: left;
}

mjx-stretchy-h {
  display: inline-table;
  width: 100%;
}

mjx-stretchy-h > * {
  display: table-cell;
  width: 0;
}

mjx-stretchy-h > * > mjx-c {
  display: inline-block;
  transform: scalex(1.0000001);
}

mjx-stretchy-h > * > mjx-c::before {
  display: inline-block;
  padding: .001em 0;
  width: initial;
}

mjx-stretchy-h > mjx-ext {
  overflow: hidden;
  width: 100%;
}

mjx-stretchy-h > mjx-ext > mjx-c::before {
  transform: scalex(500);
}

mjx-stretchy-h > mjx-ext > mjx-c {
  width: 0;
}

mjx-stretchy-h > mjx-beg > mjx-c {
  margin-right: -.1em;
}

mjx-stretchy-h > mjx-end > mjx-c {
  margin-left: -.1em;
}

mjx-stretchy-v {
  display: inline-block;
}

mjx-stretchy-v > * {
  display: block;
}

mjx-stretchy-v > mjx-beg {
  height: 0;
}

mjx-stretchy-v > mjx-end > mjx-c {
  display: block;
}

mjx-stretchy-v > * > mjx-c {
  transform: scaley(1.0000001);
  transform-origin: left center;
  overflow: hidden;
}

mjx-stretchy-v > mjx-ext {
  display: block;
  height: 100%;
  box-sizing: border-box;
  border: 0px solid transparent;
  overflow: hidden;
}

mjx-stretchy-v > mjx-ext > mjx-c::before {
  width: initial;
}

mjx-stretchy-v > mjx-ext > mjx-c {
  transform: scaleY(500) translateY(.1em);
  overflow: visible;
}

mjx-mark {
  display: inline-block;
  height: 0px;
}

mjx-mn {
  display: inline-block;
  text-align: left;
}

mjx-ms {
  display: inline-block;
  text-align: left;
}

mjx-mspace {
  display: inline-block;
  text-align: left;
}

mjx-mpadded {
  display: inline-block;
  text-align: left;
}

mjx-rbox {
  display: inline-block;
  position: relative;
}

mjx-menclose {
  display: inline-block;
  text-align: left;
  position: relative;
}

mjx-menclose > mjx-dstrike {
  display: inline-block;
  left: 0;
  top: 0;
  position: absolute;
  border-top: 0.067em solid;
  transform-origin: top left;
}

mjx-menclose > mjx-ustrike {
  display: inline-block;
  left: 0;
  bottom: 0;
  position: absolute;
  border-top: 0.067em solid;
  transform-origin: bottom left;
}

mjx-menclose > mjx-hstrike {
  border-top: 0.067em solid;
  position: absolute;
  left: 0;
  right: 0;
  bottom: 50%;
  transform: translateY(0.034em);
}

mjx-menclose > mjx-vstrike {
  border-left: 0.067em solid;
  position: absolute;
  top: 0;
  bottom: 0;
  right: 50%;
  transform: translateX(0.034em);
}

mjx-menclose > mjx-rbox {
  position: absolute;
  top: 0;
  bottom: 0;
  right: 0;
  left: 0;
  border: 0.067em solid;
  border-radius: 0.267em;
}

mjx-menclose > mjx-cbox {
  position: absolute;
  top: 0;
  bottom: 0;
  right: 0;
  left: 0;
  border: 0.067em solid;
  border-radius: 50%;
}

mjx-menclose > mjx-arrow {
  position: absolute;
  left: 0;
  bottom: 50%;
  height: 0;
  width: 0;
}

mjx-menclose > mjx-arrow > * {
  display: block;
  position: absolute;
  transform-origin: bottom;
  border-left: 0.268em solid;
  border-right: 0;
  box-sizing: border-box;
}

mjx-menclose > mjx-arrow > mjx-aline {
  left: 0;
  top: -0.034em;
  right: 0.201em;
  height: 0;
  border-top: 0.067em solid;
  border-left: 0;
}

mjx-menclose > mjx-arrow[double] > mjx-aline {
  left: 0.201em;
  height: 0;
}

mjx-menclose > mjx-arrow > mjx-rthead {
  transform: skewX(0.464rad);
  right: 0;
  bottom: -1px;
  border-bottom: 1px solid transparent;
  border-top: 0.134em solid transparent;
}

mjx-menclose > mjx-arrow > mjx-rbhead {
  transform: skewX(-0.464rad);
  transform-origin: top;
  right: 0;
  top: -1px;
  border-top: 1px solid transparent;
  border-bottom: 0.134em solid transparent;
}

mjx-menclose > mjx-arrow > mjx-lthead {
  transform: skewX(-0.464rad);
  left: 0;
  bottom: -1px;
  border-left: 0;
  border-right: 0.268em solid;
  border-bottom: 1px solid transparent;
  border-top: 0.134em solid transparent;
}

mjx-menclose > mjx-arrow > mjx-lbhead {
  transform: skewX(0.464rad);
  transform-origin: top;
  left: 0;
  top: -1px;
  border-left: 0;
  border-right: 0.268em solid;
  border-top: 1px solid transparent;
  border-bottom: 0.134em solid transparent;
}

mjx-menclose > dbox {
  position: absolute;
  top: 0;
  bottom: 0;
  left: -0.3em;
  width: 0.6em;
  border: 0.067em solid;
  border-radius: 50%;
  clip-path: inset(0 0 0 0.3em);
  box-sizing: border-box;
}

mjx-mfrac {
  display: inline-block;
  text-align: left;
}

mjx-frac {
  display: inline-block;
  vertical-align: 0.17em;
  padding: 0 .22em;
}

mjx-frac[type="d"] {
  vertical-align: .04em;
}

mjx-frac[delims] {
  padding: 0 .1em;
}

mjx-frac[atop] {
  padding: 0 .12em;
}

mjx-frac[atop][delims] {
  padding: 0;
}

mjx-dtable {
  display: inline-table;
  width: 100%;
}

mjx-dtable > * {
  font-size: 2000%;
}

mjx-dbox {
  display: block;
  font-size: 5%;
}

mjx-num {
  display: block;
  text-align: center;
}

mjx-den {
  display: block;
  text-align: center;
}

mjx-mfrac[bevelled] > mjx-num {
  display: inline-block;
}

mjx-mfrac[bevelled] > mjx-den {
  display: inline-block;
}

mjx-den[align="right"], mjx-num[align="right"] {
  text-align: right;
}

mjx-den[align="left"], mjx-num[align="left"] {
  text-align: left;
}

mjx-nstrut {
  display: inline-block;
  height: .054em;
  width: 0;
  vertical-align: -.054em;
}

mjx-nstrut[type="d"] {
  height: .217em;
  vertical-align: -.217em;
}

mjx-dstrut {
  display: inline-block;
  height: .505em;
  width: 0;
}

mjx-dstrut[type="d"] {
  height: .726em;
}

mjx-line {
  display: block;
  box-sizing: border-box;
  min-height: 1px;
  height: .06em;
  border-top: .06em solid;
  margin: .06em -.1em;
  overflow: hidden;
}

mjx-line[type="d"] {
  margin: .18em -.1em;
}

mjx-msqrt {
  display: inline-block;
  text-align: left;
}

mjx-root {
  display: inline-block;
  white-space: nowrap;
}

mjx-surd {
  display: inline-block;
  vertical-align: top;
}

mjx-sqrt {
  display: inline-block;
  padding-top: .07em;
}

mjx-sqrt > mjx-box {
  border-top: .07em solid;
}

mjx-sqrt.mjx-tall > mjx-box {
  padding-left: .3em;
  margin-left: -.3em;
}

mjx-mroot {
  display: inline-block;
  text-align: left;
}

mjx-msub {
  display: inline-block;
  text-align: left;
}

mjx-msup {
  display: inline-block;
  text-align: left;
}

mjx-msubsup {
  display: inline-block;
  text-align: left;
}

mjx-script {
  display: inline-block;
  padding-right: .05em;
}

mjx-script > * {
  display: block;
}

mjx-munder {
  display: inline-block;
  text-align: left;
}

mjx-over {
  text-align: left;
}

mjx-munder:not([limits="false"]) {
  display: inline-table;
}

mjx-munder > mjx-row {
  text-align: left;
}

mjx-under {
  padding-bottom: .1em;
}

mjx-mover {
  display: inline-block;
  text-align: left;
}

mjx-mover:not([limits="false"]) {
  padding-top: .1em;
}

mjx-mover:not([limits="false"]) > * {
  display: block;
  text-align: left;
}

mjx-munderover {
  display: inline-block;
  text-align: left;
}

mjx-munderover:not([limits="false"]) {
  padding-top: .1em;
}

mjx-munderover:not([limits="false"]) > * {
  display: block;
}

mjx-mmultiscripts {
  display: inline-block;
  text-align: left;
}

mjx-prescripts {
  display: inline-table;
  padding-left: .05em;
}

mjx-scripts {
  display: inline-table;
  padding-right: .05em;
}

mjx-prescripts > mjx-row > mjx-cell {
  text-align: right;
}

mjx-mfenced {
  display: inline-block;
  text-align: left;
}

mjx-mtable {
  display: inline-block;
  text-align: center;
  vertical-align: .25em;
  position: relative;
  box-sizing: border-box;
}

mjx-labels {
  position: absolute;
  left: 0;
  top: 0;
}

mjx-table {
  display: inline-block;
  vertical-align: -.5ex;
}

mjx-table > mjx-itable {
  vertical-align: middle;
  text-align: left;
  box-sizing: border-box;
}

mjx-labels > mjx-itable {
  position: absolute;
  top: 0;
}

mjx-mtable[justify="left"] {
  text-align: left;
}

mjx-mtable[justify="right"] {
  text-align: right;
}

mjx-mtable[justify="left"][side="left"] {
  padding-right: 0 ! important;
}

mjx-mtable[justify="left"][side="right"] {
  padding-left: 0 ! important;
}

mjx-mtable[justify="right"][side="left"] {
  padding-right: 0 ! important;
}

mjx-mtable[justify="right"][side="right"] {
  padding-left: 0 ! important;
}

mjx-mtable[align] {
  vertical-align: baseline;
}

mjx-mtable[align="top"] > mjx-table {
  vertical-align: top;
}

mjx-mtable[align="bottom"] > mjx-table {
  vertical-align: bottom;
}

mjx-mtable[side="right"] mjx-labels {
  min-width: 100%;
}

mjx-mtr {
  display: table-row;
  text-align: left;
}

mjx-mtr[rowalign="top"] > mjx-mtd {
  vertical-align: top;
}

mjx-mtr[rowalign="center"] > mjx-mtd {
  vertical-align: middle;
}

mjx-mtr[rowalign="bottom"] > mjx-mtd {
  vertical-align: bottom;
}

mjx-mtr[rowalign="baseline"] > mjx-mtd {
  vertical-align: baseline;
}

mjx-mtr[rowalign="axis"] > mjx-mtd {
  vertical-align: .25em;
}

mjx-mlabeledtr {
  display: table-row;
  text-align: left;
}

mjx-mlabeledtr[rowalign="top"] > mjx-mtd {
  vertical-align: top;
}

mjx-mlabeledtr[rowalign="center"] > mjx-mtd {
  vertical-align: middle;
}

mjx-mlabeledtr[rowalign="bottom"] > mjx-mtd {
  vertical-align: bottom;
}

mjx-mlabeledtr[rowalign="baseline"] > mjx-mtd {
  vertical-align: baseline;
}

mjx-mlabeledtr[rowalign="axis"] > mjx-mtd {
  vertical-align: .25em;
}

mjx-mtd {
  display: table-cell;
  text-align: center;
  padding: .215em .4em;
}

mjx-mtd:first-child {
  padding-left: 0;
}

mjx-mtd:last-child {
  padding-right: 0;
}

mjx-mtable > * > mjx-itable > *:first-child > mjx-mtd {
  padding-top: 0;
}

mjx-mtable > * > mjx-itable > *:last-child > mjx-mtd {
  padding-bottom: 0;
}

mjx-tstrut {
  display: inline-block;
  height: 1em;
  vertical-align: -.25em;
}

mjx-labels[align="left"] > mjx-mtr > mjx-mtd {
  text-align: left;
}

mjx-labels[align="right"] > mjx-mtr > mjx-mtd {
  text-align: right;
}

mjx-mtr mjx-mtd[rowalign="top"], mjx-mlabeledtr mjx-mtd[rowalign="top"] {
  vertical-align: top;
}

mjx-mtr mjx-mtd[rowalign="center"], mjx-mlabeledtr mjx-mtd[rowalign="center"] {
  vertical-align: middle;
}

mjx-mtr mjx-mtd[rowalign="bottom"], mjx-mlabeledtr mjx-mtd[rowalign="bottom"] {
  vertical-align: bottom;
}

mjx-mtr mjx-mtd[rowalign="baseline"], mjx-mlabeledtr mjx-mtd[rowalign="baseline"] {
  vertical-align: baseline;
}

mjx-mtr mjx-mtd[rowalign="axis"], mjx-mlabeledtr mjx-mtd[rowalign="axis"] {
  vertical-align: .25em;
}

mjx-maction {
  display: inline-block;
  text-align: left;
  position: relative;
}

mjx-maction > mjx-tool {
  display: none;
  position: absolute;
  bottom: 0;
  right: 0;
  width: 0;
  height: 0;
  z-index: 500;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

mjx-maction[toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

mjx-mglyph {
  display: inline-block;
  text-align: left;
}

mjx-mglyph > img {
  display: inline-block;
  border: 0;
  padding: 0;
}

mjx-semantics {
  display: inline-block;
  text-align: left;
}

mjx-annotation {
  display: inline-block;
  text-align: left;
}

mjx-annotation-xml {
  display: inline-block;
  text-align: left;
  font-family: initial;
  line-height: normal;
}

mjx-TeXAtom {
  display: inline-block;
  text-align: left;
}

mjx-c {
  display: inline-block;
}

mjx-utext {
  display: inline-block;
  padding: .75em 0 .2em 0;
}

mjx-c::before {
  display: block;
  width: 0;
}

.MJX-TEX {
  font-family: MJXZERO, MJXTEX;
}

.TEX-B {
  font-family: MJXZERO, MJXTEX-B;
}

.TEX-I {
  font-family: MJXZERO, MJXTEX-I;
}

.TEX-MI {
  font-family: MJXZERO, MJXTEX-MI;
}

.TEX-BI {
  font-family: MJXZERO, MJXTEX-BI;
}

.TEX-S1 {
  font-family: MJXZERO, MJXTEX-S1;
}

.TEX-S2 {
  font-family: MJXZERO, MJXTEX-S2;
}

.TEX-S3 {
  font-family: MJXZERO, MJXTEX-S3;
}

.TEX-S4 {
  font-family: MJXZERO, MJXTEX-S4;
}

.TEX-A {
  font-family: MJXZERO, MJXTEX-A;
}

.TEX-C {
  font-family: MJXZERO, MJXTEX-C;
}

.TEX-CB {
  font-family: MJXZERO, MJXTEX-CB;
}

.TEX-FR {
  font-family: MJXZERO, MJXTEX-FR;
}

.TEX-FRB {
  font-family: MJXZERO, MJXTEX-FRB;
}

.TEX-SS {
  font-family: MJXZERO, MJXTEX-SS;
}

.TEX-SSB {
  font-family: MJXZERO, MJXTEX-SSB;
}

.TEX-SSI {
  font-family: MJXZERO, MJXTEX-SSI;
}

.TEX-SC {
  font-family: MJXZERO, MJXTEX-SC;
}

.TEX-T {
  font-family: MJXZERO, MJXTEX-T;
}

.TEX-V {
  font-family: MJXZERO, MJXTEX-V;
}

.TEX-VB {
  font-family: MJXZERO, MJXTEX-VB;
}

mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c {
  font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important;
}

@font-face /* 0 */ {
  font-family: MJXZERO;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff");
}

@font-face /* 1 */ {
  font-family: MJXTEX;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff");
}

@font-face /* 2 */ {
  font-family: MJXTEX-B;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff");
}

@font-face /* 3 */ {
  font-family: MJXTEX-I;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff");
}

@font-face /* 4 */ {
  font-family: MJXTEX-MI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff");
}

@font-face /* 5 */ {
  font-family: MJXTEX-BI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff");
}

@font-face /* 6 */ {
  font-family: MJXTEX-S1;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff");
}

@font-face /* 7 */ {
  font-family: MJXTEX-S2;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff");
}

@font-face /* 8 */ {
  font-family: MJXTEX-S3;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff");
}

@font-face /* 9 */ {
  font-family: MJXTEX-S4;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff");
}

@font-face /* 10 */ {
  font-family: MJXTEX-A;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff");
}

@font-face /* 11 */ {
  font-family: MJXTEX-C;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff");
}

@font-face /* 12 */ {
  font-family: MJXTEX-CB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff");
}

@font-face /* 13 */ {
  font-family: MJXTEX-FR;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff");
}

@font-face /* 14 */ {
  font-family: MJXTEX-FRB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff");
}

@font-face /* 15 */ {
  font-family: MJXTEX-SS;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff");
}

@font-face /* 16 */ {
  font-family: MJXTEX-SSB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff");
}

@font-face /* 17 */ {
  font-family: MJXTEX-SSI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff");
}

@font-face /* 18 */ {
  font-family: MJXTEX-SC;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff");
}

@font-face /* 19 */ {
  font-family: MJXTEX-T;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff");
}

@font-face /* 20 */ {
  font-family: MJXTEX-V;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff");
}

@font-face /* 21 */ {
  font-family: MJXTEX-VB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff");
}

mjx-c.mjx-c20::before {
  padding: 0 0.25em 0 0;
  content: " ";
}

mjx-c.mjx-c25::before {
  padding: 0.75em 0.833em 0.056em 0;
  content: "%";
}

mjx-c.mjx-c28::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: "(";
}

mjx-c.mjx-c29::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: ")";
}

mjx-c.mjx-c2B::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "+";
}

mjx-c.mjx-c2C::before {
  padding: 0.121em 0.278em 0.194em 0;
  content: ",";
}

mjx-c.mjx-c2E::before {
  padding: 0.12em 0.278em 0 0;
  content: ".";
}

mjx-c.mjx-c2F::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "/";
}

mjx-c.mjx-c30::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "0";
}

mjx-c.mjx-c31::before {
  padding: 0.666em 0.5em 0 0;
  content: "1";
}

mjx-c.mjx-c32::before {
  padding: 0.666em 0.5em 0 0;
  content: "2";
}

mjx-c.mjx-c33::before {
  padding: 0.665em 0.5em 0.022em 0;
  content: "3";
}

mjx-c.mjx-c34::before {
  padding: 0.677em 0.5em 0 0;
  content: "4";
}

mjx-c.mjx-c35::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "5";
}

mjx-c.mjx-c36::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "6";
}

mjx-c.mjx-c37::before {
  padding: 0.676em 0.5em 0.022em 0;
  content: "7";
}

mjx-c.mjx-c38::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "8";
}

mjx-c.mjx-c39::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "9";
}

mjx-c.mjx-c3A::before {
  padding: 0.43em 0.278em 0 0;
  content: ":";
}

mjx-c.mjx-c3C::before {
  padding: 0.54em 0.778em 0.04em 0;
  content: "<";
}

mjx-c.mjx-c3D::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "=";
}

mjx-c.mjx-c3E::before {
  padding: 0.54em 0.778em 0.04em 0;
  content: ">";
}

mjx-c.mjx-c46::before {
  padding: 0.68em 0.653em 0 0;
  content: "F";
}

mjx-c.mjx-c4B::before {
  padding: 0.683em 0.778em 0 0;
  content: "K";
}

mjx-c.mjx-c4C::before {
  padding: 0.683em 0.625em 0 0;
  content: "L";
}

mjx-c.mjx-c61::before {
  padding: 0.448em 0.5em 0.011em 0;
  content: "a";
}

mjx-c.mjx-c65::before {
  padding: 0.448em 0.444em 0.011em 0;
  content: "e";
}

mjx-c.mjx-c67::before {
  padding: 0.453em 0.5em 0.206em 0;
  content: "g";
}

mjx-c.mjx-c69::before {
  padding: 0.669em 0.278em 0 0;
  content: "i";
}

mjx-c.mjx-c6B::before {
  padding: 0.694em 0.528em 0 0;
  content: "k";
}

mjx-c.mjx-c6C::before {
  padding: 0.694em 0.278em 0 0;
  content: "l";
}

mjx-c.mjx-c6D::before {
  padding: 0.442em 0.833em 0 0;
  content: "m";
}

mjx-c.mjx-c6E::before {
  padding: 0.442em 0.556em 0 0;
  content: "n";
}

mjx-c.mjx-c6F::before {
  padding: 0.448em 0.5em 0.01em 0;
  content: "o";
}

mjx-c.mjx-c70::before {
  padding: 0.442em 0.556em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c72::before {
  padding: 0.442em 0.392em 0 0;
  content: "r";
}

mjx-c.mjx-c73::before {
  padding: 0.448em 0.394em 0.011em 0;
  content: "s";
}

mjx-c.mjx-c74::before {
  padding: 0.615em 0.389em 0.01em 0;
  content: "t";
}

mjx-c.mjx-c79::before {
  padding: 0.431em 0.528em 0.204em 0;
  content: "y";
}

mjx-c.mjx-c7A::before {
  padding: 0.431em 0.444em 0 0;
  content: "z";
}

mjx-c.mjx-c7B::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "{";
}

mjx-c.mjx-c7C::before {
  padding: 0.75em 0.278em 0.249em 0;
  content: "|";
}

mjx-c.mjx-c7D::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "}";
}

mjx-c.mjx-c7E::before {
  padding: 0.318em 0.5em 0 0;
  content: "~";
}

mjx-c.mjx-cB1::before {
  padding: 0.666em 0.778em 0 0;
  content: "\B1";
}

mjx-c.mjx-cD7::before {
  padding: 0.491em 0.778em 0 0;
  content: "\D7";
}

mjx-c.mjx-c3A9::before {
  padding: 0.704em 0.722em 0 0;
  content: "\3A9";
}

mjx-c.mjx-c2032::before {
  padding: 0.56em 0.275em 0 0;
  content: "\2032";
}

mjx-c.mjx-c2061::before {
  padding: 0 0 0 0;
  content: "";
}

mjx-c.mjx-c210E.TEX-I::before {
  padding: 0.694em 0.576em 0.011em 0;
  content: "h";
}

mjx-c.mjx-c2113::before {
  padding: 0.705em 0.417em 0.02em 0;
  content: "\2113";
}

mjx-c.mjx-c211D.TEX-A::before {
  padding: 0.683em 0.722em 0 0;
  content: "R";
}

mjx-c.mjx-c2192::before {
  padding: 0.511em 1em 0.011em 0;
  content: "\2192";
}

mjx-c.mjx-c2194::before {
  padding: 0.511em 1em 0.011em 0;
  content: "\2194";
}

mjx-c.mjx-c21A6::before {
  padding: 0.511em 1em 0.011em 0;
  content: "\21A6";
}

mjx-c.mjx-c2208::before {
  padding: 0.54em 0.667em 0.04em 0;
  content: "\2208";
}

mjx-c.mjx-c2212::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "\2212";
}

mjx-c.mjx-c2217::before {
  padding: 0.465em 0.5em 0 0;
  content: "\2217";
}

mjx-c.mjx-c221A::before {
  padding: 0.8em 0.853em 0.2em 0;
  content: "\221A";
}

[noIC] mjx-c.mjx-c221A:last-child::before {
  padding-right: 0.833em;
}

mjx-c.mjx-c221E::before {
  padding: 0.442em 1em 0.011em 0;
  content: "\221E";
}

mjx-c.mjx-c223C::before {
  padding: 0.367em 0.778em 0 0;
  content: "\223C";
}

mjx-c.mjx-c2264::before {
  padding: 0.636em 0.778em 0.138em 0;
  content: "\2264";
}

mjx-c.mjx-c2265::before {
  padding: 0.636em 0.778em 0.138em 0;
  content: "\2265";
}

mjx-c.mjx-c226A::before {
  padding: 0.568em 1em 0.067em 0;
  content: "\226A";
}

mjx-c.mjx-c226B::before {
  padding: 0.567em 1em 0.067em 0;
  content: "\226B";
}

mjx-c.mjx-c2286::before {
  padding: 0.636em 0.778em 0.138em 0;
  content: "\2286";
}

mjx-c.mjx-c22C5::before {
  padding: 0.31em 0.278em 0 0;
  content: "\22C5";
}

mjx-c.mjx-c1D400.TEX-B::before {
  padding: 0.698em 0.869em 0 0;
  content: "A";
}

mjx-c.mjx-c1D403.TEX-B::before {
  padding: 0.686em 0.882em 0 0;
  content: "D";
}

mjx-c.mjx-c1D404.TEX-B::before {
  padding: 0.68em 0.756em 0 0;
  content: "E";
}

mjx-c.mjx-c1D40D.TEX-B::before {
  padding: 0.686em 0.9em 0 0;
  content: "N";
}

mjx-c.mjx-c1D412.TEX-B::before {
  padding: 0.697em 0.639em 0.011em 0;
  content: "S";
}

mjx-c.mjx-c1D413.TEX-B::before {
  padding: 0.675em 0.8em 0 0;
  content: "T";
}

mjx-c.mjx-c1D431.TEX-B::before {
  padding: 0.444em 0.607em 0 0;
  content: "x";
}

mjx-c.mjx-c1D434.TEX-I::before {
  padding: 0.716em 0.75em 0 0;
  content: "A";
}

mjx-c.mjx-c1D436.TEX-I::before {
  padding: 0.705em 0.76em 0.022em 0;
  content: "C";
}

[noIC] mjx-c.mjx-c1D436.TEX-I:last-child::before {
  padding-right: 0.715em;
}

mjx-c.mjx-c1D437.TEX-I::before {
  padding: 0.683em 0.828em 0 0;
  content: "D";
}

mjx-c.mjx-c1D43A.TEX-I::before {
  padding: 0.705em 0.786em 0.022em 0;
  content: "G";
}

mjx-c.mjx-c1D43B.TEX-I::before {
  padding: 0.683em 0.888em 0 0;
  content: "H";
}

[noIC] mjx-c.mjx-c1D43B.TEX-I:last-child::before {
  padding-right: 0.831em;
}

mjx-c.mjx-c1D43C.TEX-I::before {
  padding: 0.683em 0.504em 0 0;
  content: "I";
}

[noIC] mjx-c.mjx-c1D43C.TEX-I:last-child::before {
  padding-right: 0.44em;
}

mjx-c.mjx-c1D43E.TEX-I::before {
  padding: 0.683em 0.889em 0 0;
  content: "K";
}

[noIC] mjx-c.mjx-c1D43E.TEX-I:last-child::before {
  padding-right: 0.849em;
}

mjx-c.mjx-c1D43F.TEX-I::before {
  padding: 0.683em 0.681em 0 0;
  content: "L";
}

mjx-c.mjx-c1D440.TEX-I::before {
  padding: 0.683em 1.051em 0 0;
  content: "M";
}

[noIC] mjx-c.mjx-c1D440.TEX-I:last-child::before {
  padding-right: 0.97em;
}

mjx-c.mjx-c1D441.TEX-I::before {
  padding: 0.683em 0.888em 0 0;
  content: "N";
}

[noIC] mjx-c.mjx-c1D441.TEX-I:last-child::before {
  padding-right: 0.803em;
}

mjx-c.mjx-c1D442.TEX-I::before {
  padding: 0.704em 0.763em 0.022em 0;
  content: "O";
}

mjx-c.mjx-c1D443.TEX-I::before {
  padding: 0.683em 0.751em 0 0;
  content: "P";
}

[noIC] mjx-c.mjx-c1D443.TEX-I:last-child::before {
  padding-right: 0.642em;
}

mjx-c.mjx-c1D445.TEX-I::before {
  padding: 0.683em 0.759em 0.021em 0;
  content: "R";
}

mjx-c.mjx-c1D446.TEX-I::before {
  padding: 0.705em 0.645em 0.022em 0;
  content: "S";
}

[noIC] mjx-c.mjx-c1D446.TEX-I:last-child::before {
  padding-right: 0.613em;
}

mjx-c.mjx-c1D447.TEX-I::before {
  padding: 0.677em 0.704em 0 0;
  content: "T";
}

[noIC] mjx-c.mjx-c1D447.TEX-I:last-child::before {
  padding-right: 0.584em;
}

mjx-c.mjx-c1D44E.TEX-I::before {
  padding: 0.441em 0.529em 0.01em 0;
  content: "a";
}

mjx-c.mjx-c1D44F.TEX-I::before {
  padding: 0.694em 0.429em 0.011em 0;
  content: "b";
}

mjx-c.mjx-c1D450.TEX-I::before {
  padding: 0.442em 0.433em 0.011em 0;
  content: "c";
}

mjx-c.mjx-c1D451.TEX-I::before {
  padding: 0.694em 0.52em 0.01em 0;
  content: "d";
}

mjx-c.mjx-c1D452.TEX-I::before {
  padding: 0.442em 0.466em 0.011em 0;
  content: "e";
}

mjx-c.mjx-c1D453.TEX-I::before {
  padding: 0.705em 0.55em 0.205em 0;
  content: "f";
}

[noIC] mjx-c.mjx-c1D453.TEX-I:last-child::before {
  padding-right: 0.49em;
}

mjx-c.mjx-c1D454.TEX-I::before {
  padding: 0.442em 0.477em 0.205em 0;
  content: "g";
}

mjx-c.mjx-c1D456.TEX-I::before {
  padding: 0.661em 0.345em 0.011em 0;
  content: "i";
}

mjx-c.mjx-c1D457.TEX-I::before {
  padding: 0.661em 0.412em 0.204em 0;
  content: "j";
}

mjx-c.mjx-c1D458.TEX-I::before {
  padding: 0.694em 0.521em 0.011em 0;
  content: "k";
}

mjx-c.mjx-c1D459.TEX-I::before {
  padding: 0.694em 0.298em 0.011em 0;
  content: "l";
}

mjx-c.mjx-c1D45A.TEX-I::before {
  padding: 0.442em 0.878em 0.011em 0;
  content: "m";
}

mjx-c.mjx-c1D45B.TEX-I::before {
  padding: 0.442em 0.6em 0.011em 0;
  content: "n";
}

mjx-c.mjx-c1D45C.TEX-I::before {
  padding: 0.441em 0.485em 0.011em 0;
  content: "o";
}

mjx-c.mjx-c1D45D.TEX-I::before {
  padding: 0.442em 0.503em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c1D45F.TEX-I::before {
  padding: 0.442em 0.451em 0.011em 0;
  content: "r";
}

mjx-c.mjx-c1D460.TEX-I::before {
  padding: 0.442em 0.469em 0.01em 0;
  content: "s";
}

mjx-c.mjx-c1D461.TEX-I::before {
  padding: 0.626em 0.361em 0.011em 0;
  content: "t";
}

mjx-c.mjx-c1D462.TEX-I::before {
  padding: 0.442em 0.572em 0.011em 0;
  content: "u";
}

mjx-c.mjx-c1D463.TEX-I::before {
  padding: 0.443em 0.485em 0.011em 0;
  content: "v";
}

mjx-c.mjx-c1D464.TEX-I::before {
  padding: 0.443em 0.716em 0.011em 0;
  content: "w";
}

mjx-c.mjx-c1D465.TEX-I::before {
  padding: 0.442em 0.572em 0.011em 0;
  content: "x";
}

mjx-c.mjx-c1D466.TEX-I::before {
  padding: 0.442em 0.49em 0.205em 0;
  content: "y";
}

mjx-c.mjx-c1D467.TEX-I::before {
  padding: 0.442em 0.465em 0.011em 0;
  content: "z";
}

mjx-c.mjx-c1D477.TEX-BI::before {
  padding: 0.686em 0.847em 0 0;
  content: "P";
}

[noIC] mjx-c.mjx-c1D477.TEX-BI:last-child::before {
  padding-right: 0.723em;
}

mjx-c.mjx-c1D54A.TEX-A::before {
  padding: 0.702em 0.556em 0.012em 0;
  content: "S";
}

mjx-c.mjx-c1D5A2.TEX-SS::before {
  padding: 0.705em 0.639em 0.011em 0;
  content: "C";
}

mjx-c.mjx-c1D6FC.TEX-I::before {
  padding: 0.442em 0.64em 0.011em 0;
  content: "\3B1";
}

mjx-c.mjx-c1D6FD.TEX-I::before {
  padding: 0.705em 0.566em 0.194em 0;
  content: "\3B2";
}

mjx-c.mjx-c1D6FE.TEX-I::before {
  padding: 0.441em 0.543em 0.216em 0;
  content: "\3B3";
}

[noIC] mjx-c.mjx-c1D6FE.TEX-I:last-child::before {
  padding-right: 0.518em;
}

mjx-c.mjx-c1D700.TEX-I::before {
  padding: 0.452em 0.466em 0.022em 0;
  content: "\3B5";
}

mjx-c.mjx-c1D705.TEX-I::before {
  padding: 0.442em 0.576em 0.011em 0;
  content: "\3BA";
}

mjx-c.mjx-c1D706.TEX-I::before {
  padding: 0.694em 0.583em 0.012em 0;
  content: "\3BB";
}

mjx-c.mjx-c1D707.TEX-I::before {
  padding: 0.442em 0.603em 0.216em 0;
  content: "\3BC";
}

mjx-c.mjx-c1D70F.TEX-I::before {
  padding: 0.431em 0.517em 0.013em 0;
  content: "\3C4";
}

[noIC] mjx-c.mjx-c1D70F.TEX-I:last-child::before {
  padding-right: 0.437em;
}

mjx-c.mjx-c1D716.TEX-I::before {
  padding: 0.431em 0.406em 0.011em 0;
  content: "\3F5";
}

mjx-c.mjx-c1D7CE.TEX-B::before {
  padding: 0.654em 0.575em 0.01em 0;
  content: "0";
}

mjx-c.mjx-c1D7D2.TEX-B::before {
  padding: 0.656em 0.575em 0 0;
  content: "4";
}

mjx-c.mjx-c1D7D3.TEX-B::before {
  padding: 0.655em 0.575em 0.011em 0;
  content: "5";
}

mjx-c.mjx-c2D.TEX-B::before {
  padding: 0.278em 0.383em 0 0;
  content: "-";
}

mjx-c.mjx-c2E.TEX-B::before {
  padding: 0.171em 0.319em 0 0;
  content: ".";
}

mjx-c.mjx-c2D.TEX-MI::before {
  padding: 0.251em 0.358em 0 0;
  content: "-";
}

mjx-c.mjx-c2DC.TEX-S1::before {
  padding: 0.722em 0.556em 0 0;
  content: "\2DC";
}

mjx-c.mjx-c2211.TEX-S1::before {
  padding: 0.75em 1.056em 0.25em 0;
  content: "\2211";
}

mjx-c.mjx-c221A.TEX-S1::before {
  padding: 0.85em 1.02em 0.35em 0;
  content: "\221A";
}

[noIC] mjx-c.mjx-c221A.TEX-S1:last-child::before {
  padding-right: 1em;
}

mjx-c.mjx-c28.TEX-S2::before {
  padding: 1.15em 0.597em 0.649em 0;
  content: "(";
}

mjx-c.mjx-c29.TEX-S2::before {
  padding: 1.15em 0.597em 0.649em 0;
  content: ")";
}

mjx-c.mjx-c44.TEX-C::before {
  padding: 0.683em 0.771em 0 0;
  content: "D";
}

mjx-c.mjx-c4F.TEX-C::before {
  padding: 0.705em 0.796em 0.022em 0;
  content: "O";
}

mjx-c.mjx-c53.TEX-C::before {
  padding: 0.705em 0.642em 0.022em 0;
  content: "S";
}

[noIC] mjx-c.mjx-c53.TEX-C:last-child::before {
  padding-right: 0.606em;
}

mjx-c.mjx-c54.TEX-C::before {
  padding: 0.717em 0.833em 0.068em 0;
  content: "T";
}

[noIC] mjx-c.mjx-c54.TEX-C:last-child::before {
  padding-right: 0.545em;
}

mjx-c.mjx-c57.TEX-C::before {
  padding: 0.683em 1.034em 0.053em 0;
  content: "W";
}

[noIC] mjx-c.mjx-c57.TEX-C:last-child::before {
  padding-right: 0.988em;
}
</style></head><body><div id="__next"><nav class="navbar navbar-inverse navbar-fixed-top" role="navigation"><div class="container"><div class="navbar-header"><button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand home push-link" href="https://openreview.net/"><strong>OpenReview</strong>.net</a></div><div id="navbar" class="navbar-collapse collapse"><form class="navbar-form navbar-left profile-search" role="search"><div class="form-group has-feedback"><input type="text" name="term" class="form-control" value="" placeholder="Search OpenReview..." autocomplete="off"><span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span></div><input type="hidden" name="group" value="all"><input type="hidden" name="content" value="all"><input type="hidden" name="source" value="all"><input type="hidden" name="sort" value="cdate:desc"></form><ul class="nav navbar-nav navbar-right"><li id="user-menu"><a href="https://openreview.net/login">Login</a></li></ul></div></div></nav><div id="or-banner" class="banner" style=""><div class="container"><div class="row"><div class="col-xs-12"><a title="Venue Homepage" href="https://openreview.net/group?id=ICLR.cc/2021"><img class="icon" src="./poster_files/arrow_left.svg" alt="back arrow">Go to <strong>ICLR 2021</strong> homepage</a></div></div></div></div><div id="flash-message-container" class="alert alert-danger fixed-overlay" role="alert" style="display:none"><div class="container"><div class="row"><div class="col-xs-12"><div class="alert-content"><button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button></div></div></div></div></div><div class="container"><div class="row"><div class="col-xs-12"><main id="content" class="group  "><div id="group-container"><div id="header" class="venue-header" style="display: block;"><h1>International Conference on Learning Representations</h1>
<h3>ICLR 2021</h3>

  <h4>
      <span class="venue-location">
        <span class="glyphicon glyphicon-globe" aria-hidden="true"></span> Vienna, Austria
      </span>
      <span class="venue-date">
        <span class="glyphicon glyphicon-calendar" aria-hidden="true"></span> May 04 2021
      </span>
      <span class="venue-website">
        <span class="glyphicon glyphicon-new-window" aria-hidden="true"></span> <a href="https://iclr.cc/" title="International Conference on Learning Representations Homepage" target="_blank">https://iclr.cc/</a>
      </span>
      <span class="venue-contact">
        <span class="glyphicon glyphicon-envelope" aria-hidden="true"></span> <a href="mailto:iclr2021programchairs@googlegroups.com" target="_blank">iclr2021programchairs@googlegroups.com</a>
      </span>
  </h4>

<div class="description">
    <p class="dark">Please see the venue website for more information.<br> </p><p class="dark"><strong>Abstract Submission End:</strong> Sep 28 2020 03:00PM UTC-0</p><p class="dark"><strong>Paper Submission End:</strong> Oct 2 2020 03:00PM UTC-0</p>
  <p></p>
</div>
</div><div id="invitation"></div><div id="notes">
<div class="tabs-container " style=""><ul class="nav nav-tabs" role="tablist">
    <li role="presentation" style="display: none;">
      <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#your-consoles" aria-controls="your-consoles" role="tab" data-toggle="tab" data-tab-index="0" data-modify-history="true">
        Your Consoles
      </a>
    </li>
    <li role="presentation">
      <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#oral-presentations" aria-controls="oral-presentations" role="tab" data-toggle="tab" data-tab-index="1" data-modify-history="true">
        Oral Presentations
      </a>
    </li>
    <li role="presentation">
      <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#spotlight-presentations" aria-controls="spotlight-presentations" role="tab" data-toggle="tab" data-tab-index="2" data-modify-history="true">
        Spotlight Presentations
      </a>
    </li>
    <li role="presentation" class="active">
      <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#poster-presentations" aria-controls="poster-presentations" role="tab" data-toggle="tab" data-tab-index="3" data-modify-history="true" aria-expanded="true">
        Poster Presentations
      </a>
    </li>
    <li role="presentation">
      <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#withdrawn-rejected-submissions" aria-controls="withdrawn-rejected-submissions" role="tab" data-toggle="tab" data-tab-index="4" data-modify-history="true">
        Withdrawn/Rejected Submissions
      </a>
    </li>
</ul>

<div class="tab-content">
    <div role="tabpanel" class="tab-pane fade  " id="your-consoles">
      
    </div>
    <div role="tabpanel" class="tab-pane fade  " id="oral-presentations">
      
    </div>
    <div role="tabpanel" class="tab-pane fade  " id="spotlight-presentations">
      
    </div>
    <div role="tabpanel" class="tab-pane fade   active in" id="poster-presentations">
      
    <ul class="list-unstyled submissions-list">
    <li class="note " data-id="YwpZmcAehZ" data-number="181">
      <h4>
        <a href="https://openreview.net/forum?id=YwpZmcAehZ">
            Revisiting Dynamic Convolution via Matrix Decomposition
        </a>
      
        
          <a href="https://openreview.net/pdf?id=YwpZmcAehZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yunsheng_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yunsheng_Li1">Yunsheng Li</a>, <a href="https://openreview.net/profile?id=~Yinpeng_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yinpeng_Chen1">Yinpeng Chen</a>, <a href="https://openreview.net/profile?id=~Xiyang_Dai2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiyang_Dai2">Xiyang Dai</a>, <a href="https://openreview.net/profile?email=mengcliu%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mengcliu@microsoft.com">mengchen liu</a>, <a href="https://openreview.net/profile?id=~Dongdong_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dongdong_Chen1">Dongdong Chen</a>, <a href="https://openreview.net/profile?email=yu.ye%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yu.ye@microsoft.com">Ye Yu</a>, <a href="https://openreview.net/profile?id=~Lu_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lu_Yuan1">Lu Yuan</a>, <a href="https://openreview.net/profile?id=~Zicheng_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zicheng_Liu1">Zicheng Liu</a>, <a href="https://openreview.net/profile?id=~Mei_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mei_Chen2">Mei Chen</a>, <a href="https://openreview.net/profile?id=~Nuno_Vasconcelos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nuno_Vasconcelos1">Nuno Vasconcelos</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#YwpZmcAehZ-details-930" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YwpZmcAehZ-details-930"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">supervised representation learning, efficient network, dynamic network, matrix decomposition</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent research in dynamic convolution shows substantial performance boost for efficient CNNs, due to the adaptive aggregation of K static convolution kernels. It has two limitations: (a) it increases the number of convolutional weights by K-times, and (b) the joint optimization of dynamic attention and static convolution kernels is challenging. In this paper, we revisit it from a new perspective of matrix decomposition and reveal the key issue is that dynamic convolution applies dynamic attention over channel groups after projecting into a higher dimensional latent space. To address this issue, we propose dynamic channel fusion to replace dynamic attention over channel groups. Dynamic channel fusion not only enables significant dimension reduction of the latent space, but also mitigates the joint optimization difficulty. As a result, our method is easier to train and requires significantly fewer parameters without sacrificing accuracy. Source code is at https://github.com/liyunsheng13/dcd.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Efficient network with dynamic matrix decomposition</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="A5VV3UyIQz" data-number="217">
      <h4>
        <a href="https://openreview.net/forum?id=A5VV3UyIQz">
            Explainable Deep One-Class Classification
        </a>
      
        
          <a href="https://openreview.net/pdf?id=A5VV3UyIQz" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Philipp_Liznerski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philipp_Liznerski1">Philipp Liznerski</a>, <a href="https://openreview.net/profile?id=~Lukas_Ruff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lukas_Ruff1">Lukas Ruff</a>, <a href="https://openreview.net/profile?id=~Robert_A._Vandermeulen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Robert_A._Vandermeulen2">Robert A. Vandermeulen</a>, <a href="https://openreview.net/profile?email=b_franks12%40cs.uni-kl.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="b_franks12@cs.uni-kl.de">Billy Joe Franks</a>, <a href="https://openreview.net/profile?id=~Marius_Kloft1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marius_Kloft1">Marius Kloft</a>, <a href="https://openreview.net/profile?id=~Klaus_Robert_Muller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Klaus_Robert_Muller1">Klaus Robert Muller</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#A5VV3UyIQz-details-994" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="A5VV3UyIQz-details-994"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">anomaly-detection, deep-learning, explanations, interpretability, xai, one-class-classification, deep-anomaly-detection, novelty-detection, outlier-detection</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep one-class classification variants for anomaly detection learn a mapping that concentrates nominal samples in feature space causing anomalies to be mapped away. Because this transformation is highly non-linear, finding interpretations poses a significant challenge. In this paper we present an explainable deep one-class classification method, Fully Convolutional Data Description (FCDD), where the mapped samples are themselves also an explanation heatmap. FCDD yields competitive detection performance and provides reasonable explanations on common anomaly detection benchmarks with CIFAR-10 and ImageNet. On MVTec-AD, a recent manufacturing dataset offering ground-truth anomaly maps, FCDD sets a new state of the art in the unsupervised setting. Our method can incorporate ground-truth anomaly maps during training and using even a few of these (~5) improves performance significantly. Finally, using FCDD's explanations we demonstrate the vulnerability of deep one-class classification models to spurious image features such as image watermarks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce an approach to explainable deep anomaly detection based on fully convolutional neural networks. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="lU5Rs_wCweN" data-number="416">
      <h4>
        <a href="https://openreview.net/forum?id=lU5Rs_wCweN">
            Taking Notes on the Fly Helps Language Pre-Training
        </a>
      
        
          <a href="https://openreview.net/pdf?id=lU5Rs_wCweN" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Qiyu_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qiyu_Wu1">Qiyu Wu</a>, <a href="https://openreview.net/profile?id=~Chen_Xing2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chen_Xing2">Chen Xing</a>, <a href="https://openreview.net/profile?email=yatli%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yatli@microsoft.com">Yatao Li</a>, <a href="https://openreview.net/profile?id=~Guolin_Ke3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guolin_Ke3">Guolin Ke</a>, <a href="https://openreview.net/profile?id=~Di_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Di_He1">Di He</a>, <a href="https://openreview.net/profile?id=~Tie-Yan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tie-Yan_Liu1">Tie-Yan Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 14 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>22 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#lU5Rs_wCweN-details-72" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lU5Rs_wCweN-details-72"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Natural Language Processing, Pre-training</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">How to make unsupervised language pre-training more efficient and less resource-intensive is an important research direction in NLP. In this paper, we focus on improving the efficiency of language pre-training methods through providing better data utilization. It is well-known that in language data corpus, words follow a heavy-tail distribution. A large proportion of words appear only very few times and the embeddings of rare words are usually poorly optimized. We argue that such embeddings carry inadequate semantic signals, which could make the data utilization inefficient and slow down the pre-training of the entire model. To mitigate this problem, we propose Taking Notes on the Fly (TNF), which takes notes for rare words on the fly during pre-training to help the model understand them when they occur next time. Specifically, TNF maintains a note dictionary and saves a rare word's contextual information in it as notes when the rare word occurs in a sentence. When the same rare word occurs again during training, the note information saved beforehand can be employed to enhance the semantics of the current sentence. By doing so, TNF provides a better data utilization since cross-sentence information is employed to cover the inadequate semantics caused by rare words in the sentences. We implement TNF on both BERT and ELECTRA to check its efficiency and effectiveness.  Experimental results show that TNF's training time is 60% less than its backbone pre-training models when reaching the same performance.  When trained with same number of iterations, TNF outperforms its backbone methods on most of downstream tasks and the average GLUE score. Code is attached in the supplementary material.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=lU5Rs_wCweN&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We improve the efficiency of language pre-training methods through providing better data utilization.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="l-LGlk4Yl6G" data-number="549">
      <h4>
        <a href="https://openreview.net/forum?id=l-LGlk4Yl6G">
            Mixed-Features Vectors and Subspace Splitting
        </a>
      
        
          <a href="https://openreview.net/pdf?id=l-LGlk4Yl6G" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alejandro_Pimentel-Alarc%C3%B3n2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alejandro_Pimentel-Alarcón2">Alejandro Pimentel-Alarcón</a>, <a href="https://openreview.net/profile?id=~Daniel_L._Pimentel-Alarc%C3%B3n1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_L._Pimentel-Alarcón1">Daniel L. Pimentel-Alarcón</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#l-LGlk4Yl6G-details-528" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="l-LGlk4Yl6G-details-528"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Motivated by metagenomics, recommender systems, dictionary learning, and related problems, this paper introduces subspace splitting(SS): the task of clustering the entries of what we call amixed-features vector, that is, a vector whose subsets of coordinates agree with a collection of subspaces. We derive precise identifiability conditions under which SS is well-posed, thus providing the first fundamental theory for this problem. We also propose the first three practical SS algorithms, each with advantages and disadvantages: a random sampling method , a projection-based greedy heuristic , and an alternating Lloyd-type algorithm ; all allow noise, outliers, and missing data. Our extensive experiments outline the performance of our algorithms, and in lack of other SS algorithms, for reference we compare against methods for tightly related problems, like robust matched subspace detection and maximum feasible subsystem, which are special simpler cases of SS.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="o966_Is_nPA" data-number="555">
      <h4>
        <a href="https://openreview.net/forum?id=o966_Is_nPA">
            Neural Pruning via Growing Regularization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=o966_Is_nPA" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Huan_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huan_Wang3">Huan Wang</a>, <a href="https://openreview.net/profile?id=~Can_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Can_Qin1">Can Qin</a>, <a href="https://openreview.net/profile?id=~Yulun_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yulun_Zhang1">Yulun Zhang</a>, <a href="https://openreview.net/profile?id=~Yun_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yun_Fu1">Yun Fu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#o966_Is_nPA-details-499" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="o966_Is_nPA-details-499"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">model compression, deep neural network pruning, Hessian matrix, regularization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Regularization has long been utilized to learn sparsity in deep neural network pruning. However, its role is mainly explored in the small penalty strength regime. In this work, we extend its application to a new scenario where the regularization grows large gradually to tackle two central problems of pruning: pruning schedule and weight importance scoring. (1) The former topic is newly brought up in this work, which we find critical to the pruning performance while receives little research attention. Specifically, we propose an L2 regularization variant with rising penalty factors and show it can bring significant accuracy gains compared with its one-shot counterpart, even when the same weights are removed. (2) The growing penalty scheme also brings us an approach to exploit the Hessian information for more accurate pruning without knowing their specific values, thus not bothered by the common Hessian approximation problems. Empirically, the proposed algorithms are easy to implement and scalable to large datasets and networks in both structured and unstructured pruning. Their effectiveness is demonstrated with modern deep neural networks on the CIFAR and ImageNet datasets, achieving competitive results compared to many state-of-the-art algorithms. Our code and trained models are publicly available at https://github.com/mingsun-tse/regularization-pruning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose two new deep network pruning algorithms based a growing regularization paradigm.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="6k7VdojAIK" data-number="767">
      <h4>
        <a href="https://openreview.net/forum?id=6k7VdojAIK">
            Practical Massively Parallel Monte-Carlo Tree Search Applied to Molecular Design
        </a>
      
        
          <a href="https://openreview.net/pdf?id=6k7VdojAIK" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xiufeng_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiufeng_Yang1">Xiufeng Yang</a>, <a href="https://openreview.net/profile?id=~Tanuj_Aasawat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tanuj_Aasawat1">Tanuj Aasawat</a>, <a href="https://openreview.net/profile?id=~Kazuki_Yoshizoe2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kazuki_Yoshizoe2">Kazuki Yoshizoe</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#6k7VdojAIK-details-777" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6k7VdojAIK-details-777"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">parallel Monte Carlo Tree Search (MCTS), Upper Confidence bound applied to Trees (UCT), molecular design</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">It is common practice to use large computational resources to train neural networks, known from many examples, such as reinforcement learning applications. However, while massively parallel computing is often used for training models, it is rarely used to search solutions for combinatorial optimization problems. This paper proposes a novel massively parallel Monte-Carlo Tree Search (MP-MCTS) algorithm that works efficiently for a 1,000 worker scale on a distributed memory environment using multiple compute nodes and applies it to molecular design. This paper is the first work that applies distributed MCTS to a real-world and non-game problem. Existing works on large-scale parallel MCTS show efficient scalability in terms of the number of rollouts up to 100 workers. Still, they suffer from the degradation in the quality of the solutions. MP-MCTS maintains the search quality at a larger scale. By running MP-MCTS on 256 CPU cores for only 10 minutes, we obtained candidate molecules with similar scores to non-parallel MCTS running for 42 hours. Moreover, our results based on parallel MCTS (combined with a simple RNN model) significantly outperform existing state-of-the-art work. Our method is generic and is expected to speed up other applications of MCTS.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Novel massively parallel MCTS achieves state-of-the-art score in molecular design benchmark.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=6k7VdojAIK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="5jRVa89sZk" data-number="823">
      <h4>
        <a href="https://openreview.net/forum?id=5jRVa89sZk">
            Empirical Analysis of Unlabeled Entity Problem in Named Entity Recognition
        </a>
      
        
          <a href="https://openreview.net/pdf?id=5jRVa89sZk" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yangming_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yangming_Li1">Yangming Li</a>, <a href="https://openreview.net/profile?id=~lemao_liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~lemao_liu1">lemao liu</a>, <a href="https://openreview.net/profile?id=~Shuming_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuming_Shi1">Shuming Shi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#5jRVa89sZk-details-509" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5jRVa89sZk-details-509"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Named Entity Recognition, Unlabeled Entity Problem, Negative Sampling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In many scenarios, named entity recognition (NER) models severely suffer from unlabeled entity problem, where the entities of a sentence may not be fully annotated. Through empirical studies performed on synthetic datasets, we find two causes of performance degradation. One is the reduction of annotated entities and the other is treating unlabeled entities as negative instances. The first cause has less impact than the second one and can be mitigated by adopting pretraining language models. The second cause seriously misguides a model in training and greatly affects its performances. Based on the above observations, we propose a general approach, which can almost eliminate the misguidance brought by unlabeled entities. The key idea is to use negative sampling that, to a large extent, avoids training NER models with unlabeled entities. Experiments on synthetic datasets and real-world datasets show that our model is robust to unlabeled entity problem and surpasses prior baselines. On well-annotated datasets, our model is competitive with the state-of-the-art method.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This work studys what are the impacts of unlabeled entity problem on NER models and how to effectively eliminate them by a general method.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="O-6Pm_d_Q-" data-number="863">
      <h4>
        <a href="https://openreview.net/forum?id=O-6Pm_d_Q-">
            Deep Networks and the Multiple Manifold Problem
        </a>
      
        
          <a href="https://openreview.net/pdf?id=O-6Pm_d_Q-" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sam_Buchanan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sam_Buchanan1">Sam Buchanan</a>, <a href="https://openreview.net/profile?id=~Dar_Gilboa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dar_Gilboa1">Dar Gilboa</a>, <a href="https://openreview.net/profile?id=~John_Wright1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_Wright1">John Wright</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#O-6Pm_d_Q--details-800" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="O-6Pm_d_Q--details-800"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning, overparameterized neural networks, low-dimensional structure</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the multiple manifold problem, a binary classification task modeled on applications in machine vision, in which a deep fully-connected neural network is trained to separate two low-dimensional submanifolds of the unit sphere. We provide an analysis of the one-dimensional case, proving for a simple manifold configuration that when the network depth <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="0" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></mjx-assistive-mml></mjx-container> is large relative to certain geometric and statistical properties of the data, the network width <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="1" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container> grows as a sufficiently large polynomial in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="2" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></mjx-assistive-mml></mjx-container>, and the number of i.i.d. samples from the manifolds is polynomial in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="3" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></mjx-assistive-mml></mjx-container>, randomly-initialized gradient descent rapidly learns to classify the two manifolds perfectly with high probability. Our analysis demonstrates concrete benefits of depth and width in the context of a practically-motivated model problem: the depth acts as a fitting resource, with larger depths corresponding to smoother networks that can more readily separate the class manifolds, and the width acts as a statistical resource, enabling concentration of the randomly-initialized network and its gradients. The argument centers around the "neural tangent kernel" of Jacot et al. and its role in the nonasymptotic analysis of training overparameterized neural networks; to this literature, we contribute essentially optimal rates of concentration for the neural tangent kernel of deep fully-connected ReLU networks, requiring width <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="4" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2265"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-mstyle><mjx-mspace style="width: 0.167em;"></mjx-mspace></mjx-mstyle><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c70"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c6F"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c79"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>≥</mo><mi>L</mi><mstyle scriptlevel="0"><mspace width="thinmathspace"></mspace></mstyle><mrow><mi mathvariant="normal">p</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">y</mi></mrow><mo stretchy="false">(</mo><msub><mi>d</mi><mn>0</mn></msub><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> to achieve uniform concentration of the initial kernel over a <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="5" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mn>0</mn></msub></math></mjx-assistive-mml></mjx-container>-dimensional submanifold of the unit sphere <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="6" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c1D54A TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.429em;"><mjx-texatom size="s" texclass="ORD"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.201em;"><mjx-mn class="mjx-n" style="font-size: 83.3%;"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi mathvariant="double-struck">S</mi></mrow><mrow><msub><mi>n</mi><mn>0</mn></msub><mo>−</mo><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container>, and a nonasymptotic framework for establishing generalization of networks trained in the "NTK regime" with structured data. The proof makes heavy use of martingale concentration to optimally treat statistical dependencies across layers of the initial random network. This approach should be of use in establishing similar results for other network architectures.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We prove a finite-time generalization result for deep fully-connected neural networks trained by gradient descent to classify structured data, where the required width, depth, and sample complexity depend only on intrinsic properties of the data.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ZzwDy_wiWv" data-number="869">
      <h4>
        <a href="https://openreview.net/forum?id=ZzwDy_wiWv">
            Knowledge distillation via softmax regression representation learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ZzwDy_wiWv" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jing_Yang7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jing_Yang7">Jing Yang</a>, <a href="https://openreview.net/profile?id=~Brais_Martinez3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brais_Martinez3">Brais Martinez</a>, <a href="https://openreview.net/profile?id=~Adrian_Bulat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adrian_Bulat1">Adrian Bulat</a>, <a href="https://openreview.net/profile?id=~Georgios_Tzimiropoulos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Georgios_Tzimiropoulos1">Georgios Tzimiropoulos</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 21 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ZzwDy_wiWv-details-858" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZzwDy_wiWv-details-858"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper addresses the problem of model compression via knowledge distillation. We advocate for a method that optimizes the output feature of the penultimate layer of the student network and hence is directly related to representation learning. Previous distillation methods which typically impose direct feature matching between the student and the teacher do not take into account the classification problem at hand. On the contrary, our distillation method decouples representation learning and classification and utilizes the teacher's pre-trained classifier to train the student's penultimate layer feature. In particular, for the same input image, we wish the teacher's and student's feature to produce the same output when passed through the teacher's classifier which is achieved with a simple <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="7" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> loss. Our method is extremely simple to implement and straightforward to train and is shown to consistently outperform previous state-of-the-art methods over a large set of experimental settings including different (a) network architectures, (b) teacher-student capacities, (c) datasets, and (d) domains. The code will be available at \url{https://github.com/jingyang2017/KD_SRRL}.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="7wCBOfJ8hJM" data-number="971">
      <h4>
        <a href="https://openreview.net/forum?id=7wCBOfJ8hJM">
            Nearest Neighbor Machine Translation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=7wCBOfJ8hJM" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Urvashi_Khandelwal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Urvashi_Khandelwal1">Urvashi Khandelwal</a>, <a href="https://openreview.net/profile?id=~Angela_Fan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Angela_Fan2">Angela Fan</a>, <a href="https://openreview.net/profile?id=~Dan_Jurafsky1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Jurafsky1">Dan Jurafsky</a>, <a href="https://openreview.net/profile?id=~Luke_Zettlemoyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luke_Zettlemoyer1">Luke Zettlemoyer</a>, <a href="https://openreview.net/profile?id=~Mike_Lewis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mike_Lewis1">Mike Lewis</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#7wCBOfJ8hJM-details-911" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7wCBOfJ8hJM-details-911"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">nearest neighbors, machine translation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We introduce <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="8" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>-nearest-neighbor machine translation (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="9" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>NN-MT), which predicts tokens with a nearest-neighbor classifier over a large datastore of cached examples, using representations from a neural translation model for similarity search. This approach requires no additional training and scales to give the decoder direct access to billions of examples at test time, resulting in a highly expressive model that consistently improves performance across many settings. Simply adding nearest-neighbor search improves a state-of-the-art German-English translation model by 1.5 BLEU. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="10" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>NN-MT allows a single model to be adapted to diverse domains by using a domain-specific datastore, improving results by an average of 9.2 BLEU over zero-shot transfer, and achieving new state-of-the-art results---without training on these domains. A massively multilingual model can also be specialized for particular language pairs, with improvements of 3 BLEU for translating from English into German and Chinese. Qualitatively, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="11" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>NN-MT is easily interpretable; it combines source and target context to retrieve highly relevant examples.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We augment the decoder of a pre-trained machine translation model with a nearest neighbor classifier, substantially improving performance in the single language-pair, multilingual and domain adaptation settings, without any additional training.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="3SqrRe8FWQ-" data-number="1002">
      <h4>
        <a href="https://openreview.net/forum?id=3SqrRe8FWQ-">
            WrapNet:  Neural Net Inference with Ultra-Low-Precision Arithmetic
        </a>
      
        
          <a href="https://openreview.net/pdf?id=3SqrRe8FWQ-" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Renkun_Ni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Renkun_Ni1">Renkun Ni</a>, <a href="https://openreview.net/profile?email=hmchu%40cs.umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="hmchu@cs.umd.edu">Hong-min Chu</a>, <a href="https://openreview.net/profile?id=~Oscar_Castaneda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Oscar_Castaneda1">Oscar Castaneda</a>, <a href="https://openreview.net/profile?id=~Ping-yeh_Chiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ping-yeh_Chiang1">Ping-yeh Chiang</a>, <a href="https://openreview.net/profile?id=~Christoph_Studer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christoph_Studer1">Christoph Studer</a>, <a href="https://openreview.net/profile?id=~Tom_Goldstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tom_Goldstein1">Tom Goldstein</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#3SqrRe8FWQ--details-980" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3SqrRe8FWQ--details-980"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">quantization, efficient inference</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Low-precision neural networks represent both weights and activations with few bits, drastically reducing the cost of multiplications. Meanwhile, these products are accumulated using high-precision (typically 32-bit) additions.  Additions dominate the arithmetic complexity of inference in quantized (e.g., binary) nets, and high precision is needed to avoid overflow. To further optimize inference, we propose WrapNet, an architecture that adapts neural networks to use low-precision (8-bit) additions while achieving classification accuracy comparable to their 32-bit counterparts. We achieve resilience to low-precision accumulation by inserting a cyclic activation layer that makes results invariant to overflow. We demonstrate the efficacy of our approach using both software and hardware platforms.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We adapt neural networks to integer overflow and extreme low-bit accumulator, and show the efficacy on both software and hardware platforms. </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=3SqrRe8FWQ-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="oZIvHV04XgC" data-number="1192">
      <h4>
        <a href="https://openreview.net/forum?id=oZIvHV04XgC">
            Wandering within a world: Online contextualized few-shot learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=oZIvHV04XgC" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mengye_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mengye_Ren1">Mengye Ren</a>, <a href="https://openreview.net/profile?id=~Michael_Louis_Iuzzolino1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Louis_Iuzzolino1">Michael Louis Iuzzolino</a>, <a href="https://openreview.net/profile?id=~Michael_Curtis_Mozer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Curtis_Mozer1">Michael Curtis Mozer</a>, <a href="https://openreview.net/profile?id=~Richard_Zemel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Richard_Zemel1">Richard Zemel</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#oZIvHV04XgC-details-675" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oZIvHV04XgC-details-675"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Few-shot learning, continual learning, lifelong learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We aim to bridge the gap between typical human and machine-learning environments by extending the standard framework of few-shot learning to an online, continual setting. In this setting, episodes do not have separate training and testing phases, and instead models are evaluated online while learning novel classes. As in the real world, where the presence of spatiotemporal context helps us retrieve learned skills in the past, our online few-shot learning setting also features an underlying context that changes throughout time. Object classes are correlated within a context and inferring the correct context can lead to better performance. Building upon this setting, we propose a new few-shot learning dataset based on large scale indoor imagery that mimics the visual
      experience of an agent wandering within a world. Furthermore, we convert popular few-shot learning approaches into online versions and we also propose a new model that can make use of spatiotemporal contextual information from the recent past.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a new continual few-shot learning paradigm and a new model.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=oZIvHV04XgC&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="pW2Q2xLwIMD" data-number="1223">
      <h4>
        <a href="https://openreview.net/forum?id=pW2Q2xLwIMD">
            Few-Shot Learning via Learning the Representation, Provably
        </a>
      
        
          <a href="https://openreview.net/pdf?id=pW2Q2xLwIMD" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Simon_Shaolei_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Shaolei_Du1">Simon Shaolei Du</a>, <a href="https://openreview.net/profile?id=~Wei_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Hu1">Wei Hu</a>, <a href="https://openreview.net/profile?id=~Sham_M._Kakade1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sham_M._Kakade1">Sham M. Kakade</a>, <a href="https://openreview.net/profile?id=~Jason_D._Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jason_D._Lee1">Jason D. Lee</a>, <a href="https://openreview.net/profile?id=~Qi_Lei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qi_Lei1">Qi Lei</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 30 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#pW2Q2xLwIMD-details-710" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pW2Q2xLwIMD-details-710"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">representation learning, statistical learning theory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper studies few-shot learning via representation learning, where one uses <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="12" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container> source tasks with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="13" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mn>1</mn></msub></math></mjx-assistive-mml></mjx-container> data per task to learn a representation in order to reduce the sample complexity of a target task for which there is only <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="14" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c226A"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mo>≪</mo><msub><mi>n</mi><mn>1</mn></msub><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> data. Specifically, we focus on the setting where there exists a good common representation between source and target, and our goal is to understand how much a sample size reduction is possible. First, we study the setting where this common representation is low-dimensional and provide a risk bound of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="15" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.06em; padding-left: 0.215em; margin-bottom: -0.215em;"><mjx-mo class="mjx-n"><mjx-c class="mjx-c7E"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mrow size="s"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.201em;"><mjx-mn class="mjx-n" style="font-size: 83.3%;"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mfrac space="3"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msub size="s"><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.201em;"><mjx-mn class="mjx-n" style="font-size: 83.3%;"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mi>O</mi><mo stretchy="false">~</mo></mover></mrow><mo stretchy="false">(</mo><mfrac><mrow><mi>d</mi><mi>k</mi></mrow><mrow><msub><mi>n</mi><mn>1</mn></msub><mi>T</mi></mrow></mfrac><mo>+</mo><mfrac><mi>k</mi><msub><mi>n</mi><mn>2</mn></msub></mfrac><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> on the target task for the linear representation class; here <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="16" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></mjx-assistive-mml></mjx-container> is the ambient input dimension and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="17" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c226A"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi><mo stretchy="false">(</mo><mo>≪</mo><mi>d</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> is the dimension of the representation. This result bypasses the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="18" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A9"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mfrac><mn>1</mn><mi>T</mi></mfrac><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> barrier under the i.i.d. task assumption, and can capture the desired property that all <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="19" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mn>1</mn></msub><mi>T</mi></math></mjx-assistive-mml></mjx-container> samples from source tasks can be \emph{pooled} together for representation learning. We further extend this result to handle a general representation function class and obtain a similar result. Next, we consider the setting where the common representation may be high-dimensional but is capacity-constrained (say in norm); here, we again demonstrate the advantage of representation learning in both high-dimensional linear regression and neural networks, and show that representation learning can fully utilize all <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="20" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mn>1</mn></msub><mi>T</mi></math></mjx-assistive-mml></mjx-container> samples from source tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We study when and how much representation learning can help few-shot learning by drastically reducing sample complexity on the target task.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="QkRbdiiEjM" data-number="1400">
      <h4>
        <a href="https://openreview.net/forum?id=QkRbdiiEjM">
            AdaGCN: Adaboosting Graph Convolutional Networks into Deep Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=QkRbdiiEjM" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ke_Sun3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ke_Sun3">Ke Sun</a>, <a href="https://openreview.net/profile?id=~Zhanxing_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhanxing_Zhu1">Zhanxing Zhu</a>, <a href="https://openreview.net/profile?id=~Zhouchen_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhouchen_Lin1">Zhouchen Lin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#QkRbdiiEjM-details-518" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QkRbdiiEjM-details-518"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Graph Neural Networks, AdaBoost</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The design of deep graph models still remains to be investigated and the crucial part is how to explore and exploit the knowledge from different hops of neighbors in an efficient way. In this paper, we propose a novel RNN-like deep graph neural network architecture by incorporating AdaBoost into the computation of network; and the proposed graph convolutional network called AdaGCN~(Adaboosting Graph Convolutional Network) has the ability to efficiently extract knowledge from high-order neighbors of current nodes and then integrates knowledge from different hops of neighbors into the network in an Adaboost way. Different from other graph neural networks that directly stack many graph convolution layers, AdaGCN shares the same base neural network architecture among all ``layers'' and is recursively optimized, which is similar to an RNN. Besides, We also theoretically established the connection between AdaGCN and existing graph convolutional methods, presenting the benefits of our proposal. Finally, extensive experiments demonstrate the consistent state-of-the-art prediction performance on graphs across different label rates and the computational advantage of our approach AdaGCN~\footnote{Code is available at \url{https://github.com/datake/AdaGCN}.}.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a novel RNN-like deep graph neural network architecture by incorporating AdaBoost into the computation of network.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ee6W5UgQLa" data-number="1545">
      <h4>
        <a href="https://openreview.net/forum?id=ee6W5UgQLa">
            MultiModalQA: complex question answering over text, tables and images
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ee6W5UgQLa" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alon_Talmor1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alon_Talmor1">Alon Talmor</a>, <a href="https://openreview.net/profile?email=oriy%40mail.tau.ac.il" class="profile-link" data-toggle="tooltip" data-placement="top" title="oriy@mail.tau.ac.il">Ori Yoran</a>, <a href="https://openreview.net/profile?email=amnoncatav%40mail.tau.ac.il" class="profile-link" data-toggle="tooltip" data-placement="top" title="amnoncatav@mail.tau.ac.il">Amnon Catav</a>, <a href="https://openreview.net/profile?email=lahav%40mail.tau.ac.il" class="profile-link" data-toggle="tooltip" data-placement="top" title="lahav@mail.tau.ac.il">Dan Lahav</a>, <a href="https://openreview.net/profile?id=~Yizhong_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yizhong_Wang2">Yizhong Wang</a>, <a href="https://openreview.net/profile?id=~Akari_Asai2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Akari_Asai2">Akari Asai</a>, <a href="https://openreview.net/profile?id=~Gabriel_Ilharco1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gabriel_Ilharco1">Gabriel Ilharco</a>, <a href="https://openreview.net/profile?id=~Hannaneh_Hajishirzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hannaneh_Hajishirzi1">Hannaneh Hajishirzi</a>, <a href="https://openreview.net/profile?id=~Jonathan_Berant1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Berant1">Jonathan Berant</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ee6W5UgQLa-details-656" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ee6W5UgQLa-details-656"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">NLP, Question Answering, Dataset, Multi-Modal, Multi-Hop</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">When answering complex questions, people can seamlessly combine information from visual, textual and tabular sources. 
      While interest in models that reason over multiple pieces of evidence has surged in recent years, there has been relatively little work on question answering models that reason across multiple modalities.
      In this paper, we present MultiModalQA (MMQA): a challenging question answering dataset that requires joint reasoning over text, tables and images. 
      We create MMQA using a new framework for generating complex multi-modal questions at scale, harvesting tables from Wikipedia, and attaching images and text paragraphs using entities that appear in each table. We then define a formal language that allows us to take questions that can be answered from a single modality, and combine them to generate cross-modal questions. Last, crowdsourcing workers take these automatically generated questions and rephrase them into more fluent language.
      We create 29,918 questions through this procedure, and empirically demonstrate the necessity of a multi-modal multi-hop approach to solve our task: our multi-hop model, ImplicitDecomp, achieves an average F1 of 51.7  over cross-modal questions, substantially outperforming a strong baseline that achieves 38.2 F1, but still lags significantly behind human performance, which is at 90.1 F1.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">MultiModalQA: A question answering dataset that requires multi-modal multi-hop reasoning over wikipedia text, tables and images, accompanied by a new multi-hop model for tackling the task.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=ee6W5UgQLa&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="73WTGs96kho" data-number="1574">
      <h4>
        <a href="https://openreview.net/forum?id=73WTGs96kho">
            Net-DNF: Effective Deep Modeling of Tabular Data
        </a>
      
        
          <a href="https://openreview.net/pdf?id=73WTGs96kho" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Liran_Katzir1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liran_Katzir1">Liran Katzir</a>, <a href="https://openreview.net/profile?id=~Gal_Elidan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gal_Elidan1">Gal Elidan</a>, <a href="https://openreview.net/profile?id=~Ran_El-Yaniv1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ran_El-Yaniv1">Ran El-Yaniv</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">01 Oct 2020 (modified: 24 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#73WTGs96kho-details-122" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="73WTGs96kho-details-122"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural Networks, Architectures, Tabular Data, Predictive Modeling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A challenging open question in deep learning is how to handle tabular data. Unlike domains such as image and natural language processing, where deep architectures prevail, there is still no widely accepted neural architecture that dominates tabular data. As a step toward bridging this gap, we present Net-DNF a novel generic architecture whose inductive bias elicits models whose structure corresponds to logical Boolean formulas in disjunctive normal form (DNF) over affine soft-threshold decision terms. Net-DNFs also promote localized decisions that are taken over small subsets of the features. We present an extensive experiments showing that Net-DNFs significantly and consistently outperform fully connected networks over tabular data. With relatively few hyperparameters, Net-DNFs open the door to practical end-to-end handling of tabular data using neural networks. We present ablation studies, which justify the design choices of Net-DNF including the inductive bias elements, namely, Boolean formulation, locality, and feature selection. 
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Neural network architecture for tabular data</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=73WTGs96kho&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="7R7fAoUygoa" data-number="1938">
      <h4>
        <a href="https://openreview.net/forum?id=7R7fAoUygoa">
            Optimal Regularization can Mitigate Double Descent
        </a>
      
        
          <a href="https://openreview.net/pdf?id=7R7fAoUygoa" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Preetum_Nakkiran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Preetum_Nakkiran1">Preetum Nakkiran</a>, <a href="https://openreview.net/profile?email=pvenkat%40g.harvard.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="pvenkat@g.harvard.edu">Prayaag Venkat</a>, <a href="https://openreview.net/profile?id=~Sham_M._Kakade1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sham_M._Kakade1">Sham M. Kakade</a>, <a href="https://openreview.net/profile?id=~Tengyu_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tengyu_Ma1">Tengyu Ma</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#7R7fAoUygoa-details-392" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7R7fAoUygoa-details-392"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">double descent, generalization, regularization, regression, monotonicity</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent empirical and theoretical studies have shown that many learning algorithms -- from linear regression to neural networks -- can have test performance that is non-monotonic in quantities such the sample size and model size. This striking phenomenon, often referred to as "double descent", has raised questions of if we need to re-think our current understanding of generalization. In this work, we study whether the double-descent phenomenon can be avoided by using optimal regularization. Theoretically, we prove that for certain linear regression models with isotropic data distribution, optimally-tuned <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="21" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> regularization achieves monotonic test performance as we grow either the sample size or the model size.
      We also demonstrate empirically that optimally-tuned <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="22" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> regularization can mitigate double descent for more general models, including neural networks.
      Our results suggest that it may also be informative to study the test risk scalings of various algorithms in the context of appropriately tuned regularization.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Optimal regularization can provably avoid double-descent in certain settings.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="3jjmdp7Hha" data-number="2063">
      <h4>
        <a href="https://openreview.net/forum?id=3jjmdp7Hha">
            Meta Back-Translation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=3jjmdp7Hha" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hieu_Pham1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hieu_Pham1">Hieu Pham</a>, <a href="https://openreview.net/profile?id=~Xinyi_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinyi_Wang1">Xinyi Wang</a>, <a href="https://openreview.net/profile?id=~Yiming_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yiming_Yang1">Yiming Yang</a>, <a href="https://openreview.net/profile?id=~Graham_Neubig1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Graham_Neubig1">Graham Neubig</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#3jjmdp7Hha-details-37" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3jjmdp7Hha-details-37"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">meta learning, machine translation, back translation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Back-translation is an effective strategy to improve the performance of Neural Machine Translation~(NMT) by generating pseudo-parallel data. However, several recent works have found that better translation quality in the pseudo-parallel data does not necessarily lead to a better final translation model, while lower-quality but diverse data often yields stronger results instead.
      In this paper we propose a new way to generate pseudo-parallel data for back-translation that directly optimizes the final model performance.  Specifically, we propose a meta-learning framework where the back-translation model learns to match the forward-translation model's gradients on the development data with those on the pseudo-parallel data. In our evaluations in both the standard datasets WMT En-De'14 and WMT En-Fr'14, as well as a multilingual translation setting, our method leads to significant improvements over strong baselines. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Use meta learning to teach the back-translation model to generate better back-translated sentences.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=3jjmdp7Hha&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="nkIDwI6oO4_" data-number="2099">
      <h4>
        <a href="https://openreview.net/forum?id=nkIDwI6oO4_">
            Learning A Minimax Optimizer: A Pilot Study
        </a>
      
        
          <a href="https://openreview.net/pdf?id=nkIDwI6oO4_" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jiayi_Shen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiayi_Shen1">Jiayi Shen</a>, <a href="https://openreview.net/profile?id=~Xiaohan_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaohan_Chen1">Xiaohan Chen</a>, <a href="https://openreview.net/profile?id=~Howard_Heaton2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Howard_Heaton2">Howard Heaton</a>, <a href="https://openreview.net/profile?id=~Tianlong_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianlong_Chen1">Tianlong Chen</a>, <a href="https://openreview.net/profile?id=~Jialin_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jialin_Liu1">Jialin Liu</a>, <a href="https://openreview.net/profile?id=~Wotao_Yin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wotao_Yin1">Wotao Yin</a>, <a href="https://openreview.net/profile?id=~Zhangyang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhangyang_Wang1">Zhangyang Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 14 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#nkIDwI6oO4_-details-567" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="nkIDwI6oO4_-details-567"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Learning to Optimize, Minimax Optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Solving continuous minimax optimization is of extensive practical interest, yet notoriously unstable and difficult. This paper introduces the learning to optimize(L2O) methodology to the minimax problems for the first time and addresses its accompanying unique challenges. We first present Twin-L2O, the first dedicated minimax L2O method consisting of two LSTMs for updating min and max variables separately. The decoupled design is found to facilitate learning, particularly when the min and max variables are highly asymmetric. Empirical experiments on a variety of minimax problems corroborate the effectiveness of Twin-L2O. We then discuss a crucial concern of Twin-L2O, i.e., its inevitably limited generalizability to unseen optimizees. To address this issue, we present two complementary strategies. Our first solution, Enhanced Twin-L2O, is empirically applicable for general minimax problems, by improving L2O training via leveraging curriculum learning. Our second alternative, called Safeguarded Twin-L2O, is a preliminary theoretical exploration stating that under some strong assumptions, it is possible to theoretically establish the convergence of Twin-L2O. We benchmark our algorithms on several testbed problems and compare against state-of-the-art minimax solvers. The code is available at:  https://github.com/VITA-Group/L2O-Minimax.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper introduces the learning to optimize (L2O) methodology, called Twin L2O, for minimax optimization consisting of two LSTMs.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ajOrOhQOsYx" data-number="2140">
      <h4>
        <a href="https://openreview.net/forum?id=ajOrOhQOsYx">
            A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ajOrOhQOsYx" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Leon_Lang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leon_Lang1">Leon Lang</a>, <a href="https://openreview.net/profile?id=~Maurice_Weiler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maurice_Weiler1">Maurice Weiler</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ajOrOhQOsYx-details-840" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ajOrOhQOsYx-details-840"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Group Equivariant Convolution, Steerable Kernel, Quantum Mechanics, Wigner-Eckart Theorem, Representation Theory, Harmonic Analysis, Peter-Weyl Theorem</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Group equivariant convolutional networks (GCNNs) endow classical convolutional networks with additional symmetry priors, which can lead to a considerably improved performance. Recent advances in the theoretical description of GCNNs revealed that such models can generally be understood as performing convolutions with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="23" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></mjx-assistive-mml></mjx-container>-steerable kernels, that is, kernels that satisfy an equivariance constraint themselves. While the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="24" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></mjx-assistive-mml></mjx-container>-steerability constraint has been derived, it has to date only been solved for specific use cases - a general characterization of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="25" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></mjx-assistive-mml></mjx-container>-steerable kernel spaces is still missing. This work provides such a characterization for the practically relevant case of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="26" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></mjx-assistive-mml></mjx-container> being any compact group. Our investigation is motivated by a striking analogy between the constraints underlying steerable kernels on the one hand and spherical tensor operators from quantum mechanics on the other hand. By generalizing the famous Wigner-Eckart theorem for spherical tensor operators, we prove that steerable kernel spaces are fully understood and parameterized in terms of 1) generalized reduced matrix elements, 2) Clebsch-Gordan coefficients, and 3) harmonic basis functions on homogeneous spaces.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We parameterize equivariant convolution kernels by proving a generalization of the Wigner-Eckart theorem for spherical tensor operators.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="enoVQWLsfyL" data-number="2156">
      <h4>
        <a href="https://openreview.net/forum?id=enoVQWLsfyL">
            Viewmaker Networks: Learning Views for Unsupervised Representation Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=enoVQWLsfyL" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alex_Tamkin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_Tamkin1">Alex Tamkin</a>, <a href="https://openreview.net/profile?id=~Mike_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mike_Wu1">Mike Wu</a>, <a href="https://openreview.net/profile?id=~Noah_Goodman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noah_Goodman1">Noah Goodman</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 22 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#enoVQWLsfyL-details-402" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="enoVQWLsfyL-details-402"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">unsupervised learning, self-supervised, representation learning, contrastive learning, views, data augmentation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Many recent methods for unsupervised representation learning train models to be invariant to different "views," or distorted versions of an input. However, designing these views requires considerable trial and error by human experts, hindering widespread adoption of unsupervised representation learning methods across domains and modalities. To address this, we propose viewmaker networks: generative models that learn to produce useful views from a given input. Viewmakers are stochastic bounded adversaries: they produce views by generating and then adding an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="27" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container>-bounded perturbation to the input, and are trained adversarially with respect to the main encoder network. Remarkably, when pretraining on CIFAR-10, our learned views enable comparable transfer accuracy to the well-tuned SimCLR augmentations---despite not including transformations like cropping or color jitter. Furthermore, our learned views significantly outperform baseline augmentations on speech recordings (+9 points on average) and wearable sensor data (+17 points on average). Viewmaker views can also be combined with handcrafted views: they improve robustness to common image corruptions and can increase transfer performance in cases where handcrafted views are less explored. These results suggest that viewmakers may provide a path towards more general representation learning algorithms---reducing the domain expertise and effort needed to pretrain on a much wider set of domains. Code is available at https://github.com/alextamkin/viewmaker.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a new generative model that produces views for self-supervised learning, matching or outperforming hand-crafted views on image, speech, and wearable sensor datasets</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=enoVQWLsfyL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="23ZjUGpjcc" data-number="2224">
      <h4>
        <a href="https://openreview.net/forum?id=23ZjUGpjcc">
            Scalable Transfer Learning with Expert Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=23ZjUGpjcc" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Joan_Puigcerver1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joan_Puigcerver1">Joan Puigcerver</a>, <a href="https://openreview.net/profile?id=~Carlos_Riquelme_Ruiz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Carlos_Riquelme_Ruiz1">Carlos Riquelme Ruiz</a>, <a href="https://openreview.net/profile?email=basilm%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="basilm@google.com">Basil Mustafa</a>, <a href="https://openreview.net/profile?id=~Cedric_Renggli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cedric_Renggli1">Cedric Renggli</a>, <a href="https://openreview.net/profile?id=~Andr%C3%A9_Susano_Pinto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~André_Susano_Pinto1">André Susano Pinto</a>, <a href="https://openreview.net/profile?id=~Sylvain_Gelly1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sylvain_Gelly1">Sylvain Gelly</a>, <a href="https://openreview.net/profile?id=~Daniel_Keysers2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Keysers2">Daniel Keysers</a>, <a href="https://openreview.net/profile?id=~Neil_Houlsby1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neil_Houlsby1">Neil Houlsby</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#23ZjUGpjcc-details-514" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="23ZjUGpjcc-details-514"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Transfer Learning, Expert Models, Few Shot</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Transfer of pre-trained representations can improve sample efficiency and reduce computational requirements for new tasks. However, representations used for transfer are usually generic, and are not tailored to a particular distribution of downstream tasks. We explore the use of expert representations for transfer with a simple, yet effective, strategy. We train a diverse set of experts by exploiting existing label structures, and use cheap-to-compute performance proxies to select the relevant expert for each target task. This strategy scales the process of transferring to new tasks, since it does not revisit the pre-training data during transfer. Accordingly, it requires little extra compute per target task, and results in a speed-up of 2-3 orders of magnitude compared to competing approaches. Further, we provide an adapter-based architecture able to compress many experts into a single model. We evaluate our approach on two different data sources and demonstrate that it outperforms baselines on over 20 diverse vision tasks in both cases.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=23ZjUGpjcc&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Ovp8dvB8IBH" data-number="2248">
      <h4>
        <a href="https://openreview.net/forum?id=Ovp8dvB8IBH">
            Negative Data Augmentation 
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Ovp8dvB8IBH" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Abhishek_Sinha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abhishek_Sinha1">Abhishek Sinha</a>, <a href="https://openreview.net/profile?id=~Kumar_Ayush2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kumar_Ayush2">Kumar Ayush</a>, <a href="https://openreview.net/profile?id=~Jiaming_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiaming_Song1">Jiaming Song</a>, <a href="https://openreview.net/profile?id=~Burak_Uzkent1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Burak_Uzkent1">Burak Uzkent</a>, <a href="https://openreview.net/profile?id=~Hongxia_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongxia_Jin1">Hongxia Jin</a>, <a href="https://openreview.net/profile?id=~Stefano_Ermon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefano_Ermon1">Stefano Ermon</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Ovp8dvB8IBH-details-656" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ovp8dvB8IBH-details-656"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">generative models, self-supervised learning, data augmentation, anomaly detection</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Data augmentation is often used to enlarge datasets with synthetic samples generated in accordance with the underlying data distribution. To enable a wider range of augmentations, we explore negative data augmentation strategies (NDA) that intentionally create out-of-distribution samples. We show that such negative out-of-distribution samples provide information on the support of the data distribution,  and can be leveraged for generative modeling and representation learning. We introduce a new GAN training objective where we use NDA as an additional source of synthetic data for the discriminator. We prove that under suitable conditions, optimizing the resulting objective still recovers the true data distribution but can directly bias the generator towards avoiding samples that lack the desired structure. Empirically, models trained with our method achieve improved conditional/unconditional image generation along with improved anomaly detection capabilities. Further, we incorporate the same negative data augmentation strategy in a contrastive learning framework for self-supervised representation learning on images and videos, achieving improved performance on downstream image classification, object detection, and action recognition tasks. These results suggest that prior knowledge on what does not constitute valid data is an effective form of weak supervision across a range of unsupervised learning tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a framework to do Negative Data Augmentation for generative models and self-supervised learning</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="JCRblSgs34Z" data-number="2340">
      <h4>
        <a href="https://openreview.net/forum?id=JCRblSgs34Z">
            Fantastic Four: Differentiable and Efficient Bounds on Singular Values of Convolution Layers
        </a>
      
        
          <a href="https://openreview.net/pdf?id=JCRblSgs34Z" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sahil_Singla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sahil_Singla1">Sahil Singla</a>, <a href="https://openreview.net/profile?id=~Soheil_Feizi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soheil_Feizi2">Soheil Feizi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#JCRblSgs34Z-details-446" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JCRblSgs34Z-details-446"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">spectral regularization, spectral normalization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In deep neural networks, the spectral norm of the Jacobian of a layer bounds the factor by which the norm of a signal changes during forward/backward propagation. Spectral norm regularizations have been shown to improve generalization, robustness and optimization of deep learning methods. Existing methods to compute the spectral norm of convolution layers either rely on heuristics that are efficient in computation but lack guarantees or are theoretically-sound but computationally expensive. In this work, we obtain the best of both worlds by deriving {\it four} provable upper bounds on the spectral norm of a standard 2D multi-channel convolution layer. These bounds are differentiable and can be computed efficiently during training with negligible overhead. One of these bounds is in fact the popular heuristic method of Miyato et al. (multiplied by a constant factor depending on filter sizes). Each of these four bounds can achieve the tightest gap depending on convolution filters. Thus, we propose to use the minimum of these four bounds as a tight, differentiable and efficient upper bound on the spectral norm of convolution layers. Moreover, our spectral bound is an effective regularizer and can be used to bound either the lipschitz constant or curvature values (eigenvalues of the Hessian) of neural networks. Through experiments on MNIST and CIFAR-10, we demonstrate the effectiveness of our spectral bound in improving generalization and robustness of deep networks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We derive four provable upper bounds on the largest singular value of convolution layers that are differentiable, independent of size of input image and can be computed efficiently during training with negligible overhead.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=JCRblSgs34Z&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Ozk9MrX1hvA" data-number="2524">
      <h4>
        <a href="https://openreview.net/forum?id=Ozk9MrX1hvA">
            CoDA: Contrast-enhanced and Diversity-promoting Data Augmentation for Natural Language Understanding
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Ozk9MrX1hvA" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yanru_Qu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanru_Qu1">Yanru Qu</a>, <a href="https://openreview.net/profile?id=~Dinghan_Shen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dinghan_Shen1">Dinghan Shen</a>, <a href="https://openreview.net/profile?id=~Yelong_Shen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yelong_Shen2">Yelong Shen</a>, <a href="https://openreview.net/profile?email=ssajeev%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ssajeev@microsoft.com">Sandra Sajeev</a>, <a href="https://openreview.net/profile?id=~Weizhu_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weizhu_Chen1">Weizhu Chen</a>, <a href="https://openreview.net/profile?id=~Jiawei_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiawei_Han1">Jiawei Han</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Ozk9MrX1hvA-details-30" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ozk9MrX1hvA-details-30"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">data augmentation, natural language understanding, consistency training, contrastive learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Data augmentation has been demonstrated as an effective strategy for improving model generalization and data efficiency.  However, due to the discrete nature of natural language, designing label-preserving transformations for text data tends to be more challenging. In this paper, we propose a novel data augmentation frame-work dubbed CoDA, which synthesizes diverse and informative augmented examples by integrating multiple transformations organically.  Moreover, a contrastive regularization is introduced to capture the global relationship among all the data samples.  A momentum encoder along with a memory bank is further leveraged to better estimate the contrastive loss. To verify the effectiveness of the proposed framework, we apply CoDA to Transformer-based models on a wide range of natural language understanding tasks. On the GLUE benchmark, CoDA gives rise to an average improvement of 2.2%while applied to the Roberta-large model. More importantly, it consistently exhibits stronger results relative to several competitive data augmentation and adversarial training baselines (including the low-resource settings). Extensive experiments show that the proposed contrastive objective can be flexibly combined with various data augmentation approaches to further boost their performance, highlighting the wide applicability of the CoDA framework.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="4RbdgBh9gE" data-number="2547">
      <h4>
        <a href="https://openreview.net/forum?id=4RbdgBh9gE">
            Teaching with Commentaries
        </a>
      
        
          <a href="https://openreview.net/pdf?id=4RbdgBh9gE" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Aniruddh_Raghu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aniruddh_Raghu1">Aniruddh Raghu</a>, <a href="https://openreview.net/profile?id=~Maithra_Raghu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maithra_Raghu1">Maithra Raghu</a>, <a href="https://openreview.net/profile?id=~Simon_Kornblith1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Kornblith1">Simon Kornblith</a>, <a href="https://openreview.net/profile?id=~David_Duvenaud2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Duvenaud2">David Duvenaud</a>, <a href="https://openreview.net/profile?id=~Geoffrey_Hinton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Geoffrey_Hinton1">Geoffrey Hinton</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#4RbdgBh9gE-details-260" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4RbdgBh9gE-details-260"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">learning to teach, metalearning, hypergradients</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Effective training of deep neural networks can be challenging, and there remain many open questions on how to best learn these models. Recently developed methods to improve neural network training examine teaching: providing learned information during the training process to improve downstream model performance. In this paper, we take steps towards extending the scope of teaching. We propose a flexible teaching framework using commentaries,  learned meta-information helpful for training on a particular task. We present gradient-based methods to learn commentaries, leveraging recent work on implicit differentiation for scalability. We explore diverse applications of commentaries, from weighting training examples, to parameterising label-dependent data augmentation policies, to representing attention masks that highlight salient image regions. We find that commentaries can improve training speed and/or performance, and provide insights about the dataset and training process. We also observe that commentaries generalise: they can be reused when training new models to obtain performance benefits, suggesting a use-case where commentaries are stored with a dataset and leveraged in future for improved model training. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a flexible framework for neural network teaching, demonstrate it in various settings, and find that it can improve performance and yield insights about datasets and the training process.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="UFGEelJkLu5" data-number="2674">
      <h4>
        <a href="https://openreview.net/forum?id=UFGEelJkLu5">
            MixKD: Towards Efficient Distillation of Large-scale Language Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=UFGEelJkLu5" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kevin_J_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kevin_J_Liang1">Kevin J Liang</a>, <a href="https://openreview.net/profile?id=~Weituo_Hao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weituo_Hao1">Weituo Hao</a>, <a href="https://openreview.net/profile?id=~Dinghan_Shen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dinghan_Shen1">Dinghan Shen</a>, <a href="https://openreview.net/profile?id=~Yufan_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yufan_Zhou1">Yufan Zhou</a>, <a href="https://openreview.net/profile?id=~Weizhu_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weizhu_Chen1">Weizhu Chen</a>, <a href="https://openreview.net/profile?id=~Changyou_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changyou_Chen1">Changyou Chen</a>, <a href="https://openreview.net/profile?id=~Lawrence_Carin2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lawrence_Carin2">Lawrence Carin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 22 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>22 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#UFGEelJkLu5-details-816" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UFGEelJkLu5-details-816"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Natural Language Processing, Representation Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Large-scale language models have recently demonstrated impressive empirical performance. Nevertheless, the improved results are attained at the price of bigger models, more power consumption, and slower inference, which hinder their applicability to low-resource (both memory and computation) platforms. Knowledge distillation (KD) has been demonstrated as an effective framework for compressing such big models. However, large-scale neural network systems are prone to memorize training instances, and thus tend to make inconsistent predictions when the data distribution is altered slightly. Moreover, the student model has few opportunities to request useful information from the teacher model when there is limited task-specific data available. To address these issues, we propose MixKD, a data-agnostic distillation framework that leverages mixup, a simple yet efficient data augmentation approach, to endow the resulting model with stronger generalization ability. Concretely, in addition to the original training examples, the student model is encouraged to mimic the teacher's behavior on the linear interpolation of example pairs as well. We prove from a theoretical perspective that under reasonable conditions MixKD gives rise to a smaller gap between the generalization error and the empirical error. To verify its effectiveness, we conduct experiments on the GLUE benchmark, where MixKD consistently leads to significant gains over the standard KD training, and outperforms several competitive baselines. Experiments under a limited-data setting and ablation studies further demonstrate the advantages of the proposed approach.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose MixKD, a distillation framework leveraging mixup for large-scale language models.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="N6JECD-PI5w" data-number="2701">
      <h4>
        <a href="https://openreview.net/forum?id=N6JECD-PI5w">
            FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders
        </a>
      
        
          <a href="https://openreview.net/pdf?id=N6JECD-PI5w" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Pengyu_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pengyu_Cheng1">Pengyu Cheng</a>, <a href="https://openreview.net/profile?id=~Weituo_Hao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weituo_Hao1">Weituo Hao</a>, <a href="https://openreview.net/profile?id=~Siyang_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siyang_Yuan1">Siyang Yuan</a>, <a href="https://openreview.net/profile?id=~Shijing_Si1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shijing_Si1">Shijing Si</a>, <a href="https://openreview.net/profile?id=~Lawrence_Carin2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lawrence_Carin2">Lawrence Carin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#N6JECD-PI5w-details-313" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="N6JECD-PI5w-details-313"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Fairness, Contrastive Learning, Mutual Information, Pretrained Text Encoders</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Pretrained text encoders, such as BERT, have been applied increasingly in various natural language processing (NLP) tasks, and have recently demonstrated significant performance gains. However, recent studies have demonstrated the existence of social bias in these pretrained NLP models. Although prior works have made progress on word-level debiasing, improved sentence-level fairness of pretrained encoders still lacks exploration. In this paper, we proposed the first neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, we introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoders, while continuously showing desirable performance on downstream tasks. Moreover, our post hoc method does not require any retraining of the text encoders, further enlarging FairFil's application space.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A debiasing method for large-scale pretrained text encoders via contrastive learning.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="T1XmO8ScKim" data-number="2769">
      <h4>
        <a href="https://openreview.net/forum?id=T1XmO8ScKim">
            Probabilistic Numeric Convolutional Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=T1XmO8ScKim" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Marc_Anton_Finzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marc_Anton_Finzi1">Marc Anton Finzi</a>, <a href="https://openreview.net/profile?id=~Roberto_Bondesan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roberto_Bondesan1">Roberto Bondesan</a>, <a href="https://openreview.net/profile?id=~Max_Welling1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Max_Welling1">Max Welling</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#T1XmO8ScKim-details-278" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="T1XmO8ScKim-details-278"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">probabilistic numerics, gaussian processes, discretization error, pde, superpixel, irregularly spaced time series, misssing data, spatial uncertainty</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Continuous input signals like images and time series that are irregularly sampled or have missing values are challenging for existing deep learning methods. Coherently defined feature representations must depend on the values in unobserved regions of the input. Drawing from the work in probabilistic numerics, we propose Probabilistic Numeric Convolutional Neural Networks which represent features as Gaussian processes, providing a probabilistic description of discretization error. We then define a convolutional layer as the evolution of a PDE defined on this GP, followed by a nonlinearity. This approach also naturally admits steerable equivariant convolutions under e.g. the rotation group. In experiments we show that our approach yields a <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="28" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn><mo>×</mo></math></mjx-assistive-mml></mjx-container> reduction of error from the previous state of the art on the SuperPixel-MNIST dataset and competitive performance on the medical time series dataset PhysioNet2012.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We build a neural network which integrates internal discretization error and missing values probabilistically with GPs</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=T1XmO8ScKim&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="hkMoYYEkBoI" data-number="3045">
      <h4>
        <a href="https://openreview.net/forum?id=hkMoYYEkBoI">
            Computational Separation Between Convolutional and Fully-Connected Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=hkMoYYEkBoI" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~eran_malach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~eran_malach1">eran malach</a>, <a href="https://openreview.net/profile?id=~Shai_Shalev-Shwartz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shai_Shalev-Shwartz1">Shai Shalev-Shwartz</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#hkMoYYEkBoI-details-37" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hkMoYYEkBoI-details-37"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural Networks, Deep Learning, Convolutional Networks, Fully-Connected Networks, Gradient Descent</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Convolutional neural networks (CNN) exhibit unmatched performance in a multitude of computer vision tasks. However, the advantage of using convolutional networks over fully-connected networks is not understood from a theoretical perspective. In this work, we show how convolutional networks can leverage locality in the data, and thus achieve a computational advantage over fully-connected networks. Specifically, we show a class of problems that can be efficiently solved using convolutional networks trained with gradient-descent, but at the same time is hard to learn using a polynomial-size fully-connected network.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show a computational separation between convolutional and fully-connected networks, proving that the former can leverage strong local structure in the data.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=hkMoYYEkBoI&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="nzpLWnVAyah" data-number="3092">
      <h4>
        <a href="https://openreview.net/forum?id=nzpLWnVAyah">
            On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines
        </a>
      
        
          <a href="https://openreview.net/pdf?id=nzpLWnVAyah" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Marius_Mosbach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marius_Mosbach1">Marius Mosbach</a>, <a href="https://openreview.net/profile?id=~Maksym_Andriushchenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maksym_Andriushchenko1">Maksym Andriushchenko</a>, <a href="https://openreview.net/profile?id=~Dietrich_Klakow1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dietrich_Klakow1">Dietrich Klakow</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#nzpLWnVAyah-details-750" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="nzpLWnVAyah-details-750"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">fine-tuning stability, transfer learning, pretrained language model, BERT</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Fine-tuning pre-trained transformer-based language models such as BERT has become a common practice dominating leaderboards across various NLP benchmarks. Despite the strong empirical performance of fine-tuned models, fine-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Previous literature (Devlin et al., 2019; Lee et al., 2020; Dodge et al., 2020) identified two potential reasons for the observed instability: catastrophic forgetting and small size of the fine-tuning datasets. In this paper, we show that both hypotheses fail to explain the fine-tuning instability. We analyze BERT, RoBERTa, and ALBERT, fine-tuned on commonly used datasets from the GLUE benchmark, and show that the observed instability is caused by optimization difficulties that lead to vanishing gradients. Additionally, we show that the remaining variance of the downstream task performance can be attributed to differences in generalization where fine-tuned models with the same training loss exhibit noticeably different test performance. Based on our analysis, we present a simple but strong baseline that makes fine-tuning BERT-based models significantly more stable than the previously proposed approaches. Code to reproduce our results is available online: https://github.com/uds-lsv/bert-stable-fine-tuning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We provide an analysis of the fine-tuning instability of BERT-based models and present a simple method to fix it.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="kvhzKz-_DMF" data-number="3366">
      <h4>
        <a href="https://openreview.net/forum?id=kvhzKz-_DMF">
            Variational Information Bottleneck for Effective Low-Resource Fine-Tuning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=kvhzKz-_DMF" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Rabeeh_Karimi_mahabadi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rabeeh_Karimi_mahabadi2">Rabeeh Karimi mahabadi</a>, <a href="https://openreview.net/profile?id=~Yonatan_Belinkov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yonatan_Belinkov1">Yonatan Belinkov</a>, <a href="https://openreview.net/profile?id=~James_Henderson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_Henderson1">James Henderson</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#kvhzKz-_DMF-details-378" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kvhzKz-_DMF-details-378"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Transfer learning, NLP, large-scale pre-trained language models, over-fitting, robust, biases, variational information bottleneck</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task.  We propose to use Variational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting.  Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks.  Our code is publicly available in https://github.com/rabeehk/vibert.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose to use Variational Information Bottleneck  to suppress irrelevant features for an effective fine-tuning of large-scale language models in low-resource scenarios. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=kvhzKz-_DMF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="01olnfLIbD" data-number="3659">
      <h4>
        <a href="https://openreview.net/forum?id=01olnfLIbD">
            Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching
        </a>
      
        
          <a href="https://openreview.net/pdf?id=01olnfLIbD" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jonas_Geiping1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonas_Geiping1">Jonas Geiping</a>, <a href="https://openreview.net/profile?id=~Liam_H_Fowl1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liam_H_Fowl1">Liam H Fowl</a>, <a href="https://openreview.net/profile?id=~W._Ronny_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~W._Ronny_Huang1">W. Ronny Huang</a>, <a href="https://openreview.net/profile?id=~Wojciech_Czaja1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wojciech_Czaja1">Wojciech Czaja</a>, <a href="https://openreview.net/profile?id=~Gavin_Taylor1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gavin_Taylor1">Gavin Taylor</a>, <a href="https://openreview.net/profile?id=~Michael_Moeller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Moeller1">Michael Moeller</a>, <a href="https://openreview.net/profile?id=~Tom_Goldstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tom_Goldstein1">Tom Goldstein</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#01olnfLIbD-details-904" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="01olnfLIbD-details-904"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Data Poisoning, ImageNet, Large-scale, Gradient Alignment, Security, Backdoor Attacks, from-scratch, clean-label</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Data Poisoning attacks modify training data to maliciously control a model trained on such data.
      In this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a
      particularly malicious poisoning attack that is both ``from scratch" and ``clean label", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. 
      Previous poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.
      The central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.
      Finally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=01olnfLIbD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="XPZIaotutsD" data-number="3690">
      <h4>
        <a href="https://openreview.net/forum?id=XPZIaotutsD">
            DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION
        </a>
      
        
          <a href="https://openreview.net/pdf?id=XPZIaotutsD" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Pengcheng_He2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pengcheng_He2">Pengcheng He</a>, <a href="https://openreview.net/profile?id=~Xiaodong_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaodong_Liu1">Xiaodong Liu</a>, <a href="https://openreview.net/profile?id=~Jianfeng_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianfeng_Gao1">Jianfeng Gao</a>, <a href="https://openreview.net/profile?id=~Weizhu_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weizhu_Chen1">Weizhu Chen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#XPZIaotutsD-details-673" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XPZIaotutsD-details-673"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Transformer, Attention, Natural Language Processing, Language Model Pre-training, Position Encoding</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models’ 
       generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understand(NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, outperforming the human baseline by a decent margin (90.3 versus
      89.8). The pre-trained DeBERTa models and the source code were released at: https://github.com/microsoft/DeBERTa.
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A new model architecture DeBERTa is proposed that improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=XPZIaotutsD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="CBmJwzneppz" data-number="3820">
      <h4>
        <a href="https://openreview.net/forum?id=CBmJwzneppz">
            Optimism in Reinforcement Learning with Generalized Linear Function Approximation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=CBmJwzneppz" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yining_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yining_Wang1">Yining Wang</a>, <a href="https://openreview.net/profile?id=~Ruosong_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruosong_Wang1">Ruosong Wang</a>, <a href="https://openreview.net/profile?id=~Simon_Shaolei_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Shaolei_Du1">Simon Shaolei Du</a>, <a href="https://openreview.net/profile?id=~Akshay_Krishnamurthy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Akshay_Krishnamurthy1">Akshay Krishnamurthy</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#CBmJwzneppz-details-511" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CBmJwzneppz-details-511"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, optimism, exploration, function approximation, theory, regret analysis, provable sample efficiency</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We design a new provably efficient algorithm for episodic reinforcement learning with generalized linear function approximation. We analyze the algorithm under a new expressivity assumption that we call ``optimistic closure,'' which is strictly weaker than assumptions from prior analyses for the linear setting. With optimistic closure, we prove that our algorithm enjoys a regret bound of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="29" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.06em; padding-left: 0.187em; margin-bottom: -0.597em;"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2DC TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom><mjx-mrow><mjx-mo class="mjx-lop"><mjx-c class="mjx-c28 TEX-S2"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43B TEX-I"></mjx-c></mjx-mi><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.123em;"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.289em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt><mjx-mo class="mjx-lop"><mjx-c class="mjx-c29 TEX-S2"></mjx-c></mjx-mo></mjx-mrow></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mi>O</mi><mo>~</mo></mover></mrow><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi>H</mi><msqrt><msup><mi>d</mi><mn>3</mn></msup><mi>T</mi></msqrt><mo data-mjx-texclass="CLOSE">)</mo></mrow></math></mjx-assistive-mml></mjx-container> where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="30" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>H</mi></math></mjx-assistive-mml></mjx-container> is the horizon, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="31" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></mjx-assistive-mml></mjx-container> is the dimensionality of the state-action features and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="32" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container> is the number of episodes. This is the first statistically and computationally efficient algorithm for reinforcement learning with generalized linear functions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A provably efficient (statistically and computationally) algorithm for reinforcement learning with generalized linear function approximation and no explicit dynamics assumptions.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="6DOZ8XNNfGN" data-number="880">
      <h4>
        <a href="https://openreview.net/forum?id=6DOZ8XNNfGN">
            Graph Traversal with Tensor Functionals: A Meta-Algorithm for Scalable Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=6DOZ8XNNfGN" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Elan_Sopher_Markowitz2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Elan_Sopher_Markowitz2">Elan Sopher Markowitz</a>, <a href="https://openreview.net/profile?email=keshavba%40usc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="keshavba@usc.edu">Keshav Balasubramanian</a>, <a href="https://openreview.net/profile?email=mehrnoom%40usc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mehrnoom@usc.edu">Mehrnoosh Mirtaheri</a>, <a href="https://openreview.net/profile?id=~Sami_Abu-El-Haija1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sami_Abu-El-Haija1">Sami Abu-El-Haija</a>, <a href="https://openreview.net/profile?id=~Bryan_Perozzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bryan_Perozzi1">Bryan Perozzi</a>, <a href="https://openreview.net/profile?id=~Greg_Ver_Steeg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Greg_Ver_Steeg1">Greg Ver Steeg</a>, <a href="https://openreview.net/profile?id=~Aram_Galstyan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aram_Galstyan1">Aram Galstyan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#6DOZ8XNNfGN-details-520" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6DOZ8XNNfGN-details-520"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Graph, Learning, Algorithm, Scale, Message Passing, Node Embeddings</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Graph Representation Learning (GRL) methods have impacted fields from chemistry to social science. However, their algorithmic implementations are specialized to specific use-cases e.g. "message passing" methods are run differently from "node embedding" ones. Despite their apparent differences, all these methods utilize the graph structure,  and therefore, their learning can be approximated with stochastic graph traversals.  We propose Graph Traversal via Tensor Functionals (GTTF), a unifying meta-algorithm framework for easing the implementation of diverse graph algorithms and enabling transparent and efficient scaling to large graphs.  GTTF is founded upon a data structure (stored as a sparse tensor) and a stochastic graph traversal algorithm (described using tensor operations). The algorithm is a functional that accept two functions, and can be specialized to obtain a variety of GRL models and objectives, simply by changing those two functions. We show for a wide class of methods, our algorithm learns in an unbiased fashion and, in expectation, approximates the learning as if the specialized implementations were run directly.
      With these capabilities, we scale otherwise non-scalable methods to set state-of-the-art on large graph datasets while being more efficient than existing GRL libraries -- with only a handful of lines of code for each method specialization.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">GTTF is a meta-algorithm, upon which, many algorithms for graph learning can be implemented, automatically giving them efficiency and scale, yet unbiased learning.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Qm7R_SdqTpT" data-number="1046">
      <h4>
        <a href="https://openreview.net/forum?id=Qm7R_SdqTpT">
            Diverse Video Generation using a Gaussian Process Trigger
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Qm7R_SdqTpT" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Gaurav_Shrivastava1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gaurav_Shrivastava1">Gaurav Shrivastava</a>, <a href="https://openreview.net/profile?id=~Abhinav_Shrivastava2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abhinav_Shrivastava2">Abhinav Shrivastava</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Qm7R_SdqTpT-details-849" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Qm7R_SdqTpT-details-849"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">video synthesis, future frame generation, video generation, gaussian process priors, diverse video generation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Generating future frames given a few context (or past) frames is a challenging task. It requires modeling the temporal coherence of videos as well as multi-modality in terms of diversity in the potential future states. Current variational approaches for video generation tend to marginalize over multi-modal future outcomes. Instead, we propose to explicitly model the multi-modality in the future outcomes and leverage it to sample diverse futures. Our approach, Diverse Video Generator, uses a GP to learn priors on future states given the past and maintains a probability distribution over possible futures given a particular sample. We leverage the changes in this distribution over time to control the sampling of diverse future states by estimating the end of on-going sequences. In particular, we use the variance of GP over the output function space to trigger a change in the action sequence. We achieve state-of-the-art results on diverse future frame generation in terms of reconstruction quality and diversity of the generated sequences.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Diverse future frame synthesis by modeling the diversity of future states using a Gaussian Process, and using Bayesian inference to sample diverse future states.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Qm7R_SdqTpT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="lqU2cs3Zca" data-number="2220">
      <h4>
        <a href="https://openreview.net/forum?id=lqU2cs3Zca">
            Signatory: differentiable computations of the signature and logsignature transforms, on both CPU and GPU
        </a>
      
        
          <a href="https://openreview.net/pdf?id=lqU2cs3Zca" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Patrick_Kidger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Patrick_Kidger1">Patrick Kidger</a>, <a href="https://openreview.net/profile?email=tlyons%40maths.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="tlyons@maths.ox.ac.uk">Terry Lyons</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 06 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#lqU2cs3Zca-details-878" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lqU2cs3Zca-details-878"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">signature, logsignature, gpu, library, open source</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Signatory is a library for calculating and performing functionality related to the signature and logsignature transforms. The focus is on machine learning, and as such includes features such as CPU parallelism, GPU support, and backpropagation. To our knowledge it is the first GPU-capable library for these operations. Signatory implements new features not available in previous libraries, such as efficient precomputation strategies. Furthermore, several novel algorithmic improvements are introduced, producing substantial real-world speedups even on the CPU without parallelism. The library operates as a Python wrapper around C++, and is compatible with the PyTorch ecosystem. It may be installed directly via \texttt{pip}. Source code, documentation, examples, benchmarks and tests may be found at \texttt{\url{https://github.com/patrick-kidger/signatory}}. The license is Apache-2.0.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Differentiable, GPU-capable implementations of the (log)signature transform via novel algorithms.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=lqU2cs3Zca&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="0-EYBhgw80y" data-number="322">
      <h4>
        <a href="https://openreview.net/forum?id=0-EYBhgw80y">
            MoPro: Webly Supervised Learning with Momentum Prototypes
        </a>
      
        
          <a href="https://openreview.net/pdf?id=0-EYBhgw80y" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Junnan_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junnan_Li2">Junnan Li</a>, <a href="https://openreview.net/profile?id=~Caiming_Xiong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Caiming_Xiong1">Caiming Xiong</a>, <a href="https://openreview.net/profile?id=~Steven_Hoi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_Hoi2">Steven Hoi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#0-EYBhgw80y-details-334" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0-EYBhgw80y-details-334"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">webly-supervised learning, weakly-supervised learning, contrastive learning, representation learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a webly-supervised representation learning method that does not suffer from the annotation unscalability of supervised learning, nor the computation unscalability of self-supervised learning. Most existing works on webly-supervised representation learning adopt a vanilla supervised learning method without accounting for the prevalent noise in the training data, whereas most prior methods in learning with label noise are less effective for real-world large-scale noisy data. We propose momentum prototypes (MoPro), a simple contrastive learning method that achieves online label noise correction, out-of-distribution sample removal, and representation learning. MoPro achieves state-of-the-art performance on WebVision, a weakly-labeled noisy dataset. MoPro also shows superior performance when the pretrained model is transferred to down-stream image classification and detection tasks. It outperforms the ImageNet supervised pretrained model by +10.5 on 1-shot classification on VOC, and outperforms the best self-supervised pretrained model by +17.3 when finetuned on 1% of ImageNet labeled samples. Furthermore, MoPro is more robust to distribution shifts. Code and pretrained models are available at https://github.com/salesforce/MoPro.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">MoPro is a new webly-supervised learning framework which advances representation learning using freely-available Web images.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=0-EYBhgw80y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="04cII6MumYV" data-number="365">
      <h4>
        <a href="https://openreview.net/forum?id=04cII6MumYV">
            A Universal Representation Transformer Layer for Few-Shot Image Classification
        </a>
      
        
          <a href="https://openreview.net/pdf?id=04cII6MumYV" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Lu_Liu7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lu_Liu7">Lu Liu</a>, <a href="https://openreview.net/profile?id=~William_L._Hamilton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_L._Hamilton1">William L. Hamilton</a>, <a href="https://openreview.net/profile?id=~Guodong_Long2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guodong_Long2">Guodong Long</a>, <a href="https://openreview.net/profile?id=~Jing_Jiang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jing_Jiang6">Jing Jiang</a>, <a href="https://openreview.net/profile?id=~Hugo_Larochelle1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hugo_Larochelle1">Hugo Larochelle</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#04cII6MumYV-details-739" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="04cII6MumYV-details-739"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Few-shot classification aims to recognize unseen classes when presented with only a small number of samples. We consider the problem of multi-domain few-shot image classification, where unseen classes and examples come from diverse data sources. This problem has seen growing interest and has inspired the development of benchmarks such as Meta-Dataset. A key challenge in this multi-domain setting is to effectively integrate the feature representations from the diverse set of training domains. Here, we propose a Universal Representation Transformer (URT) layer, that meta-learns to leverage universal features for few-shot classification by dynamically re-weighting and composing the most appropriate domain-specific representations. In experiments, we show that URT sets a new state-of-the-art result on Meta-Dataset. Specifically, it achieves top-performance on the highest number of data sources compared to competing methods. We analyze variants of URT and present a visualization of the attention score heatmaps that sheds light on how the model performs cross-domain generalization.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=04cII6MumYV&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">code at: https://github.com/liulu112601/URT</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="TtYSU29zgR" data-number="1564">
      <h4>
        <a href="https://openreview.net/forum?id=TtYSU29zgR">
            Primal Wasserstein Imitation Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=TtYSU29zgR" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Robert_Dadashi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Robert_Dadashi2">Robert Dadashi</a>, <a href="https://openreview.net/profile?id=~Leonard_Hussenot1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leonard_Hussenot1">Leonard Hussenot</a>, <a href="https://openreview.net/profile?id=~Matthieu_Geist1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthieu_Geist1">Matthieu Geist</a>, <a href="https://openreview.net/profile?id=~Olivier_Pietquin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Olivier_Pietquin1">Olivier Pietquin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#TtYSU29zgR-details-108" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TtYSU29zgR-details-108"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement Learning, Inverse Reinforcement Learning, Imitation Learning, Optimal Transport, Wasserstein distance</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Imitation Learning (IL) methods seek to match the behavior of an agent with that of an expert. In the present work, we propose a new IL method based on a conceptually simple algorithm: Primal Wasserstein Imitation Learning (PWIL), which ties to the primal form of the Wasserstein distance between the expert and the agent state-action distributions. We present a reward function which is derived offline, as opposed to recent adversarial IL algorithms that learn a reward function through interactions with the environment, and which requires little fine-tuning. We show that we can recover expert behavior on a variety of continuous control tasks of the MuJoCo domain in a sample efficient manner in terms of agent interactions and of expert interactions with the environment. Finally, we show that the behavior of the agent we train matches the behavior of the expert with the Wasserstein distance, rather than the commonly used proxy of performance.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A new Imitation Learning method based on optimal transport.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=TtYSU29zgR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="MIDckA56aD" data-number="859">
      <h4>
        <a href="https://openreview.net/forum?id=MIDckA56aD">
            Learning perturbation sets for robust machine learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=MIDckA56aD" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Eric_Wong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eric_Wong1">Eric Wong</a>, <a href="https://openreview.net/profile?id=~J_Zico_Kolter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~J_Zico_Kolter1">J Zico Kolter</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#MIDckA56aD-details-504" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MIDckA56aD-details-504"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">adversarial examples, perturbation sets, robust machine learning, conditional variational autoencoder</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. In this paper, we aim to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, we use a conditional generator that defines the perturbation set over a constrained region of the latent space. We formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, our approach can generate a variety of perturbations at different complexities and scales, ranging from baseline spatial transformations, through common image corruptions, to lighting variations. We measure the quality of our learned perturbation sets both quantitatively and qualitatively, finding that our models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, we leverage our learned perturbation sets to train models which are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations, while improving generalization on non-adversarial data. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found at https://github.com/locuslab/perturbation_learning. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We learn to characterize real-world changes in well-defined perturbation sets, which allow us train models which are empirically and certifiably robust to real-world adversarial changes. </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=MIDckA56aD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="XI-OJ5yyse" data-number="1903">
      <h4>
        <a href="https://openreview.net/forum?id=XI-OJ5yyse">
            CopulaGNN: Towards Integrating Representational and Correlational Roles of Graphs in Graph Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=XI-OJ5yyse" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jiaqi_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiaqi_Ma1">Jiaqi Ma</a>, <a href="https://openreview.net/profile?id=~Bo_Chang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Chang1">Bo Chang</a>, <a href="https://openreview.net/profile?id=~Xuefei_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuefei_Zhang1">Xuefei Zhang</a>, <a href="https://openreview.net/profile?id=~Qiaozhu_Mei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qiaozhu_Mei1">Qiaozhu Mei</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#XI-OJ5yyse-details-924" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XI-OJ5yyse-details-924"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Graph Neural Network, Gaussian Copula, Gaussian Graphical Model</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Graph-structured data are ubiquitous. However, graphs encode diverse types of information and thus play different roles in data representation. In this paper, we distinguish the \textit{representational} and the \textit{correlational} roles played by the graphs in node-level prediction tasks, and we investigate how Graph Neural Network (GNN) models can effectively leverage both types of information. Conceptually, the representational information provides guidance for the model to construct better node features; while the correlational information indicates the correlation between node outcomes conditional on node features. Through a simulation study, we find that many popular GNN models are incapable of effectively utilizing the correlational information. By leveraging the idea of the copula, a principled way to describe the dependence among multivariate random variables, we offer a general solution. The proposed Copula Graph Neural Network (CopulaGNN) can take a wide range of GNN models as base models and utilize both representational and correlational information stored in the graphs. Experimental results on two types of regression tasks verify the effectiveness of the proposed method.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We distinguish the representational and the correlational information encoded by the graphs in node-level prediction tasks, and propose a novel Copula Graph Neural Network to effectively leverage both information.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="8Ln-Bq0mZcy" data-number="1315">
      <h4>
        <a href="https://openreview.net/forum?id=8Ln-Bq0mZcy">
            On the Critical Role of Conventions in Adaptive Human-AI Collaboration
        </a>
      
        
          <a href="https://openreview.net/pdf?id=8Ln-Bq0mZcy" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Andy_Shih1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andy_Shih1">Andy Shih</a>, <a href="https://openreview.net/profile?id=~Arjun_Sawhney1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arjun_Sawhney1">Arjun Sawhney</a>, <a href="https://openreview.net/profile?id=~Jovana_Kondic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jovana_Kondic1">Jovana Kondic</a>, <a href="https://openreview.net/profile?id=~Stefano_Ermon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefano_Ermon1">Stefano Ermon</a>, <a href="https://openreview.net/profile?id=~Dorsa_Sadigh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dorsa_Sadigh1">Dorsa Sadigh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#8Ln-Bq0mZcy-details-897" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8Ln-Bq0mZcy-details-897"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multi-agent games, emergent behavior, transfer learning, human-AI collaboration</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Humans can quickly adapt to new partners in collaborative tasks (e.g. playing basketball), because they understand which fundamental skills of the task (e.g. how to dribble, how to shoot) carry over across new partners. Humans can also quickly adapt to similar tasks with the same partners by carrying over conventions that they have developed (e.g. raising hand signals pass the ball), without learning to coordinate from scratch. To collaborate seamlessly with humans, AI agents should adapt quickly to new partners and new tasks as well. However, current approaches have not attempted to distinguish between the complexities intrinsic to a task and the conventions used by a partner, and more generally there has been little focus on leveraging conventions for adapting to new settings. In this work, we propose a learning framework that teases apart rule-dependent representation from convention-dependent representation in a principled way. We show that, under some assumptions, our rule-dependent representation is a sufficient statistic of the distribution over best-response strategies across partners. Using this separation of representations, our agents are able to adapt quickly to new partners, and to coordinate with old partners on new tasks in a zero-shot manner. We experimentally validate our approach on three collaborative tasks varying in complexity: a contextual multi-armed bandit, a block placing task, and the card game Hanabi.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Training agents that can adapt to new settings in multi-agent games.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=8Ln-Bq0mZcy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="i80OPhOCVH2" data-number="1144">
      <h4>
        <a href="https://openreview.net/forum?id=i80OPhOCVH2">
            On the Bottleneck of Graph Neural Networks and its Practical Implications
        </a>
      
        
          <a href="https://openreview.net/pdf?id=i80OPhOCVH2" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Uri_Alon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Uri_Alon1">Uri Alon</a>, <a href="https://openreview.net/profile?id=~Eran_Yahav1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eran_Yahav1">Eran Yahav</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#i80OPhOCVH2-details-736" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="i80OPhOCVH2-details-736"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">graphs, GNNs, limitations, understanding, bottleneck, over-squashing</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008), one of the major problems in training GNNs was their struggle to propagate information between distant nodes in the graph.
      We propose a new explanation for this problem: GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially growing information into fixed-size vectors.
      As a result, GNNs fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction.
      In this paper, we highlight the inherent problem of over-squashing in GNNs:
      we demonstrate that the bottleneck hinders popular GNNs from fitting long-range signals in the training data;
      we further show that GNNs that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and GGNN;
      finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffers from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights. 
      Our code is available at https://github.com/tech-srl/bottleneck/ .</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A novel observation on GNN limitations: in long-range problems, a computational bottleneck causes over-squashing of information.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="42kiJ7n_8xO" data-number="2285">
      <h4>
        <a href="https://openreview.net/forum?id=42kiJ7n_8xO">
            The geometry of integration in text classification RNNs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=42kiJ7n_8xO" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kyle_Aitken1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyle_Aitken1">Kyle Aitken</a>, <a href="https://openreview.net/profile?id=~Vinay_Venkatesh_Ramasesh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vinay_Venkatesh_Ramasesh1">Vinay Venkatesh Ramasesh</a>, <a href="https://openreview.net/profile?id=~Ankush_Garg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ankush_Garg1">Ankush Garg</a>, <a href="https://openreview.net/profile?id=~Yuan_Cao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuan_Cao2">Yuan Cao</a>, <a href="https://openreview.net/profile?id=~David_Sussillo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Sussillo1">David Sussillo</a>, <a href="https://openreview.net/profile?id=~Niru_Maheswaranathan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Niru_Maheswaranathan1">Niru Maheswaranathan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#42kiJ7n_8xO-details-439" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="42kiJ7n_8xO-details-439"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Recurrent neural networks, dynamical systems, interpretability, document classification, reverse engineering</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Despite the widespread application of recurrent neural networks (RNNs), a unified understanding of how RNNs solve particular tasks remains elusive.  In particular, it is unclear what dynamical patterns arise in trained RNNs, and how those pat-terns depend on the training dataset or task.  This work addresses these questions in the context of text classification, building on earlier work studying the dynamics of binary sentiment-classification networks (Maheswaranathan et al., 2019).  We study text-classification tasks beyond the binary case, exploring the dynamics ofRNNs trained on both natural and synthetic datasets.  These dynamics, which we find to be both interpretable and low-dimensional, share a common mechanism across architectures and datasets:  specifically, these text-classification networks use low-dimensional attractor manifolds to accumulate evidence for each class as they process the text.  The dimensionality and geometry of the attractor manifold are determined by the structure of the training dataset, with the dimensionality reflecting the number of scalar quantities the network remembers in order to classify.In categorical classification, for example, we show that this dimensionality is one less than the number of classes. Correlations in the dataset, such as those induced by ordering, can further reduce the dimensionality of the attractor manifold; we show how to predict this reduction using simple word-count statistics computed on the training dataset. To the degree that integration of evidence towards a decision is a common computational primitive, this work continues to lay the foundation for using dynamical systems techniques to study the inner workings of RNNs.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We study text classification RNNs using tools from dynamical systems analysis, finding and explaining the geometry of low-dimensional attractor manifolds.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=42kiJ7n_8xO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="jh-rTtvkGeM" data-number="1902">
      <h4>
        <a href="https://openreview.net/forum?id=jh-rTtvkGeM">
            Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability
        </a>
      
        
          <a href="https://openreview.net/pdf?id=jh-rTtvkGeM" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jeremy_Cohen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jeremy_Cohen1">Jeremy Cohen</a>, <a href="https://openreview.net/profile?email=skaur%40andrew.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="skaur@andrew.cmu.edu">Simran Kaur</a>, <a href="https://openreview.net/profile?id=~Yuanzhi_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuanzhi_Li1">Yuanzhi Li</a>, <a href="https://openreview.net/profile?id=~J_Zico_Kolter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~J_Zico_Kolter1">J Zico Kolter</a>, <a href="https://openreview.net/profile?id=~Ameet_Talwalkar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ameet_Talwalkar1">Ameet Talwalkar</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>33 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#jh-rTtvkGeM-details-304" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jh-rTtvkGeM-details-304"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">optimization, trajectory, stability, sharpness, implicit bias, implicit regularization, L-smoothness, deep learning theory, science of deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum eigenvalue of the training loss Hessian hovers just above the value <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="33" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mtext class="mjx-n"><mjx-c class="mjx-c28"></mjx-c><mjx-c class="mjx-c73"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c70"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c73"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c7A"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c29"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2</mn><mrow><mo>/</mo></mrow><mtext>(step size)</mtext></math></mjx-assistive-mml></mjx-container>, and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training. We hope that our findings will inspire future efforts aimed at rigorously understanding optimization at the Edge of Stability.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We trained neural networks using full-batch gradient descent -- you won't believe what happens next!</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="SK7A5pdrgov" data-number="3586">
      <h4>
        <a href="https://openreview.net/forum?id=SK7A5pdrgov">
            CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=SK7A5pdrgov" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ossama_Ahmed1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ossama_Ahmed1">Ossama Ahmed</a>, <a href="https://openreview.net/profile?id=~Frederik_Tr%C3%A4uble1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frederik_Träuble1">Frederik Träuble</a>, <a href="https://openreview.net/profile?id=~Anirudh_Goyal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anirudh_Goyal1">Anirudh Goyal</a>, <a href="https://openreview.net/profile?id=~Alexander_Neitz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Neitz1">Alexander Neitz</a>, <a href="https://openreview.net/profile?id=~Manuel_Wuthrich1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manuel_Wuthrich1">Manuel Wuthrich</a>, <a href="https://openreview.net/profile?id=~Yoshua_Bengio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoshua_Bengio1">Yoshua Bengio</a>, <a href="https://openreview.net/profile?id=~Bernhard_Sch%C3%B6lkopf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernhard_Schölkopf1">Bernhard Schölkopf</a>, <a href="https://openreview.net/profile?id=~Stefan_Bauer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefan_Bauer1">Stefan Bauer</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#SK7A5pdrgov-details-741" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SK7A5pdrgov-details-741"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, transfer learning, sim2real transfer, domain adaptation, causality, generalization, robotics</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Despite recent successes of reinforcement learning (RL), it remains a challenge for agents to transfer learned skills to related environments. To facilitate research addressing this problem, we proposeCausalWorld, a benchmark for causal structure and transfer learning in a robotic manipulation environment. The environment is a simulation of an open-source robotic platform, hence offering the possibility of sim-to-real transfer. Tasks consist of constructing 3D shapes from a set of blocks - inspired by how children learn to build complex structures. The key strength of CausalWorld is that it provides a combinatorial family of such tasks with common causal structure and underlying factors (including, e.g., robot and object masses, colors, sizes). The user (or the agent) may intervene on all causal variables, which allows  for  fine-grained  control  over  how  similar  different  tasks  (or  task  distributions) are. One can thus easily define training and evaluation distributions of a desired difficulty level,  targeting a specific form of generalization (e.g.,  only changes in appearance or object mass). Further, this common parametrization facilitates defining curricula by interpolating between an initial and a target task. While users may define their own task distributions, we present eight meaningful distributions as concrete benchmarks, ranging from simple to very challenging, all of which require long-horizon planning as well as precise low-level motor control. Finally, we provide baseline results for a subset of these tasks on distinct training curricula and corresponding evaluation protocols, verifying the feasibility of the tasks in this benchmark.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A benchmark to address the challenge of agents transferring their learned skills to related environments; primarily for causal structure and transfer learning.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="jrA5GAccy_" data-number="1481">
      <h4>
        <a href="https://openreview.net/forum?id=jrA5GAccy_">
            Empirical or Invariant Risk Minimization? A Sample Complexity Perspective
        </a>
      
        
          <a href="https://openreview.net/pdf?id=jrA5GAccy_" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kartik_Ahuja1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kartik_Ahuja1">Kartik Ahuja</a>, <a href="https://openreview.net/profile?email=wangsidi76%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="wangsidi76@gmail.com">Jun Wang</a>, <a href="https://openreview.net/profile?id=~Amit_Dhurandhar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amit_Dhurandhar1">Amit Dhurandhar</a>, <a href="https://openreview.net/profile?id=~Karthikeyan_Shanmugam1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Karthikeyan_Shanmugam1">Karthikeyan Shanmugam</a>, <a href="https://openreview.net/profile?id=~Kush_R._Varshney1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kush_R._Varshney1">Kush R. Varshney</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#jrA5GAccy_-details-750" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jrA5GAccy_-details-750"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">invariant risk minimization, IRM</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recently, invariant risk minimization (IRM) was proposed as a promising solution to address out-of-distribution (OOD) generalization. However, it is unclear when IRM should be preferred over the widely-employed empirical risk minimization (ERM) framework. In this work, we analyze both these frameworks from the perspective of sample complexity, thus taking a firm step towards answering this important question. We find that depending on the type of data generation mechanism, the two approaches might have very different finite sample and asymptotic behavior. For example, in the covariate shift setting we see that the two approaches not only arrive at the same asymptotic solution, but also have similar finite sample behavior with no clear winner. For other distribution shifts such as those involving confounders or anti-causal variables, however, the two approaches arrive at different asymptotic solutions where IRM is guaranteed to be close to the desired OOD solutions in the finite sample regime, while ERM is biased even asymptotically.  We further investigate how different factors --- the number of environments, complexity of the model, and IRM penalty weight ---  impact the sample complexity of IRM in relation to its distance from the OOD solutions. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">In this work, we provide a sample complexity comparison of the recent invariant risk minimization (IRM) framework with the classic empirical risk minimization (ERM) to answer when is IRM better than ERM in terms of out-of-distribution generalization?</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=jrA5GAccy_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="V5j-jdoDDP" data-number="1272">
      <h4>
        <a href="https://openreview.net/forum?id=V5j-jdoDDP">
            Scaling Symbolic Methods using Gradients for Neural Model Explanation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=V5j-jdoDDP" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=subhamsahoo%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="subhamsahoo@google.com">Subham Sekhar Sahoo</a>, <a href="https://openreview.net/profile?id=~Subhashini_Venugopalan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Subhashini_Venugopalan2">Subhashini Venugopalan</a>, <a href="https://openreview.net/profile?id=~Li_Li8" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Li_Li8">Li Li</a>, <a href="https://openreview.net/profile?id=~Rishabh_Singh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rishabh_Singh1">Rishabh Singh</a>, <a href="https://openreview.net/profile?id=~Patrick_Riley2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Patrick_Riley2">Patrick Riley</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#V5j-jdoDDP-details-719" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="V5j-jdoDDP-details-719"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural Model Explanation, SMT Solvers, Symbolic Methods</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Symbolic techniques based on Satisfiability Modulo Theory (SMT) solvers have been proposed for analyzing and verifying neural network properties, but their usage has been fairly limited owing to their poor scalability with larger networks. In this work, we propose a technique for combining gradient-based methods with symbolic techniques to scale such analyses and demonstrate its application for model explanation. In particular, we apply this technique to identify minimal regions in an input that are most relevant for a neural network's prediction. Our approach uses gradient information (based on Integrated Gradients) to focus on a subset of neurons in the first layer, which allows our technique to scale to large networks. The corresponding SMT constraints encode the minimal input mask discovery problem such that after masking the input, the activations of the selected neurons are still above a threshold. After solving for the minimal masks, our approach scores the mask regions to generate a relative ordering of the features within the mask. This produces a saliency map which explains" where a model is looking" when making a prediction. We evaluate our technique on three datasets-MNIST, ImageNet, and Beer Reviews, and demonstrate both quantitatively and qualitatively that the regions generated by our approach are sparser and achieve higher saliency scores compared to the gradient-based methods alone. Code and examples are at - https://github.com/google-research/google-research/tree/master/smug_saliency</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="dgd4EJqsbW5" data-number="2381">
      <h4>
        <a href="https://openreview.net/forum?id=dgd4EJqsbW5">
            Control-Aware Representations for Model-based Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=dgd4EJqsbW5" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=bcui%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="bcui@fb.com">Brandon Cui</a>, <a href="https://openreview.net/profile?id=~Yinlam_Chow1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yinlam_Chow1">Yinlam Chow</a>, <a href="https://openreview.net/profile?id=~Mohammad_Ghavamzadeh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammad_Ghavamzadeh2">Mohammad Ghavamzadeh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#dgd4EJqsbW5-details-868" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dgd4EJqsbW5-details-868"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A major challenge in modern reinforcement learning (RL) is efficient control of dynamical systems from high-dimensional sensory observations.   Learning controllable embedding (LCE) is a promising approach that addresses this challenge by embedding the observations into a lower-dimensional latent space, estimating the latent dynamics, and utilizing it to perform control in the latent space.  Two important questions in this area are how to learn a representation that is amenable to the control problem at hand, and how to achieve an end-to-end framework for representation learning and control.  In this paper, we take a few steps towards addressing these questions. We first formulate a LCE model to learn representations that are suitable to be used by a policy iteration style algorithm in the latent space.We call this model control-aware representation learning(CARL). We derive a loss function and three implementations for CARL. In the offline implementation, we replace the locally-linear control algorithm (e.g., iLQR) used by the existing LCE methods with a RL algorithm, namely model-based soft actor-critic, and show that it results in significant improvement. In online CARL, we interleave representation learning and control, and demonstrate further gain in performance.  Finally, we propose value-guided CARL, a variation in which we optimize a weighted version of the CARL loss function, where the weights depend on the TD-error of the current policy. We evaluate the proposed algorithms by extensive experiments on benchmark tasks and compare them with several LCE baselines.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=dgd4EJqsbW5&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tc5qisoB-C" data-number="1334">
      <h4>
        <a href="https://openreview.net/forum?id=tc5qisoB-C">
            C-Learning: Learning to Achieve Goals via Recursive Classification
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tc5qisoB-C" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Benjamin_Eysenbach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Eysenbach1">Benjamin Eysenbach</a>, <a href="https://openreview.net/profile?id=~Ruslan_Salakhutdinov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruslan_Salakhutdinov1">Ruslan Salakhutdinov</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tc5qisoB-C-details-902" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tc5qisoB-C-details-902"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, goal reaching, density estimation, Q-learning, hindsight relabeling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned reinforcement learning (RL), is centered around learning a conditional probability density function over future states. Instead of directly estimating this density function, we indirectly estimate this density function by training a classifier to predict whether an observation comes from the future. Via Bayes' rule, predictions from our classifier can be transformed into predictions over future states. Importantly, an off-policy variant of our algorithm allows us to predict the future state distribution of a new policy, without collecting new experience. This variant allows us to optimize functionals of a policy's future state distribution, such as the density of reaching a particular goal state. While conceptually similar to Q-learning, our work lays a principled foundation for goal-conditioned RL as density estimation, providing justification for goal-conditioned methods used in prior work. This foundation makes hypotheses about Q-learning, including the optimal goal-sampling ratio, which we confirm experimentally. Moreover, our proposed method is competitive with prior goal-conditioned RL methods.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We reframe the goal-conditioned RL problem as one of predicting and controlling the future state of the world, and derive a principled algorithm to solve this problem. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="guetrIHLFGI" data-number="2172">
      <h4>
        <a href="https://openreview.net/forum?id=guetrIHLFGI">
            The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers
        </a>
      
        
          <a href="https://openreview.net/pdf?id=guetrIHLFGI" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Preetum_Nakkiran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Preetum_Nakkiran1">Preetum Nakkiran</a>, <a href="https://openreview.net/profile?id=~Behnam_Neyshabur1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Behnam_Neyshabur1">Behnam Neyshabur</a>, <a href="https://openreview.net/profile?id=~Hanie_Sedghi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hanie_Sedghi1">Hanie Sedghi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#guetrIHLFGI-details-628" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="guetrIHLFGI-details-628"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">generalization, optimization, online learning, understanding deep learning, empirical investigation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a new framework for reasoning about generalization in deep learning. 
      The core idea is to couple the Real World, where optimizers take stochastic gradient steps on the empirical loss, to an Ideal World, where optimizers take steps on the population loss. This leads to an alternate decomposition of test error into: (1) the Ideal World test error plus (2) the gap between the two worlds. If the gap (2) is universally small, this reduces the problem of generalization in offline learning to the problem of optimization in online learning.
      We then give empirical evidence that this gap between worlds can be small in realistic deep learning settings, in particular supervised image classification. For example, CNNs generalize better than MLPs on image distributions in the Real World, but this is "because" they optimize faster on the population loss in the Ideal World. This suggests our framework is a useful tool for understanding generalization in deep learning, and lays the foundation for future research in this direction. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show empirical evidence that the performance gap between offline generalization and online optimization is small and propose an alternative framework for studying generalization.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-Hs_otp2RB" data-number="3195">
      <h4>
        <a href="https://openreview.net/forum?id=-Hs_otp2RB">
            Improving VAEs' Robustness to Adversarial Attack
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-Hs_otp2RB" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Matthew_JF_Willetts1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthew_JF_Willetts1">Matthew JF Willetts</a>, <a href="https://openreview.net/profile?email=acamuto%40turing.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="acamuto@turing.ac.uk">Alexander Camuto</a>, <a href="https://openreview.net/profile?id=~Tom_Rainforth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tom_Rainforth1">Tom Rainforth</a>, <a href="https://openreview.net/profile?id=~S_Roberts1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~S_Roberts1">S Roberts</a>, <a href="https://openreview.net/profile?email=cholmes%40stats.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="cholmes@stats.ox.ac.uk">Christopher C Holmes</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 21 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-Hs_otp2RB-details-339" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-Hs_otp2RB-details-339"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep generative models, variational autoencoders, robustness, adversarial attack</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Variational autoencoders (VAEs) have recently been shown to be vulnerable to adversarial attacks, wherein they are fooled into reconstructing a chosen target image. However, how to defend against such attacks remains an open problem. We make significant advances in addressing this issue by introducing methods for producing adversarially robust VAEs. Namely, we first demonstrate that methods proposed to obtain disentangled latent representations produce VAEs that are more robust to these attacks. However, this robustness comes at the cost of reducing the quality of the reconstructions. We ameliorate this by applying disentangling methods to hierarchical VAEs. The resulting models produce high--fidelity autoencoders that are also adversarially robust. We confirm their capabilities on several different datasets and with current state-of-the-art VAE adversarial attacks, and also show that they increase the robustness of downstream tasks to attack.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that regularisation methods first developed to obtain 'disentangled' VAEs increase the robustness of VAEs to adversarial attack; leveraging this insight we propose an even-more-robust hierarchical VAE.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=-Hs_otp2RB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Qm8UNVCFdh" data-number="894">
      <h4>
        <a href="https://openreview.net/forum?id=Qm8UNVCFdh">
            What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Qm8UNVCFdh" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kiana_Ehsani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kiana_Ehsani1">Kiana Ehsani</a>, <a href="https://openreview.net/profile?id=~Daniel_Gordon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Gordon1">Daniel Gordon</a>, <a href="https://openreview.net/profile?id=~Thomas_Hai_Dang_Nguyen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Hai_Dang_Nguyen1">Thomas Hai Dang Nguyen</a>, <a href="https://openreview.net/profile?id=~Roozbeh_Mottaghi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roozbeh_Mottaghi1">Roozbeh Mottaghi</a>, <a href="https://openreview.net/profile?id=~Ali_Farhadi3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ali_Farhadi3">Ali Farhadi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 07 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Qm8UNVCFdh-details-666" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Qm8UNVCFdh-details-666"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">representation learning, computer vision</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ``"muscly-supervised" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We learn a muscly-supervised visual representation from human's interactions with the visual world.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Qm8UNVCFdh&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="lWaz5a9lcFU" data-number="67">
      <h4>
        <a href="https://openreview.net/forum?id=lWaz5a9lcFU">
            EEC: Learning to Encode and Regenerate Images for Continual Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=lWaz5a9lcFU" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ali_Ayub1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ali_Ayub1">Ali Ayub</a>, <a href="https://openreview.net/profile?id=~Alan_Wagner2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alan_Wagner2">Alan Wagner</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#lWaz5a9lcFU-details-181" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lWaz5a9lcFU-details-181"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Continual Learning, Catastrophic Forgetting, Cognitively-inspired Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The two main impediments to continual learning are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17% over state-of-the-art methods on benchmark datasets, while requiring 78% less storage space.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We train autoencoders with Neural Style Transfer to replay old tasks data for continual learning. The encoded features are converted into centroids and covariances to keep memory footprint from growing while keeping classifier performance stable.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=lWaz5a9lcFU&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="edJ_HipawCa" data-number="396">
      <h4>
        <a href="https://openreview.net/forum?id=edJ_HipawCa">
            Impact of Representation Learning in Linear Bandits
        </a>
      
        
          <a href="https://openreview.net/pdf?id=edJ_HipawCa" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jiaqi_Yang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiaqi_Yang2">Jiaqi Yang</a>, <a href="https://openreview.net/profile?id=~Wei_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Hu1">Wei Hu</a>, <a href="https://openreview.net/profile?id=~Jason_D._Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jason_D._Lee1">Jason D. Lee</a>, <a href="https://openreview.net/profile?id=~Simon_Shaolei_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Shaolei_Du1">Simon Shaolei Du</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 14 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#edJ_HipawCa-details-652" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="edJ_HipawCa-details-652"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">linear bandits, representation learning, multi-task learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study how representation learning can improve the efficiency of bandit problems. We study the setting where we play <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="34" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container> linear bandits with dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="35" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></mjx-assistive-mml></mjx-container> concurrently, and these <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="36" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container> bandit tasks share a common <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="37" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c226A"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi><mo stretchy="false">(</mo><mo>≪</mo><mi>d</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> dimensional linear representation. For the finite-action setting, we present a new algorithm which achieves <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="38" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.06em; padding-left: 0.187em; margin-bottom: -0.597em;"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2DC TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.155em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-msqrt space="3"><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.155em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mi>O</mi><mo>~</mo></mover></mrow><mo stretchy="false">(</mo><mi>T</mi><msqrt><mi>k</mi><mi>N</mi></msqrt><mo>+</mo><msqrt><mi>d</mi><mi>k</mi><mi>N</mi><mi>T</mi></msqrt><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> regret, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="39" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></mjx-assistive-mml></mjx-container> is the number of rounds we play for each bandit. When <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="40" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container> is sufficiently large, our algorithm significantly outperforms the naive algorithm (playing <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="41" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container> bandits independently) that achieves <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="42" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mover><mjx-over style="padding-bottom: 0.06em; padding-left: 0.187em; margin-bottom: -0.597em;"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2DC TEX-S1"></mjx-c></mjx-mo></mjx-over><mjx-base><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi></mjx-base></mjx-mover></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.156em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mi>O</mi><mo>~</mo></mover></mrow><mo stretchy="false">(</mo><mi>T</mi><msqrt><mi>d</mi><mi>N</mi></msqrt><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> regret. We also provide an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="43" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A9"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.155em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-msqrt space="3"><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.155em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><mi>T</mi><msqrt><mi>k</mi><mi>N</mi></msqrt><mo>+</mo><msqrt><mi>d</mi><mi>k</mi><mi>N</mi><mi>T</mi></msqrt><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> regret lower bound, showing that our algorithm is minimax-optimal up to poly-logarithmic factors.  Furthermore, we extend our algorithm to the infinite-action setting and obtain a corresponding regret bound which demonstrates the benefit of representation learning in certain regimes. We also present experiments on synthetic and real-world data to illustrate our theoretical findings and demonstrate the effectiveness of our proposed algorithms.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show representation learning provably improves multi-task linear bandits.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="XjYgR6gbCEc" data-number="1427">
      <h4>
        <a href="https://openreview.net/forum?id=XjYgR6gbCEc">
            MODALS: Modality-agnostic Automated Data Augmentation in the Latent Space
        </a>
      
        
          <a href="https://openreview.net/pdf?id=XjYgR6gbCEc" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tsz-Him_Cheung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tsz-Him_Cheung1">Tsz-Him Cheung</a>, <a href="https://openreview.net/profile?id=~Dit-Yan_Yeung2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dit-Yan_Yeung2">Dit-Yan Yeung</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#XjYgR6gbCEc-details-83" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XjYgR6gbCEc-details-83"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning, data augmentation, automated data augmentation, latent space</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Data augmentation is an efficient way to expand a training dataset by creating additional artificial data. While data augmentation is found to be effective in improving the generalization capabilities of models for various machine learning tasks, the underlying augmentation methods are usually manually designed and carefully evaluated for each data modality separately, like image processing functions for image data and word-replacing rules for text data. In this work, we propose an automated data augmentation approach called MODALS (Modality-agnostic Automated Data Augmentation in the Latent Space) to augment data for any modality in a generic way. MODALS exploits automated data augmentation to fine-tune four universal data transformation operations in the latent space to adapt the transform to data of different modalities. Through comprehensive experiments, we demonstrate the effectiveness of MODALS on multiple datasets for text, tabular, time-series and image modalities.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">MODALS is an automated data augmentation framework that fine-tunes four universal data transformation operations in the latent space to augment data of different modalities.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="3T9iFICe0Y9" data-number="2870">
      <h4>
        <a href="https://openreview.net/forum?id=3T9iFICe0Y9">
            The Recurrent Neural Tangent Kernel
        </a>
      
        
          <a href="https://openreview.net/pdf?id=3T9iFICe0Y9" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sina_Alemohammad1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sina_Alemohammad1">Sina Alemohammad</a>, <a href="https://openreview.net/profile?id=~Zichao_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zichao_Wang1">Zichao Wang</a>, <a href="https://openreview.net/profile?id=~Randall_Balestriero1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Randall_Balestriero1">Randall Balestriero</a>, <a href="https://openreview.net/profile?id=~Richard_Baraniuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Richard_Baraniuk1">Richard Baraniuk</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#3T9iFICe0Y9-details-788" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3T9iFICe0Y9-details-788"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural Tangent Kernel, Recurrent Neural Network, Gaussian Process, Overparameterization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The study of deep neural networks (DNNs) in the infinite-width limit, via the so-called neural tangent kernel (NTK) approach, has provided new insights into the dynamics of learning, generalization, and the impact of initialization. One key DNN architecture remains to be kernelized, namely, the recurrent neural network (RNN).  In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK), which provides new insights into the behavior of  overparametrized RNNs. A key property of the RNTK should greatly benefit practitioners is its ability to compare inputs of different length. To this end, we characterize how the RNTK weights different time steps to form its output under different initialization parameters and nonlinearity choices. A synthetic and 56 real-world data experiments demonstrate that the RNTK offers significant performance gains over other kernels, including standard NTKs, across a wide array of data sets. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=3T9iFICe0Y9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="MBpHUFrcG2x" data-number="2275">
      <h4>
        <a href="https://openreview.net/forum?id=MBpHUFrcG2x">
            Projected Latent Markov Chain Monte Carlo: Conditional Sampling of Normalizing Flows
        </a>
      
        
          <a href="https://openreview.net/pdf?id=MBpHUFrcG2x" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Chris_Cannella1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chris_Cannella1">Chris Cannella</a>, <a href="https://openreview.net/profile?id=~Mohammadreza_Soltani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammadreza_Soltani1">Mohammadreza Soltani</a>, <a href="https://openreview.net/profile?id=~Vahid_Tarokh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vahid_Tarokh1">Vahid Tarokh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 24 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#MBpHUFrcG2x-details-697" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MBpHUFrcG2x-details-697"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Conditional Sampling, Normalizing Flows, Markov Chain Monte Carlo, Missing Data Inference</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We introduce Projected Latent Markov Chain Monte Carlo (PL-MCMC), a technique for sampling from the exact conditional distributions learned by normalizing flows. As a conditional sampling method, PL-MCMC enables Monte Carlo Expectation Maximization (MC-EM) training of normalizing flows from incomplete data. Through experimental tests applying normalizing flows to missing data tasks for a variety of data sets, we demonstrate the efficacy of PL-MCMC for conditional sampling from normalizing flows.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce and demonstrate a novel MCMC technique for sampling from the exact conditional distributions known by normalizing flows.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=MBpHUFrcG2x&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="NjF772F4ZZR" data-number="1698">
      <h4>
        <a href="https://openreview.net/forum?id=NjF772F4ZZR">
            Learning the Pareto Front with Hypernetworks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=NjF772F4ZZR" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Aviv_Navon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aviv_Navon1">Aviv Navon</a>, <a href="https://openreview.net/profile?id=~Aviv_Shamsian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aviv_Shamsian1">Aviv Shamsian</a>, <a href="https://openreview.net/profile?id=~Ethan_Fetaya1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ethan_Fetaya1">Ethan Fetaya</a>, <a href="https://openreview.net/profile?id=~Gal_Chechik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gal_Chechik1">Gal Chechik</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#NjF772F4ZZR-details-106" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NjF772F4ZZR-details-106"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multi-objective optimization, multi-task learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Multi-objective optimization (MOO) problems are prevalent in machine learning. These problems have a set of optimal solutions, called the Pareto front, where each point on the front represents a different trade-off between possibly conflicting objectives. Recent MOO methods can target a specific desired ray in loss space however, most approaches still face two grave limitations: (i) A separate model has to be trained for each point on the front; and (ii) The exact trade-off must be known before the optimization process. Here, we tackle the problem of learning the entire Pareto front, with the capability of selecting a desired operating point on the front after training. We call this new setup Pareto-Front Learning (PFL).
      
      We describe an approach to PFL implemented using HyperNetworks, which we term Pareto HyperNetworks (PHNs). PHN learns the entire Pareto front simultaneously using a single hypernetwork, which receives as input a desired preference vector and returns a Pareto-optimal model whose loss vector is in the desired ray. The unified model is runtime efficient compared to training multiple models and generalizes to new operating points not used during training. We evaluate our method on a wide set of problems, from multi-task regression and classification to fairness. PHNs learn the entire Pareto front at roughly the same time as learning a single point on the front and at the same time reach a better solution set. PFL opens the door to new applications where models are selected based on preferences that are only available at run time.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A novel approach for learning the entire Pareto front using hypernetworks</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="YLewtnvKgR7" data-number="1741">
      <h4>
        <a href="https://openreview.net/forum?id=YLewtnvKgR7">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
      
        
          <a href="https://openreview.net/pdf?id=YLewtnvKgR7" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ali_Harakeh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ali_Harakeh1">Ali Harakeh</a>, <a href="https://openreview.net/profile?id=~Steven_L._Waslander1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_L._Waslander1">Steven L. Waslander</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 13 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#YLewtnvKgR7-details-793" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YLewtnvKgR7-details-793"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Object Detection, Predictive Uncertainty Estimation, Proper Scoring Rules, Variance Networks, Energy Score, Computer Vision</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Predictive uncertainty estimation is an essential next step for the reliable deployment of deep object detectors in safety-critical tasks. In this work, we focus on estimating predictive distributions for bounding box regression output with variance networks. We show that in the context of object detection, training variance networks with negative log likelihood (NLL) can lead to high entropy predictive distributions regardless of the correctness of the output mean. We propose to use the energy score as a non-local proper scoring rule and find that when used for training, the energy score leads to better calibrated and lower entropy predictive distributions than NLL. We also address the widespread use of non-proper scoring metrics for evaluating predictive distributions from deep object detectors by proposing an alternate evaluation approach founded on proper scoring rules. Using the proposed evaluation tools, we show that although variance networks can be used to produce high quality predictive distributions, ad-hoc approaches used by seminal object detectors for choosing regression targets during training do not provide wide enough data support for reliable variance learning. We hope that our work helps shift evaluation in probabilistic object detection to better align with predictive uncertainty evaluation in other machine learning domains. Code for all models, evaluation, and datasets is available at: https://github.com/asharakeh/probdet.git.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Y9McSeEaqUh" data-number="1583">
      <h4>
        <a href="https://openreview.net/forum?id=Y9McSeEaqUh">
            Predicting Classification Accuracy When Adding New Unobserved Classes
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Y9McSeEaqUh" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yuli_Slavutsky1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuli_Slavutsky1">Yuli Slavutsky</a>, <a href="https://openreview.net/profile?id=~Yuval_Benjamini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuval_Benjamini1">Yuval Benjamini</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Y9McSeEaqUh-details-280" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Y9McSeEaqUh-details-280"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">multiclass classification, classification, extrapolation, accuracy, ROC</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Multiclass classifiers are often designed and evaluated only on a sample from the classes on which they will eventually be applied. Hence, their final accuracy remains unknown. In this work we study how a classifier’s performance over the initial class sample can be used to extrapolate its expected accuracy on a larger, unobserved set of classes. For this, we define a measure of separation between correct and incorrect classes that is independent of the number of classes: the "reversed ROC" (rROC), which is obtained by replacing the roles of classes and data-points in the common ROC. We show that the classification accuracy is a function of the rROC in multiclass classifiers, for which the learned representation of data from the initial class sample remains unchanged when new classes are added. Using these results we formulate a robust neural-network-based algorithm, "CleaneX", which learns to estimate the accuracy of such classifiers on arbitrarily large sets of classes. Unlike previous methods, our method uses both the observed accuracies of the classifier and densities of classification scores, and therefore achieves remarkably better predictions than current state-of-the-art methods on both simulations and real datasets of object detection, face recognition, and brain decoding.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A new prediction method of multiclass classification accuracy for an increased number of classes.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="POWv6hDd9XH" data-number="1">
      <h4>
        <a href="https://openreview.net/forum?id=POWv6hDd9XH">
            BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction
        </a>
      
        
          <a href="https://openreview.net/pdf?id=POWv6hDd9XH" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yuhang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuhang_Li1">Yuhang Li</a>, <a href="https://openreview.net/profile?id=~Ruihao_Gong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruihao_Gong1">Ruihao Gong</a>, <a href="https://openreview.net/profile?id=~Xu_Tan3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xu_Tan3">Xu Tan</a>, <a href="https://openreview.net/profile?id=~Yang_Yang22" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Yang22">Yang Yang</a>, <a href="https://openreview.net/profile?id=~Peng_Hu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peng_Hu3">Peng Hu</a>, <a href="https://openreview.net/profile?id=~Qi_Zhang15" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qi_Zhang15">Qi Zhang</a>, <a href="https://openreview.net/profile?id=~Fengwei_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fengwei_Yu1">Fengwei Yu</a>, <a href="https://openreview.net/profile?id=~Wei_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Wang3">Wei Wang</a>, <a href="https://openreview.net/profile?id=~Shi_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shi_Gu1">Shi Gu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 28 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#POWv6hDd9XH-details-224" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="POWv6hDd9XH-details-224"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Post Training Quantization, Mixed Precision, Second-order analysis</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the challenging task of neural network quantization without end-to-end retraining, called Post-training Quantization (PTQ). PTQ usually requires a small subset of training data but produces less powerful quantized models than Quantization-Aware Training (QAT). In this work, we propose a novel PTQ framework, dubbed BRECQ, which pushes the limits of bitwidth in PTQ down to INT2 for the first time. BRECQ leverages the basic building blocks in neural networks and reconstructs them one-by-one. In a comprehensive theoretical study of the second-order error, we show that BRECQ achieves a good balance between cross-layer dependency and generalization error. To further employ the power of quantization, the mixed precision technique is incorporated in our framework by approximating the inter-layer and intra-layer sensitivity. Extensive experiments on various handcrafted and searched neural architectures are conducted for both image classification and object detection tasks. And for the first time we prove that, without bells and whistles, PTQ can attain 4-bit ResNet and MobileNetV2 comparable with QAT and enjoy 240 times faster production of quantized models.  Codes are available at https://github.com/yhhhli/BRECQ.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ixpSxO9flk3" data-number="2046">
      <h4>
        <a href="https://openreview.net/forum?id=ixpSxO9flk3">
            No MCMC for me: Amortized sampling for fast and stable training of energy-based models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ixpSxO9flk3" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Will_Sussman_Grathwohl2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Will_Sussman_Grathwohl2">Will Sussman Grathwohl</a>, <a href="https://openreview.net/profile?email=jacob.jin.kelly%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jacob.jin.kelly@gmail.com">Jacob Jin Kelly</a>, <a href="https://openreview.net/profile?id=~Milad_Hashemi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Milad_Hashemi1">Milad Hashemi</a>, <a href="https://openreview.net/profile?id=~Mohammad_Norouzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammad_Norouzi1">Mohammad Norouzi</a>, <a href="https://openreview.net/profile?id=~Kevin_Swersky1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kevin_Swersky1">Kevin Swersky</a>, <a href="https://openreview.net/profile?id=~David_Duvenaud2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Duvenaud2">David Duvenaud</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 13 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>34 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ixpSxO9flk3-details-284" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ixpSxO9flk3-details-284"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Generative Models, EBM, Energy-Based Models, Energy Based Models, semi-supervised learning, JEM</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a new generator-based approach for training EBMs and demonstrate that it trains models which obtain high likelihood and overcomes stability issues common in EBM training.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="jLoC4ez43PZ" data-number="318">
      <h4>
        <a href="https://openreview.net/forum?id=jLoC4ez43PZ">
            GraphCodeBERT: Pre-training Code Representations with Data Flow
        </a>
      
        
          <a href="https://openreview.net/pdf?id=jLoC4ez43PZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Daya_Guo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daya_Guo2">Daya Guo</a>, <a href="https://openreview.net/profile?email=shuoren%40buaa.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="shuoren@buaa.edu.cn">Shuo Ren</a>, <a href="https://openreview.net/profile?email=lushuai96%40pku.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="lushuai96@pku.edu.cn">Shuai Lu</a>, <a href="https://openreview.net/profile?email=zyfeng%40ir.hit.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="zyfeng@ir.hit.edu.cn">Zhangyin Feng</a>, <a href="https://openreview.net/profile?id=~Duyu_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Duyu_Tang1">Duyu Tang</a>, <a href="https://openreview.net/profile?id=~Shujie_LIU1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shujie_LIU1">Shujie LIU</a>, <a href="https://openreview.net/profile?id=~Long_Zhou2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Long_Zhou2">Long Zhou</a>, <a href="https://openreview.net/profile?id=~Nan_Duan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nan_Duan1">Nan Duan</a>, <a href="https://openreview.net/profile?id=~Alexey_Svyatkovskiy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexey_Svyatkovskiy1">Alexey Svyatkovskiy</a>, <a href="https://openreview.net/profile?email=shengyfu%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="shengyfu@microsoft.com">Shengyu Fu</a>, <a href="https://openreview.net/profile?email=michele.tufano%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="michele.tufano@microsoft.com">Michele Tufano</a>, <a href="https://openreview.net/profile?email=shao.deng%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="shao.deng@microsoft.com">Shao Kun Deng</a>, <a href="https://openreview.net/profile?email=colin.clement%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="colin.clement@microsoft.com">Colin Clement</a>, <a href="https://openreview.net/profile?email=dawn.drain%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dawn.drain@microsoft.com">Dawn Drain</a>, <a href="https://openreview.net/profile?email=neels%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="neels@microsoft.com">Neel Sundaresan</a>, <a href="https://openreview.net/profile?email=issjyin%40mail.sysu.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="issjyin@mail.sysu.edu.cn">Jian Yin</a>, <a href="https://openreview.net/profile?email=djiang%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="djiang@microsoft.com">Daxin Jiang</a>, <a href="https://openreview.net/profile?id=~Ming_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ming_Zhou1">Ming Zhou</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#jLoC4ez43PZ-details-713" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jLoC4ez43PZ-details-713"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Pre-training, BERT, Code Representations, Code Structure, Data Flow</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of "where-the-value-comes-from" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="iaO86DUuKi" data-number="883">
      <h4>
        <a href="https://openreview.net/forum?id=iaO86DUuKi">
            Conservative Safety Critics for Exploration
        </a>
      
        
          <a href="https://openreview.net/pdf?id=iaO86DUuKi" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Homanga_Bharadhwaj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Homanga_Bharadhwaj1">Homanga Bharadhwaj</a>, <a href="https://openreview.net/profile?id=~Aviral_Kumar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aviral_Kumar2">Aviral Kumar</a>, <a href="https://openreview.net/profile?id=~Nicholas_Rhinehart1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_Rhinehart1">Nicholas Rhinehart</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>, <a href="https://openreview.net/profile?id=~Florian_Shkurti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Shkurti1">Florian Shkurti</a>, <a href="https://openreview.net/profile?id=~Animesh_Garg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Animesh_Garg1">Animesh Garg</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#iaO86DUuKi-details-219" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iaO86DUuKi-details-219"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Safe exploration, Reinforcement Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Safe exploration presents a major challenge in reinforcement learning (RL): when active data collection requires deploying partially trained policies, we must ensure that these policies avoid catastrophically unsafe regions, while still enabling trial and error learning. In this paper, we target the problem of safe exploration in RL, by learning a conservative safety estimate of environment states through a critic, and provably upper bound the likelihood of catastrophic failures at every training iteration. We theoretically characterize the tradeoff between safety and policy improvement, show that the safety constraints are satisfied with high probability during training, derive provable convergence guarantees for our approach which is no worse asymptotically then standard RL, and empirically demonstrate the efficacy of the proposed approach on a suite of challenging navigation, manipulation, and locomotion tasks. Our results demonstrate that the proposed approach can achieve competitive task performance, while incurring significantly lower catastrophic failure rates during training as compared to prior methods. Videos are at this URL https://sites.google.com/view/conservative-safety-critics/</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Safe exploration in reinforcement learning can be achieved by constraining policy learning with conservative safety estimates of the environment.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=iaO86DUuKi&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="uKhGRvM8QNH" data-number="182">
      <h4>
        <a href="https://openreview.net/forum?id=uKhGRvM8QNH">
            Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors
        </a>
      
        
          <a href="https://openreview.net/pdf?id=uKhGRvM8QNH" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Linfeng_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Linfeng_Zhang2">Linfeng Zhang</a>, <a href="https://openreview.net/profile?id=~Kaisheng_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaisheng_Ma1">Kaisheng Ma</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#uKhGRvM8QNH-details-297" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uKhGRvM8QNH-details-297"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Knowledge Distillation, Object Detection, Teacher-Student Learning, Non-Local Modules, Attention Modules</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Knowledge distillation, in which a student model is trained to mimic a teacher model, has been proved as an effective technique for model compression and model accuracy boosting. However, most knowledge distillation methods, designed for image classification, have failed on more challenging tasks, such as object detection. In this paper, we suggest that the failure of knowledge distillation on object detection is mainly caused by two reasons: (1) the imbalance between pixels of foreground and background and (2) lack of distillation on the relation between different pixels. Observing the above reasons, we propose attention-guided distillation and non-local distillation to address the two problems, respectively.  Attention-guided distillation is proposed to find the crucial pixels of foreground objects with attention mechanism and then make the students take more effort to learn their features. Non-local distillation is proposed to enable students to learn not only the feature of an individual pixel but also the relation between different pixels captured by non-local modules. Experiments show that our methods achieve excellent AP improvements on both one-stage and two-stage, both anchor-based and anchor-free detectors. For example, Faster RCNN (ResNet101 backbone) with our distillation achieves 43.9 AP on COCO2017, which is 4.1 higher than the baseline. Codes have been released on Github.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose two knowledge distillation methods on object detection - attention-guided distillation and non-local distillation which lead to 4.1 AP improvements on Faster RCNN101 in MS COCO2017.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=uKhGRvM8QNH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="whE31dn74cL" data-number="2930">
      <h4>
        <a href="https://openreview.net/forum?id=whE31dn74cL">
            A Temporal Kernel Approach for Deep Learning with Continuous-time Information
        </a>
      
        
          <a href="https://openreview.net/pdf?id=whE31dn74cL" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Da_Xu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Da_Xu2">Da Xu</a>, <a href="https://openreview.net/profile?id=~Chuanwei_Ruan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuanwei_Ruan1">Chuanwei Ruan</a>, <a href="https://openreview.net/profile?id=~Evren_Korpeoglu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Evren_Korpeoglu1">Evren Korpeoglu</a>, <a href="https://openreview.net/profile?id=~Sushant_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sushant_Kumar1">Sushant Kumar</a>, <a href="https://openreview.net/profile?id=~Kannan_Achan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kannan_Achan1">Kannan Achan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#whE31dn74cL-details-169" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="whE31dn74cL-details-169"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Kernel Learning, Continuous-time System, Spectral Distribution, Random Feature, Reparameterization, Learning Theory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Sequential deep learning models such as RNN, causal CNN and attention mechanism do not readily consume continuous-time information. Discretizing the temporal data, as we show, causes inconsistency even for simple continuous-time processes. Current approaches often handle time in a heuristic manner to be consistent with the existing deep learning architectures and implementations. In this paper, we provide a principled way to characterize continuous-time systems using deep learning tools. Notably, the proposed approach applies to all the major deep learning architectures and requires little modifications to the implementation. The critical insight is to represent the continuous-time system by composing neural networks with a temporal kernel, where we gain our intuition from the recent advancements in understanding deep learning with Gaussian process and neural tangent kernel. To represent the temporal kernel, we introduce the random feature approach and convert the kernel learning problem to spectral density estimation under reparameterization. We further prove the convergence and consistency results even when the temporal kernel is non-stationary, and the spectral density is misspecified. The simulations and real-data experiments demonstrate the empirical effectiveness of our temporal kernel approach in a broad range of settings.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a temporal kernel learning approach based on random features and reparameterization to characterize the continuous-time information in deep learning models.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=whE31dn74cL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Srmggo3b3X6" data-number="1303">
      <h4>
        <a href="https://openreview.net/forum?id=Srmggo3b3X6">
            For self-supervised learning, Rationality implies generalization, provably
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Srmggo3b3X6" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yamini_Bansal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yamini_Bansal1">Yamini Bansal</a>, <a href="https://openreview.net/profile?id=~Gal_Kaplun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gal_Kaplun1">Gal Kaplun</a>, <a href="https://openreview.net/profile?id=~Boaz_Barak2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boaz_Barak2">Boaz Barak</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Srmggo3b3X6-details-823" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Srmggo3b3X6-details-823"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep Learning Theory, Generalization Bounds, Self-supervised learning, Representation learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We prove a new upper bound on the generalization gap of classifiers that are obtained by first using self-supervision to learn a representation <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="44" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi></math></mjx-assistive-mml></mjx-container> of the training~data, and then fitting a simple (e.g., linear) classifier <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="45" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi></math></mjx-assistive-mml></mjx-container> to the labels. Specifically, we show that (under the assumptions described below) the generalization gap of such classifiers tends to zero if <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="46" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ss"><mjx-c class="mjx-c1D5A2 TEX-SS"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c226A"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="sans-serif">C</mi></mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo><mo>≪</mo><mi>n</mi></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="47" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ss"><mjx-c class="mjx-c1D5A2 TEX-SS"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="sans-serif">C</mi></mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> is an appropriately-defined measure of the simple classifier <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="48" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi></math></mjx-assistive-mml></mjx-container>'s complexity, and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="49" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container> is the number of training samples. We stress that our bound is independent of the complexity of the representation <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="50" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>r</mi></math></mjx-assistive-mml></mjx-container>. 
      We do not make any structural or conditional-independence  assumptions on the representation-learning task, which can use the same training dataset that is later used for classification. Rather, we assume that the training procedure satisfies certain natural noise-robustness (adding small amount of label noise causes small degradation in performance) and rationality  (getting the wrong label is not better than getting no label at all) conditions that widely hold across many standard architectures.
      We also conduct an extensive empirical study of the generalization gap and the quantities used in our assumptions for a variety of self-supervision based algorithms, including SimCLR, AMDIM and BigBiGAN,  on the CIFAR-10 and ImageNet datasets. We show that, unlike standard supervised classifiers, these algorithms display small generalization gap, and the bounds we prove on this gap are often non vacuous. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We prove and empirically demonstrate generalization bounds for algorithms that fit a simple classifier on a representation learned via self-supervision; we obtain non-vacuous bounds for such top-performing algorithms on both CIFAR-10 and ImageNet.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Wi5KUNlqWty" data-number="1028">
      <h4>
        <a href="https://openreview.net/forum?id=Wi5KUNlqWty">
            How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Wi5KUNlqWty" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Dongkwan_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dongkwan_Kim1">Dongkwan Kim</a>, <a href="https://openreview.net/profile?id=~Alice_Oh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alice_Oh1">Alice Oh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Wi5KUNlqWty-details-905" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Wi5KUNlqWty-details-905"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Graph Neural Network, Attention Mechanism, Self-supervised Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Attention mechanism in graph neural networks is designed to assign larger weights to important neighbor nodes for better representation. However, what graph attention learns is not understood well, particularly when graphs are noisy. In this paper, we propose a self-supervised graph attention network (SuperGAT), an improved graph attention model for noisy graphs. Specifically, we exploit two attention forms compatible with a self-supervised task to predict edges, whose presence and absence contain the inherent information about the importance of the relationships between nodes. By encoding edges, SuperGAT learns more expressive attention in distinguishing mislinked neighbors. We find two graph characteristics influence the effectiveness of attention forms and self-supervision: homophily and average degree. Thus, our recipe provides guidance on which attention design to use when those two graph characteristics are known. Our experiment on 17 real-world datasets demonstrates that our recipe generalizes across 15 datasets of them, and our models designed by recipe show improved performance over baselines.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a method that self-supervise graph attention through edges and it should be designed according to the average degree and homophily of graphs.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="DEa4JdMWRHp" data-number="1672">
      <h4>
        <a href="https://openreview.net/forum?id=DEa4JdMWRHp">
            Interpretable Models for Granger Causality Using Self-explaining Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=DEa4JdMWRHp" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ri%C4%8Dards_Marcinkevi%C4%8Ds1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ričards_Marcinkevičs1">Ričards Marcinkevičs</a>, <a href="https://openreview.net/profile?id=~Julia_E_Vogt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Julia_E_Vogt1">Julia E Vogt</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#DEa4JdMWRHp-details-169" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DEa4JdMWRHp-details-169"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">time series, Granger causality, interpretability, inference, neural networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Exploratory analysis of time series data can yield a better understanding of complex dynamical systems. Granger causality is a practical framework for analysing interactions in sequential data, applied in a wide range of domains. In this paper, we propose a novel framework for inferring multivariate Granger causality under nonlinear dynamics based on an extension of self-explaining neural networks. This framework is more interpretable than other neural-network-based techniques for inferring Granger causality, since in addition to relational inference, it also allows detecting signs of Granger-causal effects and inspecting their variability over time. In comprehensive experiments on simulated data, we show that our framework performs on par with several powerful baseline methods at inferring Granger causality and that it achieves better performance at inferring interaction signs. The results suggest that our framework is a viable and more interpretable alternative to sparse-input neural networks for inferring Granger causality.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an interpretable framework for inferring Granger causality based on self-explaining neural networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=DEa4JdMWRHp&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-QxT4mJdijq" data-number="1447">
      <h4>
        <a href="https://openreview.net/forum?id=-QxT4mJdijq">
            Meta-learning Symmetries by Reparameterization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-QxT4mJdijq" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Allan_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Allan_Zhou1">Allan Zhou</a>, <a href="https://openreview.net/profile?email=tknowles%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="tknowles@stanford.edu">Tom Knowles</a>, <a href="https://openreview.net/profile?id=~Chelsea_Finn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chelsea_Finn1">Chelsea Finn</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-QxT4mJdijq-details-755" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-QxT4mJdijq-details-755"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">meta-learning, equivariance, convolution, symmetry</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Many successful deep learning architectures are equivariant to certain transformations in order to conserve parameters and improve generalization: most famously, convolution layers are equivariant to shifts of the input. This approach only works when practitioners know the symmetries of the task and can manually construct an architecture with the corresponding equivariances. Our goal is an approach for learning equivariances from data, without needing to design custom task-specific architectures. We present a method for learning and encoding equivariances into networks by learning corresponding parameter sharing patterns from data. Our method can provably represent equivariance-inducing parameter sharing for any finite group of symmetry transformations. Our experiments suggest that it can automatically learn to encode equivariances to common transformations used in image processing tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A method for automatically meta-learning and encoding equivariances into neural networks.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="eIHYL6fpbkA" data-number="507">
      <h4>
        <a href="https://openreview.net/forum?id=eIHYL6fpbkA">
            Removing Undesirable Feature Contributions Using Out-of-Distribution Data
        </a>
      
        
          <a href="https://openreview.net/pdf?id=eIHYL6fpbkA" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Saehyung_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Saehyung_Lee1">Saehyung Lee</a>, <a href="https://openreview.net/profile?id=~Changhwa_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changhwa_Park1">Changhwa Park</a>, <a href="https://openreview.net/profile?id=~Hyungyu_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hyungyu_Lee2">Hyungyu Lee</a>, <a href="https://openreview.net/profile?id=~Jihun_Yi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jihun_Yi1">Jihun Yi</a>, <a href="https://openreview.net/profile?id=~Jonghyun_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonghyun_Lee1">Jonghyun Lee</a>, <a href="https://openreview.net/profile?id=~Sungroh_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sungroh_Yoon1">Sungroh Yoon</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#eIHYL6fpbkA-details-775" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eIHYL6fpbkA-details-775"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">adversarial training, adversarial robustness, generalization, out-of-distribution</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Several data augmentation methods deploy unlabeled-in-distribution (UID) data to bridge the gap between the training and inference of neural networks. However, these methods have clear limitations in terms of availability of UID data and dependence of algorithms on pseudo-labels. Herein, we propose a data augmentation method to improve generalization in both adversarial and standard learning by using out-of-distribution (OOD) data that are devoid of the abovementioned issues. We show how to improve generalization theoretically using OOD data in each learning scenario and complement our theoretical analysis with experiments on CIFAR-10, CIFAR-100, and a subset of ImageNet. The results indicate that undesirable features are shared even among image data that seem to have little correlation from a human point of view. We also present the advantages of the proposed method through comparison with other data augmentation methods, which can be used in the absence of UID data. Furthermore, we demonstrate that the proposed method can further improve the existing state-of-the-art adversarial training.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a simple method, Out-of-distribution data Augmented Training (OAT), to leverage OOD data for adversarial and standard learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="a2gqxKDvYys" data-number="3587">
      <h4>
        <a href="https://openreview.net/forum?id=a2gqxKDvYys">
            Mind the Gap when Conditioning Amortised Inference in Sequential Latent-Variable Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=a2gqxKDvYys" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Justin_Bayer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Justin_Bayer1">Justin Bayer</a>, <a href="https://openreview.net/profile?id=~Maximilian_Soelch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maximilian_Soelch1">Maximilian Soelch</a>, <a href="https://openreview.net/profile?id=~Atanas_Mirchev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Atanas_Mirchev1">Atanas Mirchev</a>, <a href="https://openreview.net/profile?id=~Baris_Kayalibay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Baris_Kayalibay1">Baris Kayalibay</a>, <a href="https://openreview.net/profile?id=~Patrick_van_der_Smagt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Patrick_van_der_Smagt1">Patrick van der Smagt</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#a2gqxKDvYys-details-389" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="a2gqxKDvYys-details-389"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">variational inference, state-space models, amortized inference, recurrent networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Amortised inference enables scalable learning of sequential latent-variable models (LVMs) with the evidence lower bound (ELBO). In this setting, variational posteriors are often only partially conditioned. While the true posteriors depend, e.g., on the entire sequence of observations, approximate posteriors are only informed by past observations. This mimics the Bayesian filter---a mixture of smoothing posteriors. Yet, we show that the ELBO objective forces partially-conditioned amortised posteriors to approximate products of smoothing posteriors instead. Consequently, the learned generative model is compromised. We demonstrate these theoretical findings in three scenarios: traffic flow, handwritten digits, and aerial vehicle dynamics. Using fully-conditioned approximate posteriors, performance improves in terms of generative modelling and multi-step prediction.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show how a common model assumption in amortised variational inference with sequential LVMS leads to a suboptimality and how to prevent it.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="0IO5VdnSAaH" data-number="519">
      <h4>
        <a href="https://openreview.net/forum?id=0IO5VdnSAaH">
            On the Universality of the Double Descent Peak in Ridgeless Regression
        </a>
      
        
          <a href="https://openreview.net/pdf?id=0IO5VdnSAaH" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~David_Holzm%C3%BCller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Holzmüller1">David Holzmüller</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#0IO5VdnSAaH-details-446" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0IO5VdnSAaH-details-446"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Double Descent, Interpolation Peak, Linear Regression, Random Features, Random Weights Neural Networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We prove a non-asymptotic distribution-independent lower bound for the expected mean squared generalization error caused by label noise in ridgeless linear regression. Our lower bound generalizes a similar known result to the overparameterized (interpolating) regime. In contrast to most previous works, our analysis applies to a broad class of input distributions with almost surely full-rank feature matrices, which allows us to cover various types of deterministic or random feature maps. Our lower bound is asymptotically sharp and implies that in the presence of label noise, ridgeless linear regression does not perform well around the interpolation threshold for any of these feature maps. We analyze the imposed assumptions in detail and provide a theory for analytic (random) feature maps. Using this theory, we can show that our assumptions are satisfied for input distributions with a (Lebesgue) density and feature maps given by random deep neural networks with analytic activation functions like sigmoid, tanh, softplus or GELU. As further examples, we show that feature maps from random Fourier features and polynomial kernels also satisfy our assumptions. We complement our theory with further experimental and analytic results.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We prove a distribution-independent lower bound for the generalization error of ridgeless (random) features regression under weak assumptions, showing universal sensitivity to label noise around the interpolation threshold.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=0IO5VdnSAaH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="DNl5s5BXeBn" data-number="1240">
      <h4>
        <a href="https://openreview.net/forum?id=DNl5s5BXeBn">
            Fair Mixup: Fairness via Interpolation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=DNl5s5BXeBn" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ching-Yao_Chuang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ching-Yao_Chuang1">Ching-Yao Chuang</a>, <a href="https://openreview.net/profile?id=~Youssef_Mroueh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youssef_Mroueh1">Youssef Mroueh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#DNl5s5BXeBn-details-730" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DNl5s5BXeBn-details-730"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">fairness, data augmentation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Training classifiers under fairness constraints such as group fairness, regularizes the disparities of predictions between the groups. Nevertheless, even though the constraints are satisfied during training, they might not generalize at evaluation time. To improve the generalizability of fair classifiers, we propose fair mixup, a new data augmentation strategy for imposing the fairness constraint. In particular, we show that fairness can be achieved by regularizing the models on paths of interpolated samples  between the groups. We use mixup, a powerful data augmentation strategy  to generate these interpolates. We analyze fair mixup and empirically show that it ensures a better generalization for both accuracy and fairness measurement in tabular, vision, and language benchmarks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-bdp_8Itjwp" data-number="1776">
      <h4>
        <a href="https://openreview.net/forum?id=-bdp_8Itjwp">
            Self-supervised Learning from a Multi-view Perspective
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-bdp_8Itjwp" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yao-Hung_Hubert_Tsai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yao-Hung_Hubert_Tsai1">Yao-Hung Hubert Tsai</a>, <a href="https://openreview.net/profile?id=~Yue_Wu17" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yue_Wu17">Yue Wu</a>, <a href="https://openreview.net/profile?id=~Ruslan_Salakhutdinov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruslan_Salakhutdinov1">Ruslan Salakhutdinov</a>, <a href="https://openreview.net/profile?id=~Louis-Philippe_Morency1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Louis-Philippe_Morency1">Louis-Philippe Morency</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 21 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-bdp_8Itjwp-details-860" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-bdp_8Itjwp-details-860"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Self-supervised Learning, Unsupervised Learning, Multi-view Representation Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">As a subset of unsupervised representation learning, self-supervised representation learning adopts self-defined signals as supervision and uses the learned representation for downstream tasks, such as object detection and image captioning. Many proposed approaches for self-supervised learning follow naturally a multi-view perspective, where the input (e.g., original images) and the self-supervised signals (e.g., augmented images) can be seen as two redundant views of the data. Building from this multi-view perspective, this paper provides an information-theoretical framework to better understand the properties that encourage successful self-supervised learning. Specifically, we demonstrate that self-supervised learned representations can extract task-relevant information and discard task-irrelevant information. Our theoretical framework paves the way to a larger space of self-supervised learning objective design. In particular, we propose a composite objective that bridges the gap between prior contrastive and predictive learning objectives, and introduce an additional objective term to discard task-irrelevant information. To verify our analysis, we conduct controlled experiments to evaluate the impact of the composite objectives. We also explore our framework's empirical generalization beyond the multi-view perspective, where the cross-view redundancy may not be clearly observed.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">From a multi-view learning perspective, this paper provides both theoretical and empirical analysis on self-supervised learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=-bdp_8Itjwp&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="IMPA6MndSXU" data-number="953">
      <h4>
        <a href="https://openreview.net/forum?id=IMPA6MndSXU">
            Integrating Categorical Semantics into Unsupervised Domain Translation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=IMPA6MndSXU" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Samuel_Lavoie-Marchildon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samuel_Lavoie-Marchildon1">Samuel Lavoie-Marchildon</a>, <a href="https://openreview.net/profile?id=~Faruk_Ahmed1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Faruk_Ahmed1">Faruk Ahmed</a>, <a href="https://openreview.net/profile?id=~Aaron_Courville3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aaron_Courville3">Aaron Courville</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#IMPA6MndSXU-details-347" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IMPA6MndSXU-details-347"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Unsupervised Domain Translation, Unsupervised Learning, Image-to-Image Translation, Deep Learning, Representation Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While unsupervised domain translation (UDT) has seen a lot of success recently, we argue that mediating its translation via categorical semantic features could broaden its applicability. In particular, we demonstrate that categorical semantics improves the translation between perceptually different domains sharing multiple object categories. We propose a method to learn, in an unsupervised manner, categorical semantic features (such as object labels) that are invariant of the source and target domains. We show that conditioning the style encoder of unsupervised domain translation methods on the learned categorical semantics leads to a translation preserving the digits on MNIST<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="51" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2194"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">↔</mo></math></mjx-assistive-mml></mjx-container>SVHN and to a more realistic stylization on Sketches<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="52" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo accent="false" stretchy="false">→</mo></math></mjx-assistive-mml></mjx-container>Reals.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a method for learning domain invariant categorial semantics which enable UDT on two setups.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=IMPA6MndSXU&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="aYuZO9DIdnn" data-number="523">
      <h4>
        <a href="https://openreview.net/forum?id=aYuZO9DIdnn">
            The Unreasonable Effectiveness of Patches in Deep Convolutional Kernels Methods
        </a>
      
        
          <a href="https://openreview.net/pdf?id=aYuZO9DIdnn" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Louis_THIRY1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Louis_THIRY1">Louis THIRY</a>, <a href="https://openreview.net/profile?id=~Michael_Arbel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Arbel1">Michael Arbel</a>, <a href="https://openreview.net/profile?id=~Eugene_Belilovsky1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eugene_Belilovsky1">Eugene Belilovsky</a>, <a href="https://openreview.net/profile?id=~Edouard_Oyallon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edouard_Oyallon1">Edouard Oyallon</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#aYuZO9DIdnn-details-616" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="aYuZO9DIdnn-details-616"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">convolutional kernel methods, image classification</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A recent line of work showed that  various forms of convolutional  kernel methods can be competitive with standard supervised deep convolutional networks on datasets like CIFAR-10, obtaining accuracies in the range of 87-90% while being more amenable to theoretical analysis. In this work, we highlight the importance of a data-dependent feature extraction step that is key to the obtain good performance in convolutional kernel methods. This step typically corresponds to a whitened dictionary of patches, and gives rise to a data-driven convolutional kernel methods.We extensively study its effect, demonstrating it is the key ingredient for high performance of these methods. Specifically, we show that one of the simplest instances of such kernel methods, based on a single layer of  image patches followed by a linear classifier is already obtaining classification accuracies on CIFAR-10 in the same range as previous more sophisticated convolutional kernel methods. We scale this method to the challenging ImageNet dataset, showing such a simple approach can exceed all existing non-learned representation methods. This is a new baseline for object recognition without representation learning methods, that  initiates the investigation of  convolutional kernel models  on ImageNet. We conduct experiments to analyze the dictionary that we used, our ablations showing they exhibit low-dimensional properties.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Patch-based representation is a key ingredient for competitive convolutional kernel methods.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=aYuZO9DIdnn&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="MmCRswl1UYl" data-number="662">
      <h4>
        <a href="https://openreview.net/forum?id=MmCRswl1UYl">
            Open Question Answering over Tables and Text
        </a>
      
        
          <a href="https://openreview.net/pdf?id=MmCRswl1UYl" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Wenhu_Chen3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenhu_Chen3">Wenhu Chen</a>, <a href="https://openreview.net/profile?id=~Ming-Wei_Chang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ming-Wei_Chang3">Ming-Wei Chang</a>, <a href="https://openreview.net/profile?id=~Eva_Schlinger2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eva_Schlinger2">Eva Schlinger</a>, <a href="https://openreview.net/profile?id=~William_Yang_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_Yang_Wang2">William Yang Wang</a>, <a href="https://openreview.net/profile?id=~William_W._Cohen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_W._Cohen2">William W. Cohen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#MmCRswl1UYl-details-411" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MmCRswl1UYl-details-411"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Question Answering, Tabular Data, Open-domain, Retrieval</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In open question answering (QA), the answer to a question is produced by retrieving and then analyzing documents that might contain answers to the question.  Most open QA systems have considered only retrieving information from unstructured text.  Here we consider for the first time open QA over {\em both} tabular and textual data and present a new large-scale dataset \emph{Open Table-and-Text Question Answering} (OTT-QA) to evaluate performance on this task. Most questions in OTT-QA require multi-hop inference across tabular data and unstructured text, and the evidence required to answer a question can be distributed in different ways over these two types of input, making evidence retrieval challenging---our baseline model using an iterative retriever and BERT-based reader achieves an exact match score less than 10\%. We then propose two novel techniques to address the challenge of retrieving and aggregating evidence for OTT-QA. The first technique is to use ``early fusion'' to group multiple highly relevant tabular and textual units into a fused block, which provides more context for the retriever to search for.  The second technique is to use a cross-block reader to model the cross-dependency between multiple retrieved evidence with global-local sparse attention. Combining these two techniques improves the score significantly, to above 27\%.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose the new task of answering open-domain questions answering over web tables and text and design new techniques: 1) fused retrieval 2) cross-block reader to resolve the challenges posed in the new task.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=MmCRswl1UYl&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9uvhpyQwzM_" data-number="2857">
      <h4>
        <a href="https://openreview.net/forum?id=9uvhpyQwzM_">
            Evaluation of Similarity-based Explanations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9uvhpyQwzM_" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kazuaki_Hanawa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kazuaki_Hanawa1">Kazuaki Hanawa</a>, <a href="https://openreview.net/profile?id=~Sho_Yokoi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sho_Yokoi1">Sho Yokoi</a>, <a href="https://openreview.net/profile?id=~Satoshi_Hara1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Satoshi_Hara1">Satoshi Hara</a>, <a href="https://openreview.net/profile?id=~Kentaro_Inui1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kentaro_Inui1">Kentaro Inui</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9uvhpyQwzM_-details-309" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9uvhpyQwzM_-details-309"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Interpretability, Explainability</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Explaining the predictions made by complex machine learning models helps users to understand and accept the predicted outputs with confidence. One promising way is to use similarity-based explanation that provides similar instances as evidence to support model predictions. Several relevance metrics are used for this purpose. In this study, we investigated relevance metrics that can provide reasonable explanations to users. Specifically, we adopted three tests to evaluate whether the relevance metrics satisfy the minimal requirements for similarity-based explanation. Our experiments revealed that the cosine similarity of the gradients of the loss performs best, which would be a recommended choice in practice. In addition, we showed that some metrics perform poorly in our tests and analyzed the reasons of their failure. We expect our insights to help practitioners in selecting appropriate relevance metrics and also aid further researches for designing better relevance metrics for explanations.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We investigated empirically which of the relevance metrics (e.g. similarity of hidden layer, influence function, etc.) are appropriate for similarity-based explanation.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="wXgk_iCiYGo" data-number="3008">
      <h4>
        <a href="https://openreview.net/forum?id=wXgk_iCiYGo">
            A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima
        </a>
      
        
          <a href="https://openreview.net/pdf?id=wXgk_iCiYGo" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zeke_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zeke_Xie1">Zeke Xie</a>, <a href="https://openreview.net/profile?id=~Issei_Sato1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Issei_Sato1">Issei Sato</a>, <a href="https://openreview.net/profile?id=~Masashi_Sugiyama1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Masashi_Sugiyama1">Masashi Sugiyama</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#wXgk_iCiYGo-details-255" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wXgk_iCiYGo-details-255"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning dynamics, SGD, diffusion, flat minima, stochastic optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Stochastic Gradient Descent (SGD) and its variants are mainstream methods for training deep networks in practice. SGD is known to find a flat minimum that often generalizes well. However, it is mathematically unclear how deep learning can select a flat minimum among so many minima. To answer the question quantitatively, we develop a density diffusion theory (DDT) to reveal how minima selection quantitatively depends on the minima sharpness and the hyperparameters. To the best of our knowledge, we are the first to theoretically and empirically prove that, benefited from the Hessian-dependent covariance of stochastic gradient noise, SGD favors flat minima exponentially more than sharp minima, while Gradient Descent (GD) with injected white noise favors flat minima only polynomially more than sharp minima. We also reveal that either a small learning rate or large-batch training requires exponentially many iterations to escape from minima in terms of the ratio of the batch size and learning rate. Thus, large-batch training cannot search flat minima efficiently in a realistic computational time.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We prove that, benefited from the Hessian-dependent covariance of stochastic gradient noise, SGD favors flat minima exponentially more than sharp minima.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="fgd7we_uZa6" data-number="2783">
      <h4>
        <a href="https://openreview.net/forum?id=fgd7we_uZa6">
            How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=fgd7we_uZa6" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zixiang_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zixiang_Chen1">Zixiang Chen</a>, <a href="https://openreview.net/profile?id=~Yuan_Cao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuan_Cao1">Yuan Cao</a>, <a href="https://openreview.net/profile?id=~Difan_Zou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Difan_Zou1">Difan Zou</a>, <a href="https://openreview.net/profile?id=~Quanquan_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Quanquan_Gu1">Quanquan Gu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#fgd7we_uZa6-details-20" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fgd7we_uZa6-details-20"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep ReLU networks, neural tangent kernel, (stochastic) gradient descent, generalization error, classification</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A recent line of research on deep learning focuses on the extremely over-parameterized setting, and shows that when the network width is larger than a high degree polynomial of the training sample size <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="53" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container> and the inverse of the target error <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="54" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>ϵ</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container>, deep neural networks learned by (stochastic) gradient descent enjoy nice optimization and generalization guarantees. Very recently, it is shown that under certain margin assumptions on the training data, a polylogarithmic width condition suffices for two-layer ReLU networks to converge and generalize (Ji and Telgarsky, 2020). However, whether deep neural networks can be learned with such a mild over-parameterization is still an open question. In this work, we answer this question affirmatively and establish sharper learning guarantees for deep ReLU networks trained by (stochastic) gradient descent. In specific, under certain assumptions made in previous work, our optimization and generalization guarantees hold with network width polylogarithmic in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="55" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="56" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>ϵ</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container>. Our results push the study of over-parameterized deep neural networks towards more practical settings.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We establish learning guarantees for deep ReLU networks with width polylogarithmic in sample size and the inverse of the target error.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="YHdeAO61l6T" data-number="2740">
      <h4>
        <a href="https://openreview.net/forum?id=YHdeAO61l6T">
            Auction Learning as a Two-Player Game
        </a>
      
        
          <a href="https://openreview.net/pdf?id=YHdeAO61l6T" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jad_Rahme1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jad_Rahme1">Jad Rahme</a>, <a href="https://openreview.net/profile?id=~Samy_Jelassi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samy_Jelassi1">Samy Jelassi</a>, <a href="https://openreview.net/profile?id=~S._Matthew_Weinberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~S._Matthew_Weinberg1">S. Matthew Weinberg</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#YHdeAO61l6T-details-457" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YHdeAO61l6T-details-457"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Mechanism Design, Auction Theory, Game Theory, Deep Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Designing an incentive compatible auction that maximizes expected revenue is a central problem in Auction Design. While theoretical approaches to the problem have hit some limits, a recent research direction initiated by Duetting et al. (2019) consists in building neural network architectures to find optimal auctions.  We propose two conceptual deviations from their approach which result in enhanced performance. First, we use recent results in theoretical auction design to introduce a time-independent Lagrangian.  This not only circumvents the need for an expensive hyper-parameter search (as in prior work), but also provides a single metric to compare the performance of two auctions (absent from prior work). Second, the optimization procedure in previous work uses an inner maximization loop to compute optimal misreports. We amortize this process through the introduction of an additional neural network. We demonstrate the effectiveness of our approach by learning competitive or strictly improved auctions compared to prior work. Both results together further imply a novel formulation of Auction Design as a two-player game with stationary utility functions.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We formulate the auction learning problem as a two player game with a stationary utility functions and explore the advantages of such an approach.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=YHdeAO61l6T&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="sCZbhBvqQaU" data-number="3741">
      <h4>
        <a href="https://openreview.net/forum?id=sCZbhBvqQaU">
            Robust Reinforcement Learning on State Observations with Learned Optimal Adversary
        </a>
      
        
          <a href="https://openreview.net/pdf?id=sCZbhBvqQaU" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Huan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huan_Zhang1">Huan Zhang</a>, <a href="https://openreview.net/profile?id=~Hongge_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongge_Chen1">Hongge Chen</a>, <a href="https://openreview.net/profile?id=~Duane_S_Boning1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Duane_S_Boning1">Duane S Boning</a>, <a href="https://openreview.net/profile?id=~Cho-Jui_Hsieh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cho-Jui_Hsieh1">Cho-Jui Hsieh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#sCZbhBvqQaU-details-167" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sCZbhBvqQaU-details-167"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, robustness, adversarial attacks, adversarial defense</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the robustness of reinforcement learning (RL) with adversarially perturbed state observations, which aligns with the setting of many adversarial attacks to deep reinforcement learning (DRL) and is also important for rolling out real-world RL agent under unpredictable sensing noise. With a fixed agent policy, we demonstrate that an optimal adversary to perturb state observations can be found, which is guaranteed to obtain the worst case agent reward. For DRL settings, this leads to a novel empirical adversarial attack to RL agents via a learned adversary that is much stronger than previous ones. To enhance the robustness of an agent, we propose a framework of alternating training with learned adversaries (ATLA), which trains an adversary online together with the agent using policy gradient following the optimal adversarial attack framework. Additionally, inspired by the analysis of state-adversarial Markov decision process (SA-MDP), we show that past states and actions (history) can be useful for learning a robust agent, and we empirically find a LSTM based policy can be more robust under adversaries. Empirical evaluations on a few continuous control environments show that ATLA achieves state-of-the-art performance under strong adversaries. Our code is available at https://github.com/huanzhang12/ATLA_robust_RL.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We study the robustness of RL agents under perturbations on states and find that using an "optimal" adversary learned online in an alternating training manner can improve the robustness of agent policy.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-6vS_4Kfz0" data-number="3688">
      <h4>
        <a href="https://openreview.net/forum?id=-6vS_4Kfz0">
            Optimizing Memory Placement using Evolutionary Graph Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-6vS_4Kfz0" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Shauharda_Khadka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shauharda_Khadka1">Shauharda Khadka</a>, <a href="https://openreview.net/profile?email=estelle.aflalo%40intel.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="estelle.aflalo@intel.com">Estelle Aflalo</a>, <a href="https://openreview.net/profile?email=mattias.marder%40intel.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mattias.marder@intel.com">Mattias Mardar</a>, <a href="https://openreview.net/profile?email=avrech%40campus.technion.ac.il" class="profile-link" data-toggle="tooltip" data-placement="top" title="avrech@campus.technion.ac.il">Avrech Ben-David</a>, <a href="https://openreview.net/profile?id=~Santiago_Miret1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Santiago_Miret1">Santiago Miret</a>, <a href="https://openreview.net/profile?id=~Shie_Mannor2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shie_Mannor2">Shie Mannor</a>, <a href="https://openreview.net/profile?id=~Tamir_Hazan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tamir_Hazan1">Tamir Hazan</a>, <a href="https://openreview.net/profile?id=~Hanlin_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hanlin_Tang1">Hanlin Tang</a>, <a href="https://openreview.net/profile?id=~Somdeb_Majumdar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Somdeb_Majumdar1">Somdeb Majumdar</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-6vS_4Kfz0-details-685" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-6vS_4Kfz0-details-685"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement Learning, Memory Mapping, Device Placement, Evolutionary Algorithms</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">For deep neural network accelerators, memory movement is both energetically expensive and can bound computation. Therefore, optimal mapping of tensors to memory hierarchies is critical to performance. The growing complexity of neural networks calls for automated memory mapping instead of manual heuristic approaches; yet the search space of neural network computational graphs have previously been prohibitively large. We introduce Evolutionary Graph Reinforcement Learning (EGRL), a method designed for large search spaces, that combines graph neural networks, reinforcement learning, and evolutionary search. A set of fast, stateless policies guide the evolutionary search to improve its sample-efficiency. We train and validate our approach directly on the Intel NNP-I chip for inference. EGRL outperforms policy-gradient, evolutionary search and dynamic programming baselines on BERT, ResNet-101 and ResNet-50. We additionally achieve 28-78% speed-up compared to the native NNP-I compiler on all three workloads.  </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We combine evolutionary and gradient-based reinforcement learning to tackle the large search spaces needed to map tensors to memory, yielding up to 78% speedup on BERT and ResNet on a deep learning inference chip.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=-6vS_4Kfz0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="TK_6nNb_C7q" data-number="2103">
      <h4>
        <a href="https://openreview.net/forum?id=TK_6nNb_C7q">
            Hierarchical Autoregressive Modeling for Neural Video Compression
        </a>
      
        
          <a href="https://openreview.net/pdf?id=TK_6nNb_C7q" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ruihan_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruihan_Yang1">Ruihan Yang</a>, <a href="https://openreview.net/profile?id=~Yibo_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yibo_Yang1">Yibo Yang</a>, <a href="https://openreview.net/profile?id=~Joseph_Marino1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joseph_Marino1">Joseph Marino</a>, <a href="https://openreview.net/profile?id=~Stephan_Mandt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephan_Mandt1">Stephan Mandt</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#TK_6nNb_C7q-details-623" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TK_6nNb_C7q-details-623"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Compression, Video Compression, Generative Models, Autoregressive Models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent work by Marino et al. (2020) showed improved performance in sequential density estimation by combining masked autoregressive flows with hierarchical latent variable models. We draw a connection between such autoregressive generative models and the task of lossy video compression. Specifically, we view recent neural video compression methods (Lu et al., 2019; Yang et al., 2020b; Agustssonet al., 2020) as instances of a generalized stochastic temporal autoregressive transform, and propose avenues for enhancement based on this insight. Comprehensive evaluations on large-scale video data show improved rate-distortion performance over both state-of-the-art neural and conventional video compression methods.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="71zCSP_HuBN" data-number="2577">
      <h4>
        <a href="https://openreview.net/forum?id=71zCSP_HuBN">
            Individually Fair Rankings
        </a>
      
        
          <a href="https://openreview.net/pdf?id=71zCSP_HuBN" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Amanda_Bower1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amanda_Bower1">Amanda Bower</a>, <a href="https://openreview.net/profile?email=hamidef%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="hamidef@umich.edu">Hamid Eftekhari</a>, <a href="https://openreview.net/profile?id=~Mikhail_Yurochkin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mikhail_Yurochkin1">Mikhail Yurochkin</a>, <a href="https://openreview.net/profile?id=~Yuekai_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuekai_Sun1">Yuekai Sun</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#71zCSP_HuBN-details-396" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="71zCSP_HuBN-details-396"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">algorithmic fairness, learning to rank, optimal transport</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure. The crux of our method is an optimal transport-based regularizer that enforces individual fairness and an efficient algorithm for optimizing the regularizer. We show that our approach leads to certifiably individually fair LTR models and demonstrate the efficacy of our method on ranking tasks subject to demographic biases.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present an algorithm for training individually fair learning-to-rank systems using optimal transport tools.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=71zCSP_HuBN&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="pAbm1qfheGk" data-number="1699">
      <h4>
        <a href="https://openreview.net/forum?id=pAbm1qfheGk">
            Learning Neural Generative Dynamics for Molecular Conformation Generation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=pAbm1qfheGk" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Minkai_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minkai_Xu1">Minkai Xu</a>, <a href="https://openreview.net/profile?id=~Shitong_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shitong_Luo1">Shitong Luo</a>, <a href="https://openreview.net/profile?id=~Yoshua_Bengio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoshua_Bengio1">Yoshua Bengio</a>, <a href="https://openreview.net/profile?id=~Jian_Peng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jian_Peng1">Jian Peng</a>, <a href="https://openreview.net/profile?id=~Jian_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jian_Tang1">Jian Tang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 28 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#pAbm1qfheGk-details-650" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pAbm1qfheGk-details-650"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Molecular conformation generation, deep generative models, continuous normalizing flow, energy-based models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study how to generate molecule conformations (i.e., 3D structures) from a molecular graph. Traditional methods, such as molecular dynamics, sample conformations via computationally expensive simulations. Recently, machine learning methods have shown great potential by training on a large collection of conformation data. Challenges arise from the limited model capacity for capturing complex distributions of conformations and the difficulty in modeling long-range dependencies between atoms. Inspired by the recent progress in deep generative models, in this paper, we propose a novel probabilistic framework to generate valid and diverse conformations given a molecular graph. We propose a method combining the advantages of both flow-based and energy-based models, enjoying: (1) a high model capacity to estimate the multimodal conformation distribution; (2) explicitly capturing the complex long-range dependencies between atoms in the observation space. Extensive experiments demonstrate the superior performance of the proposed method on several benchmarks, including conformation generation and distance modeling tasks, with a significant improvement over existing generative models for molecular conformation sampling.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A novel probabilistic framework to generate valid and diverse molecular conformations. Reaching state-of-the-art results on conformation generation and inter-atomic distance modeling.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=pAbm1qfheGk&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="hr-3PMvDpil" data-number="426">
      <h4>
        <a href="https://openreview.net/forum?id=hr-3PMvDpil">
            Efficient Certified Defenses Against Patch Attacks on Image Classifiers
        </a>
      
        
          <a href="https://openreview.net/pdf?id=hr-3PMvDpil" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jan_Hendrik_Metzen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jan_Hendrik_Metzen1">Jan Hendrik Metzen</a>, <a href="https://openreview.net/profile?id=~Maksym_Yatsura1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maksym_Yatsura1">Maksym Yatsura</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#hr-3PMvDpil-details-261" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hr-3PMvDpil-details-261"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">robustness, certified defense, adversarial patch, aversarial examples</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with efficient inference while maintaining high performance on clean inputs. We propose BagCert, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end optimization of certified robustness against patches of different sizes and locations. On CIFAR10, BagCert certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5x5 patches.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a method for certifying robustness against adversarial patches that combines high certified accuracy with efficient inference while maintaining strong performance on clean data.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="VErQxgyrbfn" data-number="2928">
      <h4>
        <a href="https://openreview.net/forum?id=VErQxgyrbfn">
            Convex Regularization behind Neural Reconstruction
        </a>
      
        
          <a href="https://openreview.net/pdf?id=VErQxgyrbfn" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Arda_Sahiner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arda_Sahiner1">Arda Sahiner</a>, <a href="https://openreview.net/profile?id=~Morteza_Mardani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Morteza_Mardani1">Morteza Mardani</a>, <a href="https://openreview.net/profile?email=ozt%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ozt@stanford.edu">Batu Ozturkler</a>, <a href="https://openreview.net/profile?id=~Mert_Pilanci3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mert_Pilanci3">Mert Pilanci</a>, <a href="https://openreview.net/profile?id=~John_M._Pauly1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_M._Pauly1">John M. Pauly</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#VErQxgyrbfn-details-184" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="VErQxgyrbfn-details-184"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">neural networks, image reconstruction, denoising, interpretability, robustness, neural reconstruction, convex duality, inverse problems, sparsity, convex optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Neural networks have shown tremendous potential for reconstructing high-resolution images in inverse problems. The non-convex and opaque nature of neural networks, however, hinders their utility in sensitive applications such as medical imaging. To cope with this challenge, this paper advocates a convex duality framework that makes a two-layer fully-convolutional ReLU denoising network amenable to convex optimization. The convex dual network not only offers the optimum training with convex solvers, but also facilitates interpreting training and prediction. In particular, it implies training neural networks with weight decay regularization induces path sparsity while the prediction is piecewise linear filtering. A range of experiments with MNIST and fastMRI datasets confirm the efficacy of the dual network optimization problem. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This work proposes a finite-dimensional convex dual of a two-layer fully-convolutional ReLU network for denoising problems, and uses it for interpretation of neural network training and predictions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=VErQxgyrbfn&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="iKQAk8a2kM0" data-number="289">
      <h4>
        <a href="https://openreview.net/forum?id=iKQAk8a2kM0">
            Targeted Attack against Deep Neural Networks via Flipping Limited Weight Bits
        </a>
      
        
          <a href="https://openreview.net/pdf?id=iKQAk8a2kM0" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jiawang_Bai2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiawang_Bai2">Jiawang Bai</a>, <a href="https://openreview.net/profile?id=~Baoyuan_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Baoyuan_Wu1">Baoyuan Wu</a>, <a href="https://openreview.net/profile?id=~Yong_Zhang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yong_Zhang6">Yong Zhang</a>, <a href="https://openreview.net/profile?id=~Yiming_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yiming_Li1">Yiming Li</a>, <a href="https://openreview.net/profile?id=~Zhifeng_Li5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhifeng_Li5">Zhifeng Li</a>, <a href="https://openreview.net/profile?id=~Shu-Tao_Xia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shu-Tao_Xia1">Shu-Tao Xia</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 21 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#iKQAk8a2kM0-details-77" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iKQAk8a2kM0-details-77"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">targeted attack, bit-flip, weight attack</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">To explore the vulnerability of deep neural networks (DNNs), many attack paradigms have been well studied, such as the poisoning-based backdoor attack in the training stage and the adversarial attack in the inference stage. In this paper, we study a novel attack paradigm, which modifies model parameters in the deployment stage for malicious purposes. Specifically, our goal is to misclassify a specific sample into a target class without any sample modification, while not significantly reduce the prediction accuracy of other samples to ensure the stealthiness. To this end, we formulate this problem as a binary integer programming (BIP), since the parameters are stored as binary bits (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="57" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi><mo>.</mo><mi>e</mi><mo>.</mo></math></mjx-assistive-mml></mjx-container>, 0 and 1) in the memory. By utilizing the latest technique in integer programming, we equivalently reformulate this BIP problem as a continuous optimization problem, which can be effectively and efficiently solved using the alternating direction method of multipliers (ADMM) method. Consequently, the flipped critical bits can be easily determined through optimization, rather than using a heuristic strategy. Extensive experiments demonstrate the superiority of our method in attacking DNNs.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a targeted attack method against the deployed DNN via flipping a few binary weight bits.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=iKQAk8a2kM0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="5Y21V0RDBV" data-number="3332">
      <h4>
        <a href="https://openreview.net/forum?id=5Y21V0RDBV">
            Generalized Multimodal ELBO
        </a>
      
        
          <a href="https://openreview.net/pdf?id=5Y21V0RDBV" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Thomas_Marco_Sutter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Marco_Sutter1">Thomas Marco Sutter</a>, <a href="https://openreview.net/profile?id=~Imant_Daunhawer2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Imant_Daunhawer2">Imant Daunhawer</a>, <a href="https://openreview.net/profile?id=~Julia_E_Vogt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Julia_E_Vogt1">Julia E Vogt</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#5Y21V0RDBV-details-239" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5Y21V0RDBV-details-239"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multimodal, VAE, ELBO, self-supervised, generative learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Multiple data types naturally co-occur when describing real-world phenomena and learning from them is a long-standing goal in machine learning research. However, existing self-supervised generative models approximating an ELBO are not able to fulfill all desired requirements of multimodal models: their posterior approximation functions lead to a trade-off between the semantic coherence and the ability to learn the joint data distribution. We propose a new, generalized ELBO formulation for multimodal data that overcomes these limitations. The new objective encompasses two previous methods as special cases and combines their benefits without compromises. In extensive experiments, we demonstrate the advantage of the proposed method compared to state-of-the-art models in self-supervised, generative learning tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a generalized ELBO for modeling multiple data types in a scalable and self-supervised way.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=5Y21V0RDBV&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="0aW6lYOYB7d" data-number="1511">
      <h4>
        <a href="https://openreview.net/forum?id=0aW6lYOYB7d">
            Large-width functional asymptotics for deep Gaussian neural networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=0aW6lYOYB7d" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=daniele.bracale%40edu.unito.it" class="profile-link" data-toggle="tooltip" data-placement="top" title="daniele.bracale@edu.unito.it">Daniele Bracale</a>, <a href="https://openreview.net/profile?id=~Stefano_Favaro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefano_Favaro1">Stefano Favaro</a>, <a href="https://openreview.net/profile?email=sandra.fortini%40unibocconi.it" class="profile-link" data-toggle="tooltip" data-placement="top" title="sandra.fortini@unibocconi.it">Sandra Fortini</a>, <a href="https://openreview.net/profile?id=~Stefano_Peluchetti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefano_Peluchetti1">Stefano Peluchetti</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 20 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#0aW6lYOYB7d-details-18" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0aW6lYOYB7d-details-18"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning theory, infinitely wide neural network, Gaussian process, stochastic process</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we consider fully connected feed-forward deep neural networks where weights and biases are independent and identically distributed according to Gaussian distributions. Extending previous results (Matthews et al., 2018a;b;Yang, 2019)  we adopt a function-space perspective, i.e. we look at neural networks as infinite-dimensional random elements on the input space <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="58" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.41em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>I</mi></msup></math></mjx-assistive-mml></mjx-container>. Under suitable assumptions on the activation function we show that: i) a network defines a continuous Gaussian process on the input space <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="59" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.41em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>I</mi></msup></math></mjx-assistive-mml></mjx-container>; ii) a network with re-scaled weights converges weakly to a continuous Gaussian process in the large-width limit; iii) the limiting Gaussian process has almost surely locally <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="60" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FE TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>γ</mi></math></mjx-assistive-mml></mjx-container>-Hölder continuous paths, for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="61" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D6FE TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0</mn><mo>&lt;</mo><mi>γ</mi><mo>&lt;</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container>. Our results contribute to recent theoretical studies on the interplay between infinitely wide deep neural networks and Gaussian processes by establishing weak convergence in function-space with respect to a stronger metric.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We establish the convergence of infinitely wide feed-forward deep neural networks in function space.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=0aW6lYOYB7d&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="H8UHdhWG6A3" data-number="135">
      <h4>
        <a href="https://openreview.net/forum?id=H8UHdhWG6A3">
            Distributed Momentum for Byzantine-resilient Stochastic Gradient Descent
        </a>
      
        
          <a href="https://openreview.net/pdf?id=H8UHdhWG6A3" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=el-mahdi.el-mhamdi%40polytechnique.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="el-mahdi.el-mhamdi@polytechnique.edu">El Mahdi El Mhamdi</a>, <a href="https://openreview.net/profile?id=~Rachid_Guerraoui1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rachid_Guerraoui1">Rachid Guerraoui</a>, <a href="https://openreview.net/profile?id=~S%C3%A9bastien_Rouault1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sébastien_Rouault1">Sébastien Rouault</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#H8UHdhWG6A3-details-869" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="H8UHdhWG6A3-details-869"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Byzantine SGD, Distributed ML, Momentum</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Byzantine-resilient Stochastic Gradient Descent (SGD) aims at shielding model training from Byzantine faults, be they ill-labeled training datapoints, exploited software/hardware vulnerabilities, or malicious worker nodes in a distributed setting.
      Two recent attacks have been challenging state-of-the-art defenses though, often successfully precluding the model from even fitting the training set.
      The main identified weakness in current defenses is their requirement of a sufficiently low variance-norm ratio for the stochastic gradients.
      We propose a practical method which, despite increasing the variance, reduces the variance-norm ratio, mitigating the identified weakness.
      We assess the effectiveness of our method over 736 different training configurations, comprising the 2 state-of-the-art attacks and 6 defenses.
      For confidence and reproducibility purposes, each configuration is run 5 times with specified seeds (1 to 5), totalling 3680 runs.
      In our experiments, when the attack is effective enough to decrease the highest observed top-1 cross-accuracy by at least 20% compared to the unattacked run, our technique systematically increases back the highest observed accuracy, and is able to recover at least 20% in more than 60% of the cases.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">An inexpensive method to substantially improve the effectiveness of existing Byzantine-resilient SGD defenses, assessed against state-of-the-art attacks and supported by theoretical insights.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=H8UHdhWG6A3&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="agHLCOBM5jP" data-number="3284">
      <h4>
        <a href="https://openreview.net/forum?id=agHLCOBM5jP">
            Fully Unsupervised Diversity Denoising with Convolutional Variational Autoencoders
        </a>
      
        
          <a href="https://openreview.net/pdf?id=agHLCOBM5jP" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mangal_Prakash1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mangal_Prakash1">Mangal Prakash</a>, <a href="https://openreview.net/profile?id=~Alexander_Krull3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Krull3">Alexander Krull</a>, <a href="https://openreview.net/profile?id=~Florian_Jug1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Jug1">Florian Jug</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#agHLCOBM5jP-details-439" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="agHLCOBM5jP-details-439"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Diversity denoising, Unsupervised denoising, Variational Autoencoders, Noise model</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DivNoising, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DivNoising predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DivNoising samples from the posterior enable a plethora of useful applications. We are (i) showing denoising results for 13 datasets, (ii) discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are (iii) demonstrating how instance cell segmentation improves when using diverse DivNoising predictions.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">DivNoising performs fully unsupervised diversity denoising using fully convolutional variational autoencoders and achieves SOTA results for a number of well known datasets while also enabling VAE-like sampling</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="n7wIfYPdVet" data-number="1149">
      <h4>
        <a href="https://openreview.net/forum?id=n7wIfYPdVet">
            Auxiliary Learning by Implicit Differentiation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=n7wIfYPdVet" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Aviv_Navon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aviv_Navon1">Aviv Navon</a>, <a href="https://openreview.net/profile?id=~Idan_Achituve1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Idan_Achituve1">Idan Achituve</a>, <a href="https://openreview.net/profile?id=~Haggai_Maron1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haggai_Maron1">Haggai Maron</a>, <a href="https://openreview.net/profile?id=~Gal_Chechik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gal_Chechik1">Gal Chechik</a>, <a href="https://openreview.net/profile?id=~Ethan_Fetaya1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ethan_Fetaya1">Ethan Fetaya</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#n7wIfYPdVet-details-400" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="n7wIfYPdVet-details-400"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Auxiliary Learning, Multi-task Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Training neural networks with auxiliary tasks is a common practice for improving the performance on a main task of interest.
      Two main challenges arise in this multi-task learning setting: (i) designing useful auxiliary tasks; and (ii) combining auxiliary tasks into a single coherent loss. Here, we propose a novel framework, AuxiLearn, that targets both challenges based on implicit differentiation. First, when useful auxiliaries are known, we propose learning a network that combines all losses into a single coherent objective function. This network can learn non-linear interactions between tasks. Second, when no useful auxiliary task is known, we describe how to learn a network that generates a meaningful, novel auxiliary task. We evaluate AuxiLearn in a series of tasks and domains, including image segmentation and learning with attributes in the low data regime, and find that it consistently outperforms competing methods.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Learn to combine auxiliary tasks in a nonlinear fashion and to design them automatically.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=n7wIfYPdVet&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="TQt98Ya7UMP" data-number="3232">
      <h4>
        <a href="https://openreview.net/forum?id=TQt98Ya7UMP">
            Balancing Constraints and Rewards with Meta-Gradient D4PG
        </a>
      
        
          <a href="https://openreview.net/pdf?id=TQt98Ya7UMP" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=dancalian%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dancalian@google.com">Dan A. Calian</a>, <a href="https://openreview.net/profile?id=~Daniel_J_Mankowitz2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_J_Mankowitz2">Daniel J Mankowitz</a>, <a href="https://openreview.net/profile?id=~Tom_Zahavy2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tom_Zahavy2">Tom Zahavy</a>, <a href="https://openreview.net/profile?id=~Zhongwen_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhongwen_Xu1">Zhongwen Xu</a>, <a href="https://openreview.net/profile?id=~Junhyuk_Oh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junhyuk_Oh2">Junhyuk Oh</a>, <a href="https://openreview.net/profile?id=~Nir_Levine2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nir_Levine2">Nir Levine</a>, <a href="https://openreview.net/profile?id=~Timothy_Mann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Timothy_Mann1">Timothy Mann</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#TQt98Ya7UMP-details-419" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TQt98Ya7UMP-details-419"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, meta-gradients, constraints</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deploying Reinforcement Learning (RL) agents to solve real-world applications often requires satisfying complex system constraints. Often the constraint thresholds are incorrectly set due to the complex nature of a system or the inability to verify the thresholds offline (e.g, no simulator or reasonable offline evaluation procedure exists). This results in solutions where a task cannot be solved without violating the constraints. However, in many real-world cases, constraint violations are undesirable yet they are not catastrophic, motivating the need for soft-constrained RL approaches. We present two soft-constrained RL approaches that utilize meta-gradients to find a good trade-off between expected return and minimizing constraint violations. We demonstrate the effectiveness of these approaches by showing that they consistently outperform the baselines across four different Mujoco domains.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper uses meta-gradients to perform soft-constrained Reinforcement Learning (RL) optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=TQt98Ya7UMP&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="_mQp5cr_iNy" data-number="3504">
      <h4>
        <a href="https://openreview.net/forum?id=_mQp5cr_iNy">
            Adversarially Guided Actor-Critic
        </a>
      
        
          <a href="https://openreview.net/pdf?id=_mQp5cr_iNy" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yannis_Flet-Berliac1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yannis_Flet-Berliac1">Yannis Flet-Berliac</a>, <a href="https://openreview.net/profile?id=~Johan_Ferret1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Johan_Ferret1">Johan Ferret</a>, <a href="https://openreview.net/profile?id=~Olivier_Pietquin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Olivier_Pietquin1">Olivier Pietquin</a>, <a href="https://openreview.net/profile?id=~Philippe_Preux1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philippe_Preux1">Philippe Preux</a>, <a href="https://openreview.net/profile?id=~Matthieu_Geist1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthieu_Geist1">Matthieu Geist</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#_mQp5cr_iNy-details-773" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_mQp5cr_iNy-details-773"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Despite definite success in deep reinforcement learning problems, actor-critic algorithms are still confronted with sample inefficiency in complex environments, particularly in tasks where efficient exploration is a bottleneck. These methods consider a policy (the actor) and a value function (the critic) whose respective losses are built using different motivations and approaches. This paper introduces a third protagonist: the adversary. While the adversary mimics the actor by minimizing the KL-divergence between their respective action distributions, the actor, in addition to learning to solve the task, tries to differentiate itself from the adversary predictions. This novel objective stimulates the actor to follow strategies that could not have been correctly predicted from previous trajectories, making its behavior innovative in tasks where the reward is extremely rare. Our experimental analysis shows that the resulting Adversarially Guided Actor-Critic (AGAC) algorithm leads to more exhaustive exploration. Notably, AGAC outperforms current state-of-the-art methods on a set of various hard-exploration and procedurally-generated tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This method introduces an adversary to the actor-critic framework that drives the agent to explore more efficiently in hard-exploration tasks.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="KLH36ELmwIB" data-number="52">
      <h4>
        <a href="https://openreview.net/forum?id=KLH36ELmwIB">
            DARTS-: Robustly Stepping out of Performance Collapse Without Indicators
        </a>
      
        
          <a href="https://openreview.net/pdf?id=KLH36ELmwIB" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xiangxiang_Chu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiangxiang_Chu1">Xiangxiang Chu</a>, <a href="https://openreview.net/profile?email=figure1_wxx%40sjtu.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="figure1_wxx@sjtu.edu.cn">Xiaoxing Wang</a>, <a href="https://openreview.net/profile?id=~Bo_Zhang7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Zhang7">Bo Zhang</a>, <a href="https://openreview.net/profile?id=~Shun_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shun_Lu1">Shun Lu</a>, <a href="https://openreview.net/profile?email=weixiaolin02%40meituan.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="weixiaolin02@meituan.com">Xiaolin Wei</a>, <a href="https://openreview.net/profile?id=~Junchi_Yan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junchi_Yan2">Junchi Yan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#KLH36ELmwIB-details-737" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KLH36ELmwIB-details-737"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">neural architecture search, DARTS stability</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Despite the fast development of differentiable architecture search (DARTS), it suffers from a standing instability issue regarding searching performance, which extremely limits its application. Existing robustifying methods draw clues from the outcome instead of finding out the causing factor. Various indicators such as Hessian eigenvalues are proposed as a signal of performance collapse, and the searching should be stopped once an indicator reaches a preset threshold.
      However, these methods tend to easily reject good architectures if thresholds are inappropriately set, let alone the searching is intrinsically noisy. In this paper, we undertake a more subtle and direct approach to resolve the collapse. 
      We first demonstrate that skip connections with a learnable architectural coefficient can easily recover from a disadvantageous state and become dominant.  We conjecture that skip connections profit too much from this privilege, hence causing the collapse for the derived model. Therefore, we propose to factor out this benefit with an auxiliary skip connection, ensuring a fairer competition for all operations. Extensive experiments on various datasets verify that our approach can substantially improve the robustness of DARTS. Our code is available at https://github.com/Meituan-AutoML/DARTS-</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Indicator-free approach to stabilize DARTS</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=KLH36ELmwIB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="_zx8Oka09eF" data-number="2468">
      <h4>
        <a href="https://openreview.net/forum?id=_zx8Oka09eF">
            Are wider nets better given the same number of parameters?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=_zx8Oka09eF" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Anna_Golubeva1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anna_Golubeva1">Anna Golubeva</a>, <a href="https://openreview.net/profile?id=~Guy_Gur-Ari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guy_Gur-Ari1">Guy Gur-Ari</a>, <a href="https://openreview.net/profile?id=~Behnam_Neyshabur1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Behnam_Neyshabur1">Behnam Neyshabur</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#_zx8Oka09eF-details-809" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_zx8Oka09eF-details-809"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">network width, over-parametrization, understanding deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Empirical studies demonstrate that the performance of neural networks improves with increasing number of parameters. In most of these studies, the number of parameters is increased by increasing the network width. This begs the question: Is the observed improvement due to the larger number of parameters, or is it due to the larger width itself? We compare different ways of increasing model width while keeping the number of parameters constant. We show that for models initialized with a random, static sparsity pattern in the weight tensors, network width is the determining factor for good performance, while the number of weights is secondary, as long as the model achieves high training accuarcy. As a step towards understanding this effect, we analyze these models in the framework of Gaussian Process kernels. We find that the distance between the sparse finite-width model kernel and the infinite-width kernel at initialization is indicative of model performance.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that increasing network width leads to better performance even when the number of weights remains fixed.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=_zx8Oka09eF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="FZ1oTwcXchK" data-number="484">
      <h4>
        <a href="https://openreview.net/forum?id=FZ1oTwcXchK">
            Optimal Conversion of Conventional Artificial Neural Networks to Spiking Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=FZ1oTwcXchK" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Shikuang_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shikuang_Deng1">Shikuang Deng</a>, <a href="https://openreview.net/profile?id=~Shi_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shi_Gu1">Shi Gu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#FZ1oTwcXchK-details-613" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FZ1oTwcXchK-details-613"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">spiking neural network, weight balance, second-order approximation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Spiking neural networks (SNNs) are biology-inspired artificial neural networks (ANNs) that comprise of spiking neurons to process asynchronous discrete signals. While more efficient in power consumption and inference speed on the neuromorphic hardware, SNNs are usually difficult to train directly from scratch with spikes due to the discreteness. As an alternative, many efforts have been devoted to converting conventional ANNs into SNNs by copying the weights from ANNs and adjusting the spiking threshold potential of neurons in SNNs. Researchers have designed new SNN architectures and conversion algorithms to diminish the conversion error. However, an effective conversion should address the difference between the SNN and ANN architectures with an efficient approximation of the loss function, which is missing in the field. In this work, we analyze the conversion error by recursive reduction to layer-wise summation and propose a novel strategic pipeline that transfers the weights to the target SNN by combining threshold balance and soft-reset mechanisms. This pipeline enables almost no accuracy loss between the converted SNNs and conventional ANNs with only <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="62" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∼</mo><mn>1</mn><mrow><mo>/</mo></mrow><mn>10</mn></math></mjx-assistive-mml></mjx-container> of the typical SNN simulation time. Our method is promising to get implanted onto embedded platforms with better support of SNNs with limited energy and memory. Codes are available at https://github.com/Jackn0/snn_optimal_conversion_pipeline.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose and validate an optimal pipeline that efficiently converts conventional artificial neural networks to spiking neural networks with almost no accuracy loss in a fairly short simulation length. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="aDjoksTpXOP" data-number="1365">
      <h4>
        <a href="https://openreview.net/forum?id=aDjoksTpXOP">
            Deep Equals Shallow for ReLU Networks in Kernel Regimes
        </a>
      
        
          <a href="https://openreview.net/pdf?id=aDjoksTpXOP" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alberto_Bietti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alberto_Bietti1">Alberto Bietti</a>, <a href="https://openreview.net/profile?id=~Francis_Bach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Francis_Bach1">Francis Bach</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#aDjoksTpXOP-details-19" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="aDjoksTpXOP-details-19"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning, kernels, approximation, neural tangent kernels</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep networks are often considered to be more expressive than shallow ones in terms of approximation. Indeed, certain functions can be approximated by deep networks provably more efficiently than by shallow ones, however, no tractable algorithms are known for learning such deep models. Separately, a recent line of work has shown that deep networks trained with gradient descent may behave like (tractable) kernel methods in a certain over-parameterized regime, where the kernel is determined by the architecture and initialization, and this paper focuses on approximation for such kernels. We show that for ReLU activations, the kernels derived from deep fully-connected networks have essentially the same approximation properties as their shallow two-layer counterpart, namely the same eigenvalue decay for the corresponding integral operator. This highlights the limitations of the kernel framework for understanding the benefits of such deep architectures. Our main theoretical result relies on characterizing such eigenvalue decays through differentiability properties of the kernel function, which also easily applies to the study of other kernels defined on the sphere.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="uxpzitPEooJ" data-number="522">
      <h4>
        <a href="https://openreview.net/forum?id=uxpzitPEooJ">
            Graph Coarsening with Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=uxpzitPEooJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Chen_Cai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chen_Cai1">Chen Cai</a>, <a href="https://openreview.net/profile?email=wang.6150%40osu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="wang.6150@osu.edu">Dingkang Wang</a>, <a href="https://openreview.net/profile?id=~Yusu_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yusu_Wang1">Yusu Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#uxpzitPEooJ-details-187" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uxpzitPEooJ-details-187"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">graph coarsening, graph neural network, Doubly-weighted Laplace operator</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">As large scale-graphs become increasingly more prevalent, it poses significant computational challenges to process, extract and analyze large graph data. Graph coarsening is one popular technique to reduce the size of a graph while maintaining essential properties. Despite rich graph coarsening literature, there is only limited exploration of data-driven method in the field. In this work, we leverage the recent progress of deep learning on graphs for graph coarsening. We first propose a framework for measuring the quality of coarsening algorithm and show that depending on the goal, we need to carefully choose the Laplace operator on the coarse graph and associated projection/lift operators. Motivated by the observation that the current choice of edge weight for the coarse graph may be sub-optimal, we parametrize the weight assignment map with graph neural networks and train it to improve the coarsening quality in an unsupervised way. Through extensive experiments on both synthetic and real networks, we demonstrate that our method significantly improves common graph coarsening methods under various metrics, reduction ratios, graph sizes, and graph types. It generalizes to graphs of larger size (more than <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="63" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>25</mn><mo>×</mo></math></mjx-assistive-mml></mjx-container> of training graphs), adaptive to different losses (both differentiable and non-differentiable), and scales to much larger graphs than previous work.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We significantly improve the quality of existing graph coarsening algorithms with graph neural network.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tlV90jvZbw" data-number="2406">
      <h4>
        <a href="https://openreview.net/forum?id=tlV90jvZbw">
            Early Stopping in Deep Networks: Double Descent and How to Eliminate it
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tlV90jvZbw" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Reinhard_Heckel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Reinhard_Heckel1">Reinhard Heckel</a>, <a href="https://openreview.net/profile?id=~Fatih_Furkan_Yilmaz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fatih_Furkan_Yilmaz1">Fatih Furkan Yilmaz</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tlV90jvZbw-details-897" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tlV90jvZbw-details-897"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">early stopping, double descent</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Over-parameterized models, such as large deep networks, often exhibit a double descent phenomenon, whereas a function of model size, error first decreases, increases, and decreases at last. This intriguing double descent behavior also occurs as a function of training epochs and has been conjectured to arise because training epochs control the model complexity. In this paper, we show that such epoch-wise double descent occurs for a different reason: It is caused by a superposition of two or more bias-variance tradeoffs that arise because different parts of the network are learned at different epochs, and mitigating this by proper scaling of stepsizes can significantly improve the early stopping performance. We show this analytically for i) linear regression, where differently scaled features give rise to a superposition of bias-variance tradeoffs, and for ii) a wide two-layer neural network, where the first and second layers govern bias-variance tradeoffs. Inspired by this theory, we study two standard convolutional networks empirically and show that eliminating epoch-wise double descent through adjusting stepsizes of different layers improves the early stopping performance.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Epoch wise double descent can be explained as a superposition of two or more bias-variance tradeoffs that arise because different parts of the network are learned at different epochs.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="aGfU_xziEX8" data-number="209">
      <h4>
        <a href="https://openreview.net/forum?id=aGfU_xziEX8">
            Efficient Inference of Flexible Interaction in Spiking-neuron Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=aGfU_xziEX8" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Feng_Zhou9" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Feng_Zhou9">Feng Zhou</a>, <a href="https://openreview.net/profile?email=yixuan.zhang%40uts.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="yixuan.zhang@uts.edu.au">Yixuan Zhang</a>, <a href="https://openreview.net/profile?id=~Jun_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jun_Zhu2">Jun Zhu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#aGfU_xziEX8-details-958" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="aGfU_xziEX8-details-958"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">neural spike train, nonlinear Hawkes process, auxiliary latent variable, conjugacy</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Hawkes process provides an effective statistical framework for analyzing the time-dependent interaction of neuronal spiking activities. Although utilized in many real applications, the classic Hawkes process is incapable of modelling inhibitory interactions among neurons. Instead, the nonlinear Hawkes process allows for a more flexible influence pattern with excitatory or inhibitory interactions. In this paper, three sets of auxiliary latent variables (Polya-Gamma variables, latent marked Poisson processes and sparsity variables) are augmented to make functional connection weights in a Gaussian form, which allows for a simple iterative algorithm with analytical updates. As a result, an efficient expectation-maximization (EM) algorithm is derived to obtain the maximum a posteriori (MAP) estimate. We demonstrate the accuracy and efficiency performance of our algorithm on synthetic and real data. For real neural recordings, we show our algorithm can estimate the temporal dynamics of interaction and reveal the interpretable functional connectivity underlying neural spike trains. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">An efficient conjugate EM algorithm for nonlinear multivariate Hawkes processes based on auxiliary latent variables augmentation. </span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="R2ZlTVPx0Gk" data-number="205">
      <h4>
        <a href="https://openreview.net/forum?id=R2ZlTVPx0Gk">
            DICE: Diversity in Deep Ensembles via Conditional Redundancy Adversarial Estimation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=R2ZlTVPx0Gk" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alexandre_Rame1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexandre_Rame1">Alexandre Rame</a>, <a href="https://openreview.net/profile?id=~Matthieu_Cord1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthieu_Cord1">Matthieu Cord</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 04 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#R2ZlTVPx0Gk-details-10" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="R2ZlTVPx0Gk-details-10"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep Learning, Deep Ensembles, Information Theory, Information Bottleneck, Adversarial Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep ensembles perform better than a single network thanks to the diversity among their members. Recent approaches regularize predictions to increase diversity; however, they also drastically decrease individual members’ performances. In this paper, we argue that learning strategies for deep ensembles need to tackle the trade-off between ensemble diversity and individual accuracies. Motivated by arguments from information theory and leveraging recent advances in neural estimation of conditional mutual information, we introduce a novel training criterion called DICE: it increases diversity by reducing spurious correlations among features. The main idea is that features extracted from pairs of members should only share information useful for target class prediction without being conditionally redundant. Therefore, besides the classification loss with information bottleneck, we adversarially prevent features from being conditionally predictable from each other. We manage to reduce simultaneous errors while protecting class information. We obtain state-of-the-art accuracy results on CIFAR-10/100: for example, an ensemble of 5 networks trained with DICE matches an ensemble of 7 networks trained independently. We further analyze the consequences on calibration, uncertainty estimation, out-of-distribution detection and online co-distillation.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Driven by arguments from information theory, we introduce a new learning strategy for deep ensembles that increases diversity among members: we adversarially prevent features from being conditionally redundant, i.e., predictable from each other.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="KYPz4YsCPj" data-number="2646">
      <h4>
        <a href="https://openreview.net/forum?id=KYPz4YsCPj">
            Inductive Representation Learning in Temporal Networks via Causal Anonymous Walks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=KYPz4YsCPj" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yanbang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanbang_Wang1">Yanbang Wang</a>, <a href="https://openreview.net/profile?email=yenyu%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yenyu@stanford.edu">Yen-Yu Chang</a>, <a href="https://openreview.net/profile?id=~Yunyu_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yunyu_Liu1">Yunyu Liu</a>, <a href="https://openreview.net/profile?id=~Jure_Leskovec1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jure_Leskovec1">Jure Leskovec</a>, <a href="https://openreview.net/profile?id=~Pan_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pan_Li2">Pan Li</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#KYPz4YsCPj-details-590" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KYPz4YsCPj-details-590"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">temporal networks, inductive representation learning, anonymous walk, network motif</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Temporal networks serve as abstractions of many real-world dynamic systems. These networks typically evolve according to certain laws, such as the law of triadic closure, which is universal in social networks. Inductive representation learning of temporal networks should be able to capture such laws and further be applied to systems that follow the same laws but have not been unseen during the training stage. Previous works in this area depend on either network node identities or rich edge attributes and typically fail to extract these laws. Here, we propose {\em Causal Anonymous Walks (CAWs)} to inductively represent a temporal network. CAWs are extracted by temporal random walks and work as automatic retrieval of temporal network motifs to represent network dynamics while avoiding the time-consuming selection and counting of those motifs. CAWs adopt a novel anonymization strategy that replaces node identities with the hitting counts of the nodes based on a set of sampled walks to keep the method inductive, and simultaneously establish the correlation between motifs. We further propose a neural-network model CAW-N to encode CAWs, and pair it with a CAW sampling strategy with constant memory and time cost to support online training and inference. CAW-N is evaluated to predict links over 6 real temporal networks and uniformly outperforms previous SOTA methods by averaged 15\% AUC gain in the inductive setting. CAW-N also outperforms previous methods in 5 out of the 6 networks in the transductive setting.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">The paper proposes Causal Anonymous Walks (CAW) as an effective way to encode the dynamic laws that govern the evolution of temporal networks, which significantly improves inductive representation learning on those networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=KYPz4YsCPj&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="YNnpaAKeCfx" data-number="1462">
      <h4>
        <a href="https://openreview.net/forum?id=YNnpaAKeCfx">
            FairBatch: Batch Selection for Model Fairness
        </a>
      
        
          <a href="https://openreview.net/pdf?id=YNnpaAKeCfx" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yuji_Roh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuji_Roh1">Yuji Roh</a>, <a href="https://openreview.net/profile?id=~Kangwook_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kangwook_Lee1">Kangwook Lee</a>, <a href="https://openreview.net/profile?id=~Steven_Euijong_Whang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_Euijong_Whang1">Steven Euijong Whang</a>, <a href="https://openreview.net/profile?id=~Changho_Suh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changho_Suh1">Changho Suh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#YNnpaAKeCfx-details-249" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YNnpaAKeCfx-details-249"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">model fairness, bilevel optimization, batch selection</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Training a fair machine learning model is essential to prevent demographic disparity. Existing techniques for improving model fairness require broad changes in either data preprocessing or model training, rendering themselves difficult-to-adopt for potentially already complex machine learning systems. We address this problem via the lens of bilevel optimization. While keeping the standard training algorithm as an inner optimizer, we incorporate an outer optimizer so as to equip the inner problem with an additional functionality: Adaptively selecting minibatch sizes for the purpose of improving model fairness. Our batch selection algorithm, which we call FairBatch, implements this optimization and supports prominent fairness measures: equal opportunity, equalized odds, and demographic parity. FairBatch comes with a significant implementation benefit -- it does not require any modification to data preprocessing or model training. For instance, a single-line change of PyTorch code for replacing batch selection part of model training suffices to employ FairBatch. Our experiments conducted both on synthetic and benchmark real data demonstrate that FairBatch can provide such functionalities while achieving comparable (or even greater) performances against the state of the arts.  Furthermore, FairBatch can readily improve fairness of any pre-trained model simply via fine-tuning. It is also compatible with existing batch selection techniques intended for different purposes, such as faster convergence, thus gracefully achieving multiple purposes.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We address model fairness via the lens of bilevel optimization and propose a batch selection algorithm called FairBatch, which is easy to adopt, has state-of-the-art performance, and is compatible with existing batch selection techniques.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=YNnpaAKeCfx&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="QpNz8r_Ri2Y" data-number="1498">
      <h4>
        <a href="https://openreview.net/forum?id=QpNz8r_Ri2Y">
            Representation Balancing Offline Model-based Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=QpNz8r_Ri2Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Byung-Jun_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Byung-Jun_Lee1">Byung-Jun Lee</a>, <a href="https://openreview.net/profile?id=~Jongmin_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jongmin_Lee1">Jongmin Lee</a>, <a href="https://openreview.net/profile?id=~Kee-Eung_Kim4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kee-Eung_Kim4">Kee-Eung Kim</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#QpNz8r_Ri2Y-details-271" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QpNz8r_Ri2Y-details-271"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement Learning, Model-based Reinforcement Learning, Offline Reinforcement Learning, Batch Reinforcement Learning, Off-policy policy evaluation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="iOnhIy-a-0n" data-number="2336">
      <h4>
        <a href="https://openreview.net/forum?id=iOnhIy-a-0n">
            Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC via Variance Reduction
        </a>
      
        
          <a href="https://openreview.net/pdf?id=iOnhIy-a-0n" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Wei_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Deng1">Wei Deng</a>, <a href="https://openreview.net/profile?email=qif%40usc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="qif@usc.edu">Qi Feng</a>, <a href="https://openreview.net/profile?email=georgios.karagiannis%40durham.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="georgios.karagiannis@durham.ac.uk">Georgios P. Karagiannis</a>, <a href="https://openreview.net/profile?id=~Guang_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guang_Lin1">Guang Lin</a>, <a href="https://openreview.net/profile?id=~Faming_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Faming_Liang1">Faming Liang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#iOnhIy-a-0n-details-19" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iOnhIy-a-0n-details-19"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">variance reduction, replica exchange, parallel tempering, stochastic gradient Langevin dynamics, uncertainty quantification, change of measure, generalized Girsanov theorem, Dirichlet form, Markov jump process</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown promise in accelerating the convergence in non-convex learning; however, an excessively large correction for avoiding biases from noisy energy estimators has limited the potential of the acceleration. To address this issue, we study the variance reduction for noisy energy estimators, which promotes much more effective swaps. Theoretically, we provide a non-asymptotic analysis on the exponential convergence for the underlying continuous-time Markov jump process; moreover, we consider a generalized Girsanov theorem which includes the change of Poisson measure to overcome the crude discretization based on the Gr\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="64" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i" noic="true"><mjx-c class="mjx-c57 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">W</mi></mrow><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container>) distance. Numerically, we conduct extensive experiments and obtain state-of-the-art results in optimization and uncertainty estimates for synthetic experiments and image data.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a variance-reduced replica-exchange stochastic gradient Langevin dynamics to reduce the variance of the energy estimators to accelerate the convergence.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=iOnhIy-a-0n&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="E3Ys6a1NTGT" data-number="1484">
      <h4>
        <a href="https://openreview.net/forum?id=E3Ys6a1NTGT">
            The Importance of Pessimism in Fixed-Dataset Policy Optimization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=E3Ys6a1NTGT" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jacob_Buckman2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jacob_Buckman2">Jacob Buckman</a>, <a href="https://openreview.net/profile?email=cgel%40openai.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="cgel@openai.com">Carles Gelada</a>, <a href="https://openreview.net/profile?id=~Marc_G_Bellemare1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marc_G_Bellemare1">Marc G Bellemare</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#E3Ys6a1NTGT-details-603" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="E3Ys6a1NTGT-details-603"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning, reinforcement learning, offline reinforcement learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A unified conceptual and mathematical framework for fixed-dataset policy optimization algorithms, revealing the importance of uncertainty and pessimism.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="gLWj29369lW" data-number="860">
      <h4>
        <a href="https://openreview.net/forum?id=gLWj29369lW">
            Interpreting Knowledge Graph Relation Representation from Word Embeddings
        </a>
      
        
          <a href="https://openreview.net/pdf?id=gLWj29369lW" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Carl_Allen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Carl_Allen1">Carl Allen</a>, <a href="https://openreview.net/profile?id=~Ivana_Balazevic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ivana_Balazevic1">Ivana Balazevic</a>, <a href="https://openreview.net/profile?id=~Timothy_Hospedales1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Timothy_Hospedales1">Timothy Hospedales</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#gLWj29369lW-details-479" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gLWj29369lW-details-479"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">knowledge graphs, word embedding, representation learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Many models learn representations of knowledge graph data by exploiting its low-rank latent structure, encoding known relations between entities and enabling unknown facts to be inferred. To predict whether a relation holds between entities, embeddings are typically compared in the latent space following a relation-specific mapping. Whilst their predictive performance has steadily improved, how such models capture the underlying latent structure of semantic information remains unexplained. Building on recent theoretical understanding of word embeddings, we categorise knowledge graph relations into three types and for each derive explicit requirements of their representations. We show that empirical properties of relation representations and the relative performance of leading knowledge graph representation methods are justified by our analysis.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Interpreting the structure of knowledge graph relation representation using insight from word embeddings.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tL89RnzIiCd" data-number="3489">
      <h4>
        <a href="https://openreview.net/forum?id=tL89RnzIiCd">
            Hopfield Networks is All You Need
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tL89RnzIiCd" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hubert_Ramsauer2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hubert_Ramsauer2">Hubert Ramsauer</a>, <a href="https://openreview.net/profile?id=~Bernhard_Sch%C3%A4fl1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernhard_Schäfl1">Bernhard Schäfl</a>, <a href="https://openreview.net/profile?id=~Johannes_Lehner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Johannes_Lehner1">Johannes Lehner</a>, <a href="https://openreview.net/profile?id=~Philipp_Seidl1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philipp_Seidl1">Philipp Seidl</a>, <a href="https://openreview.net/profile?id=~Michael_Widrich2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Widrich2">Michael Widrich</a>, <a href="https://openreview.net/profile?id=~Lukas_Gruber2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lukas_Gruber2">Lukas Gruber</a>, <a href="https://openreview.net/profile?id=~Markus_Holzleitner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Markus_Holzleitner1">Markus Holzleitner</a>, <a href="https://openreview.net/profile?id=~Thomas_Adler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Adler1">Thomas Adler</a>, <a href="https://openreview.net/profile?email=openreview20%40kreil.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="openreview20@kreil.org">David Kreil</a>, <a href="https://openreview.net/profile?id=~Michael_K_Kopp1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_K_Kopp1">Michael K Kopp</a>, <a href="https://openreview.net/profile?id=~G%C3%BCnter_Klambauer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Günter_Klambauer1">Günter Klambauer</a>, <a href="https://openreview.net/profile?id=~Johannes_Brandstetter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Johannes_Brandstetter1">Johannes Brandstetter</a>, <a href="https://openreview.net/profile?id=~Sepp_Hochreiter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sepp_Hochreiter1">Sepp Hochreiter</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tL89RnzIiCd-details-222" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tL89RnzIiCd-details-222"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Modern Hopfield Network, Energy, Attention, Convergence, Storage Capacity, Hopfield layer, Associative Memory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated  into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes.
      These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers
      across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: \url{https://github.com/ml-jku/hopfield-layers}</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A novel continuous Hopfield network is proposed whose update rule is the attention mechanism of the transformer model and which can be integrated into deep learning architectures.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=tL89RnzIiCd&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9EKHN1jOlA" data-number="1483">
      <h4>
        <a href="https://openreview.net/forum?id=9EKHN1jOlA">
            Uncertainty Estimation and Calibration with Finite-State Probabilistic RNNs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9EKHN1jOlA" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Cheng_Wang9" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cheng_Wang9">Cheng Wang</a>, <a href="https://openreview.net/profile?id=~Carolin_Lawrence1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Carolin_Lawrence1">Carolin Lawrence</a>, <a href="https://openreview.net/profile?id=~Mathias_Niepert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mathias_Niepert1">Mathias Niepert</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 24 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9EKHN1jOlA-details-385" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9EKHN1jOlA-details-385"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">uncertainty estimation, calibration, RNN</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Uncertainty quantification is crucial for building reliable and trustable machine learning systems. We propose to estimate uncertainty in recurrent neural networks (RNNs) via stochastic discrete state transitions over recurrent timesteps. The uncertainty of the model can be quantified by running a prediction several times, each time sampling from the recurrent state transition distribution, leading to potentially different results if the model is uncertain. Alongside uncertainty quantification, our proposed method offers several advantages in different settings. The proposed method can (1) learn deterministic and probabilistic automata from data, (2) learn well-calibrated models on real-world classification tasks, (3) improve the performance of out-of-distribution detection, and (4) control the exploration-exploitation trade-off in reinforcement learning. An implementation is available.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A method to estimate and calibrate uncertainty in recurrent state transitions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=9EKHN1jOlA&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="fSTD6NFIW_b" data-number="2272">
      <h4>
        <a href="https://openreview.net/forum?id=fSTD6NFIW_b">
            Understanding the failure modes of out-of-distribution generalization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=fSTD6NFIW_b" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Vaishnavh_Nagarajan3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vaishnavh_Nagarajan3">Vaishnavh Nagarajan</a>, <a href="https://openreview.net/profile?email=ajandreassen%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ajandreassen@google.com">Anders Andreassen</a>, <a href="https://openreview.net/profile?id=~Behnam_Neyshabur1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Behnam_Neyshabur1">Behnam Neyshabur</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#fSTD6NFIW_b-details-714" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fSTD6NFIW_b-details-714"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">out-of-distribution generalization, spurious correlations, empirical risk minimization, theoretical study</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Empirical studies suggest that machine learning models often rely on features, such as the background, that may be spuriously correlated with the label only during training time, resulting in poor accuracy during test-time. In this work, we identify the fundamental factors that give rise to this behavior, by explaining why models fail this way even in easy-to-learn tasks where one would expect these models to succeed. In particular, through a theoretical study of gradient-descent-trained linear classifiers on some easy-to-learn tasks, we uncover two complementary failure modes. These modes arise from how spurious correlations induce two kinds of skews in the data: one geometric in nature and another, statistical. Finally, we construct natural modifications of image classification datasets to understand when these failure modes can arise in practice. We also design experiments to isolate the two failure modes when training modern neural networks on these datasets.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">In this theoretical study, we explain why machine learning models rely on spuriously correlated features in the dataset and fail at out-of-distribution generalization.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="45uOPa46Kh" data-number="1868">
      <h4>
        <a href="https://openreview.net/forum?id=45uOPa46Kh">
            Generative Language-Grounded Policy in Vision-and-Language Navigation with Bayes' Rule
        </a>
      
        
          <a href="https://openreview.net/pdf?id=45uOPa46Kh" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Shuhei_Kurita1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuhei_Kurita1">Shuhei Kurita</a>, <a href="https://openreview.net/profile?id=~Kyunghyun_Cho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyunghyun_Cho1">Kyunghyun Cho</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#45uOPa46Kh-details-158" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="45uOPa46Kh-details-158"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">vision-and-language-navigation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Vision-and-language navigation (VLN) is a task in which an agent is embodied in a realistic 3D environment and follows an instruction to reach the goal node. While most of the previous studies have built and investigated a discriminative approach, we notice that there are in fact two possible approaches to building such a VLN agent: discriminative and generative. In this paper, we design and investigate a generative language-grounded policy which uses a language model to compute the distribution over all possible instructions i.e. all possible sequences of vocabulary tokens given action and the transition history. In experiments, we show that the proposed generative approach outperforms the discriminative approach in the Room-2-Room (R2R) and Room-4-Room (R4R) datasets, especially in the unseen environments. We further show that the combination of the generative and discriminative policies achieves close to the state-of-the art results in the R2R dataset, demonstrating that the generative and discriminative policies capture the different aspects of VLN.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose the novel generative language-grounded policy for vision-and-language navigation(VLN).</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="d8Q1mt2Ghw" data-number="2656">
      <h4>
        <a href="https://openreview.net/forum?id=d8Q1mt2Ghw">
            Emergent Road Rules In Multi-Agent Driving Environments
        </a>
      
        
          <a href="https://openreview.net/pdf?id=d8Q1mt2Ghw" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Avik_Pal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Avik_Pal1">Avik Pal</a>, <a href="https://openreview.net/profile?id=~Jonah_Philion1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonah_Philion1">Jonah Philion</a>, <a href="https://openreview.net/profile?id=~Yuan-Hong_Liao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuan-Hong_Liao2">Yuan-Hong Liao</a>, <a href="https://openreview.net/profile?id=~Sanja_Fidler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanja_Fidler1">Sanja Fidler</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 11 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#d8Q1mt2Ghw-details-754" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="d8Q1mt2Ghw-details-754"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">For autonomous vehicles to safely share the road with human drivers, autonomous vehicles must abide by specific "road rules" that human drivers have agreed to follow. "Road rules" include rules that drivers are required to follow by law – such as the requirement that vehicles stop at red lights – as well as more subtle social rules – such as the implicit designation of fast lanes on the highway. In this paper, we provide empirical evidence that suggests that – instead of hard-coding road rules into self-driving algorithms – a scalable alternative may be to design multi-agent environments in which road rules emerge as optimal solutions to the problem of maximizing traffic flow.  We analyze what ingredients in driving environments cause the emergence of these road rules and find that two crucial factors are noisy perception and agents’ spatial density.  We provide qualitative and quantitative evidence of the emergence of seven social driving behaviors, ranging from obeying traffic signals to following lanes, all of which emerge from training agents to drive quickly to destinations without colliding. Our results add empirical support for the social road rules that countries worldwide have agreed on for safe, efficient driving.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">In multi-agent driving environments with noisy perception, driving conventions emerge</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=d8Q1mt2Ghw&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="bEoxzW_EXsa" data-number="2987">
      <h4>
        <a href="https://openreview.net/forum?id=bEoxzW_EXsa">
            Wasserstein-2 Generative Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=bEoxzW_EXsa" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alexander_Korotin2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Korotin2">Alexander Korotin</a>, <a href="https://openreview.net/profile?id=~Vage_Egiazarian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vage_Egiazarian1">Vage Egiazarian</a>, <a href="https://openreview.net/profile?id=~Arip_Asadulaev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arip_Asadulaev1">Arip Asadulaev</a>, <a href="https://openreview.net/profile?email=a.safin%40skoltech.ru" class="profile-link" data-toggle="tooltip" data-placement="top" title="a.safin@skoltech.ru">Alexander Safin</a>, <a href="https://openreview.net/profile?id=~Evgeny_Burnaev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Evgeny_Burnaev1">Evgeny Burnaev</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 14 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#bEoxzW_EXsa-details-329" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bEoxzW_EXsa-details-329"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">wasserstein-2 distance, optimal transport maps, non-minimax optimization, cycle-consistency regularization, input-convex neural networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a new end-to-end algorithm to compute optimal transport maps between continuous distributions without introducing bias or resorting to minimax optimization.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=bEoxzW_EXsa&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9r30XCjf5Dt" data-number="2512">
      <h4>
        <a href="https://openreview.net/forum?id=9r30XCjf5Dt">
            Vulnerability-Aware Poisoning Mechanism for Online RL with Unknown Dynamics
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9r30XCjf5Dt" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yanchao_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanchao_Sun1">Yanchao Sun</a>, <a href="https://openreview.net/profile?id=~Da_Huo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Da_Huo1">Da Huo</a>, <a href="https://openreview.net/profile?id=~Furong_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Furong_Huang1">Furong Huang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>25 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9r30XCjf5Dt-details-789" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9r30XCjf5Dt-details-789"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">poisoning attack, policy gradient, vulnerability of RL, deep RL</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Poisoning attacks on Reinforcement Learning (RL) systems could take advantage of RL algorithm’s vulnerabilities and cause failure of the learning. However, prior works on poisoning RL usually either unrealistically assume the attacker knows the underlying Markov Decision Process (MDP), or directly apply the poisoning methods in supervised learning to RL. In this work, we build a generic poisoning framework for online RL via a comprehensive investigation of heterogeneous poisoning models in RL. Without any prior knowledge of the MDP, we propose a strategic poisoning algorithm called Vulnerability-Aware Adversarial Critic Poison (VA2C-P), which works for on-policy deep RL agents, closing the gap that no poisoning method exists for policy-based RL agents. VA2C-P uses a novel metric, stability radius in RL, that measures the vulnerability of RL algorithms. Experiments on multiple deep RL agents and multiple environments show that our poisoning algorithm successfully prevents agents from learning a good policy or teaches the agents to converge to a target policy, with a limited attacking budget.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose the first poisoning algorithm against deep policy-based RL methods, without any prior knowledge of the environment, covering heterogeneous poisoning models.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=9r30XCjf5Dt&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="YtMG5ex0ou" data-number="871">
      <h4>
        <a href="https://openreview.net/forum?id=YtMG5ex0ou">
            Tomographic Auto-Encoder: Unsupervised Bayesian Recovery of Corrupted Data
        </a>
      
        
          <a href="https://openreview.net/pdf?id=YtMG5ex0ou" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Francesco_Tonolini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Francesco_Tonolini1">Francesco Tonolini</a>, <a href="https://openreview.net/profile?id=~Andreas_Damianou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_Damianou1">Andreas Damianou</a>, <a href="https://openreview.net/profile?id=~Pablo_Garcia_Moreno1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pablo_Garcia_Moreno1">Pablo Garcia Moreno</a>, <a href="https://openreview.net/profile?id=~Roderick_Murray-Smith1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roderick_Murray-Smith1">Roderick Murray-Smith</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#YtMG5ex0ou-details-971" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YtMG5ex0ou-details-971"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Missing value imputation, variational inference, variational auto-encoders</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a new probabilistic method for unsupervised recovery of corrupted data. Given a large ensemble of degraded samples, our method recovers accurate posteriors of clean values, allowing the exploration of the manifold of possible reconstructed data and hence characterising the underlying uncertainty. In this set-ting, direct application of classical variational methods often gives rise to collapsed densities that do not adequately explore the solution space.  Instead, we derive our novel reduced entropy condition approximate inference method that results in rich posteriors.  We test our model in a data recovery task under the common setting of missing values and noise, demonstrating superior performance to existing variational methods for imputation and de-noising with different real data sets. We further show higher classification accuracy after imputation, proving the advantage of propagating uncertainty to downstream tasks with our model.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Recovering accurate posterior distributions for unsupervised data recovery</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="0pxiMpCyBtr" data-number="1044">
      <h4>
        <a href="https://openreview.net/forum?id=0pxiMpCyBtr">
            Monotonic Kronecker-Factored Lattice
        </a>
      
        
          <a href="https://openreview.net/pdf?id=0pxiMpCyBtr" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~William_Taylor_Bakst1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_Taylor_Bakst1">William Taylor Bakst</a>, <a href="https://openreview.net/profile?id=~Nobuyuki_Morioka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nobuyuki_Morioka1">Nobuyuki Morioka</a>, <a href="https://openreview.net/profile?id=~Erez_Louidor1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Erez_Louidor1">Erez Louidor</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#0pxiMpCyBtr-details-578" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0pxiMpCyBtr-details-578"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Theory, Regularization, Algorithms, Classification, Regression, Matrix and Tensor Factorization, Fairness, Evaluation, Efficiency, Machine Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">It is computationally challenging to learn flexible monotonic functions that guarantee model behavior and provide interpretability beyond a few input features, and in a time where minimizing resource use is increasingly important, we must be able to learn such models that are still efficient. In this paper we show how to effectively and efficiently learn such functions using Kronecker-Factored Lattice (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="65" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c4B"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c46"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c4C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">F</mi><mi mathvariant="normal">L</mi></mrow></math></mjx-assistive-mml></mjx-container>), an efficient reparameterization of flexible monotonic lattice regression via Kronecker product. Both computational and storage costs scale linearly in the number of input features, which is a significant improvement over existing methods that grow exponentially. We also show that we can still properly enforce monotonicity and other shape constraints. The <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="66" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c4B"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c46"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c4C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">F</mi><mi mathvariant="normal">L</mi></mrow></math></mjx-assistive-mml></mjx-container> function class consists of products of piecewise-linear functions, and the size of the function class can be further increased through ensembling. We prove that the function class of an ensemble of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="67" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math></mjx-assistive-mml></mjx-container> base <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="68" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c4B"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c46"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c4C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">F</mi><mi mathvariant="normal">L</mi></mrow></math></mjx-assistive-mml></mjx-container> models strictly increases as <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="69" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math></mjx-assistive-mml></mjx-container> increases up to a certain threshold. Beyond this threshold, every multilinear interpolated lattice function can be expressed. Our experimental results demonstrate that <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="70" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c4B"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c46"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c4C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">F</mi><mi mathvariant="normal">L</mi></mrow></math></mjx-assistive-mml></mjx-container> trains faster with fewer parameters while still achieving accuracy and evaluation speeds comparable to or better than the baseline methods and preserving monotonicity guarantees on the learned model.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show how to effectively and efficiently learn flexible and interpretable monotonic functions using Kronecker-Factored Lattice, an efficient reparameterization of flexible monotonic lattice regression via Kronecker product.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="jM76BCb6F9m" data-number="3083">
      <h4>
        <a href="https://openreview.net/forum?id=jM76BCb6F9m">
            LEAF: A Learnable Frontend for Audio Classification
        </a>
      
        
          <a href="https://openreview.net/pdf?id=jM76BCb6F9m" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Neil_Zeghidour1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neil_Zeghidour1">Neil Zeghidour</a>, <a href="https://openreview.net/profile?id=~Olivier_Teboul2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Olivier_Teboul2">Olivier Teboul</a>, <a href="https://openreview.net/profile?id=~F%C3%A9lix_de_Chaumont_Quitry1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Félix_de_Chaumont_Quitry1">Félix de Chaumont Quitry</a>, <a href="https://openreview.net/profile?id=~Marco_Tagliasacchi3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marco_Tagliasacchi3">Marco Tagliasacchi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#jM76BCb6F9m-details-653" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jM76BCb6F9m-details-653"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">audio understanding, frontend, learnable, mel-filterbanks, time-frequency representations, sound classification</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Mel-filterbanks are fixed, engineered audio features which emulate human perception and have been used through the history of audio understanding up to today. However, their undeniable qualities are counterbalanced by the fundamental limitations of handmade representations. In this work we show that we can train a single learnable frontend that outperforms mel-filterbanks on a wide range of audio signals, including speech, music, audio events and animal sounds, providing a general-purpose learned frontend for audio classification. To do so, we introduce a new principled, lightweight, fully learnable architecture that can be used as a drop-in replacement of mel-filterbanks. Our system learns all operations of audio features extraction, from filtering to pooling, compression and normalization, and can be integrated into any neural network at a negligible parameter cost. We perform multi-task training on eight diverse audio classification tasks, and show consistent improvements of our model over mel-filterbanks and previous learnable alternatives. Moreover, our system outperforms the current state-of-the-art learnable frontend on Audioset, with orders of magnitude fewer parameters.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a lightweight learnable frontend of audio that can replace fixed features over a wide range of audio classification tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="GFsU8a0sGB" data-number="1852">
      <h4>
        <a href="https://openreview.net/forum?id=GFsU8a0sGB">
            Federated Learning via Posterior Averaging: A New Perspective and Practical Algorithms
        </a>
      
        
          <a href="https://openreview.net/pdf?id=GFsU8a0sGB" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Maruan_Al-Shedivat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maruan_Al-Shedivat1">Maruan Al-Shedivat</a>, <a href="https://openreview.net/profile?id=~Jennifer_Gillenwater1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jennifer_Gillenwater1">Jennifer Gillenwater</a>, <a href="https://openreview.net/profile?id=~Eric_Xing1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eric_Xing1">Eric Xing</a>, <a href="https://openreview.net/profile?id=~Afshin_Rostamizadeh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Afshin_Rostamizadeh1">Afshin Rostamizadeh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 04 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#GFsU8a0sGB-details-336" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GFsU8a0sGB-details-336"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">federated learning, posterior inference, MCMC</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Federated learning is typically approached as an optimization problem, where the goal is to minimize a global loss function by distributing computation across client devices that possess local data and specify different parts of the global objective.  We present an alternative perspective and formulate federated learning as a posterior inference problem, where the goal is to infer a global posterior distribution by having client devices each infer the posterior of their local data.  While exact inference is often intractable, this perspective provides a principled way to search for global optima in federated settings.  Further, starting with the analysis of federated quadratic objectives, we develop a computation- and communication-efficient approximate posterior inference algorithm—federated posterior averaging (FedPA).  Our algorithm uses MCMC for approximate inference of local posteriors on the clients and efficiently communicates their statistics to the server, where the latter uses them to refine a global estimate of the posterior mode.  Finally, we show that FedPA generalizes federated averaging (FedAvg), can similarly benefit from adaptive optimizers, and yields state-of-the-art results on four realistic and challenging benchmarks, converging faster, to better optima.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A new approach to federated learning that generalizes federated optimization, combines local MCMC-based sampling with global optimization-based posterior inference, and achieves competitive results on challenging benchmarks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="MtEE0CktZht" data-number="990">
      <h4>
        <a href="https://openreview.net/forum?id=MtEE0CktZht">
            Rank the Episodes: A Simple Approach for Exploration in Procedurally-Generated Environments
        </a>
      
        
          <a href="https://openreview.net/pdf?id=MtEE0CktZht" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Daochen_Zha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daochen_Zha1">Daochen Zha</a>, <a href="https://openreview.net/profile?email=mawenye%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mawenye@gmail.com">Wenye Ma</a>, <a href="https://openreview.net/profile?id=~Lei_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lei_Yuan1">Lei Yuan</a>, <a href="https://openreview.net/profile?id=~Xia_Hu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xia_Hu4">Xia Hu</a>, <a href="https://openreview.net/profile?id=~Ji_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ji_Liu1">Ji Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#MtEE0CktZht-details-67" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MtEE0CktZht-details-67"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement Learning, Exploration, Generalization of Reinforcement Learning, Self-Imitation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Exploration under sparse reward is a long-standing challenge of model-free reinforcement learning. The state-of-the-art methods address this challenge by introducing intrinsic rewards to encourage exploration in novel states or uncertain environment dynamics. Unfortunately, methods based on intrinsic rewards often fall short in procedurally-generated environments, where a different environment is generated in each episode so that the agent is not likely to visit the same state more than once. Motivated by how humans distinguish good exploration behaviors by looking into the entire episode, we introduce RAPID, a simple yet effective episode-level exploration method for procedurally-generated environments. RAPID regards each episode as a whole and gives an episodic exploration score from both per-episode and long-term views. Those highly scored episodes are treated as good exploration behaviors and are stored in a small ranking buffer. The agent then imitates the episodes in the buffer to reproduce the past good exploration behaviors. We demonstrate our method on several procedurally-generated MiniGrid environments, a first-person-view 3D Maze navigation task from MiniWorld, and several sparse MuJoCo tasks. The results show that RAPID significantly outperforms the state-of-the-art intrinsic reward strategies in terms of sample efficiency and final performance. The code is available at https://github.com/daochenzha/rapid</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Encouraging exploration via ranking the past episodes and reproducing past good exploration behaviors with imitation learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=MtEE0CktZht&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="6BRLOfrMhW" data-number="696">
      <h4>
        <a href="https://openreview.net/forum?id=6BRLOfrMhW">
            Partitioned Learned Bloom Filters
        </a>
      
        
          <a href="https://openreview.net/pdf?id=6BRLOfrMhW" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kapil_Vaidya1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kapil_Vaidya1">Kapil Vaidya</a>, <a href="https://openreview.net/profile?email=eric_knorr%40g.harvard.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="eric_knorr@g.harvard.edu">Eric Knorr</a>, <a href="https://openreview.net/profile?id=~Michael_Mitzenmacher1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Mitzenmacher1">Michael Mitzenmacher</a>, <a href="https://openreview.net/profile?id=~Tim_Kraska1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tim_Kraska1">Tim Kraska</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#6BRLOfrMhW-details-922" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6BRLOfrMhW-details-922"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">optimization, data structures, algorithms, theory, learned algorithms</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Bloom filters are space-efficient probabilistic data structures that are used to test whether an element is a member of a set, and may return false positives.  Recently, variations referred to as learned Bloom filters were developed that can provide improved performance in terms of the rate of false positives, by using a learned model for the represented set.  However, previous methods for learned Bloom filters do not take full advantage of the learned model.  Here we show how to frame the problem of optimal model utilization as an optimization problem, and using our framework derive algorithms that can achieve near-optimal performance in many cases.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="zeFrfgyZln" data-number="1477">
      <h4>
        <a href="https://openreview.net/forum?id=zeFrfgyZln">
            Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval
        </a>
      
        
          <a href="https://openreview.net/pdf?id=zeFrfgyZln" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Lee_Xiong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lee_Xiong1">Lee Xiong</a>, <a href="https://openreview.net/profile?id=~Chenyan_Xiong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chenyan_Xiong1">Chenyan Xiong</a>, <a href="https://openreview.net/profile?email=yeli1%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yeli1@microsoft.com">Ye Li</a>, <a href="https://openreview.net/profile?email=kwokfung.tang%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="kwokfung.tang@microsoft.com">Kwok-Fung Tang</a>, <a href="https://openreview.net/profile?email=jialliu%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jialliu@microsoft.com">Jialin Liu</a>, <a href="https://openreview.net/profile?id=~Paul_N._Bennett1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_N._Bennett1">Paul N. Bennett</a>, <a href="https://openreview.net/profile?email=jahmed%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jahmed@microsoft.com">Junaid Ahmed</a>, <a href="https://openreview.net/profile?email=arnold.overwijk%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="arnold.overwijk@microsoft.com">Arnold Overwijk</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#zeFrfgyZln-details-187" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zeFrfgyZln-details-187"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Dense Retrieval, Text Retrieval, Text Representation, Neural IR</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Conducting text retrieval in a learned dense representation space has many intriguing advantages. Yet dense retrieval (DR) often underperforms word-based sparse retrieval. In this paper, we first theoretically show the bottleneck of dense retrieval is the domination of uninformative negatives sampled in mini-batch training, which yield diminishing gradient norms, large gradient variances, and slow convergence. We then propose Approximate nearest neighbor Negative Contrastive Learning (ANCE), which selects hard training negatives globally from the entire corpus. Our experiments demonstrate the effectiveness of ANCE on web search, question answering, and in a commercial search engine, showing ANCE dot-product retrieval nearly matches the accuracy of BERT-based cascade IR pipeline. We also empirically validate our theory that negative sampling with ANCE better approximates the oracle importance sampling procedure and improves learning convergence.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper improves the learning of dense text retrieval using ANCE, which selects global negatives with bigger gradient norms using an asynchronously updated ANN index. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=zeFrfgyZln&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="1GTma8HwlYp" data-number="2750">
      <h4>
        <a href="https://openreview.net/forum?id=1GTma8HwlYp">
            AUXILIARY TASK UPDATE DECOMPOSITION: THE GOOD, THE BAD AND THE NEUTRAL
        </a>
      
        
          <a href="https://openreview.net/pdf?id=1GTma8HwlYp" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Lucio_M._Dery1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lucio_M._Dery1">Lucio M. Dery</a>, <a href="https://openreview.net/profile?id=~Yann_Dauphin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yann_Dauphin1">Yann Dauphin</a>, <a href="https://openreview.net/profile?id=~David_Grangier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Grangier1">David Grangier</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#1GTma8HwlYp-details-36" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1GTma8HwlYp-details-36"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">pre-training, multitask learning, deeplearning, gradient decomposition</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While deep learning has been very beneficial in data-rich settings, tasks with smaller training set
      often resort to pre-training or multitask learning to leverage data from other tasks. In this case,
      careful consideration is needed to select tasks and model parameterizations such that updates from
      the auxiliary tasks actually help the primary task. We seek to alleviate this burden by formulating a model-agnostic framework that performs fine-grained manipulation of the auxiliary task gradients. We propose to decompose auxiliary updates into directions which help, damage or leave the primary task loss unchanged. This allows weighting the update directions 
      differently depending on their impact on the problem of interest. We present a novel and efficient algorithm for that
      purpose and show its advantage in practice. Our method leverages efficient automatic differentiation 
      procedures and randomized singular value decomposition for scalability. We show that our framework is 
      generic and encompasses some prior work as particular cases. Our approach consistently outperforms strong and widely used baselines when leveraging out-of-distribution data for Text and Image classification tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We improve how we use auxiliary task data for model pre-training by decomposing gradient updates into components guided by the primary task</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=1GTma8HwlYp&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="v5gjXpmR8J" data-number="2654">
      <h4>
        <a href="https://openreview.net/forum?id=v5gjXpmR8J">
            SSD: A Unified Framework for Self-Supervised Outlier Detection
        </a>
      
        
          <a href="https://openreview.net/pdf?id=v5gjXpmR8J" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Vikash_Sehwag1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vikash_Sehwag1">Vikash Sehwag</a>, <a href="https://openreview.net/profile?id=~Mung_Chiang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mung_Chiang2">Mung Chiang</a>, <a href="https://openreview.net/profile?id=~Prateek_Mittal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Prateek_Mittal1">Prateek Mittal</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#v5gjXpmR8J-details-964" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="v5gjXpmR8J-details-964"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Outlier detection, Out-of-distribution detection in deep learning, Anomaly detection with deep neural networks, Self-supervised learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We ask the following question: what training information is required to design an effective outlier/out-of-distribution (OOD) detector, i.e., detecting samples that lie far away from training distribution? Since unlabeled data is easily accessible for many applications, the most compelling approach is to develop detectors based on only unlabeled in-distribution data. However, we observe that most existing detectors based on unlabeled data perform poorly, often equivalent to a random prediction. In contrast, existing state-of-the-art OOD detectors achieve impressive performance but require access to fine-grained data labels for supervised training. We propose SSD, an outlier detector based on only unlabeled in-distribution data. We use self-supervised representation learning followed by a Mahalanobis distance based detection in the feature space. We demonstrate that SSD outperforms most existing detectors based on unlabeled data by a large margin. Additionally, SSD even achieves performance on par, and sometimes even better, with supervised training based detectors.  Finally, we expand our detection framework with two key extensions. First, we formulate few-shot OOD detection, in which the detector has access to only one to five samples from each class of the targeted OOD dataset. Second, we extend our framework to incorporate training data labels, if available. We find that our novel detection framework based on SSD displays enhanced performance with these extensions, and achieves state-of-the-art performance. Our code is publicly available at https://github.com/inspire-group/SSD.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We achieve competitive performance on outlier/out-of-distribution detection using only unlabeled training data.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Y87Ri-GNHYu" data-number="1692">
      <h4>
        <a href="https://openreview.net/forum?id=Y87Ri-GNHYu">
            Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Y87Ri-GNHYu" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Valerie_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Valerie_Chen2">Valerie Chen</a>, <a href="https://openreview.net/profile?id=~Abhinav_Gupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abhinav_Gupta1">Abhinav Gupta</a>, <a href="https://openreview.net/profile?id=~Kenneth_Marino1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kenneth_Marino1">Kenneth Marino</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 07 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Y87Ri-GNHYu-details-787" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Y87Ri-GNHYu-details-787"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Complex, multi-task problems have proven to be difficult to solve efficiently in a sparse-reward reinforcement learning setting. In order to be sample efficient, multi-task learning requires reuse and sharing of low-level policies. To facilitate the automatic decomposition of hierarchical tasks, we propose the use of step-by-step human demonstrations in the form of natural language instructions and action trajectories. We introduce a dataset of such demonstrations in a crafting-based grid world. Our model consists of a high-level language generator and low-level policy, conditioned on language. We find that human demonstrations help solve the most complex tasks. We also find that incorporating natural language allows the model to generalize to unseen tasks in a zero-shot setting and to learn quickly from a few demonstrations. Generalization is not only reflected in the actions of the agent, but also in the generated natural language instructions in unseen tasks. Our approach also gives our trained agent interpretable behaviors because it is able to generate a sequence of high-level descriptions of its actions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="cO1IH43yUF" data-number="1284">
      <h4>
        <a href="https://openreview.net/forum?id=cO1IH43yUF">
            Revisiting Few-sample BERT Fine-tuning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=cO1IH43yUF" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tianyi_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianyi_Zhang2">Tianyi Zhang</a>, <a href="https://openreview.net/profile?id=~Felix_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Felix_Wu1">Felix Wu</a>, <a href="https://openreview.net/profile?id=~Arzoo_Katiyar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arzoo_Katiyar1">Arzoo Katiyar</a>, <a href="https://openreview.net/profile?id=~Kilian_Q_Weinberger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kilian_Q_Weinberger1">Kilian Q Weinberger</a>, <a href="https://openreview.net/profile?id=~Yoav_Artzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoav_Artzi1">Yoav Artzi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#cO1IH43yUF-details-77" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="cO1IH43yUF-details-77"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Fine-tuning, Optimization, BERT</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper is a study of fine-tuning of BERT contextual representations, with focus on commonly observed instabilities in few-sample scenarios. We identify several factors that cause this instability: the common use of a non-standard optimization method with biased gradient estimation; the limited applicability of significant parts of the BERT network for down-stream tasks; and the prevalent practice of using a pre-determined, and small number of training iterations. We empirically test the impact of these factors, and identify alternative practices that resolve the commonly observed instability of the process. In light of these observations, we re-visit recently proposed methods to improve few-sample fine-tuning with BERT and re-evaluate their effectiveness. Generally, we observe the impact of these methods diminishes significantly with our modified process. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=cO1IH43yUF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="K5YasWXZT3O" data-number="647">
      <h4>
        <a href="https://openreview.net/forum?id=K5YasWXZT3O">
            Tilted Empirical Risk Minimization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=K5YasWXZT3O" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tian_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tian_Li1">Tian Li</a>, <a href="https://openreview.net/profile?id=~Ahmad_Beirami1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ahmad_Beirami1">Ahmad Beirami</a>, <a href="https://openreview.net/profile?id=~Maziar_Sanjabi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maziar_Sanjabi1">Maziar Sanjabi</a>, <a href="https://openreview.net/profile?id=~Virginia_Smith1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Virginia_Smith1">Virginia Smith</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#K5YasWXZT3O-details-442" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="K5YasWXZT3O-details-442"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">exponential tilting, models of learning and generalization, label noise robustness, fairness</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Empirical risk minimization (ERM) is typically designed to perform well on the average loss, which can result in estimators that are sensitive to outliers, generalize poorly, or treat subgroups unfairly. While many methods aim to address these problems individually, in this work, we explore them through a unified framework---tilted empirical risk minimization (TERM). In particular, we show that it is possible to flexibly tune the impact of individual losses through a straightforward extension to ERM using a hyperparameter called the tilt. We provide several interpretations of the resulting framework: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to a superquantile method. We develop batch and stochastic first-order optimization methods for solving TERM, and show that the problem can be efficiently solved relative to common alternatives. Finally, we demonstrate that TERM can be used for a multitude of applications, such as enforcing fairness between subgroups, mitigating the effect of outliers, and handling class imbalance. TERM is not only competitive with existing solutions tailored to these individual problems, but can also enable entirely new applications, such as simultaneously addressing outliers and promoting fairness.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that tilted empirical risk minimization (TERM) can be used for enforcing fairness between subgroups, mitigating the effect of outliers, and handling class imbalance, all in a unified framework.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="vK9WrZ0QYQ" data-number="1340">
      <h4>
        <a href="https://openreview.net/forum?id=vK9WrZ0QYQ">
            Deep Neural Tangent Kernel and Laplace Kernel Have the Same RKHS
        </a>
      
        
          <a href="https://openreview.net/pdf?id=vK9WrZ0QYQ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Lin_Chen14" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lin_Chen14">Lin Chen</a>, <a href="https://openreview.net/profile?id=~Sheng_Xu5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sheng_Xu5">Sheng Xu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#vK9WrZ0QYQ-details-442" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vK9WrZ0QYQ-details-442"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural tangent kernel, Reproducing kernel Hilbert space, Laplace kernel, Singularity analysis</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions, when both kernels are restricted to the sphere <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="71" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c1D54A TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.429em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi mathvariant="double-struck">S</mi></mrow><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container>. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS, when it is restricted to the sphere <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="72" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c1D54A TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.429em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi mathvariant="double-struck">S</mi></mrow><mrow><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> and when it is defined on the entire <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="73" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.41em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>d</mi></msup></math></mjx-assistive-mml></mjx-container>.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We prove that the reproducing kernel Hilbert spaces of a deep neural tangent kernel and the Laplace kernel include the same set of functions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="8VXvj1QNRl1" data-number="3746">
      <h4>
        <a href="https://openreview.net/forum?id=8VXvj1QNRl1">
            On the Transfer of Disentangled Representations in Realistic Settings
        </a>
      
        
          <a href="https://openreview.net/pdf?id=8VXvj1QNRl1" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Andrea_Dittadi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrea_Dittadi1">Andrea Dittadi</a>, <a href="https://openreview.net/profile?id=~Frederik_Tr%C3%A4uble1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frederik_Träuble1">Frederik Träuble</a>, <a href="https://openreview.net/profile?id=~Francesco_Locatello1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Francesco_Locatello1">Francesco Locatello</a>, <a href="https://openreview.net/profile?id=~Manuel_Wuthrich1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manuel_Wuthrich1">Manuel Wuthrich</a>, <a href="https://openreview.net/profile?id=~Vaibhav_Agrawal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vaibhav_Agrawal1">Vaibhav Agrawal</a>, <a href="https://openreview.net/profile?id=~Ole_Winther1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ole_Winther1">Ole Winther</a>, <a href="https://openreview.net/profile?id=~Stefan_Bauer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefan_Bauer1">Stefan Bauer</a>, <a href="https://openreview.net/profile?id=~Bernhard_Sch%C3%B6lkopf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernhard_Schölkopf1">Bernhard Schölkopf</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 11 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#8VXvj1QNRl1-details-542" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8VXvj1QNRl1-details-542"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">representation learning, disentanglement, real-world</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Learning meaningful representations that disentangle the underlying structure of the data generating process is considered to be of key importance in machine learning. While disentangled representations were found to be useful for diverse tasks such as abstract reasoning and fair classification, their scalability and real-world impact remain questionable.
      We introduce a new high-resolution dataset with 1M simulated images and over 1,800 annotated real-world images of the same setup. In contrast to previous work, this new dataset exhibits correlations, a complex underlying structure, and allows to evaluate transfer to unseen simulated and real-world settings where the encoder i) remains in distribution or ii) is out of distribution.
      We propose new architectures in order to scale disentangled representation learning to realistic high-resolution settings and conduct a large-scale empirical study of disentangled representations on this dataset. We observe that disentanglement is a good predictor for out-of-distribution (OOD) task performance.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We scale disentangled representation learning to a new realistic dataset and conduct a large-scale empirical study on OOD generalization.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-bxf89v3Nx" data-number="2065">
      <h4>
        <a href="https://openreview.net/forum?id=-bxf89v3Nx">
            Calibration tests beyond classification
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-bxf89v3Nx" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~David_Widmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Widmann1">David Widmann</a>, <a href="https://openreview.net/profile?email=fredrik.lindsten%40liu.se" class="profile-link" data-toggle="tooltip" data-placement="top" title="fredrik.lindsten@liu.se">Fredrik Lindsten</a>, <a href="https://openreview.net/profile?email=dave.zachariah%40it.uu.se" class="profile-link" data-toggle="tooltip" data-placement="top" title="dave.zachariah@it.uu.se">Dave Zachariah</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-bxf89v3Nx-details-402" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-bxf89v3Nx-details-402"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">calibration, uncertainty quantification, framework, integral probability metric, maximum mean discrepancy</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression
      problems.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Unifying framework for calibration evaluations and tests of probabilistic predictive models</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="jphnJNOwe36" data-number="1977">
      <h4>
        <a href="https://openreview.net/forum?id=jphnJNOwe36">
            Overparameterisation and worst-case generalisation: friend or foe?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=jphnJNOwe36" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Aditya_Krishna_Menon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aditya_Krishna_Menon1">Aditya Krishna Menon</a>, <a href="https://openreview.net/profile?id=~Ankit_Singh_Rawat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ankit_Singh_Rawat1">Ankit Singh Rawat</a>, <a href="https://openreview.net/profile?id=~Sanjiv_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanjiv_Kumar1">Sanjiv Kumar</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#jphnJNOwe36-details-955" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jphnJNOwe36-details-955"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">overparameterisation, worst-case generalisation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Overparameterised neural networks have demonstrated the remarkable ability to perfectly fit training samples, while still generalising to unseen test samples. However, several recent works have revealed that such models' good average performance does not always translate to good worst-case performance: in particular, they may perform poorly on subgroups that are under-represented in the training set. In this paper, we show that in certain settings, overparameterised models' performance on under-represented subgroups may be improved via post-hoc processing. Specifically, such models' bias can be restricted to their classification layers, and manifest as structured prediction shifts for rare subgroups. We detail two post-hoc correction techniques to mitigate this bias, which operate purely on the outputs of standard model training. We empirically verify that with such post-hoc correction, overparameterisation can improve average and worst-case performance.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Overparameterised models' worst-subgroup performance can be improved via post-hoc processing.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=jphnJNOwe36&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="yvQKLaqNE6M" data-number="3015">
      <h4>
        <a href="https://openreview.net/forum?id=yvQKLaqNE6M">
            You Only Need Adversarial Supervision for Semantic Image Synthesis
        </a>
      
        
          <a href="https://openreview.net/pdf?id=yvQKLaqNE6M" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Edgar_Sch%C3%B6nfeld1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edgar_Schönfeld1">Edgar Schönfeld</a>, <a href="https://openreview.net/profile?id=~Vadim_Sushko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vadim_Sushko1">Vadim Sushko</a>, <a href="https://openreview.net/profile?id=~Dan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Zhang1">Dan Zhang</a>, <a href="https://openreview.net/profile?id=~Juergen_Gall1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Juergen_Gall1">Juergen Gall</a>, <a href="https://openreview.net/profile?id=~Bernt_Schiele1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernt_Schiele1">Bernt Schiele</a>, <a href="https://openreview.net/profile?id=~Anna_Khoreva1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anna_Khoreva1">Anna Khoreva</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#yvQKLaqNE6M-details-166" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="yvQKLaqNE6M-details-166"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Semantic Image Synthesis, GANs, Image Generation, Deep Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Despite their recent successes, GAN models for semantic image synthesis still suffer from poor image quality when trained with only adversarial supervision. Historically, additionally employing the VGG-based perceptual loss has helped to overcome this issue, significantly improving the synthesis quality, but at the same time limiting the progress of GAN models for semantic image synthesis. In this work, we propose a novel, simplified GAN model, which needs only adversarial supervision to achieve high quality results. We re-design the discriminator as a semantic segmentation network, directly using the given semantic label maps as the ground truth for training. By providing stronger supervision to the discriminator as well as to the generator through spatially- and semantically-aware discriminator feedback, we are able to synthesize images of higher fidelity with better alignment to their input label maps, making the use of the perceptual loss superfluous. Moreover, we enable high-quality multi-modal image synthesis through global and local sampling of a 3D noise tensor injected into the generator, which allows complete or partial image change. We show that images synthesized by our model are more diverse and follow the color and texture distributions of real images more closely. We achieve an average improvement of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="74" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c36"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>6</mn></math></mjx-assistive-mml></mjx-container> FID and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="75" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>5</mn></math></mjx-assistive-mml></mjx-container> mIoU points over the state of the art across different datasets using only adversarial supervision.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose OASIS, a novel model for multi-modal semantic image synthesis, improving over previous methods in terms of image quality and diversity while only using adversarial supervision.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="PS3IMnScugk" data-number="1966">
      <h4>
        <a href="https://openreview.net/forum?id=PS3IMnScugk">
            Learning to Recombine and Resample Data For Compositional Generalization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=PS3IMnScugk" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ekin_Aky%C3%BCrek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ekin_Akyürek1">Ekin Akyürek</a>, <a href="https://openreview.net/profile?id=~Afra_Feyza_Aky%C3%BCrek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Afra_Feyza_Akyürek1">Afra Feyza Akyürek</a>, <a href="https://openreview.net/profile?id=~Jacob_Andreas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jacob_Andreas1">Jacob Andreas</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 21 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#PS3IMnScugk-details-804" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PS3IMnScugk-details-804"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">compositional generalization, data augmentation, language processing, sequence models, generative modeling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Flexible neural sequence models outperform grammar- and automaton-based counterparts on a variety of tasks. However, neural models perform poorly in settings requiring compositional generalization beyond the training data—particularly to rare or unseen subsequences. Past work has found symbolic scaffolding (e.g. grammars or automata) essential in these settings. We describe R&amp;R, a learned data augmentation scheme that enables a large category of compositional generalizations without appeal to latent symbolic structure. R&amp;R has two components: recombination of original training examples via a prototype-based generative model and resampling of generated examples to encourage extrapolation. Training an ordinary neural sequence model on a dataset augmented with recombined and resampled examples significantly improves generalization in two language processing problems—instruction following (SCAN) and morphological analysis (SIGMORPHON 2018)—where R&amp;R enables learning of new constructions and tenses from as few as eight initial examples.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper investigates a data augmentation procedure based on two weaker principles: recombination and resampling, and finds that it is sufficient to induce many of the compositional generalizations studied in previous work. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=PS3IMnScugk&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="FOyuZ26emy" data-number="1308">
      <h4>
        <a href="https://openreview.net/forum?id=FOyuZ26emy">
            A Critique of Self-Expressive Deep Subspace Clustering
        </a>
      
        
          <a href="https://openreview.net/pdf?id=FOyuZ26emy" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Benjamin_David_Haeffele1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_David_Haeffele1">Benjamin David Haeffele</a>, <a href="https://openreview.net/profile?id=~Chong_You2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chong_You2">Chong You</a>, <a href="https://openreview.net/profile?id=~Rene_Vidal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rene_Vidal1">Rene Vidal</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#FOyuZ26emy-details-176" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FOyuZ26emy-details-176"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Subspace clustering, Manifold clustering, Theory of deep learning, Autoencoders</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Subspace clustering is an unsupervised clustering technique designed to cluster data that is supported on a union of linear subspaces, with each subspace defining a cluster with dimension lower than the ambient space. Many existing formulations for this problem are based on exploiting the self-expressive property of linear subspaces, where any point within a subspace can be represented as linear combination of other points within the subspace. To extend this approach to data supported on a union of non-linear manifolds, numerous studies have proposed learning an embedding of the original data using  a neural network which is regularized by a self-expressive loss function on the data in the embedded space to encourage a union of linear subspaces prior on the data in the embedded space. Here we show that there are a number of potential flaws with this approach which have not been adequately addressed in prior work. In particular, we show the model formulation is often ill-posed in that it can lead to a degenerate embedding of the data, which need not correspond to a union of subspaces at all and is poorly suited for clustering. We validate our theoretical results experimentally and also repeat prior experiments reported in the literature, where we conclude that a significant portion of the previously claimed performance benefits can be attributed to an ad-hoc post processing step rather than the deep subspace clustering model.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Here we show theoretically and experimentally that there are a number of flaws with many existing self-expressive deep subspace clustering models.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="O6LPudowNQm" data-number="2194">
      <h4>
        <a href="https://openreview.net/forum?id=O6LPudowNQm">
            INT: An Inequality Benchmark for Evaluating Generalization in Theorem Proving
        </a>
      
        
          <a href="https://openreview.net/pdf?id=O6LPudowNQm" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yuhuai_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuhuai_Wu1">Yuhuai Wu</a>, <a href="https://openreview.net/profile?id=~Albert_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Albert_Jiang1">Albert Jiang</a>, <a href="https://openreview.net/profile?id=~Jimmy_Ba1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jimmy_Ba1">Jimmy Ba</a>, <a href="https://openreview.net/profile?id=~Roger_Baker_Grosse1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roger_Baker_Grosse1">Roger Baker Grosse</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#O6LPudowNQm-details-466" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="O6LPudowNQm-details-466"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Theorem proving, Synthetic benchmark dataset, Generalization, Transformers, Graph neural networks, Monte Carlo Tree Search</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In learning-assisted theorem proving, one of the most critical challenges is to generalize to theorems unlike those seen at training time. In this paper, we introduce INT, an INequality Theorem proving benchmark designed to test agents’ generalization ability. INT is based on a theorem generator, which provides theoretically infinite data and allows us to measure 6 different types of generalization, each reflecting a distinct challenge, characteristic of automated theorem proving. In addition, provides a fast theorem proving environment with sequence-based and graph-based interfaces, conducive to performing learning-based research. We introduce base-lines with architectures including transformers and graph neural networks (GNNs)for INT. Using INT, we find that transformer-based agents achieve stronger test performance for most of the generalization tasks, despite having much larger out-of-distribution generalization gaps than GNNs. We further find that the addition of Monte Carlo Tree Search (MCTS) at test time helps to prove new theorems.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce INT, a synthetic INequality Theorem proving benchmark, to tackle the data sparsity and out-of-distribution problems for theorem proving and benchmarked transformer-based and GNN-based agents' generalization performance.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=O6LPudowNQm&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="BUlyHkzjgmA" data-number="2357">
      <h4>
        <a href="https://openreview.net/forum?id=BUlyHkzjgmA">
            Improved Estimation of Concentration Under <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="76" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container>-Norm Distance Metrics Using Half Spaces
        </a>
      
        
          <a href="https://openreview.net/pdf?id=BUlyHkzjgmA" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=jbp2jn%40virginia.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jbp2jn@virginia.edu">Jack Prescott</a>, <a href="https://openreview.net/profile?id=~Xiao_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiao_Zhang2">Xiao Zhang</a>, <a href="https://openreview.net/profile?id=~David_Evans1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Evans1">David Evans</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 11 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#BUlyHkzjgmA-details-689" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="BUlyHkzjgmA-details-689"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Adversarial Examples, Concentration of Measure, Gaussian Isoperimetric Inequality</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Concentration of measure has been argued to be the fundamental cause of adversarial vulnerability. Mahloujifar et al. (2019) presented an empirical way to measure the concentration of a data distribution using samples, and employed it to find lower bounds on intrinsic robustness for several benchmark datasets. However, it remains unclear whether these lower bounds are tight enough to provide a useful approximation for the intrinsic robustness of a dataset. To gain a deeper understanding of the concentration of measure phenomenon, we first extend the Gaussian Isoperimetric Inequality to non-spherical Gaussian measures and arbitrary <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="77" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container>-norms (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="78" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2265"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>≥</mo><mn>2</mn></math></mjx-assistive-mml></mjx-container>). We leverage these theoretical insights to design a method that uses half-spaces to estimate the concentration of any empirical dataset under <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="79" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container>-norm distance metrics. Our proposed algorithm is more efficient than Mahloujifar et al. (2019)'s, and experiments on synthetic datasets and image benchmarks demonstrate that it is able to find much tighter intrinsic robustness bounds. These tighter estimates provide further evidence that rules out intrinsic dataset concentration as a possible explanation for the adversarial vulnerability of state-of-the-art classifiers.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that concentration of measure does not prohibit the existence of adversarially robust classifiers using a novel method of empirical concentration estimation.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=BUlyHkzjgmA&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="LkFG3lB13U5" data-number="2569">
      <h4>
        <a href="https://openreview.net/forum?id=LkFG3lB13U5">
            Adaptive Federated Optimization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=LkFG3lB13U5" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sashank_J._Reddi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sashank_J._Reddi1">Sashank J. Reddi</a>, <a href="https://openreview.net/profile?id=~Zachary_Charles1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zachary_Charles1">Zachary Charles</a>, <a href="https://openreview.net/profile?id=~Manzil_Zaheer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manzil_Zaheer1">Manzil Zaheer</a>, <a href="https://openreview.net/profile?email=zachgarrett%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zachgarrett@google.com">Zachary Garrett</a>, <a href="https://openreview.net/profile?email=krush%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="krush@google.com">Keith Rush</a>, <a href="https://openreview.net/profile?id=~Jakub_Kone%C4%8Dn%C3%BD1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jakub_Konečný1">Jakub Konečný</a>, <a href="https://openreview.net/profile?id=~Sanjiv_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanjiv_Kumar1">Sanjiv Kumar</a>, <a href="https://openreview.net/profile?id=~Hugh_Brendan_McMahan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hugh_Brendan_McMahan1">Hugh Brendan McMahan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#LkFG3lB13U5-details-331" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LkFG3lB13U5-details-331"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Federated learning, optimization, adaptive optimization, distributed optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Standard federated optimization methods such as Federated Averaging (FedAvg) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including Adagrad, Adam, and  Yogi, and analyze their convergence in the presence of heterogeneous data for general non-convex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose adaptive federated optimization techniques, and highlight their improved performance over popular methods such as FedAvg.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=LkFG3lB13U5&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="1OCTOShAmqB" data-number="447">
      <h4>
        <a href="https://openreview.net/forum?id=1OCTOShAmqB">
            On the Dynamics of Training Attention Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=1OCTOShAmqB" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Haoye_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haoye_Lu1">Haoye Lu</a>, <a href="https://openreview.net/profile?id=~Yongyi_Mao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yongyi_Mao2">Yongyi Mao</a>, <a href="https://openreview.net/profile?email=nayak%40uottawa.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="nayak@uottawa.ca">Amiya Nayak</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#1OCTOShAmqB-details-263" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1OCTOShAmqB-details-263"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The attention mechanism has been widely used in deep neural networks as a model component. By now, it has become a critical building block in many state-of-the-art natural language models. Despite its great success established empirically, the working mechanism of attention has not been investigated at a sufficient theoretical depth to date. In this paper, we set up a simple text classification task and study the dynamics of training a simple attention-based classification model using gradient descent. In this setting, we show that, for the discriminative words that the model should attend to, a persisting identity exists relating its embedding and the inner product of its key and the query. This allows us to prove that training must converge to attending to the discriminative words when the attention output is classified by a linear classifier. Experiments are performed, which validate our theoretical analysis and provide further insights.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="84gjULz1t5" data-number="1280">
      <h4>
        <a href="https://openreview.net/forum?id=84gjULz1t5">
            Linear Convergent Decentralized Optimization with Compression
        </a>
      
        
          <a href="https://openreview.net/pdf?id=84gjULz1t5" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xiaorui_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaorui_Liu1">Xiaorui Liu</a>, <a href="https://openreview.net/profile?email=liyao6%40msu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="liyao6@msu.edu">Yao Li</a>, <a href="https://openreview.net/profile?email=wangron6%40msu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="wangron6@msu.edu">Rongrong Wang</a>, <a href="https://openreview.net/profile?id=~Jiliang_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiliang_Tang1">Jiliang Tang</a>, <a href="https://openreview.net/profile?email=myan%40msu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="myan@msu.edu">Ming Yan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#84gjULz1t5-details-618" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="84gjULz1t5-details-618"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Decentralized Optimization, Communication Compression, Linear Convergence, Heterogeneous data</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Communication compression has become a key strategy to speed up distributed optimization. However, existing decentralized algorithms with compression mainly focus on compressing DGD-type algorithms. They are unsatisfactory in terms of convergence rate, stability, and the capability to handle heterogeneous data. Motivated by primal-dual algorithms, this paper proposes the first \underline{L}in\underline{EA}r convergent \underline{D}ecentralized algorithm with compression, LEAD. Our theory describes the coupled dynamics of the inexact primal and dual update as well as compression error, and we provide the first consensus error bound in such settings without assuming bounded gradients. Experiments on convex problems validate our theoretical analysis, and empirical study on deep neural nets shows that LEAD is applicable to non-convex problems.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A Linear Convergent Decentralized Optimization with Communication Compression</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="uR9LaO_QxF" data-number="1854">
      <h4>
        <a href="https://openreview.net/forum?id=uR9LaO_QxF">
            Efficient Transformers in Reinforcement Learning using Actor-Learner Distillation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=uR9LaO_QxF" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Emilio_Parisotto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emilio_Parisotto1">Emilio Parisotto</a>, <a href="https://openreview.net/profile?id=~Russ_Salakhutdinov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Russ_Salakhutdinov1">Russ Salakhutdinov</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#uR9LaO_QxF-details-160" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uR9LaO_QxF-details-160"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep Reinforcement Learning, Memory, Transformers, Distillation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Many real-world applications such as robotics provide hard constraints on power and compute that limit the viable model complexity of Reinforcement Learning (RL) agents. Similarly, in many distributed RL settings, acting is done on un-accelerated hardware such as CPUs, which likewise restricts model size to prevent intractable experiment run times. These "actor-latency" constrained settings present a major obstruction to the scaling up of model complexity that has recently been extremely successful in supervised learning. To be able to utilize large model capacity while still operating within the limits imposed by the system during acting, we develop an "Actor-Learner Distillation" (ALD) procedure that leverages a continual form of distillation that transfers learning progress from a large capacity learner model to a small capacity actor model. As a case study, we develop this procedure in the context of partially-observable environments, where transformer models have had large improvements over LSTMs recently, at the cost of significantly higher computational complexity. With transformer models as the learner and LSTMs as the actor, we demonstrate in several challenging memory environments that using Actor-Learner Distillation largely recovers the clear sample-efficiency gains of the transformer learner model while maintaining the fast inference and reduced total training time of the LSTM actor model.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Actor-Learner Distillation uses an continual form of distillation to attain the high sample-efficiency of transformers while maintaining the reduced total training time of LSTMs in RL applications.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="X4y_10OX-hX" data-number="868">
      <h4>
        <a href="https://openreview.net/forum?id=X4y_10OX-hX">
            Large Associative Memory Problem in Neurobiology and Machine Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=X4y_10OX-hX" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Dmitry_Krotov2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dmitry_Krotov2">Dmitry Krotov</a>, <a href="https://openreview.net/profile?id=~John_J._Hopfield1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_J._Hopfield1">John J. Hopfield</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#X4y_10OX-hX-details-614" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="X4y_10OX-hX-details-614"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">associative memory, Hopfield networks, modern Hopfield networks, neuroscience</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Dense Associative Memories or modern Hopfield networks permit storage and reliable  retrieval of an exponentially large (in the dimension of feature space) number of memories. At the same time, their naive implementation is non-biological, since it seemingly requires the existence of many-body synaptic junctions between the neurons.  We show that these models are effective descriptions of a more microscopic (written in terms of biological degrees of freedom) theory that has additional (hidden) neurons and only requires two-body interactions between them. For this reason our proposed microscopic theory is a valid model of large associative memory with a degree of biological plausibility. The dynamics of our network and its reduced dimensional equivalent both minimize energy (Lyapunov) functions. When certain dynamical variables (hidden neurons) are integrated out from our microscopic theory, one can recover many of the models that were previously discussed in the literature, e.g. the model presented in "Hopfield Networks is All You Need" paper. We also provide an alternative derivation of the energy function and the update rule proposed in the aforementioned paper and clarify the relationships between various models of this class.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Our paper proposes a microscopic biologically-plausible theory of modern Hopfield networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="LucJxySuJcE" data-number="215">
      <h4>
        <a href="https://openreview.net/forum?id=LucJxySuJcE">
            Protecting DNNs from Theft using an Ensemble of Diverse Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=LucJxySuJcE" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sanjay_Kariyappa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanjay_Kariyappa1">Sanjay Kariyappa</a>, <a href="https://openreview.net/profile?id=~Atul_Prakash1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Atul_Prakash1">Atul Prakash</a>, <a href="https://openreview.net/profile?id=~Moinuddin_K_Qureshi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Moinuddin_K_Qureshi2">Moinuddin K Qureshi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 07 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#LucJxySuJcE-details-724" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LucJxySuJcE-details-724"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Model stealing, machine learning security</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Several recent works have demonstrated highly effective model stealing (MS) attacks on Deep Neural Networks (DNNs) in black-box settings, even when the training data is unavailable. These attacks typically use some form of Out of Distribution (OOD) data to query the target model and use the predictions obtained to train a clone model. Such a clone model learns to approximate the decision boundary of the target model, achieving high accuracy on in-distribution examples. We propose Ensemble of Diverse Models (EDM) to defend against such MS attacks. EDM is made up of models that are trained to produce dissimilar predictions for OOD inputs. By using a different member of the ensemble to service different queries, our defense produces predictions that are highly discontinuous in the input space for the adversary's OOD queries. Such discontinuities cause the clone model trained on these predictions to have poor generalization on in-distribution examples. Our evaluations on several image classification tasks demonstrate that EDM defense can severely degrade the accuracy of clone models (up to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="80" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c39"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c37"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>39.7</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container>). Our defense has minimal impact on the target accuracy, negligible computational costs during inference, and is compatible with existing defenses for MS attacks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Discontinuous predictions produced by an ensemble of diverse models can be used to create an effective defense against model stealing attacks.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=LucJxySuJcE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="LVotkZmYyDi" data-number="664">
      <h4>
        <a href="https://openreview.net/forum?id=LVotkZmYyDi">
            Proximal Gradient Descent-Ascent: Variable Convergence under KŁ Geometry
        </a>
      
        
          <a href="https://openreview.net/pdf?id=LVotkZmYyDi" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ziyi_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziyi_Chen2">Ziyi Chen</a>, <a href="https://openreview.net/profile?id=~Yi_Zhou2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Zhou2">Yi Zhou</a>, <a href="https://openreview.net/profile?id=~Tengyu_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tengyu_Xu1">Tengyu Xu</a>, <a href="https://openreview.net/profile?id=~Yingbin_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yingbin_Liang1">Yingbin Liang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#LVotkZmYyDi-details-357" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LVotkZmYyDi-details-357"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Kurdyka-Łojasiewicz geometry, minimax, nonconvex, proximal gradient descent-ascent, variable convergence</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The gradient descent-ascent (GDA) algorithm has been widely applied to solve minimax optimization problems. In order to achieve convergent policy parameters for minimax optimization, it is important that GDA generates convergent variable sequences rather than convergent sequences of function value or gradient norm. However, the variable convergence of GDA has been proved only under convexity geometries, and it is lack of understanding in general nonconvex minimax optimization. This paper fills such a gap by studying the convergence of a more general proximal-GDA for regularized nonconvex-strongly-concave minimax optimization. Specifically, we show that proximal-GDA admits a novel Lyapunov function, which monotonically decreases in the minimax optimization process and drives the variable sequences to a critical point. By leveraging this Lyapunov function and the KL geometry that parameterizes the local geometries of general nonconvex functions, we formally establish the variable convergence of proximal-GDA to a certain critical point <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="81" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mo class="mjx-n" size="s"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>x</mi><mo>∗</mo></msup></math></mjx-assistive-mml></mjx-container>, i.e., <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="82" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mo class="mjx-n" size="s"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mo class="mjx-n" size="s"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mo class="mjx-n" size="s"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>t</mi></msub><mo accent="false" stretchy="false">→</mo><msup><mi>x</mi><mo>∗</mo></msup><mo>,</mo><msub><mi>y</mi><mi>t</mi></msub><mo accent="false" stretchy="false">→</mo><msup><mi>y</mi><mo>∗</mo></msup><mo stretchy="false">(</mo><msup><mi>x</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>. Furthermore, over the full spectrum of the KL-parameterized geometry, we show that proximal-GDA achieves different types of convergence rates ranging from sublinear convergence up to finite-step convergence, depending on the geometry associated with the KL parameter. This is the first theoretical result on the variable convergence for nonconvex minimax optimization. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This is the first work on variable convergence of proximal gradient descent-ascent algorithm for nonconvex minimax optimization under ubiquitous Kurdyka-Łojasiewicz geometry.  </span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ct8_a9h1M" data-number="1270">
      <h4>
        <a href="https://openreview.net/forum?id=ct8_a9h1M">
            Contextual Dropout: An Efficient Sample-Dependent Dropout Module
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ct8_a9h1M" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~XINJIE_FAN2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~XINJIE_FAN2">XINJIE FAN</a>, <a href="https://openreview.net/profile?id=~Shujian_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shujian_Zhang1">Shujian Zhang</a>, <a href="https://openreview.net/profile?email=korawat.tanwisuth%40utexas.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="korawat.tanwisuth@utexas.edu">Korawat Tanwisuth</a>, <a href="https://openreview.net/profile?id=~Xiaoning_Qian2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoning_Qian2">Xiaoning Qian</a>, <a href="https://openreview.net/profile?id=~Mingyuan_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingyuan_Zhou1">Mingyuan Zhou</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 07 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ct8_a9h1M-details-364" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ct8_a9h1M-details-364"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Efficient Inference Methods, Probabilistic Methods, Supervised Deep Networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Dropout has been demonstrated as a simple and effective module to not only regularize the training process of deep neural networks, but also provide the uncertainty estimation for prediction. However, the quality of uncertainty estimation is highly dependent on the dropout probabilities. Most current models use the same dropout distributions across all data samples due to its simplicity.  Despite the potential gains in the flexibility of modeling uncertainty, sample-dependent dropout, on the other hand, is less explored as it often encounters scalability issues or involves non-trivial model changes.  In this paper, we propose contextual dropout with an efficient structural design as a simple and scalable sample-dependent dropout module, which can be applied to a wide range of models at the expense of only slightly increased memory and computational cost. We learn the dropout probabilities with a variational objective, compatible with both Bernoulli dropout and Gaussian dropout. We apply the contextual dropout module to various models with applications to image classification and visual question answering and demonstrate the scalability of the method with large-scale datasets, such as ImageNet and VQA 2.0. Our experimental results show that the proposed method outperforms baseline methods in terms of both accuracy and quality of uncertainty estimation.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose contextual dropout as a scalable sample-dependent dropout method, which makes the dropout probabilities depend on the input covariates of each data sample.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=ct8_a9h1M&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="W1G1JZEIy5_" data-number="2175">
      <h4>
        <a href="https://openreview.net/forum?id=W1G1JZEIy5_">
            MIROSTAT: A NEURAL TEXT DECODING ALGORITHM THAT DIRECTLY CONTROLS PERPLEXITY
        </a>
      
        
          <a href="https://openreview.net/pdf?id=W1G1JZEIy5_" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sourya_Basu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sourya_Basu1">Sourya Basu</a>, <a href="https://openreview.net/profile?email=gramachandran%40salesforce.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="gramachandran@salesforce.com">Govardana Sachitanandam Ramachandran</a>, <a href="https://openreview.net/profile?id=~Nitish_Shirish_Keskar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nitish_Shirish_Keskar1">Nitish Shirish Keskar</a>, <a href="https://openreview.net/profile?id=~Lav_R._Varshney1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lav_R._Varshney1">Lav R. Varshney</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#W1G1JZEIy5_-details-9" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="W1G1JZEIy5_-details-9"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural text decoding, sampling algorithms, cross-entropy, repetitions, incoherence</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Neural text decoding algorithms strongly influence the quality of texts generated using language models, but popular algorithms like top-k, top-p (nucleus), and temperature-based sampling may yield texts that have objectionable repetition or incoherence. Although these methods generate high-quality text after ad hoc parameter tuning that depends on the language model and the length of generated text, not much is known about the control they provide over the statistics of the output. This is important, however, since recent reports show that humans prefer when perplexity is neither too much nor too little and since we experimentally show that cross-entropy (log of perplexity) has a near-linear relation with repetition. First, we provide a theoretical analysis of perplexity in top-k, top-p, and temperature sampling, under Zipfian statistics. Then, we use this analysis to design a feedback-based adaptive top-k text decoding algorithm called mirostat that generates text (of any length) with a predetermined target value of perplexity without any tuning. Experiments show that for low values of k and p, perplexity drops significantly with generated text length and leads to excessive repetitions (the boredom trap). Contrarily, for large values of k and p, perplexity increases with generated text length and leads to incoherence (confusion trap). Mirostat avoids both traps. Specifically, we show that setting target perplexity value beyond a threshold yields negligible sentence-level repetitions. Experiments with
      human raters for fluency, coherence, and quality further verify our findings.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We provide a new text decoding algorithm that directly controls perplexity and hence several important attributes of generated text.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=W1G1JZEIy5_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="kDnal_bbb-E" data-number="2807">
      <h4>
        <a href="https://openreview.net/forum?id=kDnal_bbb-E">
            DialoGraph: Incorporating Interpretable Strategy-Graph Networks into Negotiation Dialogues
        </a>
      
        
          <a href="https://openreview.net/pdf?id=kDnal_bbb-E" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Rishabh_Joshi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rishabh_Joshi1">Rishabh Joshi</a>, <a href="https://openreview.net/profile?id=~Vidhisha_Balachandran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vidhisha_Balachandran1">Vidhisha Balachandran</a>, <a href="https://openreview.net/profile?id=~Shikhar_Vashishth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shikhar_Vashishth1">Shikhar Vashishth</a>, <a href="https://openreview.net/profile?id=~Alan_Black1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alan_Black1">Alan Black</a>, <a href="https://openreview.net/profile?id=~Yulia_Tsvetkov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yulia_Tsvetkov1">Yulia Tsvetkov</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#kDnal_bbb-E-details-696" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kDnal_bbb-E-details-696"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">negotiation, dialogue, graph neural networks, interpretability, structure</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">To successfully negotiate a deal, it is not enough to communicate fluently: pragmatic planning of persuasive negotiation strategies is essential. While modern dialogue agents excel at generating fluent sentences, they still lack pragmatic grounding and cannot reason strategically. We present DialoGraph, a negotiation system that incorporates pragmatic strategies in a negotiation dialogue using graph neural networks. DialoGraph explicitly incorporates dependencies between sequences of strategies to enable improved and interpretable prediction of next optimal strategies, given the dialogue context. Our graph-based method outperforms prior state-of-the-art negotiation models both in the accuracy of strategy/dialogue act prediction and in the quality of downstream dialogue response generation. We qualitatively show further benefits of learned strategy-graphs in providing explicit associations between effective negotiation strategies over the course of the dialogue, leading to interpretable and strategic dialogues.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose DialoGraph, a negotiation dialogue system that leverages Graph Attention Networks to model complex negotiation strategies while providing interpretability for the model via intermediate structures.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="4c0J6lwQ4_" data-number="881">
      <h4>
        <a href="https://openreview.net/forum?id=4c0J6lwQ4_">
            Multi-Time Attention Networks for Irregularly Sampled Time Series
        </a>
      
        
          <a href="https://openreview.net/pdf?id=4c0J6lwQ4_" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Satya_Narayan_Shukla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Satya_Narayan_Shukla1">Satya Narayan Shukla</a>, <a href="https://openreview.net/profile?id=~Benjamin_Marlin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Marlin1">Benjamin Marlin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#4c0J6lwQ4_-details-325" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4c0J6lwQ4_-details-325"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">irregular sampling, multivariate time series, attention, missing data</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Irregular sampling occurs in many time series modeling applications where it presents a significant challenge to standard deep learning models. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. In this paper, we propose a new deep learning framework for this setting that we call Multi-Time Attention Networks. Multi-Time Attention Networks learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. We investigate the performance of this framework on interpolation and classification tasks using multiple datasets. Our results show that the proposed approach performs as well or better than a range of baseline and recently proposed models while offering significantly faster training times than current state-of-the-art methods.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper presents a new architecture for learning with sparse and irregularly sampled multivariate time series that achieves improved performance than current state-of-the-art methods while providing significantly reduced training times.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="aD1_5zowqV" data-number="742">
      <h4>
        <a href="https://openreview.net/forum?id=aD1_5zowqV">
            Learning Energy-Based Generative Models via Coarse-to-Fine Expanding and Sampling
        </a>
      
        
          <a href="https://openreview.net/pdf?id=aD1_5zowqV" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yang_Zhao5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Zhao5">Yang Zhao</a>, <a href="https://openreview.net/profile?id=~Jianwen_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianwen_Xie1">Jianwen Xie</a>, <a href="https://openreview.net/profile?id=~Ping_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ping_Li3">Ping Li</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 28 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>24 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#aD1_5zowqV-details-71" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="aD1_5zowqV-details-71"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Energy-based model, generative model, image translation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Energy-based models (EBMs) parametrized by neural networks can be trained by Markov chain Monte Carlo (MCMC) sampling-based maximum likelihood estimation. Despite the recent significant success of EBMs in data generation, the current approaches to train EBMs can be unstable and sometimes may have difficulty synthesizing diverse and high-fidelity images. In this paper, we propose to train EBMs via a multistage coarse-to-fine expanding and sampling strategy, namely CF-EBM. To improve the learning procedure, we propose an effective net architecture and advocate applying smooth activations. The resulting approach is computationally efficient and achieves the best performance on image generation amongst EBMs and the spectral normalization GAN. Furthermore, we provide a recipe for being the first successful EBM to synthesize <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="83" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>512</mn><mo>×</mo><mn>512</mn></math></mjx-assistive-mml></mjx-container>-pixel images and also improve out-of-distribution detection. In the end, we effortlessly generalize CF-EBM to the one-sided unsupervised image-to-image translation and beat baseline methods with the model size and the training budget largely reduced. In parallel, we present a gradient-based discriminative saliency method to interpret the translation dynamics which align with human behavior explicitly.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a coarse-to-fine energy-based model learning scheme for generative modeling.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=aD1_5zowqV&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="43VKWxg_Sqr" data-number="1197">
      <h4>
        <a href="https://openreview.net/forum?id=43VKWxg_Sqr">
            Unsupervised Audiovisual Synthesis via Exemplar Autoencoders
        </a>
      
        
          <a href="https://openreview.net/pdf?id=43VKWxg_Sqr" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kangle_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kangle_Deng1">Kangle Deng</a>, <a href="https://openreview.net/profile?id=~Aayush_Bansal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aayush_Bansal1">Aayush Bansal</a>, <a href="https://openreview.net/profile?id=~Deva_Ramanan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deva_Ramanan1">Deva Ramanan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#43VKWxg_Sqr-details-574" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="43VKWxg_Sqr-details-574"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">unsupervised learning, autoencoders, speech-impaired, assistive technology, audiovisual synthesis, voice conversion</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present an unsupervised approach that converts the input speech of any individual into audiovisual streams of potentially-infinitely many output speakers. Our approach builds on simple autoencoders that project out-of-sample data onto the distribution of the training set. We use exemplar autoencoders to learn the voice, stylistic prosody, and visual appearance of a specific target exemplar speech. In contrast to existing methods, the proposed approach can be easily extended to an arbitrarily large number of speakers and styles using only 3 minutes of target audio-video data, without requiring any training data for the input speaker. To do so, we learn audiovisual bottleneck representations that capture the structured linguistic content of speech. We outperform prior approaches on both audio and video synthesis.
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present an unsupervised approach that converts the input speech of any individual into audiovisual streams of potentially-infinitely many output speakers.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=43VKWxg_Sqr&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="7aL-OtQrBWD" data-number="1915">
      <h4>
        <a href="https://openreview.net/forum?id=7aL-OtQrBWD">
            A Learning Theoretic Perspective on Local Explainability
        </a>
      
        
          <a href="https://openreview.net/pdf?id=7aL-OtQrBWD" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jeffrey_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jeffrey_Li1">Jeffrey Li</a>, <a href="https://openreview.net/profile?id=~Vaishnavh_Nagarajan3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vaishnavh_Nagarajan3">Vaishnavh Nagarajan</a>, <a href="https://openreview.net/profile?id=~Gregory_Plumb2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gregory_Plumb2">Gregory Plumb</a>, <a href="https://openreview.net/profile?id=~Ameet_Talwalkar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ameet_Talwalkar1">Ameet Talwalkar</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#7aL-OtQrBWD-details-691" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7aL-OtQrBWD-details-691"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Interpretability, Learning Theory, Local Explanations, Generalization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we explore connections between interpretable machine learning and learning theory through the lens of local approximation explanations. First, we tackle the traditional problem of performance generalization and bound the test-time predictive accuracy of a model using a notion of how locally explainable it is.  Second, we explore the novel problem of explanation generalization which is an important concern for a growing class of finite sample-based local approximation explanations. Finally, we validate our theoretical results empirically and show that they reflect what can be seen in practice.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=7aL-OtQrBWD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="AHm3dbp7D1D" data-number="149">
      <h4>
        <a href="https://openreview.net/forum?id=AHm3dbp7D1D">
            SEED: Self-supervised Distillation For Visual Representation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=AHm3dbp7D1D" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhiyuan_Fang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiyuan_Fang1">Zhiyuan Fang</a>, <a href="https://openreview.net/profile?id=~Jianfeng_Wang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianfeng_Wang4">Jianfeng Wang</a>, <a href="https://openreview.net/profile?id=~Lijuan_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lijuan_Wang1">Lijuan Wang</a>, <a href="https://openreview.net/profile?id=~Lei_Zhang23" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lei_Zhang23">Lei Zhang</a>, <a href="https://openreview.net/profile?id=~Yezhou_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yezhou_Yang1">Yezhou Yang</a>, <a href="https://openreview.net/profile?id=~Zicheng_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zicheng_Liu1">Zicheng Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#AHm3dbp7D1D-details-56" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AHm3dbp7D1D-details-56"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Self Supervised Learning, Knowledge Distillation, Representation Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper is concerned with self-supervised learning for small models. The problem is motivated by our empirical studies that while the widely used contrastive self-supervised learning method has shown great progress on large model training, it does not work well for small models. To address this problem, we propose a new learning paradigm, named <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="84" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-b"><mjx-c class="mjx-c1D412 TEX-B"></mjx-c><mjx-c class="mjx-c1D404 TEX-B"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="bold">SE</mtext></math></mjx-assistive-mml></mjx-container>lf-Sup<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="85" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-b"><mjx-c class="mjx-c1D404 TEX-B"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="bold">E</mtext></math></mjx-assistive-mml></mjx-container>rvised <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="86" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-b"><mjx-c class="mjx-c1D403 TEX-B"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="bold">D</mtext></math></mjx-assistive-mml></mjx-container>istillation (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="87" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mstyle size="lg"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi></mjx-mstyle></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathsize="1.2em"><mi>S</mi></mstyle></mrow></math></mjx-assistive-mml></mjx-container>EED), where we leverage a larger network (as Teacher) to transfer its representational knowledge into a smaller architecture (as Student) in a self-supervised fashion. Instead of directly learning from unlabeled data, we train a student encoder to mimic the similarity score distribution inferred by a teacher over a set of instances. We show that <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="88" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mstyle size="lg"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi></mjx-mstyle></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathsize="1.2em"><mi>S</mi></mstyle></mrow></math></mjx-assistive-mml></mjx-container>EED dramatically boosts the performance of small networks on downstream tasks. Compared with self-supervised baselines, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="89" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mstyle size="lg"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi></mjx-mstyle></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathsize="1.2em"><mi>S</mi></mstyle></mrow></math></mjx-assistive-mml></mjx-container>EED improves the top-1 accuracy from 42.2% to 67.6% on EfficientNet-B0 and from 36.3% to 68.2% on MobileNet-v3-Large on the ImageNet-1k dataset. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="90" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mstyle size="lg"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi></mjx-mstyle></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle mathsize="1.2em"><mi>S</mi></mstyle></mrow></math></mjx-assistive-mml></mjx-container>EED, a self-supervised distillation technique for visual representation learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-mWcQVLPSPy" data-number="363">
      <h4>
        <a href="https://openreview.net/forum?id=-mWcQVLPSPy">
            Isometric Propagation Network for Generalized Zero-shot Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-mWcQVLPSPy" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Lu_Liu7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lu_Liu7">Lu Liu</a>, <a href="https://openreview.net/profile?id=~Tianyi_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianyi_Zhou1">Tianyi Zhou</a>, <a href="https://openreview.net/profile?id=~Guodong_Long2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guodong_Long2">Guodong Long</a>, <a href="https://openreview.net/profile?id=~Jing_Jiang6" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jing_Jiang6">Jing Jiang</a>, <a href="https://openreview.net/profile?id=~Xuanyi_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuanyi_Dong1">Xuanyi Dong</a>, <a href="https://openreview.net/profile?id=~Chengqi_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chengqi_Zhang1">Chengqi Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 11 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-mWcQVLPSPy-details-271" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-mWcQVLPSPy-details-271"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Zero-shot learning, isometric, prototype propagation, alignment of semantic and visual space</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Zero-shot learning (ZSL) aims to classify images of an unseen class only based on a few attributes describing that class but no access to any training sample. A popular strategy is to learn a mapping between the semantic space of class attributes and the visual space of images based on the seen classes and their data. Thus, an unseen class image can be ideally mapped to its corresponding class attributes. The key challenge is how to align the representations in the two spaces. For most ZSL settings, the attributes for each seen/unseen class are only represented by a vector while the seen-class data provide much more information. Thus, the imbalanced supervision from the semantic and the visual space can make the learned mapping easily overfitting to the seen classes. To resolve this problem, we propose Isometric Propagation Network (IPN), which learns to strengthen the relation between classes within each space and align the class dependency in the two spaces. Specifically, IPN learns to propagate the class representations on an auto-generated graph within each space. In contrast to only aligning the resulted static representation, we regularize the two dynamic propagation procedures to be isometric in terms of the two graphs' edge weights per step by minimizing a consistency loss between them. IPN achieves state-of-the-art performance on three popular ZSL benchmarks. To evaluate the generalization capability of IPN, we further build two larger benchmarks with more diverse unseen classes and demonstrate the advantages of IPN on them.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We improve the current zero-shot learning performance by a dynamic alignment between the semantic space and visual space that encourages the isometry of the class-prototype propagation procedures in the two spaces. </span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="33rtZ4Sjwjn" data-number="402">
      <h4>
        <a href="https://openreview.net/forum?id=33rtZ4Sjwjn">
            Effective and Efficient Vote Attack on Capsule Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=33rtZ4Sjwjn" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jindong_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jindong_Gu1">Jindong Gu</a>, <a href="https://openreview.net/profile?id=~Baoyuan_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Baoyuan_Wu1">Baoyuan Wu</a>, <a href="https://openreview.net/profile?id=~Volker_Tresp1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Volker_Tresp1">Volker Tresp</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#33rtZ4Sjwjn-details-647" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="33rtZ4Sjwjn-details-647"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Capsule Networks, Adversarial Attacks, Adversarial Example Detection</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Standard Convolutional Neural Networks (CNNs) can be easily fooled by images with small quasi-imperceptible artificial perturbations. As alternatives to CNNs, the recently proposed Capsule Networks (CapsNets) are shown to be more robust to white-box attack than CNNs under popular attack protocols. Besides, the class-conditional reconstruction part of CapsNets is also used to detect adversarial examples. In this work, we investigate the adversarial robustness of CapsNets, especially how the inner workings of CapsNets change when the output capsules are attacked. The first observation is that adversarial examples misled CapsNets by manipulating the votes from primary capsules. Another observation is the high computational cost, when we directly apply multi-step attack methods designed for CNNs to attack CapsNets, due to the computationally expensive routing mechanism. Motivated by these two observations, we propose a novel vote attack where we attack votes of CapsNets directly. Our vote attack is not only effective, but also efficient by circumventing the routing process. Furthermore, we integrate our vote attack into the detection-aware attack paradigm, which can successfully bypass the class-conditional reconstruction based detection method. Extensive experiments demonstrate the superior attack performance of our vote attack on CapsNets.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an effective and efficient vote attack to create adversarial examples and bypass adversarial example detection on Capsule Networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="mEdwVCRJuX4" data-number="926">
      <h4>
        <a href="https://openreview.net/forum?id=mEdwVCRJuX4">
            Heteroskedastic and Imbalanced Deep Learning with Adaptive Regularization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=mEdwVCRJuX4" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kaidi_Cao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaidi_Cao1">Kaidi Cao</a>, <a href="https://openreview.net/profile?id=~Yining_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yining_Chen1">Yining Chen</a>, <a href="https://openreview.net/profile?id=~Junwei_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junwei_Lu1">Junwei Lu</a>, <a href="https://openreview.net/profile?email=nikos.arechiga%40tri.global" class="profile-link" data-toggle="tooltip" data-placement="top" title="nikos.arechiga@tri.global">Nikos Arechiga</a>, <a href="https://openreview.net/profile?id=~Adrien_Gaidon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adrien_Gaidon1">Adrien Gaidon</a>, <a href="https://openreview.net/profile?id=~Tengyu_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tengyu_Ma1">Tengyu Ma</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#mEdwVCRJuX4-details-893" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mEdwVCRJuX4-details-893"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning, noise robust learning, imbalanced learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Real-world large-scale datasets are heteroskedastic and imbalanced --- labels have varying levels of uncertainty and label distributions are long-tailed. Heteroskedasticity and imbalance challenge deep learning algorithms due to the difficulty of distinguishing among mislabeled, ambiguous, and rare examples. Addressing heteroskedasticity and imbalance simultaneously is under-explored. We propose a data-dependent regularization technique for heteroskedastic datasets that regularizes different regions of the input space differently. Inspired by the theoretical derivation of the optimal regularization strength in a one-dimensional nonparametric classification setting, our approach adaptively regularizes the data points in higher-uncertainty, lower-density regions more heavily. We test our method on several benchmark tasks, including a real-world heteroskedastic and imbalanced dataset, WebVision. Our experiments corroborate our theory and demonstrate a significant improvement over other methods in noise-robust deep learning. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a data-dependent regularization technique for learning heteroskedastic and imbalanced datasets.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="3tFAs5E-Pe" data-number="2976">
      <h4>
        <a href="https://openreview.net/forum?id=3tFAs5E-Pe">
            Continuous Wasserstein-2 Barycenter Estimation without Minimax Optimization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=3tFAs5E-Pe" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alexander_Korotin2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Korotin2">Alexander Korotin</a>, <a href="https://openreview.net/profile?email=lingxiao%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="lingxiao@mit.edu">Lingxiao Li</a>, <a href="https://openreview.net/profile?id=~Justin_Solomon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Justin_Solomon1">Justin Solomon</a>, <a href="https://openreview.net/profile?id=~Evgeny_Burnaev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Evgeny_Burnaev1">Evgeny Burnaev</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 14 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#3tFAs5E-Pe-details-477" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3tFAs5E-Pe-details-477"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">wasserstein-2 barycenters, non-minimax optimization, cycle-consistency regularizer, input convex neural networks, continuous case</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Wasserstein barycenters provide a geometric notion of the weighted average of probability measures based on optimal transport. In this paper, we present a scalable algorithm to compute Wasserstein-2 barycenters given sample access to the input measures, which are not restricted to being discrete. While past approaches rely on entropic or quadratic regularization, we employ input convex neural networks and cycle-consistency regularization to avoid introducing bias. As a result, our approach does not resort to minimax optimization. We provide theoretical analysis on error bounds as well as empirical evidence of the effectiveness of the proposed approach in low-dimensional qualitative scenarios and high-dimensional quantitative experiments.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a new algorithm to compute Wasserstein-2 barycenters of continuous distributions powered by a straightforward optimization procedure without introducing bias or a generative model.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=3tFAs5E-Pe&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tkAtoZkcUnm" data-number="1944">
      <h4>
        <a href="https://openreview.net/forum?id=tkAtoZkcUnm">
            Neural Thompson Sampling
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tkAtoZkcUnm" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Weitong_ZHANG1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weitong_ZHANG1">Weitong ZHANG</a>, <a href="https://openreview.net/profile?id=~Dongruo_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dongruo_Zhou1">Dongruo Zhou</a>, <a href="https://openreview.net/profile?id=~Lihong_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lihong_Li1">Lihong Li</a>, <a href="https://openreview.net/profile?id=~Quanquan_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Quanquan_Gu1">Quanquan Gu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tkAtoZkcUnm-details-396" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tkAtoZkcUnm-details-396"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep Learning, Contextual Bandits, Thompson sampling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Thompson Sampling (TS) is one of the most effective algorithms for solving contextual multi-armed bandit problems. In this paper, we propose a new algorithm, called Neural Thompson Sampling, which adapts deep neural networks for both exploration and exploitation. At the core of our algorithm is a novel posterior distribution of the reward, where its mean is the neural network approximator, and its variance is built upon the neural tangent features of the corresponding neural network. We prove that, provided the underlying reward function is bounded, the proposed algorithm is guaranteed to achieve a cumulative regret of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="91" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.056em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>T</mi><mrow><mn>1</mn><mrow><mo>/</mo></mrow><mn>2</mn></mrow></msup><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, which matches the regret of other contextual bandit algorithms in terms of total round number <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="92" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container>. Experimental comparisons with other benchmark bandit algorithms on various data sets corroborate our theory.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=tkAtoZkcUnm&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose NeuralTS, a provable neural work-based Thompson sampling algorithm for stochastic contextual bandits.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="q8qLAbQBupm" data-number="1243">
      <h4>
        <a href="https://openreview.net/forum?id=q8qLAbQBupm">
            Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics
        </a>
      
        
          <a href="https://openreview.net/pdf?id=q8qLAbQBupm" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Daniel_Kunin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Kunin1">Daniel Kunin</a>, <a href="https://openreview.net/profile?id=~Javier_Sagastuy-Brena1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Javier_Sagastuy-Brena1">Javier Sagastuy-Brena</a>, <a href="https://openreview.net/profile?id=~Surya_Ganguli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Surya_Ganguli1">Surya Ganguli</a>, <a href="https://openreview.net/profile?id=~Daniel_LK_Yamins1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_LK_Yamins1">Daniel LK Yamins</a>, <a href="https://openreview.net/profile?id=~Hidenori_Tanaka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hidenori_Tanaka1">Hidenori Tanaka</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#q8qLAbQBupm-details-799" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="q8qLAbQBupm-details-799"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">learning dynamics, symmetry, loss landscape, stochastic differential equation, modified equation analysis, conservation law, hessian, geometry, physics, gradient flow</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Understanding the dynamics of neural network parameters during training is one of the key challenges in building a theoretical foundation for deep learning. A central obstacle is that the motion of a network in high-dimensional parameter space undergoes discrete finite steps along complex stochastic gradients derived from real-world datasets. We circumvent this obstacle through a unifying theoretical framework based on intrinsic symmetries embedded in a network's architecture that are present for any dataset. We show that any such symmetry imposes stringent geometric constraints on gradients and Hessians, leading to an associated conservation law in the continuous-time limit of stochastic gradient descent (SGD), akin to Noether's theorem in physics. We further show that finite learning rates used in practice can actually break these symmetry induced conservation laws. We apply tools from finite difference methods to derive modified gradient flow, a differential equation that better approximates the numerical trajectory taken by SGD at finite learning rates. We combine modified gradient flow with our framework of symmetries to derive exact integral expressions for the dynamics of certain parameter combinations. We empirically validate our analytic expressions for learning dynamics on VGG-16 trained on Tiny ImageNet. Overall, by exploiting symmetry, our work demonstrates that we can analytically describe the learning dynamics of various parameter combinations at finite learning rates and batch sizes for state of the art architectures trained on any dataset.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">By exploiting architectural symmetry, our work demonstrates that we can analytically describe the learning dynamics of various parameter combinations at finite learning rates and batch sizes for state of the art architectures trained on any dataset.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=q8qLAbQBupm&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="EoFNy62JGd" data-number="1539">
      <h4>
        <a href="https://openreview.net/forum?id=EoFNy62JGd">
            Neural gradients are near-lognormal: improved quantized  and sparse training
        </a>
      
        
          <a href="https://openreview.net/pdf?id=EoFNy62JGd" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Brian_Chmiel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brian_Chmiel1">Brian Chmiel</a>, <a href="https://openreview.net/profile?email=liadgo2%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="liadgo2@gmail.com">Liad Ben-Uri</a>, <a href="https://openreview.net/profile?id=~Moran_Shkolnik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Moran_Shkolnik1">Moran Shkolnik</a>, <a href="https://openreview.net/profile?id=~Elad_Hoffer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Elad_Hoffer1">Elad Hoffer</a>, <a href="https://openreview.net/profile?id=~Ron_Banner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ron_Banner1">Ron Banner</a>, <a href="https://openreview.net/profile?id=~Daniel_Soudry1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Soudry1">Daniel Soudry</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#EoFNy62JGd-details-277" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EoFNy62JGd-details-277"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While training can mostly be accelerated by reducing the time needed to propagate neural gradients (loss gradients with respect to the intermediate neural layer outputs) back throughout the model, most previous works focus on the quantization/pruning of weights and activations. These methods are often not applicable to neural gradients, which have very different statistical properties. Distinguished from weights and activations, we find that the distribution of neural gradients is approximately lognormal. Considering this, we suggest two closed-form analytical methods to reduce the computational and memory burdens of neural gradients. The first method optimizes the floating-point format and scale of the gradients. The second method accurately sets sparsity thresholds for gradient pruning.  Each method achieves state-of-the-art results on ImageNet. To the best of our knowledge, this paper is the first to (1) quantize the gradients to 6-bit floating-point formats, or (2) achieve up to 85% gradient sparsity --- in each case without accuracy degradation.
      Reference implementation accompanies the paper in the supplementary material.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=EoFNy62JGd&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="TTUVg6vkNjK" data-number="850">
      <h4>
        <a href="https://openreview.net/forum?id=TTUVg6vkNjK">
            RODE: Learning Roles to Decompose Multi-Agent Tasks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=TTUVg6vkNjK" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tonghan_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tonghan_Wang1">Tonghan Wang</a>, <a href="https://openreview.net/profile?id=~Tarun_Gupta3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tarun_Gupta3">Tarun Gupta</a>, <a href="https://openreview.net/profile?id=~Anuj_Mahajan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anuj_Mahajan1">Anuj Mahajan</a>, <a href="https://openreview.net/profile?id=~Bei_Peng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bei_Peng2">Bei Peng</a>, <a href="https://openreview.net/profile?id=~Shimon_Whiteson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shimon_Whiteson1">Shimon Whiteson</a>, <a href="https://openreview.net/profile?id=~Chongjie_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongjie_Zhang1">Chongjie Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#TTUVg6vkNjK-details-35" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TTUVg6vkNjK-details-35"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multi-Agent Reinforcement Learning, Role-Based Learning, Hierarchical Multi-Agent Learning, Multi-Agent Transfer Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Role-based learning holds the promise of achieving scalable multi-agent learning by decomposing complex tasks using roles. However, it is largely unclear how to efficiently discover such a set of roles. To solve this problem, we propose to first decompose joint action spaces into restricted role action spaces by clustering actions according to their effects on the environment and other agents. Learning a role selector based on action effects makes role discovery much easier because it forms a bi-level learning hierarchy: the role selector searches in a smaller role space and at a lower temporal resolution, while role policies learn in significantly reduced primitive action-observation spaces. We further integrate information about action effects into the role policies to boost learning efficiency and policy generalization. By virtue of these advances, our method (1) outperforms the current state-of-the-art MARL algorithms on 9 of the 14 scenarios that comprise the challenging StarCraft II micromanagement benchmark and (2) achieves rapid transfer to new environments with three times the number of agents. Demonstrative videos can be viewed at https://sites.google.com/view/rode-marl.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a scalable role-based multi-agent learning method which effectively discovers roles based on joint action space decomposition according to action effects, establishing a new state of the art on the StarCraft multi-agent benchmark.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=TTUVg6vkNjK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="3RLN4EPMdYd" data-number="1712">
      <h4>
        <a href="https://openreview.net/forum?id=3RLN4EPMdYd">
            Revisiting Hierarchical Approach for Persistent Long-Term Video Prediction
        </a>
      
        
          <a href="https://openreview.net/pdf?id=3RLN4EPMdYd" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Wonkwang_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wonkwang_Lee2">Wonkwang Lee</a>, <a href="https://openreview.net/profile?id=~Whie_Jung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Whie_Jung1">Whie Jung</a>, <a href="https://openreview.net/profile?id=~Han_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Han_Zhang1">Han Zhang</a>, <a href="https://openreview.net/profile?id=~Ting_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ting_Chen1">Ting Chen</a>, <a href="https://openreview.net/profile?id=~Jing_Yu_Koh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jing_Yu_Koh2">Jing Yu Koh</a>, <a href="https://openreview.net/profile?email=thomaseh%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="thomaseh@umich.edu">Thomas Huang</a>, <a href="https://openreview.net/profile?id=~Hyungsuk_Yoon2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hyungsuk_Yoon2">Hyungsuk Yoon</a>, <a href="https://openreview.net/profile?id=~Honglak_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Honglak_Lee2">Honglak Lee</a>, <a href="https://openreview.net/profile?id=~Seunghoon_Hong2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seunghoon_Hong2">Seunghoon Hong</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#3RLN4EPMdYd-details-480" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3RLN4EPMdYd-details-480"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Video prediction, generative model, long-term prediction</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Learning to predict the long-term future of video frames is notoriously challenging due to the inherent ambiguities in a distant future and dramatic amplification of prediction error over time. Despite the recent advances in the literature, existing approaches are limited to moderately short-term prediction (less than a few seconds), while extrapolating it to a longer future quickly leads to destruction in structure and content. In this work, we revisit the hierarchical models in video prediction. Our method generates future frames by first estimating a sequence of dense semantic structures and subsequently translating the estimated structures to pixels by video-to-video translation model. Despite the simplicity, we show that modeling structures and their dynamics in categorical structure space with stochastic sequential estimator leads to surprisingly successful long-term prediction. We evaluate our method on two challenging video prediction scenarios, \emph{car driving} and \emph{human dancing}, and demonstrate that it can generate complicated scene structures and motions over a very long time horizon (\ie~thousands frames), setting a new standard of video prediction with orders of magnitude longer prediction time than existing approaches. Video results are available at https://1konny.github.io/HVP/.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a simple yet effective hierarchical video prediction model that can synthesize future frames orders of magnitude longer than existing methods (thousands frames)</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="vyY0jnWG-tK" data-number="3030">
      <h4>
        <a href="https://openreview.net/forum?id=vyY0jnWG-tK">
            Physics-aware, probabilistic model order reduction with guaranteed stability
        </a>
      
        
          <a href="https://openreview.net/pdf?id=vyY0jnWG-tK" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sebastian_Kaltenbach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sebastian_Kaltenbach1">Sebastian Kaltenbach</a>, <a href="https://openreview.net/profile?id=~Phaedon_Stelios_Koutsourelakis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Phaedon_Stelios_Koutsourelakis1">Phaedon Stelios Koutsourelakis</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#vyY0jnWG-tK-details-148" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vyY0jnWG-tK-details-148"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">inductive bias, probabilistic generative models, state-space models, model order reduction, slowness, long-term stability</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Given (small amounts of) time-series' data from  a high-dimensional, fine-grained, multiscale dynamical system, we propose a generative framework for learning an effective, lower-dimensional, coarse-grained dynamical model that is predictive of the fine-grained system's long-term evolution but also of its behavior under different initial conditions.
      We target fine-grained models as they arise in physical applications (e.g. molecular dynamics, agent-based models), the dynamics  of which are strongly non-stationary but their transition to equilibrium is governed by unknown slow processes which are largely inaccessible by brute-force simulations.
      Approaches based on domain knowledge heavily rely on physical insight in identifying temporally slow features and fail to enforce the long-term stability of the learned dynamics. On the other hand, purely statistical frameworks lack interpretability and rely on large amounts of expensive simulation data (long and multiple trajectories) as they cannot infuse domain knowledge. 
      The generative framework proposed achieves  the aforementioned desiderata by  employing a flexible prior on the complex plane for the latent, slow processes, and  an intermediate layer of physics-motivated latent variables that reduces reliance on data and imbues inductive bias. In contrast to existing schemes, it does not require  the a priori definition of projection operators from the fine-grained description and addresses simultaneously the tasks of dimensionality reduction and model estimation.
      We demonstrate its efficacy and accuracy in multiscale physical systems of particle dynamics where probabilistic, long-term predictions of phenomena not contained in the training data are produced.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a novel physics-aware, generative, probabilistic state-space model for learning an effective, lower-dimensional description that can produce long-term predictions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="kLbhLJ8OT12" data-number="443">
      <h4>
        <a href="https://openreview.net/forum?id=kLbhLJ8OT12">
            Modelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System
        </a>
      
        
          <a href="https://openreview.net/pdf?id=kLbhLJ8OT12" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jianhong_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianhong_Wang1">Jianhong Wang</a>, <a href="https://openreview.net/profile?id=~Yuan_Zhang7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuan_Zhang7">Yuan Zhang</a>, <a href="https://openreview.net/profile?id=~Tae-Kyun_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tae-Kyun_Kim2">Tae-Kyun Kim</a>, <a href="https://openreview.net/profile?email=yunjie.gu%40imperial.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="yunjie.gu@imperial.ac.uk">Yunjie Gu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 20 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#kLbhLJ8OT12-details-744" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kLbhLJ8OT12-details-744"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Task-oriented Dialogue System, Natural Language Processing, Hierarchical Reinforcement Learning, Policy Optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Designing task-oriented dialogue systems is a challenging research topic, since it needs not only to generate utterances fulfilling user requests but also to guarantee the comprehensibility. Many previous works trained end-to-end (E2E) models with supervised learning (SL), however, the bias in annotated system utterances remains as a bottleneck. Reinforcement learning (RL) deals with the problem through using non-differentiable evaluation metrics (e.g., the success rate) as rewards. Nonetheless, existing works with RL showed that the comprehensibility of generated system utterances could be corrupted when improving the performance on fulfilling user requests. In our work, we (1) propose modelling the hierarchical structure between dialogue policy and natural language generator (NLG) with the option framework, called HDNO, where the latent dialogue act is applied to avoid designing specific dialogue act representations; (2) train HDNO via hierarchical reinforcement learning (HRL), as well as suggest the asynchronous updates between dialogue policy and NLG during training to theoretically guarantee their convergence to a local maximizer; and (3) propose using a discriminator modelled with language models as an additional reward to further improve the comprehensibility. We test HDNO on MultiWoz 2.0 and MultiWoz 2.1, the datasets on multi-domain dialogues, in comparison with word-level E2E model trained with RL, LaRL and HDSA, showing improvements on the performance evaluated by automatic evaluation metrics and human evaluation. Finally, we demonstrate the semantic meanings of latent dialogue acts to show the explanability for HDNO.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a novel algorithm called HDNO for policy optimization for task-oriented dialogue system so that the performance on the comprehensibility of generated responses is improved compared with other RL-based algorithms.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="hb1sDDSLbV" data-number="2015">
      <h4>
        <a href="https://openreview.net/forum?id=hb1sDDSLbV">
            Learning explanations that are hard to vary
        </a>
      
        
          <a href="https://openreview.net/pdf?id=hb1sDDSLbV" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Giambattista_Parascandolo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Giambattista_Parascandolo1">Giambattista Parascandolo</a>, <a href="https://openreview.net/profile?id=~Alexander_Neitz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Neitz1">Alexander Neitz</a>, <a href="https://openreview.net/profile?id=~ANTONIO_ORVIETO2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~ANTONIO_ORVIETO2">ANTONIO ORVIETO</a>, <a href="https://openreview.net/profile?id=~Luigi_Gresele1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luigi_Gresele1">Luigi Gresele</a>, <a href="https://openreview.net/profile?id=~Bernhard_Sch%C3%B6lkopf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernhard_Schölkopf1">Bernhard Schölkopf</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#hb1sDDSLbV-details-717" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hb1sDDSLbV-details-717"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">invariances, consistency, gradient alignment</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we investigate the principle that good explanations are hard to vary in the context of deep learning.
      We show that averaging gradients across examples -- akin to a logical OR of patterns -- can favor memorization and `patchwork' solutions that sew together different strategies, instead of identifying invariances.
      To inspect this, we first formalize a notion of consistency for minima of the loss surface, which measures to what extent a minimum appears only when examples are pooled.
      We then propose and experimentally validate a simple alternative algorithm based on a logical AND, that focuses on invariances and prevents memorization in a set of real-world tasks. 
      Finally, using a synthetic dataset with a clear distinction between invariant and spurious mechanisms, we dissect learning signals and compare this approach to well-established regularizers.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=hb1sDDSLbV&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="rWZz3sJfCkm" data-number="3692">
      <h4>
        <a href="https://openreview.net/forum?id=rWZz3sJfCkm">
            Efficient Generalized Spherical CNNs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=rWZz3sJfCkm" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Oliver_Cobb1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Oliver_Cobb1">Oliver Cobb</a>, <a href="https://openreview.net/profile?email=christopher.wallis%40kagenova.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="christopher.wallis@kagenova.com">Christopher G. R. Wallis</a>, <a href="https://openreview.net/profile?email=augustine.mavor-parker%40kagenova.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="augustine.mavor-parker@kagenova.com">Augustine N. Mavor-Parker</a>, <a href="https://openreview.net/profile?email=auggie.marignier%40kagenova.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="auggie.marignier@kagenova.com">Augustin Marignier</a>, <a href="https://openreview.net/profile?email=matt.price%40kagenova.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="matt.price@kagenova.com">Matthew A. Price</a>, <a href="https://openreview.net/profile?email=mayeul.davezac%40kagenova.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mayeul.davezac@kagenova.com">Mayeul d'Avezac</a>, <a href="https://openreview.net/profile?id=~Jason_McEwen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jason_McEwen1">Jason McEwen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 08 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#rWZz3sJfCkm-details-38" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rWZz3sJfCkm-details-38"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Many problems across computer vision and the natural sciences require the analysis of spherical data, for which representations may be learned efficiently by encoding equivariance to rotational symmetries.  We present a generalized spherical CNN framework that encompasses various existing approaches and allows them to be leveraged alongside each other.  The only existing non-linear spherical CNN layer that is strictly equivariant has complexity <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="93" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><msup><mi>C</mi><mn>2</mn></msup><msup><mi>L</mi><mn>5</mn></msup><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="94" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi></math></mjx-assistive-mml></mjx-container> is a measure of representational capacity and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="95" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></mjx-assistive-mml></mjx-container> the spherical harmonic bandlimit.  Such a high computational cost often prohibits the use of strictly equivariant spherical CNNs.  We develop two new strictly equivariant layers with reduced complexity <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="96" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c34"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mi>C</mi><msup><mi>L</mi><mn>4</mn></msup><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="97" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mi class="mjx-n" space="2"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mi>C</mi><msup><mi>L</mi><mn>3</mn></msup><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mi>L</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, making larger, more expressive models computationally feasible.  Moreover, we adopt efficient sampling theory to achieve further computational savings. We show that these developments allow the construction of more expressive hybrid models that achieve state-of-the-art accuracy and parameter efficiency on spherical benchmark problems.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ULQdiUTHe3y" data-number="3522">
      <h4>
        <a href="https://openreview.net/forum?id=ULQdiUTHe3y">
            Collective Robustness Certificates: Exploiting Interdependence in Graph Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ULQdiUTHe3y" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jan_Schuchardt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jan_Schuchardt1">Jan Schuchardt</a>, <a href="https://openreview.net/profile?id=~Aleksandar_Bojchevski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aleksandar_Bojchevski1">Aleksandar Bojchevski</a>, <a href="https://openreview.net/profile?id=~Johannes_Klicpera1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Johannes_Klicpera1">Johannes Klicpera</a>, <a href="https://openreview.net/profile?id=~Stephan_G%C3%BCnnemann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephan_Günnemann1">Stephan Günnemann</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ULQdiUTHe3y-details-182" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ULQdiUTHe3y-details-182"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Robustness certificates, Adversarial robustness, Graph neural networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In tasks like node classification, image segmentation, and named-entity recognition we have a classifier that simultaneously outputs multiple predictions (a vector of labels) based on a single input, i.e. a single graph, image, or document respectively. Existing adversarial robustness certificates consider each prediction independently and are thus overly pessimistic for such tasks. They implicitly assume that an adversary can use different perturbed inputs to attack different predictions, ignoring the fact that we have a single shared input. We propose the first collective robustness certificate which computes the number of predictions that are simultaneously guaranteed to remain stable under perturbation, i.e. cannot be attacked. We focus on Graph Neural Networks and leverage their locality property - perturbations only affect the predictions in a close neighborhood - to fuse multiple single-node certificates into a drastically stronger collective certificate. For example, on the Citeseer dataset our collective certificate for node classification increases the average number of certifiable feature perturbations from <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="98" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>7</mn></math></mjx-assistive-mml></mjx-container> to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="99" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>351</mn></math></mjx-assistive-mml></mjx-container>.
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We fuse multiple single-prediction certificates into a drastically stronger collective certificate leveraging the locality property of Graph Neural Networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=ULQdiUTHe3y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xjXg0bnoDmS" data-number="1135">
      <h4>
        <a href="https://openreview.net/forum?id=xjXg0bnoDmS">
            Entropic gradient descent algorithms and wide flat minima
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xjXg0bnoDmS" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Fabrizio_Pittorino1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fabrizio_Pittorino1">Fabrizio Pittorino</a>, <a href="https://openreview.net/profile?id=~Carlo_Lucibello1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Carlo_Lucibello1">Carlo Lucibello</a>, <a href="https://openreview.net/profile?id=~Christoph_Feinauer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christoph_Feinauer1">Christoph Feinauer</a>, <a href="https://openreview.net/profile?id=~Gabriele_Perugini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gabriele_Perugini1">Gabriele Perugini</a>, <a href="https://openreview.net/profile?id=~Carlo_Baldassi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Carlo_Baldassi1">Carlo Baldassi</a>, <a href="https://openreview.net/profile?email=elizaveta.demyanenko%40phd.unibocconi.it" class="profile-link" data-toggle="tooltip" data-placement="top" title="elizaveta.demyanenko@phd.unibocconi.it">Elizaveta Demyanenko</a>, <a href="https://openreview.net/profile?id=~Riccardo_Zecchina1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Riccardo_Zecchina1">Riccardo Zecchina</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 13 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xjXg0bnoDmS-details-816" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xjXg0bnoDmS-details-816"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">flat minima, entropic algorithms, statistical physics, belief-propagation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The properties of flat minima in the empirical risk landscape of neural networks have been debated for some time. Increasing evidence suggests they possess better generalization capabilities with respect to sharp ones. In this work we first discuss the relationship between alternative measures of flatness: The local entropy, which is useful for analysis and algorithm development, and the local energy, which is easier to compute and was shown empirically in extensive tests on state-of-the-art networks to be the best predictor of generalization capabilities. We show semi-analytically in simple controlled scenarios that these two measures correlate strongly with each other and with generalization. Then, we extend the analysis to the deep learning scenario by extensive numerical validations. We study two algorithms, Entropy-SGD and Replicated-SGD, that explicitly include the local entropy in the optimization objective. We devise a training schedule by which we consistently find flatter minima (using both flatness measures), and improve the generalization error for common architectures (e.g. ResNet, EfficientNet).</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Relation between local entropy, flat minima, entropic algorithms and good generalization.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=xjXg0bnoDmS&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="jDdzh5ul-d" data-number="1350">
      <h4>
        <a href="https://openreview.net/forum?id=jDdzh5ul-d">
            Achieving Linear Speedup with Partial Worker Participation in Non-IID Federated Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=jDdzh5ul-d" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Haibo_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haibo_Yang1">Haibo Yang</a>, <a href="https://openreview.net/profile?email=myfang%40iastate.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="myfang@iastate.edu">Minghong Fang</a>, <a href="https://openreview.net/profile?id=~Jia_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jia_Liu1">Jia Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#jDdzh5ul-d-details-266" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jDdzh5ul-d-details-266"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Federated learning (FL) is a distributed machine learning architecture that leverages a large number of workers to jointly learn a model with decentralized data. FL has received increasing attention in recent years thanks to its data privacy protection, communication efficiency and a linear speedup for convergence in training (i.e., convergence performance increases linearly with respect to the number of workers). However, existing studies on linear speedup for convergence are only limited to the assumptions of i.i.d. datasets across workers and/or full worker participation, both of which rarely hold in practice. So far, it remains an open question whether or not the linear speedup for convergence is achievable under non-i.i.d. datasets with partial worker participation in FL.  In this paper, we show that the answer is affirmative. Specifically, we show that the federated averaging (FedAvg) algorithm (with two-sided learning rates) on non-i.i.d. datasets in non-convex settings achieves a convergence rate <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="100" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msqrt size="s"><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.16em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mfrac space="3"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mfrac><mn>1</mn><msqrt><mi>m</mi><mi>K</mi><mi>T</mi></msqrt></mfrac><mo>+</mo><mfrac><mn>1</mn><mi>T</mi></mfrac><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> for full worker participation and a convergence rate <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="101" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msqrt size="s"><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.16em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mfrac space="3"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mfrac><mn>1</mn><msqrt><mi>n</mi><mi>K</mi><mi>T</mi></msqrt></mfrac><mo>+</mo><mfrac><mn>1</mn><mi>T</mi></mfrac><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> for partial worker participation, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="102" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container> is the number of local steps, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="103" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container> is the number of total communication rounds, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="104" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi></math></mjx-assistive-mml></mjx-container> is the total worker number and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="105" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container> is the worker number in one communication round if for partial worker participation. Our results also reveal that the local steps in FL could help the convergence and show that the maximum number of local steps can be improved to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="106" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi><mrow><mo>/</mo></mrow><mi>m</mi></math></mjx-assistive-mml></mjx-container>. We conduct extensive experiments on MNIST and CIFAR-10 to verify our theoretical results.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-GLNZeVDuik" data-number="1592">
      <h4>
        <a href="https://openreview.net/forum?id=-GLNZeVDuik">
            Categorical Normalizing Flows via Continuous Transformations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-GLNZeVDuik" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Phillip_Lippe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Phillip_Lippe1">Phillip Lippe</a>, <a href="https://openreview.net/profile?id=~Efstratios_Gavves1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Efstratios_Gavves1">Efstratios Gavves</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-GLNZeVDuik-details-619" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-GLNZeVDuik-details-619"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Normalizing Flows, Density Estimation, Graph Generation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Despite their popularity, to date, the application of normalizing flows on categorical data stays limited. The current practice of using dequantization to map discrete data to a continuous space is inapplicable as categorical data has no intrinsic order. Instead, categorical data have complex and latent relations that must be inferred, like the synonymy between words. In this paper, we investigate Categorical Normalizing Flows, that is normalizing flows for categorical data. By casting the encoding of categorical data in continuous space as a variational inference problem, we jointly optimize the continuous representation and the model likelihood. Using a factorized decoder, we introduce an inductive bias to model any interactions in the normalizing flow. As a consequence, we do not only simplify the optimization compared to having a joint decoder, but also make it possible to scale up to a large number of categories that is currently impossible with discrete normalizing flows. Based on Categorical Normalizing Flows, we propose GraphCNF a permutation-invariant generative model on graphs. GraphCNF implements a three step approach modeling the nodes, edges, and adjacency matrix stepwise to increase efficiency. On molecule generation, GraphCNF outperforms both one-shot and autoregressive flow-based state-of-the-art.
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=-GLNZeVDuik&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We explore the application of normalizing flows on categorical data and propose a permutation-invariant generative model on graphs, called GraphCNF.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Xv_s64FiXTv" data-number="1122">
      <h4>
        <a href="https://openreview.net/forum?id=Xv_s64FiXTv">
            Learning to Represent Action Values as a Hypergraph on the Action Vertices
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Xv_s64FiXTv" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Arash_Tavakoli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arash_Tavakoli1">Arash Tavakoli</a>, <a href="https://openreview.net/profile?id=~Mehdi_Fatemi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mehdi_Fatemi1">Mehdi Fatemi</a>, <a href="https://openreview.net/profile?id=~Petar_Kormushev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Petar_Kormushev1">Petar Kormushev</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>19 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Xv_s64FiXTv-details-690" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Xv_s64FiXTv-details-690"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, structural credit assignment, structural inductive bias, multi-dimensional discrete action spaces, learning action representations</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Action-value estimation is a critical component of many reinforcement learning (RL) methods whereby sample complexity relies heavily on how fast a good estimator for action value can be learned. By viewing this problem through the lens of representation learning, good representations of both state and action can facilitate action-value estimation. While advances in deep learning have seamlessly driven progress in learning state representations, given the specificity of the notion of agency to RL, little attention has been paid to learning action representations. We conjecture that leveraging the combinatorial structure of multi-dimensional action spaces is a key ingredient for learning good representations of action. To test this, we set forth the action hypergraph networks framework---a class of functions for learning action representations in multi-dimensional discrete action spaces with a structural inductive bias. Using this framework we realise an agent class based on a combination with deep Q-networks, which we dub hypergraph Q-networks. We show the effectiveness of our approach on a myriad of domains: illustrative prediction problems under minimal confounding effects, Atari 2600 games, and discretised physical control benchmarks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper introduces a class of models, called action hypergraph networks, for action-value estimation by leveraging the combinatorial structure of multi-dimensional discrete action spaces.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="6puUoArESGp" data-number="1263">
      <h4>
        <a href="https://openreview.net/forum?id=6puUoArESGp">
            Debiasing Concept-based Explanations with Causal Analysis
        </a>
      
        
          <a href="https://openreview.net/pdf?id=6puUoArESGp" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mohammad_Taha_Bahadori1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammad_Taha_Bahadori1">Mohammad Taha Bahadori</a>, <a href="https://openreview.net/profile?id=~David_Heckerman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Heckerman1">David Heckerman</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 04 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#6puUoArESGp-details-50" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6puUoArESGp-details-50"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Interpretability, Concept-based Explanation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Studying the concept-based explanation techniques, we provided evidences for potential existence of spurious association between the features and concepts due to  unobserved latent variables or noise. We proposed a new causal prior graph that models the impact of the noise and latent confounding fron the estimated concepts. We showed that using the labels as instruments, we can remove the impact of the context from the explanations. Our experiments showed that our debiasing technique not only improves the quality of the explanations, but also improve the accuracy of predicting labels through the concepts. As future work, we will investigate other two-stage-regression techniques to
      find the most accurate debiasing method.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We use a technique from instrumental variables literature and remove the impact of noise and latent confounding from concept-based explanations.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ADWd4TJO13G" data-number="1652">
      <h4>
        <a href="https://openreview.net/forum?id=ADWd4TJO13G">
            Lifelong Learning of Compositional Structures
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ADWd4TJO13G" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jorge_A_Mendez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jorge_A_Mendez1">Jorge A Mendez</a>, <a href="https://openreview.net/profile?id=~ERIC_EATON1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~ERIC_EATON1">ERIC EATON</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ADWd4TJO13G-details-164" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ADWd4TJO13G-details-164"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">lifelong learning, continual learning, compositional learning, modular networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A hallmark of human intelligence is the ability to construct self-contained chunks of knowledge and adequately reuse them in novel combinations for solving different yet structurally related problems. Learning such compositional structures has been a significant challenge for artificial systems, due to the combinatorial nature of the underlying search problem. To date, research into compositional learning has largely proceeded separately from work on lifelong or continual learning. We integrate these two lines of work to present a general-purpose framework for lifelong learning of compositional structures that can be used for solving a stream of related tasks. Our framework separates the learning process into two broad stages: learning how to best combine existing components in order to assimilate a novel problem, and learning how to adapt the set of existing components to accommodate the new problem. This separation explicitly handles the trade-off between the stability required to remember how to solve earlier tasks and the flexibility required to solve new tasks, as we show empirically in an extensive evaluation.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We create a general-purpose framework for lifelong learning of compositional structures that splits the learning process into two stages: assimilation of new tasks with existing components, and accommodation of new knowledge into the components.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xpFFI_NtgpW" data-number="3039">
      <h4>
        <a href="https://openreview.net/forum?id=xpFFI_NtgpW">
            Rethinking Embedding Coupling in Pre-trained Language Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xpFFI_NtgpW" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hyung_Won_Chung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hyung_Won_Chung1">Hyung Won Chung</a>, <a href="https://openreview.net/profile?id=~Thibault_Fevry1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thibault_Fevry1">Thibault Fevry</a>, <a href="https://openreview.net/profile?email=henrytsai%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="henrytsai@google.com">Henry Tsai</a>, <a href="https://openreview.net/profile?id=~Melvin_Johnson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Melvin_Johnson1">Melvin Johnson</a>, <a href="https://openreview.net/profile?id=~Sebastian_Ruder2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sebastian_Ruder2">Sebastian Ruder</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xpFFI_NtgpW-details-138" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xpFFI_NtgpW-details-138"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">natural language processing, transfer learning, efficiency, pre-training</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art pre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to significantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By reallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on standard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that allocating additional capacity to the output embedding provides benefits to the model that persist through the fine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the number of parameters at the fine-tuning stage. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Decoupling output embedding shapes leads to more transferable Transformer layers and prevents over-specialization of a Transformer's upper layers to the pre-training task.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="gwnoVHIES05" data-number="394">
      <h4>
        <a href="https://openreview.net/forum?id=gwnoVHIES05">
            Creative Sketch Generation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=gwnoVHIES05" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Songwei_Ge2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Songwei_Ge2">Songwei Ge</a>, <a href="https://openreview.net/profile?id=~Vedanuj_Goswami1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vedanuj_Goswami1">Vedanuj Goswami</a>, <a href="https://openreview.net/profile?id=~Larry_Zitnick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Larry_Zitnick1">Larry Zitnick</a>, <a href="https://openreview.net/profile?id=~Devi_Parikh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Devi_Parikh1">Devi Parikh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 24 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#gwnoVHIES05-details-510" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gwnoVHIES05-details-510"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">creativity, sketches, part-based, GAN, dataset, generative art</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Sketching or doodling is a popular creative activity that people engage in. However, most existing work in automatic sketch understanding or generation has focused on sketches that are quite mundane. In this work, we introduce two datasets of creative sketches -- Creative Birds and Creative Creatures -- containing 10k sketches each along with part annotations. We propose DoodlerGAN -- a part-based Generative Adversarial Network (GAN) -- to generate unseen compositions of novel part appearances. Quantitative evaluations as well as human studies demonstrate that sketches generated by our approach are more creative and of higher quality than existing approaches. In fact, in Creative Birds, subjects prefer sketches generated by DoodlerGAN over those drawn by humans!</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce two creative sketch datasets and DoodlerGAN -- a part-based GAN model that generates creative sketches.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="eJIJF3-LoZO" data-number="1248">
      <h4>
        <a href="https://openreview.net/forum?id=eJIJF3-LoZO">
            Concept Learners for Few-Shot Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=eJIJF3-LoZO" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kaidi_Cao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaidi_Cao1">Kaidi Cao</a>, <a href="https://openreview.net/profile?id=~Maria_Brbic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maria_Brbic1">Maria Brbic</a>, <a href="https://openreview.net/profile?id=~Jure_Leskovec1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jure_Leskovec1">Jure Leskovec</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#eJIJF3-LoZO-details-283" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eJIJF3-LoZO-details-283"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">few-shot learning, meta learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Developing algorithms that are able to generalize to a novel task given only a few labeled examples represents a fundamental challenge in closing the gap between machine- and human-level performance. The core of human cognition lies in the structured, reusable concepts that help us to rapidly adapt to new tasks and provide reasoning behind our decisions. However, existing meta-learning methods learn complex representations across prior labeled tasks without imposing any structure on the learned representations. Here we propose COMET, a meta-learning method that improves generalization ability by learning to learn along human-interpretable concept dimensions. Instead of learning a joint unstructured metric space, COMET learns mappings of high-level concepts into semi-structured metric spaces, and effectively combines the outputs of independent concept learners. We evaluate our model on few-shot tasks from diverse domains, including fine-grained image classification, document categorization  and cell type annotation on a novel dataset from a biological domain developed in our work. COMET significantly outperforms strong meta-learning baselines, achieving 6-15% relative improvement on the most challenging 1-shot learning tasks, while unlike existing methods providing interpretations behind the model's predictions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">COMET learns generalizable representations along human-understandable concept dimensions.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="6xHJ37MVxxp" data-number="214">
      <h4>
        <a href="https://openreview.net/forum?id=6xHJ37MVxxp">
            Domain Generalization with MixStyle
        </a>
      
        
          <a href="https://openreview.net/pdf?id=6xHJ37MVxxp" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kaiyang_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaiyang_Zhou1">Kaiyang Zhou</a>, <a href="https://openreview.net/profile?id=~Yongxin_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yongxin_Yang1">Yongxin Yang</a>, <a href="https://openreview.net/profile?id=~Yu_Qiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Qiao1">Yu Qiao</a>, <a href="https://openreview.net/profile?id=~Tao_Xiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Xiang1">Tao Xiang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 05 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#6xHJ37MVxxp-details-181" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6xHJ37MVxxp-details-181"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Domain Generalization, Style Mixing</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs.~sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">MixStyle makes CNNs more domain-generalizable by mixing instance-level feature statistics of training samples across domains.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ujmgfuxSLrO" data-number="2100">
      <h4>
        <a href="https://openreview.net/forum?id=ujmgfuxSLrO">
            DeLighT: Deep and Light-weight Transformer
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ujmgfuxSLrO" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sachin_Mehta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sachin_Mehta1">Sachin Mehta</a>, <a href="https://openreview.net/profile?id=~Marjan_Ghazvininejad1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marjan_Ghazvininejad1">Marjan Ghazvininejad</a>, <a href="https://openreview.net/profile?email=sviyer%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sviyer@fb.com">Srinivasan Iyer</a>, <a href="https://openreview.net/profile?id=~Luke_Zettlemoyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luke_Zettlemoyer1">Luke Zettlemoyer</a>, <a href="https://openreview.net/profile?id=~Hannaneh_Hajishirzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hannaneh_Hajishirzi1">Hannaneh Hajishirzi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>36 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ujmgfuxSLrO-details-835" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ujmgfuxSLrO-details-835"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Transformers, Sequence Modeling, Machine Translation, Language Modeling, Representation learning, Efficient Networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We introduce a deep and light-weight transformer, DeLighT, that delivers similar or better performance than standard transformer-based models with significantly fewer parameters. DeLighT more efficiently allocates parameters both (1) within each Transformer block using the DeLighT transformation, a deep and light-weight transformation and (2) across blocks using block-wise scaling, that allows for shallower and narrower DeLighT blocks near the input and wider and deeper DeLighT blocks near the output. Overall, DeLighT networks are 2.5 to 4 times deeper than standard transformer models and yet have fewer parameters and operations. Experiments on benchmark machine translation and language modeling tasks show that DeLighT matches or improves the performance of baseline Transformers with 2 to 3 times fewer parameters on average. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Deep and light-weight transformer that matches or improves the performance of baseline Transformers with 2 to 3 times fewer parameters on standard machine translation and language modeling tasks</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="pqZV_srUVmK" data-number="1194">
      <h4>
        <a href="https://openreview.net/forum?id=pqZV_srUVmK">
            Single-Timescale Actor-Critic Provably Finds Globally Optimal Policy
        </a>
      
        
          <a href="https://openreview.net/pdf?id=pqZV_srUVmK" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zuyue_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zuyue_Fu1">Zuyue Fu</a>, <a href="https://openreview.net/profile?id=~Zhuoran_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhuoran_Yang1">Zhuoran Yang</a>, <a href="https://openreview.net/profile?id=~Zhaoran_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhaoran_Wang1">Zhaoran Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 08 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#pqZV_srUVmK-details-742" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pqZV_srUVmK-details-742"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the global convergence and global optimality of actor-critic, one of the most popular families of reinforcement learning algorithms. While most existing works on actor-critic employ bi-level or two-timescale updates, we focus on the more practical single-timescale setting, where the actor and critic are updated simultaneously. Specifically, in each iteration, the critic update is obtained by applying the Bellman evaluation operator only once while the actor is updated in the policy gradient direction computed using the critic. Moreover, we consider two function approximation settings where both the actor and critic are represented by linear or deep neural networks. For both cases, we prove that the actor sequence converges to a globally optimal policy at a sublinear <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="107" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.052em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>K</mi><mrow><mo>−</mo><mn>1</mn><mrow><mo>/</mo></mrow><mn>2</mn></mrow></msup><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> rate, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="108" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container> is the number of iterations. To the best of our knowledge, we establish the rate of convergence and global optimality of single-timescale actor-critic with linear function approximation for the first time. Moreover, under the broader scope of policy optimization with nonlinear function approximation, we prove that actor-critic with deep neural network finds the globally optimal policy at a sublinear rate for the first time. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="0oabwyZbOu" data-number="1860">
      <h4>
        <a href="https://openreview.net/forum?id=0oabwyZbOu">
            Mastering Atari with Discrete World Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=0oabwyZbOu" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Danijar_Hafner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Danijar_Hafner1">Danijar Hafner</a>, <a href="https://openreview.net/profile?id=~Timothy_P_Lillicrap1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Timothy_P_Lillicrap1">Timothy P Lillicrap</a>, <a href="https://openreview.net/profile?id=~Mohammad_Norouzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammad_Norouzi1">Mohammad Norouzi</a>, <a href="https://openreview.net/profile?id=~Jimmy_Ba1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jimmy_Ba1">Jimmy Ba</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#0oabwyZbOu-details-229" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0oabwyZbOu-details-229"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Atari, world models, model-based reinforcement learning, reinforcement learning, planning, actor critic</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer V2 reaches 200M frames and exceeds the final performance of the top single-GPU agents IQN and Rainbow.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">DreamerV2 is the first agent based on a world model to achieve human-level performance on the Atari benchmark.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="kW_zpEmMLdP" data-number="666">
      <h4>
        <a href="https://openreview.net/forum?id=kW_zpEmMLdP">
            Learning Neural Event Functions for Ordinary Differential Equations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=kW_zpEmMLdP" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ricky_T._Q._Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ricky_T._Q._Chen1">Ricky T. Q. Chen</a>, <a href="https://openreview.net/profile?id=~Brandon_Amos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brandon_Amos1">Brandon Amos</a>, <a href="https://openreview.net/profile?id=~Maximilian_Nickel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maximilian_Nickel1">Maximilian Nickel</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#kW_zpEmMLdP-details-347" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kW_zpEmMLdP-details-347"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">differential equations, implicit differentiation, point processes</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The existing Neural ODE formulation relies on an explicit knowledge of the termination time. We extend Neural ODEs to implicitly defined termination criteria modeled by neural event functions, which can be chained together and differentiated through. Neural Event ODEs are capable of modeling discrete and instantaneous changes in a continuous-time system, without prior knowledge of when these changes should occur or how many such changes should exist. We test our approach in modeling hybrid discrete- and continuous- systems such as switching dynamical systems and collision in multi-body systems, and we propose simulation-based training of point processes with applications in discrete control.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We discuss how event handling in ODE solvers can be differentiated through, allowing us to extend Neural ODEs to cases of implicitly defined termination times and enabling learning of discrete events and discontinuous dynamics.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Q4EUywJIkqr" data-number="223">
      <h4>
        <a href="https://openreview.net/forum?id=Q4EUywJIkqr">
            Contemplating Real-World Object Classification
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Q4EUywJIkqr" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~ali_borji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~ali_borji1">ali borji</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 28 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Q4EUywJIkqr-details-764" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Q4EUywJIkqr-details-764"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">object recognition, deep learning, ObjectNet, Robustness</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep object recognition models have been very successful over benchmark
      datasets such as ImageNet. How accurate and robust are they to distribution
      shifts arising from natural and synthetic variations in datasets? Prior research on
      this problem has primarily focused on ImageNet variations (e.g., ImageNetV2,
      ImageNet-A). To avoid potential inherited biases in these studies, we take a
      different approach. Specifically, we reanalyze the ObjectNet dataset recently
      proposed by Barbu et al. containing objects in daily life situations. They showed
      a dramatic performance drop of the state of the art object recognition models on
      this dataset. Due to the importance and implications of their results regarding
      the generalization ability of deep models, we take a second look at their analysis.
      We find that applying deep models to the isolated objects, rather than the entire
      scene as is done in the original paper, results in around 20-30% performance
      improvement. Relative to the numbers reported in Barbu et al., around 10-15%
      of the performance loss is recovered, without any test time data augmentation.
      Despite this gain, however, we conclude that deep models still suffer drastically
      on the ObjectNet dataset. We also investigate the robustness of models against
      synthetic image perturbations such as geometric transformations (e.g., scale,
      rotation, translation), natural image distortions (e.g., impulse noise, blur) as well
      as adversarial attacks (e.g., FGSM and PGD-5). Our results indicate that limiting
      the object area as much as possible (i.e., from the entire image to the bounding
      box to the segmentation mask) leads to consistent improvement in accuracy and
      robustness. Finally, through a qualitative analysis of ObjectNet data, we find that
      i) a large number of images in this dataset are hard to recognize even for humans,
      and ii) easy (hard) samples for models match with easy (hard) samples for humans.
      Overall, our analysis shows that ObjecNet is still a challenging test platform that
      can be used to measure the generalization ability of models. The code and data
      are available in [masked due to blind review].</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We address whether current deep learning models are able to solve object recognition in real world and how robust they are to synthetic and natural distribution shifts.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="XQQA6-So14" data-number="665">
      <h4>
        <a href="https://openreview.net/forum?id=XQQA6-So14">
            Neural Spatio-Temporal Point Processes
        </a>
      
        
          <a href="https://openreview.net/pdf?id=XQQA6-So14" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ricky_T._Q._Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ricky_T._Q._Chen1">Ricky T. Q. Chen</a>, <a href="https://openreview.net/profile?id=~Brandon_Amos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brandon_Amos1">Brandon Amos</a>, <a href="https://openreview.net/profile?id=~Maximilian_Nickel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maximilian_Nickel1">Maximilian Nickel</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#XQQA6-So14-details-982" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XQQA6-So14-details-982"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">point processes, normalizing flows, differential equations</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a new class of parameterizations for spatio-temporal point processes which leverage Neural ODEs as a computational method and enable flexible, high-fidelity models of discrete events that are localized in continuous time and space. Central to our approach is a combination of continuous-time neural networks with two novel neural architectures, \ie, Jump and Attentive Continuous-time Normalizing Flows. This approach allows us to learn complex distributions for both the spatial and temporal domain and to condition non-trivially on the observed event history. We validate our models on data sets from a wide variety of contexts such as seismology, epidemiology, urban mobility, and neuroscience.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We motivate the use of Continuous-time Normalizing Flows for building spatio-temporal point processes, and discuss modeling conditional dependencies with recurrent- or attention-based Neural ODEs.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="PpshD0AXfA" data-number="1980">
      <h4>
        <a href="https://openreview.net/forum?id=PpshD0AXfA">
            Generative Time-series Modeling with Fourier Flows
        </a>
      
        
          <a href="https://openreview.net/pdf?id=PpshD0AXfA" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ahmed_Alaa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ahmed_Alaa1">Ahmed Alaa</a>, <a href="https://openreview.net/profile?id=~Alex_James_Chan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_James_Chan1">Alex James Chan</a>, <a href="https://openreview.net/profile?id=~Mihaela_van_der_Schaar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihaela_van_der_Schaar2">Mihaela van der Schaar</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#PpshD0AXfA-details-51" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PpshD0AXfA-details-51"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Generating synthetic time-series data is crucial in various application domains, such as medical prognosis, wherein research is hamstrung by the lack of access to data due to concerns over privacy. Most of the recently proposed methods for generating synthetic time-series rely on implicit likelihood modeling using generative adversarial networks (GANs)—but such models can be difficult to train, and may jeopardize privacy by “memorizing” temporal patterns in training data. In this paper, we propose an explicit likelihood model based on a novel class of normalizing flows that view time-series data in the frequency-domain rather than the time-domain. The proposed flow, dubbed a Fourier flow, uses a discrete Fourier transform (DFT) to convert variable-length time-series with arbitrary sampling periods into fixed-length spectral representations, then applies a (data-dependent) spectral filter to the frequency-transformed time-series. We show that, by virtue of the DFT analytic properties, the Jacobian determinants and inverse mapping for the Fourier flow can be computed efficiently in linearithmic time, without imposing explicit structural constraints as in existing flows such as NICE (Dinh et al. (2014)), RealNVP (Dinh et al. (2016)) and GLOW (Kingma &amp; Dhariwal (2018)). Experiments show that Fourier flows perform competitively compared to state-of-the-art baselines.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="6FqKiVAdI3Y" data-number="1158">
      <h4>
        <a href="https://openreview.net/forum?id=6FqKiVAdI3Y">
            DOP: Off-Policy Multi-Agent Decomposed Policy Gradients
        </a>
      
        
          <a href="https://openreview.net/pdf?id=6FqKiVAdI3Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yihan_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yihan_Wang1">Yihan Wang</a>, <a href="https://openreview.net/profile?id=~Beining_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Beining_Han1">Beining Han</a>, <a href="https://openreview.net/profile?id=~Tonghan_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tonghan_Wang1">Tonghan Wang</a>, <a href="https://openreview.net/profile?id=~Heng_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Heng_Dong1">Heng Dong</a>, <a href="https://openreview.net/profile?id=~Chongjie_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongjie_Zhang1">Chongjie Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#6FqKiVAdI3Y-details-594" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6FqKiVAdI3Y-details-594"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multi-Agent Reinforcement Learning, Multi-Agent Policy Gradients</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Multi-agent policy gradient (MAPG) methods recently witness vigorous progress. However, there is a significant performance discrepancy between MAPG methods and state-of-the-art multi-agent value-based approaches. In this paper, we investigate causes that hinder the performance of MAPG algorithms and present a multi-agent decomposed policy gradient method (DOP). This method introduces the idea of value function decomposition into the multi-agent actor-critic framework. Based on this idea, DOP supports efficient off-policy learning and addresses the issue of centralized-decentralized mismatch and credit assignment in both discrete and continuous action spaces. We formally show that DOP critics have sufficient representational capability to guarantee convergence. In addition, empirical evaluations on the StarCraft II micromanagement benchmark and multi-agent particle environments demonstrate that DOP outperforms both state-of-the-art value-based and policy-based multi-agent reinforcement learning algorithms. Demonstrative videos are available at https://sites.google.com/view/dop-mapg/.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an off-policy multi-agent decomposed policy gradient method, addressing the drawbacks that prevent existing multi-agent policy gradient methods from achieving state-of-the-art performance.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=6FqKiVAdI3Y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="BbNIbVPJ-42" data-number="1273">
      <h4>
        <a href="https://openreview.net/forum?id=BbNIbVPJ-42">
            The Risks of Invariant Risk Minimization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=BbNIbVPJ-42" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Elan_Rosenfeld1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Elan_Rosenfeld1">Elan Rosenfeld</a>, <a href="https://openreview.net/profile?id=~Pradeep_Kumar_Ravikumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pradeep_Kumar_Ravikumar1">Pradeep Kumar Ravikumar</a>, <a href="https://openreview.net/profile?id=~Andrej_Risteski2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrej_Risteski2">Andrej Risteski</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>21 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#BbNIbVPJ-42-details-35" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="BbNIbVPJ-42-details-35"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">out-of-distribution generalization, causality, representation learning, deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Invariant Causal Prediction (Peters et al., 2016) is a technique for out-of-distribution generalization which assumes that some aspects of the data distribution vary across the training set but that the underlying causal mechanisms remain constant. Recently, Arjovsky et al. (2019) proposed Invariant Risk Minimization (IRM), an objective based on this idea for learning deep, invariant features of data which are a complex function of latent variables; many alternatives have subsequently been suggested.  However, formal guarantees for all of these works are severely lacking.  In this paper,  we present the first analysis of classification under the IRM objective—as well as these recently proposed alternatives—under a fairly natural and general model. In the linear case, we show simple conditions under which the optimal solution succeeds or, more often, fails to recover the optimal invariant predictor. We furthermore present the very first results in the non-linear regime: we demonstrate that IRM can fail catastrophically unless the test data is sufficiently similar to the training distribution—this is precisely the issue that it was intended to solve. Thus, in this setting we find that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We formally demonstrate that Invariant Risk Minimization and related alternative objectives often perform no better than standard ERM.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=BbNIbVPJ-42&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="GTGb3M_KcUl" data-number="174">
      <h4>
        <a href="https://openreview.net/forum?id=GTGb3M_KcUl">
            DynaTune: Dynamic Tensor Program Optimization in Deep Neural Network Compilation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=GTGb3M_KcUl" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Minjia_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minjia_Zhang1">Minjia Zhang</a>, <a href="https://openreview.net/profile?email=t-meli%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="t-meli@microsoft.com">Menghao Li</a>, <a href="https://openreview.net/profile?id=~Chi_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chi_Wang3">Chi Wang</a>, <a href="https://openreview.net/profile?email=mingqli%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mingqli@microsoft.com">Mingqin Li</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#GTGb3M_KcUl-details-166" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GTGb3M_KcUl-details-166"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Efficient Deep Learning Inference, Scalability, Code Compilation, Bayesian Inference</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing methods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long optimization time to obtain a desired latency.
      
      In this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to optimize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment results show that DynaTune is 1.2--2.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We accelerate tensor program optimization by considering it as a multi-armed bandits problem and using Bayesian inference to achieve fast convergence.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Xb8xvrtB8Ce" data-number="160">
      <h4>
        <a href="https://openreview.net/forum?id=Xb8xvrtB8Ce">
            Bag of Tricks for Adversarial Training
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Xb8xvrtB8Ce" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tianyu_Pang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianyu_Pang1">Tianyu Pang</a>, <a href="https://openreview.net/profile?id=~Xiao_Yang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiao_Yang4">Xiao Yang</a>, <a href="https://openreview.net/profile?id=~Yinpeng_Dong2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yinpeng_Dong2">Yinpeng Dong</a>, <a href="https://openreview.net/profile?id=~Hang_Su3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hang_Su3">Hang Su</a>, <a href="https://openreview.net/profile?id=~Jun_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jun_Zhu2">Jun Zhu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 24 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Xb8xvrtB8Ce-details-285" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Xb8xvrtB8Ce-details-285"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Adversarial Training, Robustness, Adversarial Examples</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Adversarial training (AT) is one of the most effective strategies for promoting model robustness. However, recent benchmarks show that most of the proposed improvements on AT are less effective than simply early stopping the training procedure. This counter-intuitive fact motivates us to investigate the implementation details of tens of AT methods. Surprisingly, we find that the basic settings (e.g., weight decay, training schedule, etc.) used in these methods are highly inconsistent. In this work, we provide comprehensive evaluations on CIFAR-10, focusing on the effects of mostly overlooked training tricks and hyperparameters for adversarially trained models. Our empirical observations suggest that adversarial robustness is much more sensitive to some basic training settings than we thought. For example, a slightly different value of weight decay can reduce the model robust accuracy by more than <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="109" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>7</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container>, which is probable to override the potential promotion induced by the proposed methods. We conclude a baseline training setting and re-implement previous defenses to achieve new state-of-the-art results. These facts also appeal to more concerns on the overlooked confounders when benchmarking defenses.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Empirical evaluation of basic training tricks used in adversarial training</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Xb8xvrtB8Ce&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="2VXyy9mIyU3" data-number="1201">
      <h4>
        <a href="https://openreview.net/forum?id=2VXyy9mIyU3">
            Learning with Instance-Dependent Label Noise: A Sample Sieve Approach
        </a>
      
        
          <a href="https://openreview.net/pdf?id=2VXyy9mIyU3" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hao_Cheng5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Cheng5">Hao Cheng</a>, <a href="https://openreview.net/profile?id=~Zhaowei_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhaowei_Zhu1">Zhaowei Zhu</a>, <a href="https://openreview.net/profile?id=~Xingyu_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xingyu_Li2">Xingyu Li</a>, <a href="https://openreview.net/profile?id=~Yifei_Gong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yifei_Gong1">Yifei Gong</a>, <a href="https://openreview.net/profile?id=~Xing_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xing_Sun1">Xing Sun</a>, <a href="https://openreview.net/profile?id=~Yang_Liu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Liu3">Yang Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#2VXyy9mIyU3-details-78" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2VXyy9mIyU3-details-78"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Learning with noisy labels, instance-based label noise, deep neural networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Human-annotated labels are often prone to noise, and the presence of such noise will degrade the performance of the resulting deep neural network (DNN) models. Much of the literature (with several recent exceptions) of learning with noisy labels focuses on the case when the label noise is independent of features. Practically, annotations errors tend to be instance-dependent and often depend on the difficulty levels of recognizing a certain task. Applying existing results from instance-independent settings would require a significant amount of estimation of noise rates. Therefore, providing theoretically rigorous solutions for learning with instance-dependent label noise remains a challenge. In this paper, we propose CORES<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="110" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-n"></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mrow><mn>2</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> (COnfidence REgularized Sample Sieve), which progressively sieves out corrupted examples. The implementation of CORES<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="111" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-n"></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mrow><mn>2</mn></mrow></msup></math></mjx-assistive-mml></mjx-container>  does not require specifying noise rates and yet we are able to provide theoretical guarantees of CORES<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="112" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-n"></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mrow><mn>2</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> in filtering out the corrupted examples. This high-quality sample sieve allows us to treat clean examples and the corrupted ones separately in training a DNN solution, and such a separation is shown to be advantageous in the instance-dependent noise setting. We demonstrate the performance of CORES<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="113" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-n"></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mrow><mn>2</mn></mrow></msup></math></mjx-assistive-mml></mjx-container> on CIFAR10 and CIFAR100 datasets with synthetic instance-dependent label noise and Clothing1M with real-world human noise. As of independent interests, our sample sieve provides a generic machinery for anatomizing noisy datasets and provides a flexible interface for various robust training techniques to further improve the performance. Code is available at https://github.com/UCSC-REAL/cores.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper proposes a dynamic sample sieve method with strong theoretical guarantees to avoid overfitting to instance-based label noise.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=2VXyy9mIyU3&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="fmtSg8591Q" data-number="718">
      <h4>
        <a href="https://openreview.net/forum?id=fmtSg8591Q">
            Efficient Reinforcement Learning in Factored MDPs with Application to Constrained RL
        </a>
      
        
          <a href="https://openreview.net/pdf?id=fmtSg8591Q" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xiaoyu_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoyu_Chen2">Xiaoyu Chen</a>, <a href="https://openreview.net/profile?id=~Jiachen_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiachen_Hu1">Jiachen Hu</a>, <a href="https://openreview.net/profile?id=~Lihong_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lihong_Li1">Lihong Li</a>, <a href="https://openreview.net/profile?id=~Liwei_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liwei_Wang1">Liwei Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#fmtSg8591Q-details-115" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fmtSg8591Q-details-115"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, factored MDP, constrained RL, learning theory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Reinforcement learning (RL) in episodic, factored Markov decision processes (FMDPs) is studied. We propose an algorithm called FMDP-BF, which leverages the factorization structure of FMDP.  The regret of FMDP-BF is shown to be exponentially smaller than that of optimal algorithms designed for non-factored MDPs, and improves on the best previous result for FMDPs~\citep{osband2014near} by a factor of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="114" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-sop"><mjx-c class="mjx-c221A TEX-S1"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.108em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i" noic="true"><mjx-c class="mjx-c53 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo></mjx-box></mjx-sqrt></mjx-msqrt></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msqrt><mi>n</mi><mi>H</mi><mo data-mjx-texclass="ORD" stretchy="false">|</mo><msub><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">S</mi></mrow><mi>i</mi></msub><mo data-mjx-texclass="ORD" stretchy="false">|</mo></msqrt></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="115" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i" noic="true"><mjx-c class="mjx-c53 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo data-mjx-texclass="ORD" stretchy="false">|</mo><msub><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">S</mi></mrow><mi>i</mi></msub><mo data-mjx-texclass="ORD" stretchy="false">|</mo></math></mjx-assistive-mml></mjx-container> is the cardinality of the factored state subspace, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="116" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>H</mi></math></mjx-assistive-mml></mjx-container> is the planning horizon and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="117" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container> is the number of factored transition. To show the optimality of our bounds, we also provide a lower bound for FMDP, which indicates that our algorithm is near-optimal w.r.t. timestep <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="118" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container>, horizon <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="119" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>H</mi></math></mjx-assistive-mml></mjx-container> and factored state-action subspace cardinality. Finally, as an application, we study a new formulation of constrained RL, known as RL with knapsack constraints (RLwK), and provides the first sample-efficient algorithm based on FMDP-BF.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an efficient algorithm with near-optimal regret guarantee for factored MDP, and apply the algorithm to a new formulation of constrained RL.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="MJIve1zgR_" data-number="712">
      <h4>
        <a href="https://openreview.net/forum?id=MJIve1zgR_">
            Unbiased Teacher for Semi-Supervised Object Detection
        </a>
      
        
          <a href="https://openreview.net/pdf?id=MJIve1zgR_" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yen-Cheng_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yen-Cheng_Liu1">Yen-Cheng Liu</a>, <a href="https://openreview.net/profile?id=~Chih-Yao_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chih-Yao_Ma1">Chih-Yao Ma</a>, <a href="https://openreview.net/profile?email=zijian%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zijian@fb.com">Zijian He</a>, <a href="https://openreview.net/profile?id=~Chia-Wen_Kuo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chia-Wen_Kuo1">Chia-Wen Kuo</a>, <a href="https://openreview.net/profile?id=~Kan_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kan_Chen1">Kan Chen</a>, <a href="https://openreview.net/profile?id=~Peizhao_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peizhao_Zhang1">Peizhao Zhang</a>, <a href="https://openreview.net/profile?id=~Bichen_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bichen_Wu1">Bichen Wu</a>, <a href="https://openreview.net/profile?id=~Zsolt_Kira1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zsolt_Kira1">Zsolt Kira</a>, <a href="https://openreview.net/profile?id=~Peter_Vajda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peter_Vajda1">Peter Vajda</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#MJIve1zgR_-details-229" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MJIve1zgR_-details-229"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Object Detection</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Semi-supervised learning, i.e., training networks with both labeled and unlabeled data, has made significant progress recently. However, existing works have primarily focused on image classification tasks and neglected object detection which requires more annotation effort. In this work, we revisit the Semi-Supervised Object Detection (SS-OD) and identify the pseudo-labeling bias issue in SS-OD. To address this, we introduce Unbiased Teacher, a simple yet effective approach that jointly trains a student and a gradually progressing teacher in a mutually-beneficial manner. Together with a class-balance loss to downweight overly confident pseudo-labels, Unbiased Teacher consistently improved state-of-the-art methods by significant margins on COCO-standard, COCO-additional, and VOC datasets. Specifically, Unbiased Teacher achieves 6.8 absolute mAP improvements against state-of-the-art method when using 1% of labeled data on MS-COCO, achieves around 10 mAP improvements against the supervised baseline when using only 0.5, 1, 2% of labeled data on MS-COCO.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose Unbiased Teacher to jointly address the pseudo-labeling bias issue and the overfitting issue in semi-supervised object detection, and our model performs favorably against existing works on COCO-standard, COCO-additional, and VOC.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=MJIve1zgR_&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9l0K4OM-oXE" data-number="1631">
      <h4>
        <a href="https://openreview.net/forum?id=9l0K4OM-oXE">
            Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9l0K4OM-oXE" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yige_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yige_Li1">Yige Li</a>, <a href="https://openreview.net/profile?email=xxlv%40mail.xidian.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="xxlv@mail.xidian.edu.cn">Xixiang Lyu</a>, <a href="https://openreview.net/profile?id=~Nodens_Koren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nodens_Koren1">Nodens Koren</a>, <a href="https://openreview.net/profile?id=~Lingjuan_Lyu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lingjuan_Lyu1">Lingjuan Lyu</a>, <a href="https://openreview.net/profile?id=~Bo_Li19" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Li19">Bo Li</a>, <a href="https://openreview.net/profile?id=~Xingjun_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xingjun_Ma1">Xingjun Ma</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9l0K4OM-oXE-details-539" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9l0K4OM-oXE-details-539"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Backdoor Defense, Deep Neural Networks, Neural Attention Distillation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep neural networks (DNNs) are known vulnerable to backdoor attacks, a training time attack that injects a trigger pattern into a small proportion of training data so as to control the model's prediction at the test time. Backdoor attacks are notably dangerous since they do not affect the model's performance on clean examples, yet can fool the model to make the incorrect prediction whenever the trigger pattern appears during testing. In this paper, we propose a novel defense framework Neural Attention Distillation (NAD) to erase backdoor triggers from backdoored DNNs. NAD utilizes a teacher network to guide the finetuning of the backdoored student network on a small clean subset of data such that the intermediate-layer attention of the student network aligns with that of the teacher network. The teacher network can be obtained by an independent finetuning process on the same clean subset. We empirically show, against 6 state-of-the-art backdoor attacks,  NAD can effectively erase the backdoor triggers using only 5\% clean training data without causing obvious performance degradation on clean examples. Our code is available at https://github.com/bboylyg/NAD.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A simple but effective nerual attention distillation method for backdoor defense.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=9l0K4OM-oXE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Wga_hrCa3P3" data-number="491">
      <h4>
        <a href="https://openreview.net/forum?id=Wga_hrCa3P3">
            Contrastive  Learning  with Adversarial Perturbations for Conditional Text Generation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Wga_hrCa3P3" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Seanie_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seanie_Lee1">Seanie Lee</a>, <a href="https://openreview.net/profile?id=~Dong_Bok_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dong_Bok_Lee1">Dong Bok Lee</a>, <a href="https://openreview.net/profile?id=~Sung_Ju_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sung_Ju_Hwang1">Sung Ju Hwang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>23 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Wga_hrCa3P3-details-73" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Wga_hrCa3P3-details-73"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">conditional text generation, contrastive learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recently, sequence-to-sequence (seq2seq) models with the Transformer architecture have achieved remarkable performance on various conditional text generation tasks, such as machine translation. However, most of them are trained with teacher forcing with the ground truth label given at each time step, without being exposed to incorrectly generated tokens during training, which hurts its generalization to unseen inputs, that is known as the "exposure bias" problem. In this work, we propose to solve the conditional text generation problem by contrasting positive pairs with negative pairs, such that the model is exposed to various valid or incorrect perturbations of the inputs, for improved generalization. However, training the model with naïve contrastive learning framework using random non-target sequences as negative examples is suboptimal, since they are easily distinguishable from the correct output, especially so with models pretrained with large text corpora. Also, generating positive examples requires domain-specific augmentation heuristics which may not generalize over diverse domains. To tackle this problem, we propose a principled method to generate positive and negative samples for contrastive learning of seq2seq models. Specifically, we generate negative examples by adding small perturbations to the input sequence to minimize its conditional likelihood, and positive examples by adding  large perturbations while enforcing it to have a high conditional likelihood. Such `"hard'' positive and negative pairs generated using our method guides the model to better distinguish correct outputs from incorrect ones. We empirically show that our proposed method significantly improves the generalization of the seq2seq on three text generation tasks --- machine translation, text summarization, and question generation.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a contrastive learning with adversarial perturbation to tackle the exposure bias problem.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="WesiCoRVQ15" data-number="1454">
      <h4>
        <a href="https://openreview.net/forum?id=WesiCoRVQ15">
            When Optimizing  <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="120" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></mjx-assistive-mml></mjx-container>-Divergence is Robust with Label Noise
        </a>
      
        
          <a href="https://openreview.net/pdf?id=WesiCoRVQ15" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jiaheng_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiaheng_Wei1">Jiaheng Wei</a>, <a href="https://openreview.net/profile?id=~Yang_Liu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Liu3">Yang Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#WesiCoRVQ15-details-979" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WesiCoRVQ15-details-979"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value "><mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="121" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo>−</mo></math></mjx-assistive-mml></mjx-container>divergence, robustness, learning with noisy labels</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We show when maximizing a properly defined <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="122" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></mjx-assistive-mml></mjx-container>-divergence measure with respect to a classifier's predictions and the supervised labels is robust with label noise. Leveraging its variational form, we derive a nice decoupling property for a family of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="123" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></mjx-assistive-mml></mjx-container>-divergence measures when label noise presents, where the divergence is shown to be a linear combination of the variational difference defined on the clean distribution and a bias term introduced due to the noise. The above derivation helps us analyze the robustness of different <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="124" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></mjx-assistive-mml></mjx-container>-divergence functions. With established robustness, this family of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="125" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></mjx-assistive-mml></mjx-container>-divergence functions arises as useful metrics for the problem of learning with noisy labels, which do not require the specification of the labels' noise rate. When they are possibly not robust, we propose fixes to make them so. In addition to the analytical results, we present thorough experimental evidence. Our code is available at https://github.com/UCSC-REAL/Robust-f-divergence-measures.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value "> We show when maximizing a properly defined <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="126" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></mjx-assistive-mml></mjx-container>-divergence measure with respect to a classifier's predictions and the supervised labels is robust with label noise. </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=WesiCoRVQ15&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="VJnrYcnRc6" data-number="2818">
      <h4>
        <a href="https://openreview.net/forum?id=VJnrYcnRc6">
            Conditional Generative Modeling via Learning the Latent Space
        </a>
      
        
          <a href="https://openreview.net/pdf?id=VJnrYcnRc6" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sameera_Ramasinghe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sameera_Ramasinghe1">Sameera Ramasinghe</a>, <a href="https://openreview.net/profile?id=~Kanchana_Nisal_Ranasinghe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kanchana_Nisal_Ranasinghe1">Kanchana Nisal Ranasinghe</a>, <a href="https://openreview.net/profile?id=~Salman_Khan4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Salman_Khan4">Salman Khan</a>, <a href="https://openreview.net/profile?id=~Nick_Barnes3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nick_Barnes3">Nick Barnes</a>, <a href="https://openreview.net/profile?id=~Stephen_Gould1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephen_Gould1">Stephen Gould</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#VJnrYcnRc6-details-129" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="VJnrYcnRc6-details-129"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multimodal Spaces, Conditional Generation, Generative Modeling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Although deep learning has achieved appealing results on several machine learning tasks, most of the models are deterministic at inference, limiting their application to single-modal settings. We propose a novel general-purpose framework for conditional generation in multimodal spaces, that uses latent variables to model generalizable learning patterns while minimizing a family of regression cost functions. At inference, the latent variables are optimized to find solutions corresponding to multiple output modes.  Compared to existing generative solutions, our approach demonstrates faster and more stable convergence, and can learn better representations for downstream tasks. Importantly, it provides a simple generic model that can perform better than highly engineered pipelines tailored using domain expertise on a variety of tasks, while generating diverse outputs. Code available at https://github.com/samgregoost/cGML.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Conditional generation in continuous multimodal spaces by learning the behavior of latent variables.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=VJnrYcnRc6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="RovX-uQ1Hua" data-number="655">
      <h4>
        <a href="https://openreview.net/forum?id=RovX-uQ1Hua">
            Text Generation by Learning from Demonstrations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=RovX-uQ1Hua" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Richard_Yuanzhe_Pang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Richard_Yuanzhe_Pang1">Richard Yuanzhe Pang</a>, <a href="https://openreview.net/profile?id=~He_He2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~He_He2">He He</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#RovX-uQ1Hua-details-974" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RovX-uQ1Hua-details-974"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">text generation, learning from demonstrations, nlp</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Current approaches to text generation largely rely on autoregressive models and maximum likelihood estimation. This paradigm leads to (i) diverse but low-quality samples due to mismatched learning objective and evaluation metric (likelihood vs. quality) and (ii) exposure bias due to mismatched history distributions (gold vs. model-generated). To alleviate these problems, we frame text generation as an offline reinforcement learning (RL) problem with expert demonstrations (i.e., the reference), where the goal is to maximize quality given model-generated histories. We propose GOLD (generation by off-policy learning from demonstrations): an easy-to-optimize algorithm that learns from the demonstrations by importance weighting. Intuitively, GOLD upweights confident tokens and downweights unconfident ones in the reference during training, avoiding optimization issues faced by prior RL approaches that rely on online data collection. According to both automatic and human evaluation, models trained by GOLD outperform those trained by MLE and policy gradient on summarization, question generation, and machine translation. Further, our models are less sensitive to decoding algorithms and alleviate exposure bias.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="_X_4Akcd8Re" data-number="2388">
      <h4>
        <a href="https://openreview.net/forum?id=_X_4Akcd8Re">
            Learning Long-term Visual Dynamics with Region Proposal Interaction Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=_X_4Akcd8Re" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Haozhi_Qi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haozhi_Qi1">Haozhi Qi</a>, <a href="https://openreview.net/profile?id=~Xiaolong_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaolong_Wang3">Xiaolong Wang</a>, <a href="https://openreview.net/profile?id=~Deepak_Pathak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deepak_Pathak1">Deepak Pathak</a>, <a href="https://openreview.net/profile?id=~Yi_Ma4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Ma4">Yi Ma</a>, <a href="https://openreview.net/profile?id=~Jitendra_Malik2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jitendra_Malik2">Jitendra Malik</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Apr 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#_X_4Akcd8Re-details-115" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_X_4Akcd8Re-details-115"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">dynamics prediction, interaction networks, physical reasoning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Learning long-term dynamics models is the key to understanding physical common sense. Most existing approaches on learning dynamics from visual input sidestep long-term predictions by resorting to rapid re-planning with short-term models. This not only requires such models to be super accurate but also limits them only to tasks where an agent can continuously obtain feedback and take action at each step until completion. In this paper, we aim to leverage the ideas from success stories in visual recognition tasks to build object representations that can capture inter-object and object-environment interactions over a long range. To this end, we propose Region Proposal Interaction Networks (RPIN), which reason about each object's trajectory in a latent region-proposal feature space. Thanks to the simple yet effective object representation, our approach outperforms prior methods by a significant margin both in terms of prediction quality and their ability to plan for downstream tasks, and also generalize well to novel environments. Code, pre-trained models, and more visualization results are available at https://haozhi.io/RPIN.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose Region Proposal Interaction Networks for physical interaction prediction, which is applied across both simulation and real world environments for long-range prediction and planning.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xCxXwTzx4L1" data-number="1752">
      <h4>
        <a href="https://openreview.net/forum?id=xCxXwTzx4L1">
            ChipNet: Budget-Aware Pruning with Heaviside Continuous Approximations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xCxXwTzx4L1" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Rishabh_Tiwari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rishabh_Tiwari1">Rishabh Tiwari</a>, <a href="https://openreview.net/profile?id=~Udbhav_Bamba1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Udbhav_Bamba1">Udbhav Bamba</a>, <a href="https://openreview.net/profile?id=~Arnav_Chavan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arnav_Chavan1">Arnav Chavan</a>, <a href="https://openreview.net/profile?id=~Deepak_Gupta2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deepak_Gupta2">Deepak Gupta</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xCxXwTzx4L1-details-80" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xCxXwTzx4L1-details-80"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Structured Pruning, Budget-Aware Pruning, Budget constraints, Sparsity Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Structured pruning methods are among the effective strategies for extracting small resource-efficient convolutional neural networks from their dense counterparts with minimal loss in accuracy. However, most existing methods still suffer from one or more limitations, that include 1) the need for training the dense model from scratch with pruning-related parameters embedded in the architecture, 2) requiring model-specific hyperparameter settings, 3) inability to include budget-related constraint in the training process, and 4) instability under scenarios of extreme pruning. In this paper, we present ChipNet, a deterministic pruning strategy that employs continuous Heaviside function and a novel crispness loss to identify a highly sparse network out of an existing dense network. Our choice of continuous Heaviside function is inspired by the field of design optimization, where the material distribution task is posed as a continuous optimization problem, but only discrete values (0 or 1) are practically feasible and expected as final outcomes. Our approach's flexible design facilitates its use with different choices of budget constraints while maintaining stability for very low target budgets. Experimental results show that ChipNet outperforms state-of-the-art structured pruning methods by remarkable margins of up to 16.1% in terms of accuracy. Further, we show that the masks obtained with ChipNet are transferable across datasets. For certain cases, it was observed that masks transferred from a model trained on feature-rich teacher dataset provide better performance on the student dataset than those obtained by directly pruning on the student data itself.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A budget-aware deterministic strategy for structured pruning based on continuous Heaviside approximations and crispness loss.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=xCxXwTzx4L1&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="b7g3_ZMHnT0" data-number="2516">
      <h4>
        <a href="https://openreview.net/forum?id=b7g3_ZMHnT0">
            Learning to Deceive Knowledge Graph Augmented Models via Targeted Perturbation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=b7g3_ZMHnT0" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=mrigankraman1611%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mrigankraman1611@gmail.com">Mrigank Raman</a>, <a href="https://openreview.net/profile?id=~Aaron_Chan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aaron_Chan1">Aaron Chan</a>, <a href="https://openreview.net/profile?id=~Siddhant_Agarwal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siddhant_Agarwal1">Siddhant Agarwal</a>, <a href="https://openreview.net/profile?id=~PeiFeng_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~PeiFeng_Wang1">PeiFeng Wang</a>, <a href="https://openreview.net/profile?id=~Hansen_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hansen_Wang1">Hansen Wang</a>, <a href="https://openreview.net/profile?email=sukim%40adobe.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sukim@adobe.com">Sungchul Kim</a>, <a href="https://openreview.net/profile?id=~Ryan_Rossi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ryan_Rossi1">Ryan Rossi</a>, <a href="https://openreview.net/profile?id=~Handong_Zhao3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Handong_Zhao3">Handong Zhao</a>, <a href="https://openreview.net/profile?email=lipka%40adobe.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="lipka@adobe.com">Nedim Lipka</a>, <a href="https://openreview.net/profile?id=~Xiang_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiang_Ren1">Xiang Ren</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#b7g3_ZMHnT0-details-905" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="b7g3_ZMHnT0-details-905"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">neural symbolic reasoning, interpretability, model explanation, faithfulness, knowledge graph, commonsense question answering, recommender system</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Knowledge graphs (KGs) have helped neural models improve performance on various knowledge-intensive tasks, like question answering and item recommendation. By using attention over the KG, such KG-augmented models can also "explain" which KG information was most relevant for making a given prediction. In this paper, we question whether these models are really behaving as we expect. We show that, through a reinforcement learning policy (or even simple heuristics), one can produce deceptively perturbed KGs, which maintain the downstream performance of the original KG while significantly deviating from the original KG's semantics and structure. Our findings raise doubts about KG-augmented models' ability to reason about KG information and give sensible explanations.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">KG-augmented models and humans use KG info differently.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xzqLpqRzxLq" data-number="587">
      <h4>
        <a href="https://openreview.net/forum?id=xzqLpqRzxLq">
            IEPT: Instance-Level and Episode-Level Pretext Tasks for Few-Shot Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xzqLpqRzxLq" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Manli_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manli_Zhang1">Manli Zhang</a>, <a href="https://openreview.net/profile?id=~Jianhong_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianhong_Zhang1">Jianhong Zhang</a>, <a href="https://openreview.net/profile?id=~Zhiwu_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiwu_Lu1">Zhiwu Lu</a>, <a href="https://openreview.net/profile?id=~Tao_Xiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Xiang1">Tao Xiang</a>, <a href="https://openreview.net/profile?id=~Mingyu_Ding1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingyu_Ding1">Mingyu Ding</a>, <a href="https://openreview.net/profile?id=~Songfang_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Songfang_Huang1">Songfang Huang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xzqLpqRzxLq-details-308" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xzqLpqRzxLq-details-308"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">few-shot learning, self-supervised learning, episode-level pretext task</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The need of collecting large quantities of labeled training data for each new task has limited the usefulness of deep neural networks. Given data from a set of source tasks, this limitation can be overcome using two transfer learning approaches: few-shot learning (FSL) and self-supervised learning (SSL). The former aims to learn `how to learn' by designing learning episodes using source tasks to simulate the challenge of solving the target new task with few labeled samples. In contrast, the latter exploits an annotation-free pretext task across all source tasks in order to learn generalizable feature representations. In this work, we propose a novel Instance-level and Episode-level Pretext Task (IEPT) framework that seamlessly integrates SSL into FSL. Specifically, given an FSL episode, we first apply geometric transformations to each instance to generate extended episodes. At the instance-level, transformation recognition is performed as per standard SSL. Importantly, at the episode-level, two SSL-FSL hybrid learning objectives are devised: (1) The consistency across the predictions of an FSL classifier from different extended episodes is maximized as an episode-level pretext task. (2) The features extracted from each instance across different episodes are integrated to construct a single FSL classifier for meta-learning. Extensive experiments show that our proposed model (i.e., FSL with IEPT) achieves the new state-of-the-art. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper proposes a novel Instance-level and Episode-level Pretext Task (IEPT) framework that seamlessly integrates SSL into FSL.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="L7WD8ZdscQ5" data-number="1139">
      <h4>
        <a href="https://openreview.net/forum?id=L7WD8ZdscQ5">
            The Role of Momentum Parameters in the Optimal Convergence of Adaptive Polyak's Heavy-ball Methods
        </a>
      
        
          <a href="https://openreview.net/pdf?id=L7WD8ZdscQ5" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Wei_Tao3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Tao3">Wei Tao</a>, <a href="https://openreview.net/profile?email=ls15186322349%40163.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ls15186322349@163.com">Sheng Long</a>, <a href="https://openreview.net/profile?email=gaowei.wu%40ia.ac.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="gaowei.wu@ia.ac.cn">Gaowei Wu</a>, <a href="https://openreview.net/profile?email=qing.tao%40ia.ac.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="qing.tao@ia.ac.cn">Qing Tao</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#L7WD8ZdscQ5-details-727" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="L7WD8ZdscQ5-details-727"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep learning, convex optimization, momentum methods, adaptive heavy-ball methods, optimal convergence</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The adaptive stochastic gradient descent (SGD) with momentum has been widely adopted in deep learning as well as convex optimization. In practice, the last iterate is commonly used as the final solution. However, the available regret analysis and the setting of constant momentum parameters only guarantee the optimal convergence of the averaged solution. In this paper, we fill this theory-practice gap by investigating the convergence of the last iterate (referred to as {\it individual convergence}), which is a more difficult task than convergence analysis of the averaged solution. Specifically, in the constrained convex cases, we prove that the adaptive Polyak's Heavy-ball (HB) method, in which the step size is only updated using the exponential moving average strategy, attains an individual convergence rate of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="127" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msqrt size="s"><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.189em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mfrac><mn>1</mn><msqrt><mi>t</mi></msqrt></mfrac><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, as opposed to that of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="128" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-msqrt size="s"><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.189em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mi>t</mi></mrow><msqrt><mi>t</mi></msqrt></mfrac><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> of SGD, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="129" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>t</mi></math></mjx-assistive-mml></mjx-container> is the number of iterations. Our new analysis not only shows how the HB momentum and its time-varying weight help us to achieve the acceleration in convex optimization but also gives valuable hints how the momentum parameters should be scheduled in deep learning. Empirical results validate the correctness of our convergence analysis in optimizing convex functions and demonstrate the improved performance of the adaptive HB methods in training deep networks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A theory-practice gap in convex optimization and deep learning is bridged by giving a novel convergence analysis of the last Iterate of adaptive Heavy-ball methods.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=L7WD8ZdscQ5&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="dV19Yyi1fS3" data-number="630">
      <h4>
        <a href="https://openreview.net/forum?id=dV19Yyi1fS3">
            Training with Quantization Noise for Extreme Model Compression
        </a>
      
        
          <a href="https://openreview.net/pdf?id=dV19Yyi1fS3" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Pierre_Stock1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pierre_Stock1">Pierre Stock</a>, <a href="https://openreview.net/profile?id=~Angela_Fan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Angela_Fan2">Angela Fan</a>, <a href="https://openreview.net/profile?id=~Benjamin_Graham1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Graham1">Benjamin Graham</a>, <a href="https://openreview.net/profile?id=~Edouard_Grave1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edouard_Grave1">Edouard Grave</a>, <a href="https://openreview.net/profile?id=~R%C3%A9mi_Gribonval1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rémi_Gribonval1">Rémi Gribonval</a>, <a href="https://openreview.net/profile?id=~Herve_Jegou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Herve_Jegou1">Herve Jegou</a>, <a href="https://openreview.net/profile?id=~Armand_Joulin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Armand_Joulin1">Armand Joulin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#dV19Yyi1fS3-details-153" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dV19Yyi1fS3-details-153"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Compression, Efficiency, Product Quantization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We tackle the problem of producing compact models, maximizing their accuracy for a given model size. A standard solution is to train networks with Quantization Aware Training, where the weights are quantized during training and the gradients approximated with the Straight-Through Estimator. In this paper, we extend this approach to work with extreme compression methods where the approximations introduced by STE are severe. Our proposal is to only quantize a different random subset of weights during each forward, allowing for unbiased gradients to flow through the other weights. Controlling the amount of noise and its form allows for extreme compression rates while maintaining the performance of the original model. As a result we establish new state-of-the-art compromises between accuracy and model size both in natural language processing and image classification. For example, applying our method to state-of-the-art Transformer and ConvNet architectures, we can achieve 82.5% accuracy on MNLI by compressing RoBERTa to 14 MB and 80.0% top-1 accuracy on ImageNet by compressing an EfficientNet-B3 to 3.3 MB.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="R0a0kFI3dJx" data-number="2265">
      <h4>
        <a href="https://openreview.net/forum?id=R0a0kFI3dJx">
            Adaptive Extra-Gradient Methods for Min-Max Optimization and Games
        </a>
      
        
          <a href="https://openreview.net/pdf?id=R0a0kFI3dJx" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kimon_Antonakopoulos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kimon_Antonakopoulos1">Kimon Antonakopoulos</a>, <a href="https://openreview.net/profile?id=~Veronica_Belmega1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Veronica_Belmega1">Veronica Belmega</a>, <a href="https://openreview.net/profile?id=~Panayotis_Mertikopoulos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Panayotis_Mertikopoulos1">Panayotis Mertikopoulos</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#R0a0kFI3dJx-details-194" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="R0a0kFI3dJx-details-194"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">min-max optimization, games, mirror-prox, adaptive methods, regime agnostic methods</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present a new family of min-max optimization algorithms that automatically exploit the geometry of the gradient data observed at earlier iterations to perform more informative extra-gradient steps in later ones.
      Thanks to this adaptation mechanism, the proposed method automatically detects whether the problem is smooth or not, without requiring any prior tuning by the optimizer.
      As a result, the algorithm simultaneously achieves order-optimal convergence rates, \ie it  converges to an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="130" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D700 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ε</mi></math></mjx-assistive-mml></mjx-container>-optimal solution within <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="131" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D700 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mrow><mo>/</mo></mrow><mi>ε</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> iterations in smooth problems, and within <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="132" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D700 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mrow><mo>/</mo></mrow><msup><mi>ε</mi><mn>2</mn></msup><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> iterations in non-smooth ones. Importantly, these guarantees do not require any of the standard boundedness or Lipschitz continuity conditions that are typically assumed in the literature; in particular, they apply even to problems with singularities (such as resource allocation problems and the like). This adaptation is achieved through the use of a geometric apparatus based on Finsler metrics and a suitably chosen mirror-prox template that allows us to derive sharp convergence rates for the methods at hand.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We develop an adaptive mirror-prox method for min-max problems and games that achieves order-optimal rates in both smooth and non-smooth problems.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=R0a0kFI3dJx&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="NTEz-6wysdb" data-number="436">
      <h4>
        <a href="https://openreview.net/forum?id=NTEz-6wysdb">
            Distilling Knowledge from Reader to Retriever for Question Answering
        </a>
      
        
          <a href="https://openreview.net/pdf?id=NTEz-6wysdb" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Gautier_Izacard1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gautier_Izacard1">Gautier Izacard</a>, <a href="https://openreview.net/profile?id=~Edouard_Grave1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edouard_Grave1">Edouard Grave</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#NTEz-6wysdb-details-589" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NTEz-6wysdb-details-589"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">question answering, information retrieval</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The task of information retrieval is an important component of many natural language processing systems, such as open domain question answering. While traditional methods were based on hand-crafted features, continuous representations based on neural networks recently obtained competitive results. A challenge of using such methods is to obtain supervised data to train the retriever model, corresponding to pairs of query and support documents. In this paper, we propose a technique to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever. We evaluate our method on question answering, obtaining state-of-the-art results.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that attention scores obtained by training a model to answer questions given a set of support documents can be used to train a model to select relevant passages in a knowledge source.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="lvRTC669EY_" data-number="2803">
      <h4>
        <a href="https://openreview.net/forum?id=lvRTC669EY_">
            Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=lvRTC669EY_" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=tangzhenggang%40pku.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="tangzhenggang@pku.edu.cn">Zhenggang Tang</a>, <a href="https://openreview.net/profile?email=yc19%40mails.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="yc19@mails.tsinghua.edu.cn">Chao Yu</a>, <a href="https://openreview.net/profile?id=~Boyuan_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boyuan_Chen2">Boyuan Chen</a>, <a href="https://openreview.net/profile?id=~Huazhe_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huazhe_Xu1">Huazhe Xu</a>, <a href="https://openreview.net/profile?id=~Xiaolong_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaolong_Wang3">Xiaolong Wang</a>, <a href="https://openreview.net/profile?id=~Fei_Fang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fei_Fang1">Fei Fang</a>, <a href="https://openreview.net/profile?id=~Simon_Shaolei_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Shaolei_Du1">Simon Shaolei Du</a>, <a href="https://openreview.net/profile?id=~Yu_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Wang3">Yu Wang</a>, <a href="https://openreview.net/profile?id=~Yi_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Wu1">Yi Wu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#lvRTC669EY_-details-242" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lvRTC669EY_-details-242"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">strategic behavior, multi-agent reinforcement learning, reward randomization, diverse strategies</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a simple, general and effective technique, Reward Randomization for discovering diverse strategic policies in complex multi-agent games. Combining reward randomization and policy gradient, we derive a new algorithm, Reward-Randomized Policy Gradient (RPG). RPG is able to discover a set of multiple distinctive human-interpretable strategies in challenging temporal trust dilemmas, including grid-world games and a real-world game Agar.io, where multiple equilibria exist but standard multi-agent policy gradient algorithms always converge to a fixed one with a sub-optimal payoff for every player even using state-of-the-art exploration techniques. Furthermore, with the set of diverse strategies from RPG, we can (1) achieve higher payoffs by fine-tuning the best policy from the set; and (2) obtain an adaptive agent by using this set of strategies as its training opponents. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an MARL algorithm, RPG, which discovers diverse non-trivial strategic behavior in several challenging multi-agent games.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tu29GQT0JFy" data-number="1124">
      <h4>
        <a href="https://openreview.net/forum?id=tu29GQT0JFy">
            not-MIWAE: Deep Generative Modelling with Missing not at Random Data
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tu29GQT0JFy" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Niels_Bruun_Ipsen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Niels_Bruun_Ipsen1">Niels Bruun Ipsen</a>, <a href="https://openreview.net/profile?id=~Pierre-Alexandre_Mattei3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pierre-Alexandre_Mattei3">Pierre-Alexandre Mattei</a>, <a href="https://openreview.net/profile?id=~Jes_Frellsen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jes_Frellsen1">Jes Frellsen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tu29GQT0JFy-details-758" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tu29GQT0JFy-details-758"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">When a missing process depends on the missing values themselves, it needs to be explicitly modelled and taken into account while doing likelihood-based inference. We present an approach for building and fitting deep latent variable models (DLVMs) in cases where the missing process is dependent on the missing data. Specifically, a deep neural network enables us to flexibly model the conditional distribution of the missingness pattern given the data. This allows for incorporating prior information about the type of missingness (e.g.~self-censoring) into the model. Our inference technique, based on importance-weighted variational inference, involves maximising a lower bound of the joint likelihood. Stochastic gradients of the bound are obtained by using the reparameterisation trick both in latent space and data space. We show on various kinds of data sets and missingness patterns that explicitly modelling the missing process can be invaluable.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present an approach for building and fitting deep latent variable models (DLVMs) in cases where the missing process is dependent on the missing data.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="MBOyiNnYthd" data-number="3291">
      <h4>
        <a href="https://openreview.net/forum?id=MBOyiNnYthd">
            IDF++: Analyzing and Improving Integer Discrete Flows for Lossless Compression
        </a>
      
        
          <a href="https://openreview.net/pdf?id=MBOyiNnYthd" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Rianne_van_den_Berg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rianne_van_den_Berg1">Rianne van den Berg</a>, <a href="https://openreview.net/profile?id=~Alexey_A._Gritsenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexey_A._Gritsenko1">Alexey A. Gritsenko</a>, <a href="https://openreview.net/profile?id=~Mostafa_Dehghani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mostafa_Dehghani1">Mostafa Dehghani</a>, <a href="https://openreview.net/profile?id=~Casper_Kaae_S%C3%B8nderby1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Casper_Kaae_Sønderby1">Casper Kaae Sønderby</a>, <a href="https://openreview.net/profile?id=~Tim_Salimans1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tim_Salimans1">Tim Salimans</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#MBOyiNnYthd-details-144" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MBOyiNnYthd-details-144"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">normalizing flows, lossless source compression, generative modeling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper we analyse and improve integer discrete flows for lossless compression. Integer discrete flows are a recently proposed class of models that learn invertible transformations for integer-valued random variables. Their discrete nature makes them particularly suitable for lossless compression with entropy coding schemes. We start by investigating a recent theoretical claim that states that invertible flows for discrete random variables are less flexible than their continuous counterparts. We demonstrate with a proof that this claim does not hold for integer discrete flows due to the embedding of data with finite support into the countably infinite integer lattice. Furthermore, we zoom in on the effect of gradient bias due to the straight-through estimator in integer discrete flows, and demonstrate that its influence is highly dependent on architecture choices and less prominent than previously thought. Finally, we show how different architecture modifications improve the performance of this model class for lossless compression, and that they also enable more efficient compression: a model with half the number of flow layers performs on par with or better than the original integer discrete flow model.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We analyze and improve integer discrete normalizing flows for lossless source compression.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="unI5ucw_Jk" data-number="1306">
      <h4>
        <a href="https://openreview.net/forum?id=unI5ucw_Jk">
            Explaining by Imitating: Understanding Decisions by Interpretable Policy Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=unI5ucw_Jk" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alihan_H%C3%BCy%C3%BCk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alihan_Hüyük1">Alihan Hüyük</a>, <a href="https://openreview.net/profile?id=~Daniel_Jarrett1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Jarrett1">Daniel Jarrett</a>, <a href="https://openreview.net/profile?id=~Cem_Tekin2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cem_Tekin2">Cem Tekin</a>, <a href="https://openreview.net/profile?id=~Mihaela_van_der_Schaar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihaela_van_der_Schaar2">Mihaela van der Schaar</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#unI5ucw_Jk-details-793" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="unI5ucw_Jk-details-793"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">interpretable policy learning, understanding decision-making</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Understanding human behavior from observed data is critical for transparency and accountability in decision-making. Consider real-world settings such as healthcare, in which modeling a decision-maker’s policy is challenging—with no access to underlying states, no knowledge of environment dynamics, and no allowance for live experimentation. We desire learning a data-driven representation of decision- making behavior that (1) inheres transparency by design, (2) accommodates partial observability, and (3) operates completely offline. To satisfy these key criteria, we propose a novel model-based Bayesian method for interpretable policy learning (“Interpole”) that jointly estimates an agent’s (possibly biased) belief-update process together with their (possibly suboptimal) belief-action mapping. Through experiments on both simulated and real-world data for the problem of Alzheimer’s disease diagnosis, we illustrate the potential of our approach as an investigative device for auditing, quantifying, and understanding human decision-making behavior.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a method for learning interpretable representations of behavior to enable auditing, quantifying, and understanding human decision-making processes.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=unI5ucw_Jk&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ETBc_MIMgoX" data-number="631">
      <h4>
        <a href="https://openreview.net/forum?id=ETBc_MIMgoX">
            Learning with AMIGo: Adversarially Motivated Intrinsic Goals
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ETBc_MIMgoX" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Andres_Campero1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andres_Campero1">Andres Campero</a>, <a href="https://openreview.net/profile?id=~Roberta_Raileanu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roberta_Raileanu2">Roberta Raileanu</a>, <a href="https://openreview.net/profile?id=~Heinrich_Kuttler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Heinrich_Kuttler1">Heinrich Kuttler</a>, <a href="https://openreview.net/profile?id=~Joshua_B._Tenenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_B._Tenenbaum1">Joshua B. Tenenbaum</a>, <a href="https://openreview.net/profile?id=~Tim_Rockt%C3%A4schel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tim_Rocktäschel1">Tim Rocktäschel</a>, <a href="https://openreview.net/profile?id=~Edward_Grefenstette1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edward_Grefenstette1">Edward Grefenstette</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 24 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>25 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ETBc_MIMgoX-details-879" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ETBc_MIMgoX-details-879"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, exploration, meta-learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A key challenge for reinforcement learning (RL) consists of learning in environments with sparse extrinsic rewards. In contrast to current RL methods, humans are able to learn new skills with little or no reward by using various forms of intrinsic motivation. We propose AMIGo, a novel agent incorporating -- as form of meta-learning -- a goal-generating teacher that proposes Adversarially Motivated Intrinsic Goals to train a goal-conditioned "student" policy in the absence of (or alongside) environment reward. Specifically, through a simple but effective "constructively adversarial" objective, the teacher learns to propose increasingly challenging -- yet achievable -- goals that allow the student to learn general skills for acting in a new environment, independent of the task to be solved. We show that our method generates a natural curriculum of self-proposed goals which ultimately allows the agent to solve challenging procedurally-generated tasks where other forms of intrinsic motivation and state-of-the-art RL methods fail.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A "constructively adversarial" teacher-student setup can augment on-policy algorithms to better solve difficult exploration tasks in RL.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="wta_8Hx2KD" data-number="1278">
      <h4>
        <a href="https://openreview.net/forum?id=wta_8Hx2KD">
            Incorporating Symmetry into Deep Dynamics Models for Improved Generalization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=wta_8Hx2KD" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Rui_Wang11" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rui_Wang11">Rui Wang</a>, <a href="https://openreview.net/profile?id=~Robin_Walters1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Robin_Walters1">Robin Walters</a>, <a href="https://openreview.net/profile?id=~Rose_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rose_Yu1">Rose Yu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#wta_8Hx2KD-details-593" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wta_8Hx2KD-details-593"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep sequence model, equivariant neural network, physics-guided deep learning, AI for earth science</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent work has shown deep learning can accelerate the prediction of physical dynamics relative to numerical solvers. However, limited physical accuracy and an inability to generalize under distributional shift limit its applicability to the real world. We propose to improve accuracy and generalization by incorporating symmetries into convolutional neural networks. Specifically, we employ a variety of methods each tailored to enforce a different symmetry. Our models are both theoretically and experimentally robust to distributional shift by symmetry group transformations and enjoy favorable sample complexity. We demonstrate the advantage of our approach on a variety of physical dynamics including Rayleigh–Bénard convection and real-world ocean currents and temperatures. Compare with image or text applications, our work is a significant step towards applying equivariant neural networks to high-dimensional systems with complex dynamics.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Integrate various symmetris into deep sequence models for forecasting turbulence and ocean currents with improved accuracy and physical consistency.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=wta_8Hx2KD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="h2EbJ4_wMVq" data-number="2667">
      <h4>
        <a href="https://openreview.net/forum?id=h2EbJ4_wMVq">
            CaPC Learning: Confidential and Private Collaborative Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=h2EbJ4_wMVq" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Christopher_A._Choquette-Choo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_A._Choquette-Choo1">Christopher A. Choquette-Choo</a>, <a href="https://openreview.net/profile?id=~Natalie_Dullerud1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Natalie_Dullerud1">Natalie Dullerud</a>, <a href="https://openreview.net/profile?id=~Adam_Dziedzic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adam_Dziedzic1">Adam Dziedzic</a>, <a href="https://openreview.net/profile?id=~Yunxiang_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yunxiang_Zhang1">Yunxiang Zhang</a>, <a href="https://openreview.net/profile?id=~Somesh_Jha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Somesh_Jha1">Somesh Jha</a>, <a href="https://openreview.net/profile?id=~Nicolas_Papernot1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicolas_Papernot1">Nicolas Papernot</a>, <a href="https://openreview.net/profile?id=~Xiao_Wang11" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiao_Wang11">Xiao Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 20 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#h2EbJ4_wMVq-details-811" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="h2EbJ4_wMVq-details-811"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">machine learning, deep learning, privacy, confidentiality, security, homomorphic encryption, mpc, differential privacy</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi-party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A method that enables parties to improve their own local heterogeneous machine learning models in a collaborative setting where both confidentiality and privacy need to be preserved to prevent both explicit and implicit sharing of private data.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=h2EbJ4_wMVq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="UwGY2qjqoLD" data-number="1237">
      <h4>
        <a href="https://openreview.net/forum?id=UwGY2qjqoLD">
            Heating up decision boundaries: isocapacitory saturation, adversarial scenarios and generalization bounds
        </a>
      
        
          <a href="https://openreview.net/pdf?id=UwGY2qjqoLD" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Bogdan_Georgiev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bogdan_Georgiev1">Bogdan Georgiev</a>, <a href="https://openreview.net/profile?id=~Lukas_Franken1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lukas_Franken1">Lukas Franken</a>, <a href="https://openreview.net/profile?id=~Mayukh_Mukherjee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mayukh_Mukherjee1">Mayukh Mukherjee</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 05 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#UwGY2qjqoLD-details-387" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UwGY2qjqoLD-details-387"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Brownian motion, deep learning theory, decision boundary geometry, curvature estimates, generalization bounds, adversarial attacks/defenses</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In the present work we study classifiers' decision boundaries via Brownian motion processes in ambient data space and associated probabilistic techniques. Intuitively, our ideas correspond to placing a heat source at the decision boundary and observing how effectively the sample points warm up. We are largely motivated by the search for a soft measure that sheds further light on the decision boundary's geometry. En route, we  bridge aspects of potential theory and geometric analysis (Maz'ya 2011, Grigor'Yan and Saloff-Coste 2002) with active fields of ML research such as adversarial examples and generalization bounds. First, we focus on the geometric behavior of decision boundaries in the light of adversarial attack/defense mechanisms. Experimentally, we observe a certain capacitory trend over different adversarial defense strategies: decision boundaries locally become flatter as measured by isoperimetric inequalities (Ford et al 2019); however, our more sensitive heat-diffusion metrics  extend this analysis and further reveal that some non-trivial geometry invisible to plain distance-based methods is still preserved. Intuitively, we provide evidence that the decision boundaries nevertheless retain many persistent "wiggly and fuzzy" regions on a finer scale.
      Second, we show how Brownian hitting probabilities translate to soft generalization bounds which are in turn connected to compression and noise stability (Arora et al 2018), and these bounds are significantly stronger if the decision boundary has controlled geometric features.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a heat-theoretic/Brownian motion approach to evaluate decision boundary geometry (curvature and density) with further applications in adversarial defenses, compression and generalization estimates.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=UwGY2qjqoLD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="TR-Nj6nFx42" data-number="2733">
      <h4>
        <a href="https://openreview.net/forum?id=TR-Nj6nFx42">
            A PAC-Bayesian Approach to Generalization Bounds for Graph Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=TR-Nj6nFx42" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Renjie_Liao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Renjie_Liao1">Renjie Liao</a>, <a href="https://openreview.net/profile?id=~Raquel_Urtasun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Raquel_Urtasun1">Raquel Urtasun</a>, <a href="https://openreview.net/profile?id=~Richard_Zemel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Richard_Zemel1">Richard Zemel</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#TR-Nj6nFx42-details-118" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TR-Nj6nFx42-details-118"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">PAC Bayes, Generalization Bounds, Graph Neural Networks, Graph Convolutional Neural Networks, Message Passing GNNs</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we derive generalization bounds for two primary classes of graph neural networks (GNNs), namely graph convolutional networks (GCNs) and message passing GNNs (MPGNNs), via a PAC-Bayesian approach. Our result reveals that the maximum node degree and the spectral norm of the weights govern the generalization bounds of both models. We also show that our bound for GCNs is a natural generalization of the results developed in \citep{neyshabur2017pac} for fully-connected and convolutional neural networks. For MPGNNs, our PAC-Bayes bound improves over the Rademacher complexity based bound \citep{garg2020generalization}, showing a tighter dependency on the maximum node degree and the maximum hidden dimension. The key ingredients of our proofs are a perturbation analysis of GNNs and the generalization of PAC-Bayes analysis to non-homogeneous GNNs. We perform an empirical study on several synthetic and real-world graph datasets and verify that our PAC-Bayes bound is tighter than others. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=TR-Nj6nFx42&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xnC8YwKUE3k" data-number="3220">
      <h4>
        <a href="https://openreview.net/forum?id=xnC8YwKUE3k">
            Clairvoyance: A Pipeline Toolkit for Medical Time Series
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xnC8YwKUE3k" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Daniel_Jarrett1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Jarrett1">Daniel Jarrett</a>, <a href="https://openreview.net/profile?id=~Jinsung_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinsung_Yoon1">Jinsung Yoon</a>, <a href="https://openreview.net/profile?id=~Ioana_Bica1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ioana_Bica1">Ioana Bica</a>, <a href="https://openreview.net/profile?id=~Zhaozhi_Qian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhaozhi_Qian1">Zhaozhi Qian</a>, <a href="https://openreview.net/profile?email=ae105%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="ae105@cam.ac.uk">Ari Ercole</a>, <a href="https://openreview.net/profile?id=~Mihaela_van_der_Schaar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihaela_van_der_Schaar2">Mihaela van der Schaar</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>25 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xnC8YwKUE3k-details-244" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xnC8YwKUE3k-details-244"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reproducibility, healthcare, medical time series, pipeline toolkit, software</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Time-series learning is the bread and butter of data-driven *clinical decision support*, and the recent explosion in ML research has demonstrated great potential in various healthcare settings. At the same time, medical time-series problems in the wild are challenging due to their highly *composite* nature: They entail design choices and interactions among components that preprocess data, impute missing values, select features, issue predictions, estimate uncertainty, and interpret models. Despite exponential growth in electronic patient data, there is a remarkable gap between the potential and realized utilization of ML for clinical research and decision support. In particular, orchestrating a real-world project lifecycle poses challenges in engineering (i.e. hard to build), evaluation (i.e. hard to assess), and efficiency (i.e. hard to optimize). Designed to address these issues simultaneously, Clairvoyance proposes a unified, end-to-end, autoML-friendly pipeline that serves as a (i) software toolkit, (ii) empirical standard, and (iii) interface for optimization. Our ultimate goal lies in facilitating transparent and reproducible experimentation with complex inference workflows, providing integrated pathways for (1) personalized prediction, (2) treatment-effect estimation, and (3) information acquisition. Through illustrative examples on real-world data in outpatient, general wards, and intensive-care settings, we illustrate the applicability of the pipeline paradigm on core tasks in the healthcare journey. To the best of our knowledge, Clairvoyance is the first to demonstrate viability of a comprehensive and automatable pipeline for clinical time-series ML.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We develop and present Clairvoyance: a pipeline toolkit for medical time series.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="068E_JSq9O" data-number="393">
      <h4>
        <a href="https://openreview.net/forum?id=068E_JSq9O">
            Self-supervised Representation Learning with Relative Predictive Coding
        </a>
      
        
          <a href="https://openreview.net/pdf?id=068E_JSq9O" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yao-Hung_Hubert_Tsai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yao-Hung_Hubert_Tsai1">Yao-Hung Hubert Tsai</a>, <a href="https://openreview.net/profile?id=~Martin_Q._Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Q._Ma1">Martin Q. Ma</a>, <a href="https://openreview.net/profile?id=~Muqiao_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Muqiao_Yang1">Muqiao Yang</a>, <a href="https://openreview.net/profile?id=~Han_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Han_Zhao1">Han Zhao</a>, <a href="https://openreview.net/profile?id=~Louis-Philippe_Morency1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Louis-Philippe_Morency1">Louis-Philippe Morency</a>, <a href="https://openreview.net/profile?id=~Ruslan_Salakhutdinov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruslan_Salakhutdinov1">Ruslan Salakhutdinov</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 28 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#068E_JSq9O-details-421" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="068E_JSq9O-details-421"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">self-supervised learning, contrastive learning, dependency based method</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper introduces Relative Predictive Coding (RPC), a new contrastive representation learning objective that maintains a good balance among training stability, minibatch size sensitivity, and downstream task performance. The key to the success of RPC is two-fold. First, RPC introduces the relative parameters to regularize the objective for boundedness and low variance. Second, RPC contains no logarithm and exponential score functions, which are the main cause of training instability in prior contrastive objectives. We empirically verify the effectiveness of RPC on benchmark vision and speech self-supervised learning tasks. Lastly, we relate RPC with mutual information (MI) estimation, showing RPC can be used to estimate MI with low variance.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present RPC, the Relative Predictive Coding, that achieves a good balance among the three challenges when modeling a contrastive learning objective: training stability, sensitivity to minibatch size, and downstream task performance. </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=068E_JSq9O&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="FmMKSO4e8JK" data-number="2157">
      <h4>
        <a href="https://openreview.net/forum?id=FmMKSO4e8JK">
            Offline Model-Based Optimization via Normalized Maximum Likelihood Estimation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=FmMKSO4e8JK" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Justin_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Justin_Fu1">Justin Fu</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#FmMKSO4e8JK-details-442" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FmMKSO4e8JK-details-442"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">model-based optimization, normalized maximum likelihood</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this work we consider data-driven optimization problems where one must maximize a function given only queries at a fixed set of points. This problem setting emerges in many domains where function evaluation is a complex and expensive process, such as in the design of materials, vehicles, or neural network architectures. Because the available data typically only covers a small manifold of the possible space of inputs, a principal challenge is to be able to construct algorithms that can reason about uncertainty and out-of-distribution values, since a naive optimizer can easily exploit an estimated model to return adversarial inputs. We propose to tackle the MBO problem by leveraging the normalized maximum-likelihood (NML) estimator, which provides a principled approach to handling uncertainty and out-of-distribution inputs. While in the standard formulation NML is intractable, we propose a tractable approximation that allows us to scale our method to high-capacity neural network models. We demonstrate that our method can effectively optimize high-dimensional design problems in a variety of disciplines such as chemistry, biology, and materials engineering.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Offline, data-driven optimization using normalized maximum likelihood to produce robust function estimates.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="NQbnPjPYaG6" data-number="1833">
      <h4>
        <a href="https://openreview.net/forum?id=NQbnPjPYaG6">
            On the Impossibility of Global Convergence in Multi-Loss Optimization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=NQbnPjPYaG6" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alistair_Letcher1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alistair_Letcher1">Alistair Letcher</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#NQbnPjPYaG6-details-11" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NQbnPjPYaG6-details-11"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">impossibility, global, convergence, optimization, multi-loss, multi-player, multi-agent, gradient, descent</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any 'reasonable' algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum.  Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We prove that a set of desirable convergence properties cannot simultaneously hold for any multi-loss optimization algorithm.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=NQbnPjPYaG6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="6zaTwpNSsQ2" data-number="3654">
      <h4>
        <a href="https://openreview.net/forum?id=6zaTwpNSsQ2">
            A Block Minifloat Representation for Training Deep Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=6zaTwpNSsQ2" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sean_Fox2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sean_Fox2">Sean Fox</a>, <a href="https://openreview.net/profile?email=seyedramin.rasoulinezhad%40sydney.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="seyedramin.rasoulinezhad@sydney.edu.au">Seyedramin Rasoulinezhad</a>, <a href="https://openreview.net/profile?id=~Julian_Faraone1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Julian_Faraone1">Julian Faraone</a>, <a href="https://openreview.net/profile?email=david.boland%40sydney.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="david.boland@sydney.edu.au">david boland</a>, <a href="https://openreview.net/profile?id=~Philip_Leong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philip_Leong1">Philip Leong</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#6zaTwpNSsQ2-details-441" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6zaTwpNSsQ2-details-441"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Training Deep Neural Networks (DNN) with high efficiency can be difficult to achieve with native floating-point representations and commercially available hardware. Specialized arithmetic with custom acceleration offers perhaps the most promising alternative. Ongoing research is trending towards narrow floating-point representations, called minifloats, that pack more operations for a given silicon area and consume less power. In this paper, we introduce Block Minifloat (BM), a new spectrum of minifloat formats capable of training DNNs end-to-end with only 4-8 bit weight, activation and gradient tensors. While standard floating-point representations have two degrees of freedom, via the exponent and mantissa, BM exposes the exponent bias as an additional field for optimization. Crucially, this enables training with fewer exponent bits, yielding dense integer-like hardware for fused multiply-add (FMA) operations. For ResNet trained on ImageNet, 6-bit BM achieves almost no degradation in floating-point accuracy with FMA units that are <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="133" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c39"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>4.1</mn><mo>×</mo><mo stretchy="false">(</mo><mn>23.9</mn><mo>×</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> smaller and consume <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="134" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2.3</mn><mo>×</mo><mo stretchy="false">(</mo><mn>16.1</mn><mo>×</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> less energy than FP8 (FP32). Furthermore, our 8-bit BM format matches floating-point accuracy while delivering a higher computational density and faster expected training times.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A new number representation, comparable to recently proposed 8-bit formats, for efficiently training a subset of DNN models.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="8nl0k08uMi" data-number="954">
      <h4>
        <a href="https://openreview.net/forum?id=8nl0k08uMi">
            Selectivity considered harmful: evaluating the causal impact of class selectivity in DNNs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=8nl0k08uMi" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Matthew_L_Leavitt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthew_L_Leavitt1">Matthew L Leavitt</a>, <a href="https://openreview.net/profile?id=~Ari_S._Morcos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ari_S._Morcos1">Ari S. Morcos</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 05 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#8nl0k08uMi-details-329" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8nl0k08uMi-details-329"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">interpretability, explainability, empirical analysis, deep learning, selectivity</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The properties of individual neurons are often analyzed in order to understand the biological and artificial neural networks in which they're embedded. Class selectivity—typically defined as how different a neuron's responses are across different classes of stimuli or data samples—is commonly used for this purpose. However, it remains an open question whether it is necessary and/or sufficient for deep neural networks (DNNs) to learn class selectivity in individual units. We investigated the causal impact of class selectivity on network function by directly regularizing for or against class selectivity. Using this regularizer to reduce class selectivity across units in convolutional neural networks increased test accuracy by over 2% in ResNet18 and 1% in ResNet50 trained on Tiny ImageNet. For ResNet20 trained on CIFAR10 we could reduce class selectivity by a factor of 2.5 with no impact on test accuracy, and reduce it nearly to zero with only a small (~2%) drop in test accuracy. In contrast, regularizing to increase class selectivity significantly decreased test accuracy across all models and datasets. These results indicate that class selectivity in individual units is neither sufficient nor strictly necessary, and can even impair DNN performance. They also encourage caution when focusing on the properties of single units as representative of the mechanisms by which DNNs function.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Class selectivity in CNNs is neither sufficient nor strictly necessary for optimal test accuracy</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="WEHSlH5mOk" data-number="1372">
      <h4>
        <a href="https://openreview.net/forum?id=WEHSlH5mOk">
            Discrete Graph Structure Learning for Forecasting Multiple Time Series
        </a>
      
        
          <a href="https://openreview.net/pdf?id=WEHSlH5mOk" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Chao_Shang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chao_Shang1">Chao Shang</a>, <a href="https://openreview.net/profile?id=~Jie_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jie_Chen1">Jie Chen</a>, <a href="https://openreview.net/profile?id=~Jinbo_Bi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinbo_Bi1">Jinbo Bi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#WEHSlH5mOk-details-948" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WEHSlH5mOk-details-948"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Time series forecasting, graph neural network, graph structure learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Time series forecasting is an extensively studied subject in statistics, economics, and computer science. Exploration of the correlation and causation among the variables in a multivariate time series shows promise in enhancing the performance of a time series model. When using deep neural networks as forecasting models, we hypothesize that exploiting the pairwise information among multiple (multivariate) time series also improves their forecast. If an explicit graph structure is known, graph neural networks (GNNs) have been demonstrated as powerful tools to exploit the structure. In this work, we propose learning the structure simultaneously with the GNN if the graph is unknown. We cast the problem as learning a probabilistic graph model through optimizing the mean performance over the graph distribution. The distribution is parameterized by a neural network so that discrete graphs can be sampled differentiably through reparameterization. Empirical evaluations show that our method is simpler, more efficient, and better performing than a recently proposed bilevel learning approach for graph structure learning, as well as a broad array of forecasting models, either deep or non-deep learning based, and graph or non-graph based.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a graph neural network approach that learns a graph structure to enhance the forecasting of multiple multivariate time series.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="CR1XOQ0UTh-" data-number="1264">
      <h4>
        <a href="https://openreview.net/forum?id=CR1XOQ0UTh-">
            Contrastive Learning with Hard Negative Samples
        </a>
      
        
          <a href="https://openreview.net/pdf?id=CR1XOQ0UTh-" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Joshua_David_Robinson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_David_Robinson1">Joshua David Robinson</a>, <a href="https://openreview.net/profile?id=~Ching-Yao_Chuang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ching-Yao_Chuang1">Ching-Yao Chuang</a>, <a href="https://openreview.net/profile?id=~Suvrit_Sra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Suvrit_Sra1">Suvrit Sra</a>, <a href="https://openreview.net/profile?id=~Stefanie_Jegelka3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefanie_Jegelka3">Stefanie Jegelka</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#CR1XOQ0UTh--details-946" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CR1XOQ0UTh--details-946"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">contrastive learning, unsupervised representation learning, hard negative sampling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We consider the question: how can you sample good negative examples for contrastive learning? We argue that, as with metric learning, learning contrastive representations benefits from hard negative samples (i.e., points that are difficult to distinguish from an anchor point). The key challenge toward using hard negatives is that contrastive methods must remain unsupervised, making it infeasible to adopt existing negative sampling strategies that use label information. In response, we develop a new class of unsupervised methods for selecting hard negative samples where the user can control the amount of hardness. A limiting case of this sampling results in a representation that tightly clusters each class, and pushes different classes as far apart as possible. The proposed method improves downstream performance across multiple modalities, requires only few additional lines of code to implement, and introduces no computational overhead.
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce an unsupervised method for sampling hard negatives for contrastive learning: the resulting embeddings have desirable theoretical properties, and have improved downstream performance on multiple different data modalities. </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=CR1XOQ0UTh-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tqOvYpjPax2" data-number="1769">
      <h4>
        <a href="https://openreview.net/forum?id=tqOvYpjPax2">
            Intraclass clustering: an implicit learning ability that regularizes DNNs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tqOvYpjPax2" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Simon_Carbonnelle1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Carbonnelle1">Simon Carbonnelle</a>, <a href="https://openreview.net/profile?id=~Christophe_De_Vleeschouwer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christophe_De_Vleeschouwer1">Christophe De Vleeschouwer</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 24 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tqOvYpjPax2-details-745" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tqOvYpjPax2-details-745"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning, generalization, implicit regularization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Several works have shown that the regularization mechanisms underlying deep neural networks' generalization performances are still poorly understood. In this paper, we hypothesize that deep neural networks are regularized through their ability to extract meaningful clusters among the samples of a class. This constitutes an implicit form of regularization, as no explicit training mechanisms or supervision target such behaviour. To support our hypothesis, we design four different measures of intraclass clustering, based on the neuron- and layer-level representations of the training data. We then show that these measures constitute accurate predictors of generalization performance across variations of a large set of hyperparameters (learning rate, batch size, optimizer, weight decay, dropout rate, data augmentation, network depth and width).</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper provides empirical evidence that deep neural networks are implicitly regularized through their ability to extract meaningful clusters among the samples of a class.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=tqOvYpjPax2&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="t0TaKv0Gx6Z" data-number="3174">
      <h4>
        <a href="https://openreview.net/forum?id=t0TaKv0Gx6Z">
            Sliced Kernelized Stein Discrepancy
        </a>
      
        
          <a href="https://openreview.net/pdf?id=t0TaKv0Gx6Z" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Wenbo_Gong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenbo_Gong1">Wenbo Gong</a>, <a href="https://openreview.net/profile?id=~Yingzhen_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yingzhen_Li1">Yingzhen Li</a>, <a href="https://openreview.net/profile?id=~Jos%C3%A9_Miguel_Hern%C3%A1ndez-Lobato1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~José_Miguel_Hernández-Lobato1">José Miguel Hernández-Lobato</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#t0TaKv0Gx6Z-details-490" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="t0TaKv0Gx6Z-details-490"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">kernel methods, variational inference, particle inference</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Kernelized Stein discrepancy (KSD), though being extensively used in goodness-of-fit tests and model learning, suffers from the curse-of-dimensionality. We address this issue by proposing the sliced Stein discrepancy and its scalable and kernelized variants, which employs kernel-based test functions defined on the optimal one-dimensional projections. When applied to goodness-of-fit tests, extensive experiments show the proposed discrepancy significantly outperforms KSD and various baselines in high dimensions. For model learning, we show its advantages by training an independent component analysis when compared with existing Stein discrepancy baselines. We further propose a novel particle inference method called sliced Stein variational gradient descent (S-SVGD) which alleviates the mode-collapse issue of SVGD in training variational autoencoders.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We proposed a method to tackle the curse-of-dimensionality issue of kernelized Stein discrepancy with RBF kernel, along with a novel particle inference algorithm resolving the vanishing repulsive issue of Stein variational gradient descent.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=t0TaKv0Gx6Z&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="St1giarCHLP" data-number="1080">
      <h4>
        <a href="https://openreview.net/forum?id=St1giarCHLP">
            Denoising Diffusion Implicit Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=St1giarCHLP" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jiaming_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiaming_Song1">Jiaming Song</a>, <a href="https://openreview.net/profile?email=chenlin%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="chenlin@stanford.edu">Chenlin Meng</a>, <a href="https://openreview.net/profile?id=~Stefano_Ermon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefano_Ermon1">Stefano Ermon</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#St1giarCHLP-details-253" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="St1giarCHLP-details-253"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">generative models, variational autoencoders, denoising score matching, variational inference</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="135" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>10</mn><mo>×</mo></math></mjx-assistive-mml></mjx-container> to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="136" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>50</mn><mo>×</mo></math></mjx-assistive-mml></mjx-container> faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show and justify a GAN-like iterative generative model with relatively fast sampling, high sample quality and without any adversarial training.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="r-gPPHEjpmw" data-number="548">
      <h4>
        <a href="https://openreview.net/forum?id=r-gPPHEjpmw">
            Hierarchical Reinforcement Learning by Discovering Intrinsic Options
        </a>
      
        
          <a href="https://openreview.net/pdf?id=r-gPPHEjpmw" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jesse_Zhang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jesse_Zhang3">Jesse Zhang</a>, <a href="https://openreview.net/profile?id=~Haonan_Yu5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haonan_Yu5">Haonan Yu</a>, <a href="https://openreview.net/profile?id=~Wei_Xu13" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Xu13">Wei Xu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 11 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#r-gPPHEjpmw-details-587" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="r-gPPHEjpmw-details-587"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">hierarchical reinforcement learning, reinforcement learning, options, unsupervised skill discovery, exploration</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a hierarchical reinforcement learning method, HIDIO, that can learn task-agnostic options in a self-supervised manner while jointly learning to utilize them to solve sparse-reward tasks. Unlike current hierarchical RL approaches that tend to formulate goal-reaching low-level tasks or pre-define ad hoc lower-level policies, HIDIO encourages lower-level option learning that is independent of the task at hand, requiring few assumptions or little knowledge about the task structure. These options are learned through an intrinsic entropy minimization objective conditioned on the option sub-trajectories. The learned options are diverse and task-agnostic. In experiments on sparse-reward robotic manipulation and navigation tasks, HIDIO achieves higher success rates with greater sample efficiency than regular RL baselines and two state-of-the-art hierarchical RL methods. Code at: https://github.com/jesbu1/hidio.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Hierarchical RL that discovers short-horizon task-agnostic options to perform well on sparse reward manipulation and navigation tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="EMHoBG0avc1" data-number="1441">
      <h4>
        <a href="https://openreview.net/forum?id=EMHoBG0avc1">
            Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval
        </a>
      
        
          <a href="https://openreview.net/pdf?id=EMHoBG0avc1" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Wenhan_Xiong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenhan_Xiong1">Wenhan Xiong</a>, <a href="https://openreview.net/profile?id=~Xiang_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiang_Li2">Xiang Li</a>, <a href="https://openreview.net/profile?id=~Srini_Iyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Srini_Iyer1">Srini Iyer</a>, <a href="https://openreview.net/profile?id=~Jingfei_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jingfei_Du1">Jingfei Du</a>, <a href="https://openreview.net/profile?id=~Patrick_Lewis2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Patrick_Lewis2">Patrick Lewis</a>, <a href="https://openreview.net/profile?id=~William_Yang_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_Yang_Wang2">William Yang Wang</a>, <a href="https://openreview.net/profile?email=mehdad%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mehdad@fb.com">Yashar Mehdad</a>, <a href="https://openreview.net/profile?id=~Scott_Yih1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Scott_Yih1">Scott Yih</a>, <a href="https://openreview.net/profile?id=~Sebastian_Riedel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sebastian_Riedel1">Sebastian Riedel</a>, <a href="https://openreview.net/profile?id=~Douwe_Kiela1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Douwe_Kiela1">Douwe Kiela</a>, <a href="https://openreview.net/profile?id=~Barlas_Oguz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Barlas_Oguz1">Barlas Oguz</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#EMHoBG0avc1-details-399" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EMHoBG0avc1-details-399"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">multi-hop question answering, recursive dense retrieval, open domain complex question answering</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a simple and efficient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previous work, our method does not require access to any corpus-specific information, such as inter-document hyperlinks or human-annotated entity markers, and can be applied to any unstructured text corpus. Our system also yields a much better efficiency-accuracy trade-off, matching the best published accuracy on HotpotQA while being 10 times faster at inference time.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a simple yet effective multi-hop dense retrieval approach for answering complex open-domain questions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="gIHd-5X324" data-number="138">
      <h4>
        <a href="https://openreview.net/forum?id=gIHd-5X324">
            Rethinking Soft Labels for Knowledge Distillation: A Bias–Variance Tradeoff Perspective
        </a>
      
        
          <a href="https://openreview.net/pdf?id=gIHd-5X324" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Helong_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Helong_Zhou1">Helong Zhou</a>, <a href="https://openreview.net/profile?id=~Liangchen_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liangchen_Song1">Liangchen Song</a>, <a href="https://openreview.net/profile?id=~Jiajie_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiajie_Chen1">Jiajie Chen</a>, <a href="https://openreview.net/profile?id=~Ye_Zhou2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ye_Zhou2">Ye Zhou</a>, <a href="https://openreview.net/profile?id=~Guoli_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guoli_Wang2">Guoli Wang</a>, <a href="https://openreview.net/profile?id=~Junsong_Yuan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junsong_Yuan2">Junsong Yuan</a>, <a href="https://openreview.net/profile?id=~Qian_Zhang7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qian_Zhang7">Qian Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 13 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#gIHd-5X324-details-503" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gIHd-5X324-details-503"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Knowledge distillation, soft labels, teacher-student model</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Knowledge distillation is an effective approach to leverage a well-trained network or an ensemble of them, named as the teacher, to guide the training of a student network.  The outputs from the teacher network are used as soft labels for supervising the training of a new network.  Recent studies (M ̈uller et al., 2019; Yuan et al., 2020) revealed an intriguing property of the soft labels that making labels soft serves as a good regularization to the student network.   From the perspective of statistical learning,  regularization aims to reduce the variance,  however how bias and variance change is not clear for training with soft labels.   In this paper, we investigate the bias-variance tradeoff brought by distillation with soft labels.   Specifically,  we observe that during training the bias-variance tradeoff varies sample-wisely. Further, under the same distillation temperature setting, we observe that the distillation performance is negatively associated with the number of some specific samples, which are named as regularization samples since these samples lead to bias increasing and variance decreasing.  Nevertheless, we empirically find that completely filtering out regularization samples also deteriorates distillation performance.  Our discoveries inspired us to propose the novel weighted soft labels to help the network adaptively handle the sample-wise bias-variance tradeoff.  Experiments on standard evaluation benchmarks validate the effectiveness of our method. Our code is available in the supplementary.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">For knowledge distillation, we analyze the regularization effect introduced by soft labels from a bias-variance perspective and propose weighted soft labels to handle the tradeoff.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=gIHd-5X324&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="GMgHyUPrXa" data-number="1384">
      <h4>
        <a href="https://openreview.net/forum?id=GMgHyUPrXa">
            A Design Space Study for LISTA and Beyond
        </a>
      
        
          <a href="https://openreview.net/pdf?id=GMgHyUPrXa" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tianjian_Meng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianjian_Meng2">Tianjian Meng</a>, <a href="https://openreview.net/profile?id=~Xiaohan_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaohan_Chen1">Xiaohan Chen</a>, <a href="https://openreview.net/profile?id=~Yifan_Jiang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yifan_Jiang2">Yifan Jiang</a>, <a href="https://openreview.net/profile?id=~Zhangyang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhangyang_Wang1">Zhangyang Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#GMgHyUPrXa-details-323" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GMgHyUPrXa-details-323"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In recent years, great success has been witnessed in building problem-specific deep networks from unrolling iterative algorithms, for solving inverse problems and beyond. Unrolling is believed to incorporate the model-based prior with the learning capacity of deep learning. This paper revisits \textit{the role of unrolling as a design approach for deep networks}: to what extent its resulting special architecture is superior, and can we find better? Using LISTA for sparse recovery as a representative example, we conduct the first thorough \textit{design space study} for the unrolled models.  Among all possible variations, we focus on extensively varying the connectivity patterns and neuron types, leading to a gigantic design space arising from LISTA. To efficiently explore this space and identify top performers, we leverage the emerging tool of neural architecture search (NAS). We carefully examine the searched top architectures in a number of settings, and are able to discover networks that consistently better than LISTA. We further present more visualization and analysis to ``open the black box", and find that the searched top architectures demonstrate highly consistent and potentially transferable patterns. We hope our study to spark more reflections and explorations on how to better mingle model-based optimization prior and data-driven learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="CZ8Y3NzuVzO" data-number="1057">
      <h4>
        <a href="https://openreview.net/forum?id=CZ8Y3NzuVzO">
            What Should Not Be Contrastive in Contrastive Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=CZ8Y3NzuVzO" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tete_Xiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tete_Xiao1">Tete Xiao</a>, <a href="https://openreview.net/profile?id=~Xiaolong_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaolong_Wang3">Xiaolong Wang</a>, <a href="https://openreview.net/profile?id=~Alexei_A_Efros1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexei_A_Efros1">Alexei A Efros</a>, <a href="https://openreview.net/profile?id=~Trevor_Darrell2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Trevor_Darrell2">Trevor Darrell</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#CZ8Y3NzuVzO-details-263" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CZ8Y3NzuVzO-details-263"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Self-supervised learning, Contrastive learning, Representation learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent self-supervised contrastive methods have been able to produce impressive transferable visual representations by learning to be invariant to different data augmentations. However, these methods implicitly assume a particular set of representational invariances (e.g., invariance to color), and can perform poorly when a downstream task violates this assumption (e.g., distinguishing red vs. yellow cars). We introduce a contrastive learning framework which does not require prior knowledge of specific, task-dependent invariances. Our model learns to capture varying and invariant factors for visual representations by constructing separate embedding spaces, each of which is invariant to all but one augmentation. We use a multi-head network with a shared backbone which captures information across each augmentation and alone outperforms all baselines on downstream tasks. We further find that the concatenation of the invariant and varying spaces performs best across all tasks we investigate, including coarse-grained, fine-grained, and few-shot downstream classification tasks, and various data corruptions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="KJNcAkY8tY4" data-number="2417">
      <h4>
        <a href="https://openreview.net/forum?id=KJNcAkY8tY4">
            Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth
        </a>
      
        
          <a href="https://openreview.net/pdf?id=KJNcAkY8tY4" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Thao_Nguyen3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thao_Nguyen3">Thao Nguyen</a>, <a href="https://openreview.net/profile?id=~Maithra_Raghu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maithra_Raghu1">Maithra Raghu</a>, <a href="https://openreview.net/profile?id=~Simon_Kornblith1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Kornblith1">Simon Kornblith</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#KJNcAkY8tY4-details-706" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KJNcAkY8tY4-details-706"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Representation learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A key factor in the success of deep neural networks is the ability to scale models to improve performance by varying the architecture depth and width. This simple property of neural network design has resulted in highly effective architectures for a variety of tasks. Nevertheless, there is limited understanding of effects of depth and width on the learned representations. In this paper, we study this fundamental question. We begin by investigating how varying depth and width affects model hidden representations, finding a characteristic block structure in the hidden representations of larger capacity (wider or deeper) models. We demonstrate that this block structure arises when model capacity is large relative to the size of the training set, and is indicative of the underlying layers preserving and propagating the dominant principal component of their representations. This discovery has important ramifications for features learned by different models, namely, representations outside the block structure are often similar across architectures with varying widths and depths, but the block structure is unique to each model. We analyze the output predictions of different model architectures, finding that even when the overall accuracy is similar, wide and deep models exhibit distinctive error patterns and variations across classes.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that depth/width variations result in distinctive characteristics in the model internal representations, with resulting consequences for representations and output predictions across different model initializations and architectures. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="cR91FAodFMe" data-number="3405">
      <h4>
        <a href="https://openreview.net/forum?id=cR91FAodFMe">
            Learning to Set Waypoints for Audio-Visual Navigation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=cR91FAodFMe" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Changan_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changan_Chen2">Changan Chen</a>, <a href="https://openreview.net/profile?id=~Sagnik_Majumder1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sagnik_Majumder1">Sagnik Majumder</a>, <a href="https://openreview.net/profile?id=~Ziad_Al-Halah2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziad_Al-Halah2">Ziad Al-Halah</a>, <a href="https://openreview.net/profile?id=~Ruohan_Gao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruohan_Gao2">Ruohan Gao</a>, <a href="https://openreview.net/profile?id=~Santhosh_Kumar_Ramakrishnan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Santhosh_Kumar_Ramakrishnan1">Santhosh Kumar Ramakrishnan</a>, <a href="https://openreview.net/profile?id=~Kristen_Grauman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kristen_Grauman1">Kristen Grauman</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#cR91FAodFMe-details-278" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="cR91FAodFMe-details-278"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">visual navigation, audio visual learning, embodied vision</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g., a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements: 1) waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on two challenging datasets of real-world 3D scenes, Replica and Matterport3D. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a hierarchical reinforcement learning approach to audio-visual navigation that learns to dynamically set waypoints in an end-to-end fashion </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=cR91FAodFMe&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="yFJ67zTeI2" data-number="2660">
      <h4>
        <a href="https://openreview.net/forum?id=yFJ67zTeI2">
            Semi-supervised Keypoint Localization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=yFJ67zTeI2" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Olga_Moskvyak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Olga_Moskvyak1">Olga Moskvyak</a>, <a href="https://openreview.net/profile?id=~Frederic_Maire1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frederic_Maire1">Frederic Maire</a>, <a href="https://openreview.net/profile?id=~Feras_Dayoub1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Feras_Dayoub1">Feras Dayoub</a>, <a href="https://openreview.net/profile?id=~Mahsa_Baktashmotlagh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mahsa_Baktashmotlagh1">Mahsa Baktashmotlagh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 05 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#yFJ67zTeI2-details-361" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="yFJ67zTeI2-details-361"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">semi-supervised learning, keypoint localization, limited data, unsupervised loss</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Knowledge about the locations of keypoints of an object in an image can assist in fine-grained classification and identification tasks, particularly for the case of objects that exhibit large variations in poses that greatly influence their visual appearance, such as wild animals. However, supervised training of a keypoint detection network requires annotating a large image dataset for each animal species, which is a labor-intensive task. To reduce the need for labeled data, we propose to learn simultaneously keypoint heatmaps and pose invariant keypoint representations in a semi-supervised manner using a small set of labeled images along with a larger set of unlabeled images. Keypoint representations are learnt with a semantic keypoint consistency constraint that forces the keypoint detection network to learn similar features for the same keypoint across the dataset. Pose invariance is achieved by making keypoint representations for the image and its augmented copies closer together in feature space. Our semi-supervised approach significantly outperforms previous methods on several benchmarks for human and animal body landmark localization.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A novel method for semi-supervised keypoint localization via learning semantic keypoint representations.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=yFJ67zTeI2&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Cnon5ezMHtu" data-number="2426">
      <h4>
        <a href="https://openreview.net/forum?id=Cnon5ezMHtu">
            Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Cnon5ezMHtu" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Wuyang_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wuyang_Chen1">Wuyang Chen</a>, <a href="https://openreview.net/profile?id=~Xinyu_Gong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinyu_Gong1">Xinyu Gong</a>, <a href="https://openreview.net/profile?id=~Zhangyang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhangyang_Wang1">Zhangyang Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Cnon5ezMHtu-details-846" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Cnon5ezMHtu-details-846"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural Architecture Search, neural tangent kernel, number of linear regions</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Neural Architecture Search (NAS) has been explosively studied to automate the discovery of top-performer neural networks. Current works require heavy training of supernet or intensive architecture evaluations, thus suffering from heavy resource consumption and often incurring search bias due to truncated training or approximations. Can we select the best neural architectures without involving any training and eliminate a drastic portion of the search cost? 
      
      We provide an affirmative answer, by proposing a novel framework called \textit{training-free neural architecture search} (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="137" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-b"><mjx-c class="mjx-c1D413 TEX-B"></mjx-c><mjx-c class="mjx-c1D404 TEX-B"></mjx-c><mjx-c class="mjx-c2D TEX-B"></mjx-c><mjx-c class="mjx-c1D40D TEX-B"></mjx-c><mjx-c class="mjx-c1D400 TEX-B"></mjx-c><mjx-c class="mjx-c1D412 TEX-B"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="bold">TE-NAS</mtext></math></mjx-assistive-mml></mjx-container>). TE-NAS ranks architectures by analyzing the spectrum of the neural tangent kernel (NTK), and the number of linear regions in the input space. Both are motivated by recent theory advances in deep networks, and can be computed without any training. We show that: (1) these two measurements imply the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="138" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D44F TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">trainability</mtext></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="139" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D465 TEX-I"></mjx-c><mjx-c class="mjx-c1D45D TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D463 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">expressivity</mtext></math></mjx-assistive-mml></mjx-container> of a neural network; and (2) they strongly correlate with the network's actual test accuracy. Further on, we design a pruning-based NAS mechanism to achieve a more flexible and superior trade-off between the trainability and expressivity during the search. In NAS-Bench-201 and DARTS search spaces, TE-NAS completes high-quality search but only costs <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="140" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-b"><mjx-c class="mjx-c1D7CE TEX-B"></mjx-c><mjx-c class="mjx-c2E TEX-B"></mjx-c><mjx-c class="mjx-c1D7D3 TEX-B"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="bold">0.5</mtext></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="141" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-b"><mjx-c class="mjx-c1D7D2 TEX-B"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="bold">4</mtext></math></mjx-assistive-mml></mjx-container> GPU hours with one 1080Ti on CIFAR-10 and ImageNet, respectively. We hope our work to inspire more attempts in bridging between the theoretic findings of deep networks and practical impacts in real NAS applications.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Our TE-NAS framework analyzes the spectrum of the neural tangent kernel (NTK) and the number of linear regions in the input space, achieving high-quality architecture search while dramatically reducing the search cost to four hours on ImageNet.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="eqBwg3AcIAK" data-number="1333">
      <h4>
        <a href="https://openreview.net/forum?id=eqBwg3AcIAK">
            Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers
        </a>
      
        
          <a href="https://openreview.net/pdf?id=eqBwg3AcIAK" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Benjamin_Eysenbach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Eysenbach1">Benjamin Eysenbach</a>, <a href="https://openreview.net/profile?id=~Shreyas_Chaudhari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shreyas_Chaudhari1">Shreyas Chaudhari</a>, <a href="https://openreview.net/profile?id=~Swapnil_Asawa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Swapnil_Asawa1">Swapnil Asawa</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>, <a href="https://openreview.net/profile?id=~Ruslan_Salakhutdinov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruslan_Salakhutdinov1">Ruslan Salakhutdinov</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#eqBwg3AcIAK-details-417" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eqBwg3AcIAK-details-417"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, transfer learning, domain adaptation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a simple, practical, and intuitive approach for domain adaptation in reinforcement learning. Our approach stems from the idea that the agent's experience in the source domain should look similar to its experience in the target domain. Building off of a probabilistic view of RL, we achieve this goal by compensating for the difference in dynamics by modifying the reward function. This modified reward function is simple to estimate by learning auxiliary classifiers that distinguish source-domain transitions from target-domain transitions. Intuitively, the agent is penalized for transitions that would indicate that the agent is interacting with the source domain, rather than the target domain. Formally, we prove that applying our method in the source domain is guaranteed to obtain a near-optimal policy for the target domain, provided that the source and target domains satisfy a lightweight assumption. Our approach is applicable to domains with continuous states and actions and does not require learning an explicit model of the dynamics. On discrete and continuous control tasks, we illustrate the mechanics of our approach and demonstrate its scalability to high-dimensional~tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a method for addressing domain adaptation in RL by using a (learned) modified reward, and prove that our method recovers a near-optimal policy for the target domain.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ce6CFXBh30h" data-number="492">
      <h4>
        <a href="https://openreview.net/forum?id=ce6CFXBh30h">
            Federated Semi-Supervised Learning with Inter-Client Consistency &amp; Disjoint Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ce6CFXBh30h" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Wonyong_Jeong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wonyong_Jeong1">Wonyong Jeong</a>, <a href="https://openreview.net/profile?id=~Jaehong_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaehong_Yoon1">Jaehong Yoon</a>, <a href="https://openreview.net/profile?id=~Eunho_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eunho_Yang1">Eunho Yang</a>, <a href="https://openreview.net/profile?id=~Sung_Ju_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sung_Ju_Hwang1">Sung Ju Hwang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ce6CFXBh30h-details-543" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ce6CFXBh30h-details-543"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Federated Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While existing federated learning approaches mostly require that clients have fully-labeled data to train on, in realistic settings, data obtained at the client-side often comes without any accompanying labels. Such deficiency of labels may result from either high labeling cost, or difficulty of annotation due to the requirement of expert knowledge. Thus the private data at each client may be either partly labeled, or completely unlabeled with labeled data being available only at the server, which leads us to a new practical federated learning problem, namely Federated Semi-Supervised Learning (FSSL). In this work, we study two essential scenarios of FSSL based on the location of the labeled data. The first scenario considers a conventional case where clients have both labeled and unlabeled data (labels-at-client), and the second scenario considers a more challenging case, where the labeled data is only available at the server (labels-at-server). We then propose a novel method to tackle the problems, which we refer to as Federated Matching (FedMatch). FedMatch improves upon naive combinations of federated learning and semi-supervised learning approaches with a new inter-client consistency loss and decomposition of the parameters for disjoint learning on labeled and unlabeled data. Through extensive experimental validation of our method in the two different scenarios, we show that our method outperforms both local semi-supervised learning and baselines which naively combine federated learning with semi-supervised learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a new practical problem of federated learning with a deficiency of supervision and study two realistic scenarios with a novel method to tackle the problems, including inter-client consistency and disjoint learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=ce6CFXBh30h&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="P6_q1BRxY8Q" data-number="1756">
      <h4>
        <a href="https://openreview.net/forum?id=P6_q1BRxY8Q">
            Learning Safe Multi-agent Control with Decentralized Neural Barrier Certificates
        </a>
      
        
          <a href="https://openreview.net/pdf?id=P6_q1BRxY8Q" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zengyi_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zengyi_Qin1">Zengyi Qin</a>, <a href="https://openreview.net/profile?id=~Kaiqing_Zhang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaiqing_Zhang3">Kaiqing Zhang</a>, <a href="https://openreview.net/profile?id=~Yuxiao_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuxiao_Chen1">Yuxiao Chen</a>, <a href="https://openreview.net/profile?id=~Jingkai_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jingkai_Chen2">Jingkai Chen</a>, <a href="https://openreview.net/profile?id=~Chuchu_Fan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuchu_Fan2">Chuchu Fan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>29 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#P6_q1BRxY8Q-details-112" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="P6_q1BRxY8Q-details-112"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multi-agent, safe, control barrier function, reinforcement learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the multi-agent safe control problem where agents should avoid collisions to static obstacles and collisions with each other while reaching their goals. Our core idea is to learn the multi-agent control policy jointly with  learning the control barrier functions as safety certificates. We propose a new joint-learning framework that can be implemented in a decentralized fashion, which can adapt to an arbitrarily large number of agents. Building upon this framework, we further improve the scalability by  incorporating neural network architectures  that are invariant to the quantity and permutation of neighboring agents. In addition, we propose a new spontaneous policy refinement method to further enforce the certificate condition during testing. We provide extensive experiments to demonstrate that our method significantly outperforms other leading multi-agent control approaches in terms of maintaining safety and completing original tasks. Our approach also shows substantial generalization capability in that the control policy can be trained with 8 agents in one scenario, while being used on other scenarios with up to 1024 agents in complex multi-agent environments and dynamics. Videos and source code can be found at https://realm.mit.edu/blog/learning-safe-multi-agent-control-decentralized-neural-barrier-certificates.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a safe and remarkably scalable multi-agent control approach via jointly learning the policy and decentralized control barrier certificates.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=P6_q1BRxY8Q&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="3X64RLgzY6O" data-number="1393">
      <h4>
        <a href="https://openreview.net/forum?id=3X64RLgzY6O">
            Direction Matters: On the Implicit Bias of Stochastic Gradient Descent with Moderate Learning Rate
        </a>
      
        
          <a href="https://openreview.net/pdf?id=3X64RLgzY6O" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jingfeng_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jingfeng_Wu1">Jingfeng Wu</a>, <a href="https://openreview.net/profile?id=~Difan_Zou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Difan_Zou1">Difan Zou</a>, <a href="https://openreview.net/profile?id=~Vladimir_Braverman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vladimir_Braverman1">Vladimir Braverman</a>, <a href="https://openreview.net/profile?id=~Quanquan_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Quanquan_Gu1">Quanquan Gu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#3X64RLgzY6O-details-917" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3X64RLgzY6O-details-917"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">SGD, regularization, implicit bias</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Understanding the algorithmic bias of stochastic gradient descent (SGD) is one of the key challenges in modern machine learning and deep learning theory. Most of the existing works, however, focus on very small or even infinitesimal learning rate regime, and fail to cover practical scenarios where the learning rate is moderate and annealing. In this paper, we make an initial attempt to characterize the particular regularization effect of SGD in the moderate learning rate regime by studying its behavior for optimizing an overparameterized linear regression problem. In this case, SGD and GD are known to converge to the unique minimum-norm solution; however, with the moderate and annealing learning rate, we show that they exhibit different directional bias: SGD converges along the large eigenvalue directions of the data matrix, while GD goes after the small eigenvalue directions. Furthermore, we show that such directional bias does matter when early stopping is adopted, where the SGD output is nearly optimal but the GD output is suboptimal. Finally, our theory explains several folk arts in practice used for SGD hyperparameter tuning, such as (1) linearly scaling the initial learning rate with batch size; and (2) overrunning SGD with high learning rate even when the loss stops decreasing.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show a directional regularization effect for SGD with moderate learning rate</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=3X64RLgzY6O&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Lc28QAB4ypz" data-number="2744">
      <h4>
        <a href="https://openreview.net/forum?id=Lc28QAB4ypz">
            Fast And Slow Learning Of Recurrent Independent Mechanisms
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Lc28QAB4ypz" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kanika_Madan3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kanika_Madan3">Kanika Madan</a>, <a href="https://openreview.net/profile?id=~Nan_Rosemary_Ke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nan_Rosemary_Ke1">Nan Rosemary Ke</a>, <a href="https://openreview.net/profile?id=~Anirudh_Goyal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anirudh_Goyal1">Anirudh Goyal</a>, <a href="https://openreview.net/profile?id=~Bernhard_Sch%C3%B6lkopf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernhard_Schölkopf1">Bernhard Schölkopf</a>, <a href="https://openreview.net/profile?id=~Yoshua_Bengio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoshua_Bengio1">Yoshua Bengio</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Lc28QAB4ypz-details-481" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Lc28QAB4ypz-details-481"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">modular representations, better generalization, learning mechanisms</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Decomposing knowledge into interchangeable pieces promises a generalization advantage when there are changes in distribution. A learning agent interacting with its environment is likely to be faced with situations requiring novel combinations of existing pieces of knowledge. We hypothesize that such a decomposition of knowledge is particularly relevant for being able to generalize in a systematic way to out-of-distribution changes. To study these ideas, we propose a particular training framework in which we assume that the pieces of knowledge an agent needs and its reward function are stationary and can be re-used across tasks. An attention mechanism dynamically selects which modules can be adapted to the current task, and the parameters of the \textit{selected} modules are allowed to change quickly as the learner is confronted with variations in what it experiences, while the parameters of the attention mechanisms act as stable, slowly changing, meta-parameters. We focus on pieces of knowledge captured by an ensemble of  modules sparsely communicating with each other via a bottleneck of attention. We find that meta-learning the  modular aspects of the proposed system greatly helps in achieving faster adaptation in a reinforcement learning setup involving navigation in a partially observed grid world with image-level input.  We also find that reversing the role of parameters and meta-parameters does not work nearly as well, suggesting a particular role for fast adaptation of the dynamically selected modules.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Different time scale learning of independent mechanisms can lead to a better generalization.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Lc28QAB4ypz&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="pzpytjk3Xb2" data-number="102">
      <h4>
        <a href="https://openreview.net/forum?id=pzpytjk3Xb2">
            Policy-Driven Attack: Learning to Query for Hard-label Black-box Adversarial Examples
        </a>
      
        
          <a href="https://openreview.net/pdf?id=pzpytjk3Xb2" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ziang_Yan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziang_Yan1">Ziang Yan</a>, <a href="https://openreview.net/profile?id=~Yiwen_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yiwen_Guo1">Yiwen Guo</a>, <a href="https://openreview.net/profile?id=~Jian_Liang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jian_Liang3">Jian Liang</a>, <a href="https://openreview.net/profile?id=~Changshui_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changshui_Zhang1">Changshui Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 20 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#pzpytjk3Xb2-details-184" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pzpytjk3Xb2-details-184"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">hard-label attack, black-box attack, adversarial attack, reinforcement learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">To craft black-box adversarial examples, adversaries need to query the victim model and take proper advantage of its feedback. Existing black-box attacks generally suffer from high query complexity, especially when only the top-1 decision (i.e., the hard-label prediction) of the victim model is available.  In this paper, we propose a novel hard-label black-box attack named Policy-Driven Attack, to reduce the query complexity. Our core idea is to learn promising search directions of the adversarial examples using a well-designed policy network in a novel reinforcement learning formulation, in which the queries become more sensible. Experimental results demonstrate that our method can significantly reduce the query complexity in comparison with existing state-of-the-art hard-label black-box attacks on various image classification benchmark datasets. Code and models for reproducing our results are available at https://github.com/ZiangYan/pda.pytorch</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A novel hard-label black-box adversarial attack that introduces a reinforcement learning based formulation with a pre-trained policy network</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=pzpytjk3Xb2&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="vVjIW3sEc1s" data-number="2122">
      <h4>
        <a href="https://openreview.net/forum?id=vVjIW3sEc1s">
            A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=vVjIW3sEc1s" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Nikunj_Saunshi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nikunj_Saunshi1">Nikunj Saunshi</a>, <a href="https://openreview.net/profile?id=~Sadhika_Malladi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sadhika_Malladi2">Sadhika Malladi</a>, <a href="https://openreview.net/profile?id=~Sanjeev_Arora1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanjeev_Arora1">Sanjeev Arora</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#vVjIW3sEc1s-details-16" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vVjIW3sEc1s-details-16"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">language models, theory, representation learning, self-supervised learning, unsupervised learning, transfer learning, natural language processing</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Autoregressive language models pretrained on large text corpora to do well on next word prediction have been successful at solving many downstream tasks, even with zero-shot usage. However, there is little theoretical understanding of this success. This paper initiates a mathematical study of this phenomenon for the downstream task of text classification by considering the following questions: (1) What is the intuitive connection between the pretraining task of next word prediction and text classification? (2) How can we mathematically formalize this connection and quantify the benefit of language modeling? For (1), we hypothesize, and verify empirically, that classification tasks of interest can be reformulated as sentence completion tasks, thus making language modeling a meaningful pretraining task. With a mathematical formalization of this hypothesis, we make progress towards (2) and show that language models that are <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="142" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi></math></mjx-assistive-mml></mjx-container>-optimal in cross-entropy (log-perplexity) learn features that can linearly solve such classification tasks with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="143" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.286em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><msqrt><mi>ϵ</mi></msqrt><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> error, thus demonstrating that doing well on language modeling can be beneficial for downstream tasks. We experimentally verify various assumptions and theoretical findings, and also use insights from the analysis to design a new objective function that performs well on some classification tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We develop a mathematical framework for understanding why language model features help with downstream linear classification tasks of interest</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Naqw7EHIfrv" data-number="544">
      <h4>
        <a href="https://openreview.net/forum?id=Naqw7EHIfrv">
            Representation Learning for Sequence Data with Deep Autoencoding Predictive Components
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Naqw7EHIfrv" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Junwen_Bai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junwen_Bai1">Junwen Bai</a>, <a href="https://openreview.net/profile?id=~Weiran_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weiran_Wang1">Weiran Wang</a>, <a href="https://openreview.net/profile?id=~Yingbo_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yingbo_Zhou1">Yingbo Zhou</a>, <a href="https://openreview.net/profile?id=~Caiming_Xiong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Caiming_Xiong1">Caiming Xiong</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Naqw7EHIfrv-details-851" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Naqw7EHIfrv-details-851"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Mutual Information, Unsupervised Learning, Sequence Data, Masked Reconstruction</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose Deep Autoencoding Predictive Components (DAPC) -- a self-supervised representation learning method for sequence data, based on the intuition that useful representations of sequence data should exhibit a simple structure in the latent space. We encourage this latent structure by maximizing an estimate of \emph{predictive information} of latent feature sequences, which is the mutual information between the past and future windows at each time step. In contrast to the mutual information lower bound commonly used by contrastive learning, the estimate of predictive information we adopt is exact under a Gaussian assumption. Additionally, it can be computed without negative sampling. To reduce the degeneracy of the latent space extracted by powerful encoders and keep useful information from the inputs, we regularize predictive information learning with a challenging masked reconstruction loss. We demonstrate that our method recovers the latent space of noisy dynamical systems, extracts predictive features for forecasting tasks, and improves automatic speech recognition when used to pretrain the encoder on large amounts of unlabeled data.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ZsZM-4iMQkH" data-number="3245">
      <h4>
        <a href="https://openreview.net/forum?id=ZsZM-4iMQkH">
            A unifying view on implicit bias in training linear neural networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ZsZM-4iMQkH" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Chulhee_Yun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chulhee_Yun1">Chulhee Yun</a>, <a href="https://openreview.net/profile?id=~Shankar_Krishnan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shankar_Krishnan1">Shankar Krishnan</a>, <a href="https://openreview.net/profile?id=~Hossein_Mobahi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hossein_Mobahi2">Hossein Mobahi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 24 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ZsZM-4iMQkH-details-523" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZsZM-4iMQkH-details-523"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">implicit bias, implicit regularization, convergence, gradient flow, gradient descent</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the implicit bias of gradient flow (i.e., gradient descent with infinitesimal step size) on linear neural network training. We propose a tensor formulation of neural networks that includes fully-connected, diagonal, and convolutional networks as special cases, and investigate the linear version of the formulation called linear tensor networks. With this formulation, we can characterize the convergence direction of the network parameters as singular vectors of a tensor defined by the network. For <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="144" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></mjx-assistive-mml></mjx-container>-layer linear tensor networks that are orthogonally decomposable, we show that gradient flow on separable classification finds a stationary point of the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="145" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.177em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mrow><mn>2</mn><mrow><mo>/</mo></mrow><mi>L</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> max-margin problem in a "transformed" input space defined by the network. For underdetermined regression, we prove that gradient flow finds a global minimum which minimizes a norm-like function that interpolates between weighted <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="146" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>1</mn></msub></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="147" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> norms in the transformed input space. Our theorems subsume existing results in the literature while removing standard convergence assumptions. We also provide experiments that corroborate our analysis.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a unifying framework for analyzing implicit bias of linear networks and show theorems that extend existing results with less convergence assumptions.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tC6iW2UUbJf" data-number="1443">
      <h4>
        <a href="https://openreview.net/forum?id=tC6iW2UUbJf">
            What Makes Instance Discrimination Good for Transfer Learning?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tC6iW2UUbJf" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Nanxuan_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nanxuan_Zhao1">Nanxuan Zhao</a>, <a href="https://openreview.net/profile?id=~Zhirong_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhirong_Wu1">Zhirong Wu</a>, <a href="https://openreview.net/profile?id=~Rynson_W._H._Lau1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rynson_W._H._Lau1">Rynson W. H. Lau</a>, <a href="https://openreview.net/profile?id=~Stephen_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephen_Lin1">Stephen Lin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 31 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tC6iW2UUbJf-details-973" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tC6iW2UUbJf-details-973"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Transfer Learning, Unsupervised Learning, Self-supervised Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Contrastive visual pretraining based on the instance discrimination pretext task has made significant progress. Notably, recent work on unsupervised pretraining has shown to surpass the supervised counterpart for finetuning downstream applications such as object detection and segmentation.   It comes as a surprise that image annotations would be better left unused for transfer learning.  In this work, we investigate the following problems: What makes instance discrimination pretraining good for transfer learning? What knowledge is actually learned and transferred from these models?  From this understanding of instance discrimination, how can we better exploit human annotation labels for pretraining? Our findings are threefold. First, what truly matters for the transfer is low-level and mid-level representations, not high-level representations.  Second, the intra-category invariance enforced by the traditional supervised model weakens transferability by increasing task misalignment. Finally, supervised pretraining can be strengthened by following an exemplar-based approach without explicit constraints among the instances within the same category.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Understanding why self-supervised contrastive learning outperforms supervised counterparts for image pretraining</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=tC6iW2UUbJf&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="cTbIjyrUVwJ" data-number="733">
      <h4>
        <a href="https://openreview.net/forum?id=cTbIjyrUVwJ">
            Learning Accurate Entropy Model with Global Reference for Image Compression
        </a>
      
        
          <a href="https://openreview.net/pdf?id=cTbIjyrUVwJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yichen_Qian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yichen_Qian1">Yichen Qian</a>, <a href="https://openreview.net/profile?email=zhiyu.tzy%40alibaba-inc.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhiyu.tzy@alibaba-inc.com">Zhiyu Tan</a>, <a href="https://openreview.net/profile?id=~Xiuyu_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiuyu_Sun1">Xiuyu Sun</a>, <a href="https://openreview.net/profile?id=~Ming_Lin4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ming_Lin4">Ming Lin</a>, <a href="https://openreview.net/profile?email=yingtian.ldy%40alibaba-inc.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yingtian.ldy@alibaba-inc.com">Dongyang Li</a>, <a href="https://openreview.net/profile?email=zhenhong.szh%40alibaba-inc.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhenhong.szh@alibaba-inc.com">Zhenhong Sun</a>, <a href="https://openreview.net/profile?id=~Li_Hao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Li_Hao1">Li Hao</a>, <a href="https://openreview.net/profile?id=~Rong_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rong_Jin1">Rong Jin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#cTbIjyrUVwJ-details-745" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="cTbIjyrUVwJ-details-745"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Image compression, Entropy Model, Global Reference</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In recent deep image compression neural networks, the entropy model plays a critical role in estimating the prior distribution of deep image encodings. Existing methods combine hyperprior with local context in the entropy estimation function. This greatly limits their performance due to the absence of a global vision. In this work, we propose a novel Global Reference Model for image compression to effectively leverage both the local and the global context information, leading to an enhanced compression rate. The proposed method scans decoded latents and then finds the most relevant latent to assist the distribution estimating of the current latent. A by-product of this work is the innovation of a mean-shifting GDN module that further improves the performance. Experimental results demonstrate that the proposed model outperforms the rate-distortion performance of most of the state-of-the-art methods in the industry.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">In this paper, we propose a novel Reference-based Model for image compression to effectively leverage both the local and global context information, which yields an enhanced compression performance.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="5jzlpHvvRk" data-number="1364">
      <h4>
        <a href="https://openreview.net/forum?id=5jzlpHvvRk">
            Loss Function Discovery for Object Detection via Convergence-Simulation Driven Search
        </a>
      
        
          <a href="https://openreview.net/pdf?id=5jzlpHvvRk" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Peidong_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peidong_Liu2">Peidong Liu</a>, <a href="https://openreview.net/profile?id=~Gengwei_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gengwei_Zhang1">Gengwei Zhang</a>, <a href="https://openreview.net/profile?id=~Bochao_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bochao_Wang2">Bochao Wang</a>, <a href="https://openreview.net/profile?id=~Hang_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hang_Xu1">Hang Xu</a>, <a href="https://openreview.net/profile?id=~Xiaodan_Liang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaodan_Liang2">Xiaodan Liang</a>, <a href="https://openreview.net/profile?id=~Yong_Jiang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yong_Jiang3">Yong Jiang</a>, <a href="https://openreview.net/profile?id=~Zhenguo_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhenguo_Li1">Zhenguo Li</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 13 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#5jzlpHvvRk-details-755" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5jzlpHvvRk-details-755"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Object detection, AutoML, Evolutionary algorithm, Loss function search</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Designing proper loss functions for vision tasks has been a long-standing research direction to advance the capability of existing models. For object detection, the well-established classification and regression loss functions have been carefully designed by considering diverse learning challenges (e.g. class imbalance, hard negative samples, and scale variances). Inspired by the recent progress in network architecture search, it is interesting to explore the possibility of discovering new loss function formulations via directly searching the primitive operation combinations. So that the learned losses not only fit for diverse object detection challenges to alleviate huge human efforts, but also have better alignment with evaluation metric and good mathematical convergence property. Beyond the previous auto-loss works on face recognition and image classification, our work makes the first attempt to discover new loss functions for the challenging object detection from primitive operation levels and finds the searched losses are insightful. We propose an effective convergence-simulation driven evolutionary search algorithm, called CSE-Autoloss, for speeding up the search progress by regularizing the mathematical rationality of loss candidates via two progressive convergence simulation modules: convergence property verification and model optimization simulation. CSE-Autoloss involves the search space (i.e. 21 mathematical operators, 3 constant-type inputs, and 3 variable-type inputs) that cover a wide range of the possible variants of existing losses and discovers best-searched loss function combination within a short time (around 1.5 wall-clock days with 20x speedup in comparison to the vanilla evolutionary algorithm). We conduct extensive evaluations of loss function search on popular detectors and validate the good generalization capability of searched losses across diverse architectures and various datasets. Our experiments show that the best-discovered loss function combinations outperform default combinations (Cross-entropy/Focal loss for classification and L1 loss for regression) by 1.1% and 0.8% in terms of mAP for two-stage and one-stage detectors on COCO respectively. Our searched losses are available at https://github.com/PerdonLiu/CSE-Autoloss.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an effective convergence-simulation driven evolutionary search algorithm, called CSE-Autoloss, for object detection loss function discovery, which achieves 20x speedup via progressive convergence-simulation modules.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ldxlzGYWDmW" data-number="576">
      <h4>
        <a href="https://openreview.net/forum?id=ldxlzGYWDmW">
            Effective Abstract Reasoning with Dual-Contrast Network
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ldxlzGYWDmW" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tao_Zhuo3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Zhuo3">Tao Zhuo</a>, <a href="https://openreview.net/profile?id=~Mohan_Kankanhalli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohan_Kankanhalli1">Mohan Kankanhalli</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 08 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ldxlzGYWDmW-details-192" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ldxlzGYWDmW-details-192"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">abstract reasoning, raven's progressive matrices, deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">As a step towards improving the abstract reasoning capability of machines, we aim to solve Raven’s Progressive Matrices (RPM) with neural networks, since solving RPM puzzles is highly correlated with human intelligence. Unlike previous methods that use auxiliary annotations or assume hidden rules to produce appropriate feature representation, we only use the ground truth answer of each question for model learning,  aiming for an intelligent agent to have a strong learning capability with a small amount of supervision.  Based on the RPM problem formulation,  the correct answer filled into the missing entry of the third row/column has  to  best  satisfy  the  same  rules  shared  between  the  first  two  rows/columns.Thus  we  design  a  simple  yet  effective  Dual-Contrast  Network  (DCNet)  to  exploit the inherent structure of RPM puzzles.  Specifically, a rule contrast module is  designed  to  compare  the  latent  rules  between  the  filled  row/column  and  the first two rows/columns; a choice contrast module is designed to increase the relative differences between candidate choices.  Experimental results on the RAVEN and  PGM  datasets  show  that  DCNet  outperforms  the  state-of-the-art  methods by a large margin of 5.77%.   Further experiments on few training samples and model generalization also show the effectiveness of DCNet.  Code is available at https://github.com/visiontao/dcnet.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a simple yet effective Dual-Contrast Network (DCNet) to solve Raven's progressive matrices without using auxiliary annotations and assumptions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="7aogOj_VYO0" data-number="1722">
      <h4>
        <a href="https://openreview.net/forum?id=7aogOj_VYO0">
            Do not Let Privacy Overbill Utility:  Gradient Embedding Perturbation for Private Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=7aogOj_VYO0" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Da_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Da_Yu1">Da Yu</a>, <a href="https://openreview.net/profile?id=~Huishuai_Zhang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huishuai_Zhang3">Huishuai Zhang</a>, <a href="https://openreview.net/profile?id=~Wei_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Chen1">Wei Chen</a>, <a href="https://openreview.net/profile?id=~Tie-Yan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tie-Yan_Liu1">Tie-Yan Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#7aogOj_VYO0-details-878" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7aogOj_VYO0-details-878"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">privacy preserving machine learning, differentially private deep learning, gradient redundancy</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The privacy leakage of the model about the training data can be bounded in the differential privacy mechanism. However, for meaningful privacy parameters, a differentially private model degrades the utility drastically when the model comprises a large number of trainable parameters.  In this paper, we propose an algorithm  \emph{Gradient Embedding Perturbation (GEP)} towards training differentially private deep models with decent accuracy. Specifically, in each gradient descent step, GEP first projects individual private gradient into a non-sensitive anchor subspace, producing a low-dimensional gradient embedding and a small-norm residual gradient. Then, GEP perturbs the low-dimensional embedding and the residual gradient separately according to the privacy budget. Such a decomposition permits a small perturbation variance, which greatly helps to break the dimensional barrier of private learning. With GEP, we achieve decent accuracy with low computational cost and modest privacy guarantee for deep models.  Especially, with privacy bound <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="148" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi><mo>=</mo><mn>8</mn></math></mjx-assistive-mml></mjx-container>, we achieve <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="149" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c39"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>74.9</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> test accuracy on CIFAR10 and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="150" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c39"></mjx-c><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>95.1</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> test accuracy on  SVHN, significantly improving over existing results.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A new algorithm for differentially private learning that advances state-of-the-art performance on several benchmark datasets. Code: https://github.com/dayu11/Gradient-Embedding-Perturbation</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=7aogOj_VYO0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="04ArenGOz3" data-number="3297">
      <h4>
        <a href="https://openreview.net/forum?id=04ArenGOz3">
            Set Prediction without Imposing Structure as Conditional Density Estimation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=04ArenGOz3" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~David_W_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_W_Zhang1">David W Zhang</a>, <a href="https://openreview.net/profile?email=gertjan.burghouts%40tno.nl" class="profile-link" data-toggle="tooltip" data-placement="top" title="gertjan.burghouts@tno.nl">Gertjan J. Burghouts</a>, <a href="https://openreview.net/profile?id=~Cees_G._M._Snoek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cees_G._M._Snoek1">Cees G. M. Snoek</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 22 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#04ArenGOz3-details-815" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="04ArenGOz3-details-815"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">set prediction, energy based models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Set prediction is about learning to predict a collection of unordered variables with unknown interrelations. Training such models with set losses imposes the structure of a metric space over sets. We focus on stochastic and underdefined cases, where an incorrectly chosen loss function leads to implausible predictions. Example tasks include conditional point-cloud reconstruction and predicting future states of molecules. In this paper we propose an alternative to training via set losses, by viewing learning as conditional density estimation. Our learning framework fits deep energy-based models and approximates the intractable likelihood with gradient-guided sampling. Furthermore, we propose a stochastically augmented prediction algorithm that enables multiple predictions, reflecting the possible variations in the target set. We empirically demonstrate on a variety of datasets the capability to learn multi-modal densities and produce different plausible predictions. Our approach is competitive with previous set prediction models on standard benchmarks. More importantly, it extends the family of addressable tasks beyond those that have unambiguous predictions.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A set prediction training and prediction framework that addresses tasks with ambiguous target sets.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="e12NDM7wkEY" data-number="1734">
      <h4>
        <a href="https://openreview.net/forum?id=e12NDM7wkEY">
            Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=e12NDM7wkEY" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yaling_Tao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yaling_Tao1">Yaling Tao</a>, <a href="https://openreview.net/profile?email=kentaro1.takagi%40toshiba.co.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="kentaro1.takagi@toshiba.co.jp">Kentaro Takagi</a>, <a href="https://openreview.net/profile?email=kouta.nakata%40toshiba.co.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="kouta.nakata@toshiba.co.jp">Kouta Nakata</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#e12NDM7wkEY-details-76" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="e12NDM7wkEY-details-76"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">clustering, representation learning, deep embedding</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Clustering is one of the most fundamental tasks in machine learning. Recently, deep clustering has become a major trend in clustering techniques. Representation learning often plays an important role in the effectiveness of deep clustering, and thus can be a principal cause of performance degradation. In this paper, we propose a clustering-friendly representation learning method using instance discrimination and feature decorrelation. Our deep-learning-based representation learning method is motivated by the properties of classical spectral clustering. Instance discrimination learns similarities among data and feature decorrelation removes redundant correlation among features. We utilize an instance discrimination method in which learning individual instance classes leads to learning similarity among instances. Through detailed experiments and examination, we show that the approach can be adapted to learning a latent space for clustering. We design novel softmax-formulated decorrelation constraints for learning. In evaluations of image clustering using CIFAR-10 and ImageNet-10, our method achieves accuracy of 81.5% and 95.4%, respectively. We also show that the softmax-formulated constraints are compatible with various neural networks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a clustering-friendly representation learning method using instance discrimination and feature decorrelation, which achieves accuracy of 81.5% and 95.4% on CIFAR-10 and ImageNet-10, respectively, far above state-of-the-art values.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Xh5eMZVONGF" data-number="3338">
      <h4>
        <a href="https://openreview.net/forum?id=Xh5eMZVONGF">
            Language-Agnostic Representation Learning of Source Code from Structure and Context
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Xh5eMZVONGF" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Daniel_Z%C3%BCgner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Zügner1">Daniel Zügner</a>, <a href="https://openreview.net/profile?email=tobias.kirschstein%40tum.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="tobias.kirschstein@tum.de">Tobias Kirschstein</a>, <a href="https://openreview.net/profile?id=~Michele_Catasta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michele_Catasta1">Michele Catasta</a>, <a href="https://openreview.net/profile?id=~Jure_Leskovec1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jure_Leskovec1">Jure Leskovec</a>, <a href="https://openreview.net/profile?id=~Stephan_G%C3%BCnnemann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephan_Günnemann1">Stephan Günnemann</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Xh5eMZVONGF-details-410" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Xh5eMZVONGF-details-410"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">machine learning for code, code summarization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Source code (Context) and its parsed abstract syntax tree (AST; Structure) are two complementary representations of the same computer program. Traditionally, designers of machine learning models have relied predominantly either on Structure or Context. We propose a new model, which jointly learns on Context and Structure of source code. In contrast to previous approaches, our model uses only language-agnostic features, i.e., source code and features that can be computed directly from the AST. Besides obtaining state-of-the-art on monolingual code summarization on all five programming languages considered in this work, we propose the first multilingual code summarization model. We show that jointly training on non-parallel data from multiple programming languages improves results on all individual languages, where the strongest gains are on low-resource languages. Remarkably, multilingual training only from Context does not lead to the same improvements, highlighting the benefits of combining Structure and Context for representation learning on code.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Language-agnostic learning from Structure and Context of programs improves learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Xh5eMZVONGF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="eo6U4CAwVmg" data-number="2902">
      <h4>
        <a href="https://openreview.net/forum?id=eo6U4CAwVmg">
            Training GANs with Stronger Augmentations via Contrastive Discriminator
        </a>
      
        
          <a href="https://openreview.net/pdf?id=eo6U4CAwVmg" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jongheon_Jeong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jongheon_Jeong1">Jongheon Jeong</a>, <a href="https://openreview.net/profile?id=~Jinwoo_Shin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinwoo_Shin1">Jinwoo Shin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#eo6U4CAwVmg-details-181" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eo6U4CAwVmg-details-181"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">generative adversarial networks, contrastive learning, data augmentation, visual representation learning, unsupervised learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This "fusion" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our experimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the linear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a novel discriminator of GAN showing that contrastive representation learning, e.g., SimCLR, and GAN can benefit each other when they are jointly trained. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xHKVVHGDOEk" data-number="1227">
      <h4>
        <a href="https://openreview.net/forum?id=xHKVVHGDOEk">
            Influence Functions in Deep Learning Are Fragile
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xHKVVHGDOEk" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Samyadeep_Basu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samyadeep_Basu1">Samyadeep Basu</a>, <a href="https://openreview.net/profile?id=~Phil_Pope1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Phil_Pope1">Phil Pope</a>, <a href="https://openreview.net/profile?id=~Soheil_Feizi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soheil_Feizi2">Soheil Feizi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xHKVVHGDOEk-details-27" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xHKVVHGDOEk-details-27"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Influence Functions, Interpretability</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Influence functions approximate the effect of training samples in test-time predictions and have a wide variety of applications in machine learning interpretability and uncertainty estimation. A commonly-used (first-order) influence function can be implemented efficiently as a post-hoc method requiring access only to the gradients and Hessian of the model. For linear models, influence functions are well-defined due to the convexity of the underlying loss function and are generally accurate even across difficult settings where model changes are fairly large such as estimating group influences. Influence functions, however, are not well-understood in the context of deep learning with non-convex loss functions.  In this paper, we provide a comprehensive and large-scale empirical study of successes and failures of influence functions in neural network models trained on datasets such as Iris, MNIST, CIFAR-10 and ImageNet. Through our extensive experiments, we show that the network architecture, its depth and width, as well as the extent of model parameterization and regularization techniques have strong effects in the accuracy of influence functions. In particular, we find that (i) influence estimates are fairly accurate for shallow networks, while for deeper networks the estimates are often erroneous; (ii) for certain network architectures and datasets, training with weight-decay regularization is important to get high-quality influence estimates; and (iii) the accuracy of influence estimates can vary significantly depending on the examined test points. These results suggest that in general influence functions in deep learning are fragile and call for developing improved influence estimation methods to mitigate these issues in non-convex setups.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">End-to-end investigation of the behaviour of influence functions in deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=xHKVVHGDOEk&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="8HhkbjrWLdE" data-number="1937">
      <h4>
        <a href="https://openreview.net/forum?id=8HhkbjrWLdE">
            Separation and Concentration in Deep Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=8HhkbjrWLdE" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~John_Zarka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_Zarka1">John Zarka</a>, <a href="https://openreview.net/profile?id=~Florentin_Guth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florentin_Guth1">Florentin Guth</a>, <a href="https://openreview.net/profile?id=~St%C3%A9phane_Mallat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stéphane_Mallat1">Stéphane Mallat</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#8HhkbjrWLdE-details-563" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8HhkbjrWLdE-details-563"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">fisher ratio, neural collapse, mean separation, concentration, variance reduction, deep learning, image classification</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Numerical experiments demonstrate that deep neural network classifiers progressively separate class distributions around their mean, achieving linear separability on the training set, and increasing the Fisher discriminant ratio. We explain this mechanism with two types of operators. We prove that a rectifier without biases applied to sign-invariant tight frames can separate class means and increase Fisher ratios. On the opposite, a soft-thresholding on tight frames can reduce within-class variabilities while preserving class means. Variance reduction bounds are proved for Gaussian mixture models. For image classification, we show that separation of class means can be achieved with rectified wavelet tight frames that are not learned. It defines a scattering transform. Learning  <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="151" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mo>×</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> convolutional tight frames along scattering channels and applying a soft-thresholding reduces within-class variabilities. The resulting scattering network reaches the classification accuracy of ResNet-18 on CIFAR-10 and ImageNet, with fewer layers and no learned biases.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=8HhkbjrWLdE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="5NA1PinlGFu" data-number="3388">
      <h4>
        <a href="https://openreview.net/forum?id=5NA1PinlGFu">
            Colorization Transformer
        </a>
      
        
          <a href="https://openreview.net/pdf?id=5NA1PinlGFu" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Manoj_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manoj_Kumar1">Manoj Kumar</a>, <a href="https://openreview.net/profile?id=~Dirk_Weissenborn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dirk_Weissenborn1">Dirk Weissenborn</a>, <a href="https://openreview.net/profile?id=~Nal_Kalchbrenner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nal_Kalchbrenner1">Nal Kalchbrenner</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 07 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#5NA1PinlGFu-details-627" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5NA1PinlGFu-details-627"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present the Colorization Transformer, a novel approach for diverse high fidelity image colorization based on self-attention. Given a grayscale image, the colorization proceeds in three steps. We first use a conditional autoregressive transformer to produce a low resolution coarse coloring of the grayscale image. Our architecture adopts conditional transformer layers to effectively condition grayscale input. Two subsequent fully parallel networks upsample the coarse colored low resolution image into a finely colored high resolution image. Sampling from the Colorization Transformer produces diverse colorings whose fidelity outperforms the previous state-of-the-art on colorising ImageNet based on FID results and based on a human evaluation in a Mechanical Turk test. Remarkably, in  more than 60\% of cases human evaluators prefer the highest rated among three generated colorings over the ground truth. The code and pre-trained checkpoints for Colorization Transformer are publicly available  at https://github.com/google-research/google-research/tree/master/coltran</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Self-attention for colorization</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="kmqjgSNXby" data-number="1897">
      <h4>
        <a href="https://openreview.net/forum?id=kmqjgSNXby">
            Autoregressive Dynamics Models for Offline Policy Evaluation and Optimization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=kmqjgSNXby" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Michael_R_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_R_Zhang1">Michael R Zhang</a>, <a href="https://openreview.net/profile?id=~Thomas_Paine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Paine1">Thomas Paine</a>, <a href="https://openreview.net/profile?id=~Ofir_Nachum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ofir_Nachum1">Ofir Nachum</a>, <a href="https://openreview.net/profile?id=~Cosmin_Paduraru1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cosmin_Paduraru1">Cosmin Paduraru</a>, <a href="https://openreview.net/profile?id=~George_Tucker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~George_Tucker1">George Tucker</a>, <a href="https://openreview.net/profile?id=~ziyu_wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~ziyu_wang1">ziyu wang</a>, <a href="https://openreview.net/profile?id=~Mohammad_Norouzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammad_Norouzi1">Mohammad Norouzi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#kmqjgSNXby-details-195" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kmqjgSNXby-details-195"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Off-policy policy evaluation, autoregressive models, offline reinforcement learning, policy optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Standard dynamics models for continuous control make use of feedforward computation to predict the conditional distribution of next state and reward given current state and action using a multivariate Gaussian with a diagonal covariance structure. This modeling choice assumes that different dimensions of the next state and reward are conditionally independent given the current state and action and may be driven by the fact that fully observable physics-based simulation environments entail deterministic transition dynamics. In this paper, we challenge this conditional independence assumption and propose a family of expressive autoregressive dynamics models that generate different dimensions of the next state and reward sequentially conditioned on previous dimensions. We demonstrate that autoregressive dynamics models indeed outperform standard feedforward models in log-likelihood on heldout transitions. Furthermore, we compare different model-based and model-free off-policy evaluation (OPE) methods on RL Unplugged, a suite of offline MuJoCo datasets, and find that autoregressive dynamics models consistently outperform all baselines, achieving a new state-of-the-art. Finally, we show that autoregressive dynamics models are useful for offline policy optimization by serving as a way to enrich the replay buffer through data augmentation and improving performance using model-based planning.
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We demonstrate autoregressive dynamics models outperform standard feedforward models and other baselines in offline policy evaluation and optimization.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="6YEQUn0QICG" data-number="288">
      <h4>
        <a href="https://openreview.net/forum?id=6YEQUn0QICG">
            FedBN: Federated Learning on Non-IID Features via Local Batch Normalization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=6YEQUn0QICG" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xiaoxiao_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoxiao_Li1">Xiaoxiao Li</a>, <a href="https://openreview.net/profile?id=~Meirui_JIANG1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Meirui_JIANG1">Meirui JIANG</a>, <a href="https://openreview.net/profile?id=~Xiaofei_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaofei_Zhang1">Xiaofei Zhang</a>, <a href="https://openreview.net/profile?id=~Michael_Kamp1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Kamp1">Michael Kamp</a>, <a href="https://openreview.net/profile?id=~Qi_Dou2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qi_Dou2">Qi Dou</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#6YEQUn0QICG-details-763" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6YEQUn0QICG-details-763"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Federated Learning, Non-IID, Batch Normalization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The emerging paradigm of federated learning (FL) strives to enable collaborative training of deep models on the network edge without centrally aggregating raw data and hence improving data privacy. In most cases, the assumption of independent and identically distributed samples across local clients does not hold for federated learning setups. Under this setting, neural network training performance may vary significantly according to the data distribution and even hurt training convergence. Most of the previous work has focused on a difference in the distribution of labels or client shifts. Unlike those settings, we address an important problem of FL, e.g., different scanners/sensors in medical imaging, different scenery distribution in autonomous driving (highway vs. city), where local clients store examples with different distributions compared to other clients, which we denote as feature shift non-iid. In this work, we propose an effective method that uses local batch normalization to alleviate the feature shift before averaging models. The resulting scheme, called FedBN, outperforms both classical FedAvg, as well as the state-of-the-art for non-iid data (FedProx) on our extensive experiments. These empirical results are supported by a convergence analysis that shows in a simplified setting that FedBN has a faster convergence rate than FedAvg. Code is available at https://github.com/med-air/FedBN.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a novel and efficient federated learning aggregation method, denoted FedBN, that uses local batch normalization to effectively tackle the underexplored non-iid problem of heterogeneous feature distributions, or feature shift.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=6YEQUn0QICG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="fmOOI2a3tQP" data-number="1767">
      <h4>
        <a href="https://openreview.net/forum?id=fmOOI2a3tQP">
            Learning Robust State Abstractions for Hidden-Parameter Block MDPs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=fmOOI2a3tQP" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Amy_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amy_Zhang1">Amy Zhang</a>, <a href="https://openreview.net/profile?id=~Shagun_Sodhani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shagun_Sodhani1">Shagun Sodhani</a>, <a href="https://openreview.net/profile?id=~Khimya_Khetarpal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Khimya_Khetarpal1">Khimya Khetarpal</a>, <a href="https://openreview.net/profile?id=~Joelle_Pineau1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joelle_Pineau1">Joelle Pineau</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 13 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#fmOOI2a3tQP-details-994" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fmOOI2a3tQP-details-994"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">multi-task reinforcement learning, bisimulation, hidden-parameter mdp, block mdp</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Many control tasks exhibit similar dynamics that can be modeled as having common latent structure. Hidden-Parameter Markov Decision Processes (HiP-MDPs) explicitly model this structure to improve sample efficiency in multi-task settings.
      However, this setting makes strong assumptions on the observability of the state that limit its application in real-world scenarios with rich observation spaces.  In this work, we leverage ideas of common structure from the HiP-MDP setting, and extend it to enable robust state abstractions inspired by Block MDPs. We  derive instantiations of this new framework for  both multi-task reinforcement learning (MTRL) and  meta-reinforcement learning (Meta-RL) settings. Further, we provide transfer and generalization bounds based on task and state similarity, along with sample complexity bounds that depend on the aggregate number of samples across tasks, rather than the number of tasks, a significant improvement over prior work. To further demonstrate efficacy of the proposed method, we empirically compare and show improvement over multi-task and meta-reinforcement learning baselines.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a new framework for tackling environments with different dynamics in rich observation settings and learning abstractions with this framework for improved generalization performance.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Ti87Pv5Oc8" data-number="2513">
      <h4>
        <a href="https://openreview.net/forum?id=Ti87Pv5Oc8">
            Meta-Learning with Neural Tangent Kernels
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Ti87Pv5Oc8" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yufan_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yufan_Zhou1">Yufan Zhou</a>, <a href="https://openreview.net/profile?id=~Zhenyi_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhenyi_Wang1">Zhenyi Wang</a>, <a href="https://openreview.net/profile?email=jxian%40buffalo.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jxian@buffalo.edu">Jiayi Xian</a>, <a href="https://openreview.net/profile?id=~Changyou_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changyou_Chen1">Changyou Chen</a>, <a href="https://openreview.net/profile?id=~Jinhui_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinhui_Xu1">Jinhui Xu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 08 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Ti87Pv5Oc8-details-438" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ti87Pv5Oc8-details-438"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">meta-learning, neural tangent kernel</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Model Agnostic Meta-Learning (MAML) has emerged as a standard framework for meta-learning, where a meta-model is learned with the ability of fast adapting to new tasks. However, as a double-looped optimization problem, MAML needs to differentiate through the whole inner-loop optimization path for every outer-loop training step, which may lead to both computational inefficiency and sub-optimal solutions. In this paper, we generalize MAML to allow meta-learning to be defined in function spaces, and propose the first meta-learning paradigm in the Reproducing Kernel Hilbert Space (RKHS) induced by the meta-model's Neural Tangent Kernel (NTK). Within this paradigm, we introduce two meta-learning algorithms in the RKHS, which no longer need a sub-optimal iterative inner-loop adaptation as in the MAML framework. We achieve this goal by 1) replacing the adaptation with a fast-adaptive regularizer in the RKHS; and 2) solving the adaptation analytically based on the NTK theory. Extensive experimental studies demonstrate advantages of our paradigm in both efficiency and quality of solutions compared to related meta-learning algorithms. Another interesting feature of our proposed methods is that they are demonstrated to be more robust to adversarial attacks and out-of-distribution adaptation than popular baselines, as demonstrated in our experiments.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">First work to define meta learning in RKHS induced by Neural Tangent Kernel</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ti87Pv5Oc8&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="8xeBUgD8u9" data-number="3663">
      <h4>
        <a href="https://openreview.net/forum?id=8xeBUgD8u9">
            Continual learning in recurrent neural networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=8xeBUgD8u9" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=behret%40ethz.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="behret@ethz.ch">Benjamin Ehret</a>, <a href="https://openreview.net/profile?id=~Christian_Henning1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christian_Henning1">Christian Henning</a>, <a href="https://openreview.net/profile?id=~Maria_Cervera1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maria_Cervera1">Maria Cervera</a>, <a href="https://openreview.net/profile?id=~Alexander_Meulemans1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Meulemans1">Alexander Meulemans</a>, <a href="https://openreview.net/profile?id=~Johannes_Von_Oswald1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Johannes_Von_Oswald1">Johannes Von Oswald</a>, <a href="https://openreview.net/profile?id=~Benjamin_F_Grewe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_F_Grewe1">Benjamin F Grewe</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#8xeBUgD8u9-details-961" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8xeBUgD8u9-details-961"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Recurrent Neural Networks, Continual Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While a diverse collection of continual learning (CL) methods has been proposed to prevent catastrophic forgetting, a thorough investigation of their effectiveness for processing sequential data with recurrent neural networks (RNNs) is lacking. Here, we provide the first comprehensive evaluation of established CL methods on a variety of sequential data benchmarks. Specifically, we shed light on the particularities that arise when applying weight-importance methods, such as elastic weight consolidation, to RNNs. In contrast to feedforward networks, RNNs iteratively reuse a shared set of weights and require working memory to process input samples. We show that the performance of weight-importance methods is not directly affected by the length of the processed sequences, but rather by high working memory requirements, which lead to an increased need for stability at the cost of decreased plasticity for learning subsequent tasks. We additionally provide theoretical arguments supporting this interpretation by studying linear RNNs. Our study shows that established CL methods can be successfully ported to the recurrent case, and that a recent regularization approach based on hypernetworks outperforms weight-importance methods, thus emerging as a promising candidate for CL in RNNs. Overall, we provide insights on the differences between CL in feedforward networks and RNNs, while guiding towards effective solutions to tackle CL on sequential data.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper studies the behavior of established approaches to the problem of continual learning in the context of recurrent neural networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=8xeBUgD8u9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ZK6vTvb84s" data-number="855">
      <h4>
        <a href="https://openreview.net/forum?id=ZK6vTvb84s">
            A Trainable Optimal Transport Embedding for Feature Aggregation and its Relationship to Attention
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ZK6vTvb84s" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Gr%C3%A9goire_Mialon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Grégoire_Mialon1">Grégoire Mialon</a>, <a href="https://openreview.net/profile?id=~Dexiong_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dexiong_Chen1">Dexiong Chen</a>, <a href="https://openreview.net/profile?id=~Alexandre_d%26%23x27%3BAspremont1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexandre_d&#39;Aspremont1">Alexandre d'Aspremont</a>, <a href="https://openreview.net/profile?id=~Julien_Mairal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Julien_Mairal1">Julien Mairal</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ZK6vTvb84s-details-313" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZK6vTvb84s-details-313"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">bioinformatics, optimal transport, kernel methods, attention, transformers</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We address the problem of learning on sets of features, motivated by the need of performing pooling operations in long biological sequences of varying sizes, with long-range dependencies, and possibly few labeled data. To address this challenging task, we introduce a parametrized representation of fixed size, which  embeds and then aggregates elements from a given input set according to the optimal transport plan between the set and a trainable reference. Our approach scales to large datasets and allows end-to-end training of the reference, while also providing a simple unsupervised learning mechanism with small computational cost. Our aggregation technique admits two useful interpretations: it may be seen as a mechanism related to attention layers in neural networks, or it may be seen as a scalable surrogate of a classical optimal transport-based kernel. We experimentally demonstrate the effectiveness of our approach on biological sequences, achieving state-of-the-art results for protein fold recognition and detection of chromatin profiles tasks, and, as a proof of concept, we show promising results for processing natural language sequences. We provide an open-source implementation of our embedding that can be used alone or as a module in larger learning models at https://github.com/claying/OTK.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a new, trainable embedding for large sets of features such as biological sequences, and demonstrate its effectiveness.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=ZK6vTvb84s&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="h0de3QWtGG" data-number="843">
      <h4>
        <a href="https://openreview.net/forum?id=h0de3QWtGG">
            Learning "What-if" Explanations for Sequential Decision-Making
        </a>
      
        
          <a href="https://openreview.net/pdf?id=h0de3QWtGG" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ioana_Bica1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ioana_Bica1">Ioana Bica</a>, <a href="https://openreview.net/profile?id=~Daniel_Jarrett1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Jarrett1">Daniel Jarrett</a>, <a href="https://openreview.net/profile?email=ah2075%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="ah2075@cam.ac.uk">Alihan Hüyük</a>, <a href="https://openreview.net/profile?id=~Mihaela_van_der_Schaar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihaela_van_der_Schaar2">Mihaela van der Schaar</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#h0de3QWtGG-details-107" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="h0de3QWtGG-details-107"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">counterfactuals, explaining decision-making, preference learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Building interpretable parameterizations of real-world decision-making on the basis of demonstrated behavior--i.e. trajectories of observations and actions made by an expert maximizing some unknown reward function--is essential for introspecting and auditing policies in different institutions. In this paper, we propose learning explanations of expert decisions by modeling their reward function in terms of preferences with respect to ``"what if'' outcomes: Given the current history of observations, what would happen if we took a particular action? To learn these cost-benefit tradeoffs associated with the expert's actions, we integrate counterfactual reasoning into batch inverse reinforcement learning. This offers a principled way of defining reward functions and explaining expert behavior, and also satisfies the constraints of real-world decision-making---where active experimentation is often impossible (e.g. in healthcare). Additionally, by estimating the effects of different actions, counterfactuals readily tackle the off-policy nature of policy evaluation in the batch setting, and can naturally accommodate settings where the expert policies depend on histories of observations rather than just current states. Through illustrative experiments in both real and simulated medical environments, we highlight the effectiveness of our batch, counterfactual inverse reinforcement learning approach in recovering accurate and interpretable descriptions of behavior.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose explaining sequential decision-making by integrating counterfactual reasoning into batch inverse reinforcement learning and recovering the preferences of experts over "what-if" outcomes.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="NomEDgIEBwE" data-number="3273">
      <h4>
        <a href="https://openreview.net/forum?id=NomEDgIEBwE">
            Improving Transformation Invariance in Contrastive Representation Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=NomEDgIEBwE" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Adam_Foster1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adam_Foster1">Adam Foster</a>, <a href="https://openreview.net/profile?id=~Rattana_Pukdee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rattana_Pukdee1">Rattana Pukdee</a>, <a href="https://openreview.net/profile?id=~Tom_Rainforth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tom_Rainforth1">Tom Rainforth</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 22 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#NomEDgIEBwE-details-754" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NomEDgIEBwE-details-754"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">contrastive learning, representation learning, transformation invariance</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose methods to strengthen the invariance properties of representations obtained by contrastive learning. While existing approaches implicitly induce a degree of invariance as representations are learned, we look to more directly enforce invariance in the encoding process. To this end, we first introduce a training objective for contrastive learning that uses a novel regularizer to control how the representation changes under transformation. We show that representations trained with this objective perform better on downstream tasks and are more robust to the introduction of nuisance transformations at test time. Second, we propose a change to how test time representations are generated by introducing a feature averaging approach that combines encodings from multiple transformations of the original input, finding that this leads to across the board performance gains. Finally, we introduce the novel Spirograph dataset to explore our ideas in the context of a differentiable generative process with multiple downstream tasks, showing that our techniques for learning invariance are highly beneficial.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose methods to strengthen the invariance properties of representations obtained by contrastive learning using novel gradient regularization during training and feature averaging at test time.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=NomEDgIEBwE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="OPyWRrcjVQw" data-number="2261">
      <h4>
        <a href="https://openreview.net/forum?id=OPyWRrcjVQw">
            Shapley explainability on the data manifold
        </a>
      
        
          <a href="https://openreview.net/pdf?id=OPyWRrcjVQw" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Christopher_Frye1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Frye1">Christopher Frye</a>, <a href="https://openreview.net/profile?email=damiendemijolla%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="damiendemijolla@gmail.com">Damien de Mijolla</a>, <a href="https://openreview.net/profile?id=~Tom_Begley1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tom_Begley1">Tom Begley</a>, <a href="https://openreview.net/profile?email=laurence.c%40faculty.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="laurence.c@faculty.ai">Laurence Cowton</a>, <a href="https://openreview.net/profile?email=t-mestan%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="t-mestan@microsoft.com">Megan Stanley</a>, <a href="https://openreview.net/profile?id=~Ilya_Feige1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ilya_Feige1">Ilya Feige</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#OPyWRrcjVQw-details-141" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OPyWRrcjVQw-details-141"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Explainability in AI is crucial for model development, compliance with regulation, and providing operational nuance to predictions. The Shapley framework for explainability attributes a model’s predictions to its input features in a mathematically principled and model-agnostic way. However, general implementations of Shapley explainability make an untenable assumption: that the model’s features are uncorrelated. In this work, we demonstrate unambiguous drawbacks of this assumption and develop two solutions to Shapley explainability that respect the data manifold. One solution, based on generative modelling, provides flexible access to data imputations; the other directly learns the Shapley value-function, providing performance and stability at the cost of flexibility. While “off-manifold” Shapley values can (i) give rise to incorrect explanations, (ii) hide implicit model dependence on sensitive attributes, and (iii) lead to unintelligible explanations in higher-dimensional data, on-manifold explainability overcomes these problems.
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present drawbacks of model explanations that do not respect the data manifold, and introduce two methods for on-manifold explainability.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="gl3D-xY7wLq" data-number="2112">
      <h4>
        <a href="https://openreview.net/forum?id=gl3D-xY7wLq">
            Noise or Signal: The Role of Image Backgrounds in Object Recognition
        </a>
      
        
          <a href="https://openreview.net/pdf?id=gl3D-xY7wLq" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kai_Yuanqing_Xiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kai_Yuanqing_Xiao1">Kai Yuanqing Xiao</a>, <a href="https://openreview.net/profile?id=~Logan_Engstrom1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Logan_Engstrom1">Logan Engstrom</a>, <a href="https://openreview.net/profile?id=~Andrew_Ilyas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_Ilyas1">Andrew Ilyas</a>, <a href="https://openreview.net/profile?id=~Aleksander_Madry1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aleksander_Madry1">Aleksander Madry</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#gl3D-xY7wLq-details-805" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gl3D-xY7wLq-details-805"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Backgrounds, Model Biases, Robustness, Computer Vision</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We assess the tendency of state-of-the-art object recognition models to depend on signals from image backgrounds. We create a toolkit for disentangling foreground and background signal on ImageNet images, and find that (a) models can achieve non-trivial accuracy by relying on the background alone, (b) models often misclassify images even in the presence of correctly classified foregrounds--up to 88% of the time with adversarially chosen backgrounds, and (c) more accurate models tend to depend on backgrounds less. Our analysis of backgrounds brings us closer to understanding which correlations machine learning models use, and how they determine models' out of distribution performance.
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We develop and use a toolkit to investigate models’ use of (and reliance on) image backgrounds.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=gl3D-xY7wLq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="HOFxeCutxZR" data-number="315">
      <h4>
        <a href="https://openreview.net/forum?id=HOFxeCutxZR">
            Enjoy Your Editing: Controllable GANs for Image Editing via Latent Space Navigation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=HOFxeCutxZR" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Peiye_Zhuang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peiye_Zhuang2">Peiye Zhuang</a>, <a href="https://openreview.net/profile?id=~Oluwasanmi_O_Koyejo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Oluwasanmi_O_Koyejo1">Oluwasanmi O Koyejo</a>, <a href="https://openreview.net/profile?id=~Alex_Schwing1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_Schwing1">Alex Schwing</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#HOFxeCutxZR-details-606" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HOFxeCutxZR-details-606"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Image manipulation, GANs, latent space of GANs</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Controllable semantic image editing enables a user to change entire image attributes with a few clicks, e.g., gradually making a summer scene look like it was taken in winter. Classic approaches for this task use a Generative Adversarial Net (GAN) to learn a latent space and suitable latent-space transformations. However, current approaches often suffer from attribute edits that are entangled, global image identity changes, and diminished photo-realism. To address these concerns, we learn multiple attribute transformations simultaneously, integrate attribute regression into the training of transformation functions, and apply a content loss and an adversarial loss that encourages the maintenance of image identity and photo-realism. We propose quantitative evaluation strategies for measuring controllable editing performance, unlike prior work, which primarily focuses on qualitative evaluation. Our model permits better control for both single- and multiple-attribute editing while preserving image identity and realism during transformation. We provide empirical results for both natural and synthetic images, highlighting that our model achieves state-of-the-art performance for targeted image manipulation. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a state-of-the-art approach to semantically edit images by transferring latent vectors towards meaningful latent space directions. </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=HOFxeCutxZR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="dFwBosAcJkN" data-number="261">
      <h4>
        <a href="https://openreview.net/forum?id=dFwBosAcJkN">
            Perceptual Adversarial Robustness: Defense Against Unseen Threat Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=dFwBosAcJkN" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Cassidy_Laidlaw1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cassidy_Laidlaw1">Cassidy Laidlaw</a>, <a href="https://openreview.net/profile?id=~Sahil_Singla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sahil_Singla1">Sahil Singla</a>, <a href="https://openreview.net/profile?id=~Soheil_Feizi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soheil_Feizi2">Soheil Feizi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#dFwBosAcJkN-details-280" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dFwBosAcJkN-details-280"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A key challenge in adversarial robustness is the lack of a precise mathematical characterization of human perception, used in the definition of adversarial attacks that are imperceptible to human eyes. Most current attacks and defenses try to get around this issue by considering restrictive adversarial threat models such as those bounded by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="152" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> or <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="153" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-n" size="s"><mjx-c class="mjx-c221E"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mi mathvariant="normal">∞</mi></msub></math></mjx-assistive-mml></mjx-container> distance, spatial perturbations, etc. However, models that are robust against any of these restrictive threat models are still fragile against other threat models, i.e. they have poor generalization to unforeseen attacks. Moreover, even if a model is robust against the union of several restrictive threat models, it is still susceptible to other imperceptible adversarial examples that are not contained in any of the constituent threat models. To resolve these issues, we propose adversarial training against the set of all imperceptible adversarial examples. Since this set is intractable to compute without a human in the loop, we approximate it using deep neural networks. We call this threat model the neural perceptual threat model (NPTM); it includes adversarial examples with a bounded neural perceptual distance (a neural network-based approximation of the true perceptual distance) to natural images. Through an extensive perceptual study, we show that the neural perceptual distance correlates well with human judgements of perceptibility of adversarial examples, validating our threat model.
      
      Under the NPTM, we develop novel perceptual adversarial attacks and defenses. Because the NPTM is very broad, we find that Perceptual Adversarial Training (PAT) against a perceptual attack gives robustness against many other types of adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five diverse adversarial attacks: <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="154" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container>, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="155" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-n" size="s"><mjx-c class="mjx-c221E"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mi mathvariant="normal">∞</mi></msub></math></mjx-assistive-mml></mjx-container>, spatial, recoloring, and JPEG. We find that PAT achieves state-of-the-art robustness against the union of these five attacks—more than doubling the accuracy over the next best model—without training against any of them. That is, PAT generalizes well to unforeseen perturbation types. This is vital in sensitive applications where a particular threat model cannot be assumed, and to the best of our knowledge, PAT is the first adversarial defense with this property.
      
      Code and data are available at https://github.com/cassidylaidlaw/perceptual-advex</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Adversarial training against a perceptually-aligned attack gives high robustness against many diverse adversarial threat models.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="0cmMMy8J5q" data-number="1886">
      <h4>
        <a href="https://openreview.net/forum?id=0cmMMy8J5q">
            Zero-Cost Proxies for Lightweight NAS
        </a>
      
        
          <a href="https://openreview.net/pdf?id=0cmMMy8J5q" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mohamed_S_Abdelfattah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohamed_S_Abdelfattah1">Mohamed S Abdelfattah</a>, <a href="https://openreview.net/profile?email=a.mehrotra1%40samsung.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="a.mehrotra1@samsung.com">Abhinav Mehrotra</a>, <a href="https://openreview.net/profile?id=~%C5%81ukasz_Dudziak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Łukasz_Dudziak1">Łukasz Dudziak</a>, <a href="https://openreview.net/profile?id=~Nicholas_Donald_Lane1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_Donald_Lane1">Nicholas Donald Lane</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#0cmMMy8J5q-details-681" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0cmMMy8J5q-details-681"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">NAS, AutoML, proxy, pruning, efficient</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Neural Architecture Search (NAS) is quickly becoming the standard methodology to design neural network models. However, NAS is typically compute-intensive because multiple models need to be evaluated before choosing the best one. To reduce the computational power and time needed, a proxy task is often used for evaluating each model instead of full training. In this paper, we evaluate conventional reduced-training proxies and quantify how well they preserve ranking between neural network models during search when compared with the rankings produced by final trained accuracy. We propose a series of zero-cost proxies, based on recent pruning literature, that use just a single minibatch of training data to compute a model's score. Our zero-cost proxies use 3 orders of magnitude less computation but can match and even outperform conventional proxies. For example, Spearman's rank correlation coefficient between final validation accuracy and our best zero-cost proxy on NAS-Bench-201 is 0.82, compared to 0.61 for EcoNAS (a recently proposed reduced-training proxy). Finally, we use these zero-cost proxies to enhance existing NAS search algorithms such as random search, reinforcement learning, evolutionary search and predictor-based search. For all search methodologies and across three different NAS datasets, we are able to significantly improve sample efficiency, and thereby decrease computation, by using our zero-cost proxies. For example on NAS-Bench-101, we achieved the same accuracy 4<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="156" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></mjx-assistive-mml></mjx-container> quicker than the best previous result. Our code is made public at: https://github.com/mohsaied/zero-cost-nas.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A single minibatch of data is used to score neural networks for NAS instead of performing full training.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="p8agn6bmTbr" data-number="2269">
      <h4>
        <a href="https://openreview.net/forum?id=p8agn6bmTbr">
            Usable Information and Evolution of Optimal Representations During Training
        </a>
      
        
          <a href="https://openreview.net/pdf?id=p8agn6bmTbr" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Michael_Kleinman2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Kleinman2">Michael Kleinman</a>, <a href="https://openreview.net/profile?id=~Alessandro_Achille1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alessandro_Achille1">Alessandro Achille</a>, <a href="https://openreview.net/profile?id=~Daksh_Idnani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daksh_Idnani1">Daksh Idnani</a>, <a href="https://openreview.net/profile?id=~Jonathan_Kao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Kao1">Jonathan Kao</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#p8agn6bmTbr-details-119" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="p8agn6bmTbr-details-119"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Usable Information, Representation Learning, Learning Dynamics, Initialization, SGD</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We introduce a notion of usable information contained in the representation learned by a deep network, and use it to study how optimal representations for the task emerge during training. We show that the implicit regularization coming from training with Stochastic Gradient Descent with a high learning-rate and small batch size plays an important role in learning minimal sufficient representations for the task. In the process of arriving at a minimal sufficient representation, we find that the content of the representation changes dynamically during training. In particular, we find that semantically meaningful but ultimately irrelevant information is encoded in the early transient dynamics of training, before being later discarded. In addition, we evaluate how perturbing the initial part of training impacts the learning dynamics and the resulting representations. We show these effects on both perceptual decision-making tasks inspired by neuroscience literature, as well as on standard image classification tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="MjvduJCsE4" data-number="2105">
      <h4>
        <a href="https://openreview.net/forum?id=MjvduJCsE4">
            Exploring the Uncertainty Properties of Neural Networks’ Implicit Priors in the Infinite-Width Limit
        </a>
      
        
          <a href="https://openreview.net/pdf?id=MjvduJCsE4" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ben_Adlam1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ben_Adlam1">Ben Adlam</a>, <a href="https://openreview.net/profile?id=~Jaehoon_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaehoon_Lee2">Jaehoon Lee</a>, <a href="https://openreview.net/profile?id=~Lechao_Xiao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lechao_Xiao2">Lechao Xiao</a>, <a href="https://openreview.net/profile?id=~Jeffrey_Pennington1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jeffrey_Pennington1">Jeffrey Pennington</a>, <a href="https://openreview.net/profile?id=~Jasper_Snoek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jasper_Snoek1">Jasper Snoek</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#MjvduJCsE4-details-864" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MjvduJCsE4-details-864"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep Learning, Uncertainty, Infinite-Width Limit, Neural Network Gaussian Process, Bayesian Neural Networks, Gaussian Process</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Modern deep learning models have achieved great success in predictive accuracy for many data modalities. However, their application to many real-world tasks is restricted by poor uncertainty estimates, such as overconfidence on out-of-distribution (OOD) data and ungraceful failing under distributional shift. Previous benchmarks have found that ensembles of neural networks (NNs) are typically the best calibrated models on OOD data. Inspired by this, we leverage recent theoretical advances that characterize the function-space prior of an infinitely-wide NN as a Gaussian process, termed the neural network Gaussian process (NNGP). We use the NNGP with a softmax link function to build a probabilistic model for multi-class classification and marginalize over the latent Gaussian outputs to sample from the posterior. This gives us a better understanding of the implicit prior NNs place on function space and allows a direct comparison of the calibration of the NNGP and its finite-width analogue. We also examine the calibration of previous approaches to classification with the NNGP, which treat classification problems as regression to the one-hot labels. In this case the Bayesian posterior is exact, and we compare several heuristics to generate a categorical distribution over classes. We find these methods are well calibrated under distributional shift. Finally, we consider an infinite-width final layer in conjunction with a pre-trained embedding. This replicates the important practical use case of transfer learning and allows scaling to significantly larger datasets. As well as achieving competitive predictive accuracy, this approach is better calibrated than its finite width analogue.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We study the uncertainty properties of infinitely-wide neural networks</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="V8jrrnwGbuc" data-number="3668">
      <h4>
        <a href="https://openreview.net/forum?id=V8jrrnwGbuc">
            On the geometry of generalization and memorization in deep neural networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=V8jrrnwGbuc" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Cory_Stephenson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cory_Stephenson1">Cory Stephenson</a>, <a href="https://openreview.net/profile?id=~suchismita_padhy2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~suchismita_padhy2">suchismita padhy</a>, <a href="https://openreview.net/profile?email=abhinav.ganesh%40intel.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="abhinav.ganesh@intel.com">Abhinav Ganesh</a>, <a href="https://openreview.net/profile?email=yueh%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yueh@stanford.edu">Yue Hui</a>, <a href="https://openreview.net/profile?id=~Hanlin_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hanlin_Tang1">Hanlin Tang</a>, <a href="https://openreview.net/profile?id=~SueYeon_Chung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~SueYeon_Chung1">SueYeon Chung</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#V8jrrnwGbuc-details-402" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="V8jrrnwGbuc-details-402"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning theory, representation learning, statistical physics methods, double descent</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Understanding how large neural networks avoid memorizing training data is key to explaining their high generalization performance. To examine the structure of when and where memorization occurs in a deep network, we use a recently developed replica-based mean field theoretic geometric analysis method. We find that all layers preferentially learn from examples which share features, and link this behavior to generalization performance. Memorization predominately occurs in the deeper layers, due to decreasing object manifolds’ radius and dimension, whereas early layers are minimally affected. This predicts that generalization can be restored by reverting the final few layer weights to earlier epochs before significant memorization occurred, which is confirmed by the experiments. Additionally, by studying generalization under different model sizes, we reveal the connection between the double descent phenomenon and the underlying model geometry. Finally, analytical analysis shows that networks avoid memorization early in training because close to initialization, the gradient contribution from permuted examples are small. These findings provide quantitative evidence for the structure of memorization across layers of a deep neural network, the drivers for such structure, and its connection to manifold geometric properties.
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We analyze the representational geometry of deep neural networks during generalization and memorization, and find the structure across layers of when and where memorization occurs, as well as drivers for this emerging structure.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="YUGG2tFuPM" data-number="2210">
      <h4>
        <a href="https://openreview.net/forum?id=YUGG2tFuPM">
            Deep Partition Aggregation: Provable Defenses against General Poisoning Attacks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=YUGG2tFuPM" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alexander_Levine2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Levine2">Alexander Levine</a>, <a href="https://openreview.net/profile?id=~Soheil_Feizi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soheil_Feizi2">Soheil Feizi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#YUGG2tFuPM-details-77" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YUGG2tFuPM-details-77"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">bagging, ensemble, robustness, certificate, poisoning, smoothing</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Adversarial poisoning attacks distort training data in order to corrupt the test-time behavior of a classifier. A provable defense provides a certificate for each test sample, which is a lower bound on the magnitude of any adversarial distortion of the training set that can corrupt the test sample's classification.
      We propose two novel provable defenses against poisoning attacks: (i) Deep Partition Aggregation (DPA), a certified defense against a general poisoning threat model, defined as the insertion or deletion of a bounded number of samples to the training set --- by implication, this threat model also includes arbitrary distortions to a bounded number of images and/or labels; and (ii) Semi-Supervised DPA (SS-DPA), a certified defense against label-flipping poisoning attacks. DPA is an ensemble method where base models are trained on partitions of the training set determined by a hash function. DPA is related to both subset aggregation, a well-studied ensemble method in classical machine learning, as well as to randomized smoothing, a popular provable defense against evasion (inference) attacks. Our defense against label-flipping poison attacks, SS-DPA, uses a semi-supervised learning algorithm as its base classifier model: each base classifier is trained using the entire unlabeled training set in addition to the labels for a partition. SS-DPA significantly outperforms the existing certified defense for label-flipping attacks (Rosenfeld et al., 2020) on both MNIST and CIFAR-10: provably tolerating, for at least half of test images, over 600 label flips (vs. &lt; 200 label flips) on MNIST and over 300 label flips (vs. 175 label flips) on CIFAR-10. Against general poisoning attacks where no prior certified defenses exists, DPA can certify <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="157" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2265"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>≥</mo></math></mjx-assistive-mml></mjx-container> 50% of test images against over 500 poison image insertions on MNIST, and nine insertions on CIFAR-10. These results establish new state-of-the-art provable defenses against general and label-flipping poison attacks. Code is available at https://github.com/alevine0/DPA</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose novel certified defenses against label-flipping and general adversarial poisoning attacks. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=YUGG2tFuPM&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="V1ZHVxJ6dSS" data-number="1011">
      <h4>
        <a href="https://openreview.net/forum?id=V1ZHVxJ6dSS">
            DC3: A learning method for optimization with hard constraints
        </a>
      
        
          <a href="https://openreview.net/pdf?id=V1ZHVxJ6dSS" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Priya_L._Donti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Priya_L._Donti1">Priya L. Donti</a>, <a href="https://openreview.net/profile?id=~David_Rolnick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Rolnick1">David Rolnick</a>, <a href="https://openreview.net/profile?id=~J_Zico_Kolter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~J_Zico_Kolter1">J Zico Kolter</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Apr 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#V1ZHVxJ6dSS-details-103" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="V1ZHVxJ6dSS-details-103"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">approximate constrained optimization, implicit differentiation, optimal power flow, surrogate models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Large optimization problems with hard constraints arise in many settings, yet classical solvers are often prohibitively slow, motivating the use of deep networks as cheap "approximate solvers." Unfortunately, naive deep learning approaches typically cannot enforce the hard constraints of such problems, leading to infeasible solutions. In this work, we present Deep Constraint Completion and Correction (DC3), an algorithm to address this challenge. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. We demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid. In both cases, DC3 achieves near-optimal objective values while preserving feasibility.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We describe a method, DC3, for fast approximate solutions to optimization problems with hard constraints, which enforces feasibility via a differentiable procedure incorporated into a neural network.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="PObuuGVrGaZ" data-number="530">
      <h4>
        <a href="https://openreview.net/forum?id=PObuuGVrGaZ">
            Is Label Smoothing Truly Incompatible with Knowledge Distillation: An Empirical Study
        </a>
      
        
          <a href="https://openreview.net/pdf?id=PObuuGVrGaZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhiqiang_Shen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiqiang_Shen1">Zhiqiang Shen</a>, <a href="https://openreview.net/profile?id=~Zechun_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zechun_Liu1">Zechun Liu</a>, <a href="https://openreview.net/profile?id=~Dejia_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dejia_Xu1">Dejia Xu</a>, <a href="https://openreview.net/profile?id=~Zitian_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zitian_Chen1">Zitian Chen</a>, <a href="https://openreview.net/profile?id=~Kwang-Ting_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kwang-Ting_Cheng1">Kwang-Ting Cheng</a>, <a href="https://openreview.net/profile?id=~Marios_Savvides1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marios_Savvides1">Marios Savvides</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#PObuuGVrGaZ-details-527" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PObuuGVrGaZ-details-527"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">label smoothing, knowledge distillation, image classification, neural machine translation, binary neural networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This work aims to empirically clarify a recently discovered perspective that label smoothing is incompatible with knowledge distillation. We begin by introducing the motivation behind on how this incompatibility is raised, i.e., label smoothing erases relative information between teacher logits. We provide a novel connection on how label smoothing affects distributions of semantically similar and dissimilar classes. Then we propose a metric to quantitatively measure the degree of erased information in sample's representation. After that, we study its one-sidedness and imperfection of the incompatibility view through massive analyses, visualizations and comprehensive experiments on Image Classification, Binary Networks, and Neural Machine Translation. Finally, we broadly discuss several circumstances wherein label smoothing will indeed lose its effectiveness.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This work empirically clarifies a recently discovered perspective that label smoothing is incompatible with knowledge distillation.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Db4yerZTYkz" data-number="194">
      <h4>
        <a href="https://openreview.net/forum?id=Db4yerZTYkz">
            Shape-Texture Debiased Neural Network Training
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Db4yerZTYkz" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yingwei_Li4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yingwei_Li4">Yingwei Li</a>, <a href="https://openreview.net/profile?id=~Qihang_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qihang_Yu1">Qihang Yu</a>, <a href="https://openreview.net/profile?id=~Mingxing_Tan3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingxing_Tan3">Mingxing Tan</a>, <a href="https://openreview.net/profile?id=~Jieru_Mei2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jieru_Mei2">Jieru Mei</a>, <a href="https://openreview.net/profile?id=~Peng_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peng_Tang1">Peng Tang</a>, <a href="https://openreview.net/profile?id=~Wei_Shen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Shen2">Wei Shen</a>, <a href="https://openreview.net/profile?id=~Alan_Yuille1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alan_Yuille1">Alan Yuille</a>, <a href="https://openreview.net/profile?id=~cihang_xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~cihang_xie1">cihang xie</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Db4yerZTYkz-details-234" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Db4yerZTYkz-details-234"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">data augmentation, representation learning, debiased training</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. 
      
      Experiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Training CNNs to acquire a debiased shape-texture representation improves image recognition.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="sjuuTm4vj0" data-number="1918">
      <h4>
        <a href="https://openreview.net/forum?id=sjuuTm4vj0">
            Using latent space regression to analyze and leverage compositionality in GANs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=sjuuTm4vj0" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Lucy_Chai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lucy_Chai1">Lucy Chai</a>, <a href="https://openreview.net/profile?id=~Jonas_Wulff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonas_Wulff1">Jonas Wulff</a>, <a href="https://openreview.net/profile?id=~Phillip_Isola1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Phillip_Isola1">Phillip Isola</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#sjuuTm4vj0-details-117" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sjuuTm4vj0-details-117"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Image Synthesis, Composition, Generative Adversarial Networks, Image Editing, Interpretability</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In recent years, Generative Adversarial Networks have become ubiquitous in both research and public perception, but how GANs convert an unstructured latent code to a high quality output is still an open question. In this work, we investigate regression into the latent space as a probe to understand the compositional properties of GANs.  We find that combining the regressor and a pretrained generator provides a strong image prior, allowing us to create composite images from a collage of random image parts at inference time while maintaining global consistency. To compare compositional properties across different generators, we measure the trade-offs between reconstruction of the unrealistic input and image quality of the regenerated samples. We find that the regression approach enables more localized editing of individual image parts compared to direct editing in the latent space, and we conduct experiments to quantify this independence effect. Our method is agnostic to the semantics of edits, and does not require labels or predefined concepts during training. Beyond image composition, our method extends to a number of related applications, such as image inpainting or example-based image editing, which we demonstrate on several GANs and datasets, and
      because it uses only a single forward pass, it can operate in real-time. Code is available on our project page: https://chail.github.io/latent-composition/.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We use a latent regressor network to investigate compositional properties of image synthesis with GANs.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="RqCC_00Bg7V" data-number="2772">
      <h4>
        <a href="https://openreview.net/forum?id=RqCC_00Bg7V">
            Blending MPC &amp; Value Function Approximation for Efficient Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=RqCC_00Bg7V" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mohak_Bhardwaj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohak_Bhardwaj1">Mohak Bhardwaj</a>, <a href="https://openreview.net/profile?email=sanjiban.choudhury%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sanjiban.choudhury@gmail.com">Sanjiban Choudhury</a>, <a href="https://openreview.net/profile?id=~Byron_Boots1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Byron_Boots1">Byron Boots</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#RqCC_00Bg7V-details-160" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RqCC_00Bg7V-details-160"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, model-predictive control</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Model-Predictive Control (MPC) is a powerful tool for controlling complex, real-world systems that uses a model to make predictions about future behavior. For each state encountered, MPC solves an online optimization problem to choose a control action that will minimize future cost. This is a surprisingly effective strategy, but real-time performance requirements warrant the use of simple models. If the model is not sufficiently accurate, then the resulting controller can be biased, limiting performance. We present a framework for improving on MPC with model-free reinforcement learning (RL). The key insight is to view MPC as constructing a series of local Q-function approximations. We show that by using a parameter <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="158" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D706 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>λ</mi></math></mjx-assistive-mml></mjx-container>, similar to the trace decay parameter in TD(<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="159" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D706 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>λ</mi></math></mjx-assistive-mml></mjx-container>), we can systematically trade-off learned value estimates against the local Q-function approximations. We present a theoretical analysis that shows how error from inaccurate models in MPC and value function estimation in RL can be balanced. We further propose an algorithm that changes <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="160" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D706 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>λ</mi></math></mjx-assistive-mml></mjx-container> over time to reduce the dependence on MPC as our estimates of the value function improve, and test the efficacy our approach on challenging high-dimensional manipulation tasks with biased models in simulation. We demonstrate that our approach can obtain performance comparable with MPC with access to true dynamics even under severe model bias and is more sample efficient as compared to model-free RL.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A framework for blending model-predictive control and model-free value function learning to systematically trade-off bias due to approximate dynamics models and value functions learned from real data</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=RqCC_00Bg7V&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9YlaeLfuhJF" data-number="2132">
      <h4>
        <a href="https://openreview.net/forum?id=9YlaeLfuhJF">
            Model Patching: Closing the Subgroup Performance Gap with Data Augmentation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9YlaeLfuhJF" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Karan_Goel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Karan_Goel1">Karan Goel</a>, <a href="https://openreview.net/profile?id=~Albert_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Albert_Gu1">Albert Gu</a>, <a href="https://openreview.net/profile?id=~Yixuan_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yixuan_Li1">Yixuan Li</a>, <a href="https://openreview.net/profile?id=~Christopher_Re1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Re1">Christopher Re</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9YlaeLfuhJF-details-280" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9YlaeLfuhJF-details-280"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Robust Machine Learning, Data Augmentation, Consistency Training, Invariant Representations</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Classifiers in machine learning are often brittle when deployed. Particularly concerning are models with inconsistent performance on specific subgroups of a class, e.g., exhibiting disparities in skin cancer classification in the presence or absence of a spurious bandage. To mitigate these performance differences, we introduce model patching, a two-stage framework for improving robustness that encourages the model to be invariant to subgroup differences, and focus on class information shared by subgroups. Model patching first models subgroup features within a class and learns semantic transformations between them, and then trains a classifier with data augmentations that deliberately manipulate subgroup features. We instantiate model patching with CAMEL, which (1) uses a CycleGAN to learn the intra-class, inter-subgroup augmentations, and (2) balances subgroup performance using a theoretically-motivated subgroup consistency regularizer, accompanied by a new robust objective. We demonstrate CAMEL’s effectiveness on 3 benchmark datasets, with reductions in robust error of up to 33% relative to the best baseline. Lastly, CAMEL successfully patches a model that fails due to spurious features on a real-world skin cancer dataset.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We describe how to fix classifiers that fail on subgroups of a class using a combination of learned data augmentation &amp; consistency training to achieve subgroup invariance.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=9YlaeLfuhJF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="dKg5D1Z1Lm" data-number="2135">
      <h4>
        <a href="https://openreview.net/forum?id=dKg5D1Z1Lm">
            Non-asymptotic Confidence Intervals of Off-policy Evaluation:  Primal and Dual Bounds 
        </a>
      
        
          <a href="https://openreview.net/pdf?id=dKg5D1Z1Lm" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yihao_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yihao_Feng1">Yihao Feng</a>, <a href="https://openreview.net/profile?id=~Ziyang_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziyang_Tang1">Ziyang Tang</a>, <a href="https://openreview.net/profile?email=zhangna%40pbcsf.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhangna@pbcsf.tsinghua.edu.cn">na zhang</a>, <a href="https://openreview.net/profile?id=~qiang_liu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~qiang_liu4">qiang liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#dKg5D1Z1Lm-details-43" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dKg5D1Z1Lm-details-43"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Non-asymptotic Confidence Intervals, Off Policy Evaluation, Reinforcement Learnings</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Off-policy evaluation (OPE) is the task of estimating the expected reward of a given policy based on offline data previously collected under different policies. Therefore, OPE is a key step in applying reinforcement learning to real-world domains such as medical treatment, where interactive data collection is expensive or even unsafe. As the observed data tends to be noisy and limited, it is essential to provide rigorous  uncertainty quantification, not just a point estimation, when applying OPE to make high stakes decisions. This work considers the problem of constructing non-asymptotic confidence intervals in infinite-horizon  off-policy evaluation, which remains a challenging open question. We develop a practical algorithm through a primal-dual optimization-based approach, which leverages the kernel Bellman loss (KBL) of Feng et al. 2019 and a new  martingale concentration inequality of KBL applicable to time-dependent data with unknown mixing conditions. Our algorithm makes  minimum assumptions on the data and the function class of the Q-function,  and works for the behavior-agnostic settings where the data is collected under a mix of arbitrary unknown behavior policies.  We present empirical results that clearly demonstrate the advantages of our approach over existing methods.
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an approach  to constructing non-asymptotic confidence intervals of off-policy estimation.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Fmg_fQYUejf" data-number="3051">
      <h4>
        <a href="https://openreview.net/forum?id=Fmg_fQYUejf">
            Linear Mode Connectivity in Multitask and Continual Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Fmg_fQYUejf" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=seyediman.mirzadeh%40wsu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="seyediman.mirzadeh@wsu.edu">Seyed Iman Mirzadeh</a>, <a href="https://openreview.net/profile?id=~Mehrdad_Farajtabar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mehrdad_Farajtabar1">Mehrdad Farajtabar</a>, <a href="https://openreview.net/profile?id=~Dilan_Gorur1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dilan_Gorur1">Dilan Gorur</a>, <a href="https://openreview.net/profile?id=~Razvan_Pascanu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Razvan_Pascanu1">Razvan Pascanu</a>, <a href="https://openreview.net/profile?id=~Hassan_Ghasemzadeh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hassan_Ghasemzadeh1">Hassan Ghasemzadeh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Fmg_fQYUejf-details-93" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Fmg_fQYUejf-details-93"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">continual learning, catastrophic forgetting, mode connectivity, multitask learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Continual (sequential) training and multitask (simultaneous) training are often attempting to solve the same overall objective: to find a solution that performs well on all considered tasks. The main difference is in the training regimes, where continual learning can only have access to one task at a time, which for neural networks typically leads to catastrophic forgetting. That is, the solution found for a subsequent task does not perform well on the previous ones anymore. 
          However, the relationship between the different minima that the two training regimes arrive at is not well understood. What sets them apart? Is there a local structure that could explain the difference in performance achieved by the two different schemes? 
          Motivated by recent work showing that different minima of the same task are typically connected by very simple curves of low error, we investigate whether multitask and continual solutions are similarly connected. We empirically find that indeed such connectivity can be reliably achieved and, more interestingly, it can be done by a linear path, conditioned on having the same initialization for both. We thoroughly analyze this observation and discuss its significance for the continual learning process.
          Furthermore, we exploit this finding to propose an effective algorithm that constrains the sequentially learned minima to behave as the multitask solution.  We show that our method outperforms several state of the art continual learning algorithms on various vision benchmarks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that continual and multitask minima are connected by linear low-error paths and design an effective continual learning algorithm that exploits this property.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="BVSM0x3EDK6" data-number="1004">
      <h4>
        <a href="https://openreview.net/forum?id=BVSM0x3EDK6">
            Robust and Generalizable Visual Representation Learning via Random Convolutions
        </a>
      
        
          <a href="https://openreview.net/pdf?id=BVSM0x3EDK6" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhenlin_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhenlin_Xu1">Zhenlin Xu</a>, <a href="https://openreview.net/profile?id=~Deyi_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deyi_Liu1">Deyi Liu</a>, <a href="https://openreview.net/profile?id=~Junlin_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junlin_Yang1">Junlin Yang</a>, <a href="https://openreview.net/profile?id=~Colin_Raffel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Colin_Raffel1">Colin Raffel</a>, <a href="https://openreview.net/profile?id=~Marc_Niethammer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marc_Niethammer1">Marc Niethammer</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#BVSM0x3EDK6-details-341" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="BVSM0x3EDK6-details-341"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">domain generalization, robustness, representation learning, data augmentation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While successful for various computer vision tasks, deep neural networks have shown to be vulnerable to texture style shifts and small perturbations to which humans are robust. In this work, we show that the robustness of neural networks can be greatly improved through the use of random convolutions as data augmentation. Random convolutions are approximately shape-preserving and may distort local textures. Intuitively, randomized convolutions create an infinite number of new domains with similar global shapes but random local texture. Therefore, we explore using outputs of multi-scale random convolutions as new images or mixing them with the original images during training. When applying a network trained with our approach to unseen domains, our method consistently improves the performance on domain generalization benchmarks and is scalable to ImageNet. In particular, in the challenging scenario of generalizing to the sketch domain in PACS and to ImageNet-Sketch, our method outperforms state-of-art methods by a large margin. More interestingly, our method can benefit downstream tasks by providing a more robust pretrained visual representation.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We use random convolutions as data augmentation to train robust visual representation that generalize to new domains.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="l0mSUROpwY" data-number="1870">
      <h4>
        <a href="https://openreview.net/forum?id=l0mSUROpwY">
            Intrinsic-Extrinsic Convolution and Pooling for Learning on 3D Protein Structures
        </a>
      
        
          <a href="https://openreview.net/pdf?id=l0mSUROpwY" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Pedro_Hermosilla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pedro_Hermosilla1">Pedro Hermosilla</a>, <a href="https://openreview.net/profile?email=marco.schaefer%40uni-tuebingen.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="marco.schaefer@uni-tuebingen.de">Marco Schäfer</a>, <a href="https://openreview.net/profile?email=242528%40mail.muni.cz" class="profile-link" data-toggle="tooltip" data-placement="top" title="242528@mail.muni.cz">Matej Lang</a>, <a href="https://openreview.net/profile?email=gloria.fackelmann%40uni-ulm.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="gloria.fackelmann@uni-ulm.de">Gloria Fackelmann</a>, <a href="https://openreview.net/profile?email=pere.pau%40cs.upc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="pere.pau@cs.upc.edu">Pere-Pau Vázquez</a>, <a href="https://openreview.net/profile?email=kozlikova%40fi.muni.cz" class="profile-link" data-toggle="tooltip" data-placement="top" title="kozlikova@fi.muni.cz">Barbora Kozlikova</a>, <a href="https://openreview.net/profile?email=michael.krone%40uni-tuebingen.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="michael.krone@uni-tuebingen.de">Michael Krone</a>, <a href="https://openreview.net/profile?id=~Tobias_Ritschel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tobias_Ritschel1">Tobias Ritschel</a>, <a href="https://openreview.net/profile?id=~Timo_Ropinski2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Timo_Ropinski2">Timo Ropinski</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#l0mSUROpwY-details-818" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="l0mSUROpwY-details-818"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">classification, bioinformatics</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Proteins perform a large variety of functions in living organisms and thus play a key role in biology. However, commonly used algorithms in protein representation learning were not specifically designed for protein data, and are therefore not able to capture all relevant structural levels of a protein during learning. To fill this gap, we propose two new learning operators, specifically designed to process protein structures. First, we introduce a novel convolution operator that considers the primary, secondary, and tertiary structure of a protein by using <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="161" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container>-D convolutions defined on both the Euclidean distance, as well as multiple geodesic distances between the atoms in a multi-graph. Second, we introduce a set of hierarchical pooling operators that enable multi-scale protein analysis. We further evaluate the accuracy of our algorithms on common downstream tasks, where we outperform state-of-the-art protein learning algorithms.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a new neural network architecture to process 3D protein structures.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=l0mSUROpwY&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="XAS3uKeFWj" data-number="792">
      <h4>
        <a href="https://openreview.net/forum?id=XAS3uKeFWj">
            Variational State-Space Models for Localisation and Dense 3D Mapping in 6 DoF
        </a>
      
        
          <a href="https://openreview.net/pdf?id=XAS3uKeFWj" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Atanas_Mirchev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Atanas_Mirchev1">Atanas Mirchev</a>, <a href="https://openreview.net/profile?id=~Baris_Kayalibay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Baris_Kayalibay1">Baris Kayalibay</a>, <a href="https://openreview.net/profile?id=~Patrick_van_der_Smagt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Patrick_van_der_Smagt1">Patrick van der Smagt</a>, <a href="https://openreview.net/profile?id=~Justin_Bayer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Justin_Bayer1">Justin Bayer</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#XAS3uKeFWj-details-619" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XAS3uKeFWj-details-619"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Generative models, Bayesian inference, Variational inference, SLAM, Deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We solve the problem of 6-DoF localisation and 3D dense reconstruction in spatial environments as approximate Bayesian inference in a deep state-space model. Our approach leverages both learning and domain knowledge from multiple-view geometry and rigid-body dynamics. This results in an expressive predictive model of the world, often missing in current state-of-the-art visual SLAM solutions. The combination of variational inference, neural networks and a differentiable raycaster ensures that our model is amenable to end-to-end gradient-based optimisation. We evaluate our approach on realistic unmanned aerial vehicle flight data, nearing the performance of state-of-the-art visual-inertial odometry systems. We demonstrate the applicability of the model to generative prediction and planning.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a variational state-space model with a latent map for 6-DoF localisation, 3D dense mapping and generative modelling for planning.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=XAS3uKeFWj&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Iz3zU3M316D" data-number="279">
      <h4>
        <a href="https://openreview.net/forum?id=Iz3zU3M316D">
            AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Iz3zU3M316D" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Byeongho_Heo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Byeongho_Heo1">Byeongho Heo</a>, <a href="https://openreview.net/profile?id=~Sanghyuk_Chun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanghyuk_Chun1">Sanghyuk Chun</a>, <a href="https://openreview.net/profile?id=~Seong_Joon_Oh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seong_Joon_Oh1">Seong Joon Oh</a>, <a href="https://openreview.net/profile?id=~Dongyoon_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dongyoon_Han1">Dongyoon Han</a>, <a href="https://openreview.net/profile?id=~Sangdoo_Yun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sangdoo_Yun1">Sangdoo Yun</a>, <a href="https://openreview.net/profile?id=~Gyuwan_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gyuwan_Kim1">Gyuwan Kim</a>, <a href="https://openreview.net/profile?id=~Youngjung_Uh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youngjung_Uh2">Youngjung Uh</a>, <a href="https://openreview.net/profile?id=~Jung-Woo_Ha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jung-Woo_Ha1">Jung-Woo Ha</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>24 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Iz3zU3M316D-details-459" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Iz3zU3M316D-details-459"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">momentum optimizer, scale-invariant weights, normalize layer, effective learning rate</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Normalization techniques, such as batch normalization (BN), are a boon for modern deep learning. They let weights converge more quickly with often better generalization performances. It has been argued that the normalization-induced scale invariance among the weights provides an advantageous ground for gradient descent (GD) optimizers: the effective step sizes are automatically reduced over time, stabilizing the overall training procedure. It is often overlooked, however, that the additional introduction of momentum in GD optimizers results in a far more rapid reduction in effective step sizes for scale-invariant weights, a phenomenon that has not yet been studied and may have caused unwanted side effects in the current practice. This is a crucial issue because arguably the vast majority of modern deep neural networks consist of (1) momentum-based GD (e.g. SGD or Adam) and (2) scale-invariant parameters (e.g. more than 90% of the weights in ResNet are scale-invariant due to BN). In this paper, we verify that the widely-adopted combination of the two ingredients lead to the premature decay of effective step sizes and sub-optimal model performances. We propose a simple and effective remedy, SGDP and AdamP: get rid of the radial component, or the norm-increasing direction, at each optimizer step. Because of the scale invariance, this modification only alters the effective step sizes without changing the effective update directions, thus enjoying the original convergence properties of GD optimizers. Given the ubiquity of momentum GD and scale invariance in machine learning, we have evaluated our methods against the baselines on 13 benchmarks. They range from vision tasks like classification (e.g. ImageNet), retrieval (e.g. CUB and SOP), and detection (e.g. COCO) to language modelling (e.g. WikiText) and audio classification (e.g. DCASE) tasks. We verify that our solution brings about uniform gains in performances in those benchmarks. Source code is available at https://github.com/clovaai/adamp</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Iz3zU3M316D&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="gV3wdEOGy_V" data-number="1026">
      <h4>
        <a href="https://openreview.net/forum?id=gV3wdEOGy_V">
            MiCE: Mixture of Contrastive Experts for Unsupervised Image Clustering
        </a>
      
        
          <a href="https://openreview.net/pdf?id=gV3wdEOGy_V" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tsung_Wei_Tsai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tsung_Wei_Tsai1">Tsung Wei Tsai</a>, <a href="https://openreview.net/profile?id=~Chongxuan_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongxuan_Li1">Chongxuan Li</a>, <a href="https://openreview.net/profile?id=~Jun_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jun_Zhu2">Jun Zhu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 14 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#gV3wdEOGy_V-details-223" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gV3wdEOGy_V-details-223"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">unsupervised learning, clustering, self supervised learning, mixture of experts</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present Mixture of Contrastive Experts (MiCE), a unified probabilistic clustering framework that simultaneously exploits the discriminative representations learned by contrastive learning and the semantic structures captured by a latent mixture model. Motivated by the mixture of experts, MiCE employs a gating function to partition an unlabeled dataset into subsets according to the latent semantics and multiple experts to discriminate distinct subsets of instances assigned to them in a contrastive learning manner. To solve the nontrivial inference and learning problems caused by the latent variables, we further develop a scalable variant of the Expectation-Maximization (EM) algorithm for MiCE and provide proof of the convergence. Empirically, we evaluate the clustering performance of MiCE on four widely adopted natural image datasets. MiCE achieves significantly better results than various previous methods and a strong contrastive learning baseline.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A principled probabilistic clustering method that exploits the discriminative representations learned by contrastive learning and the semantic structures captured by a latent mixture model in a unified framework.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=gV3wdEOGy_V&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9GBZBPn0Jx" data-number="2202">
      <h4>
        <a href="https://openreview.net/forum?id=9GBZBPn0Jx">
            HalentNet: Multimodal Trajectory Forecasting with Hallucinative Intents
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9GBZBPn0Jx" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Deyao_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deyao_Zhu1">Deyao Zhu</a>, <a href="https://openreview.net/profile?id=~Mohamed_Zahran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohamed_Zahran1">Mohamed Zahran</a>, <a href="https://openreview.net/profile?id=~Li_Erran_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Li_Erran_Li1">Li Erran Li</a>, <a href="https://openreview.net/profile?id=~Mohamed_Elhoseiny1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohamed_Elhoseiny1">Mohamed Elhoseiny</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9GBZBPn0Jx-details-943" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9GBZBPn0Jx-details-943"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Motion forecasting is essential for making intelligent decisions in robotic navigation. As a result, the multi-agent behavioral prediction has become a core component of modern human-robot interaction applications such as autonomous driving. Due to various intentions and interactions among agents, agent trajectories can have multiple possible futures. Hence, the motion forecasting model's ability to cover possible modes becomes essential to enable accurate prediction. Towards this goal, we introduce HalentNet to better model the future motion distribution in addition to a traditional trajectory regression learning objective by incorporating generative augmentation losses. We model intents with unsupervised discrete random variables whose training is guided by a collaboration between two key signals: A discriminative loss that encourages intents' diversity and a hallucinative loss that explores intent transitions (i.e., mixed intents) and encourages their smoothness. This regulates the neural network behavior to be more accurately predictive on uncertain scenarios due to the active yet careful exploration of possible future agent behavior. Our model's learned representation leads to better and more semantically meaningful coverage of the trajectory distribution. Our experiments show that our method can improve over the state-of-the-art trajectory forecasting benchmarks, including vehicles and pedestrians, for about 20% on average FDE and 50% on road boundary violation rate when predicting 6 seconds future. We also conducted human experiments to show that our predicted trajectories received 39.6% more votes than the runner-up approach and 32.2% more votes than our variant without hallucinative mixed intent loss. The code will be released soon. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="p5uylG94S68" data-number="3299">
      <h4>
        <a href="https://openreview.net/forum?id=p5uylG94S68">
            Model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=p5uylG94S68" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Bal%C3%A1zs_K%C3%A9gl2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Balázs_Kégl2">Balázs Kégl</a>, <a href="https://openreview.net/profile?email=gabriel.j.hurtado%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="gabriel.j.hurtado@gmail.com">Gabriel Hurtado</a>, <a href="https://openreview.net/profile?id=~Albert_Thomas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Albert_Thomas1">Albert Thomas</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>23 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#p5uylG94S68-details-35" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="p5uylG94S68-details-35"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">model-based reinforcement learning, generative models, mixture density nets, dynamic systems, heteroscedasticity</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We contribute to micro-data model-based reinforcement learning (MBRL) by rigorously comparing popular generative models using a fixed (random shooting) control agent. We find that on an environment that requires multimodal posterior predictives, mixture density nets outperform all other models by a large margin. When multimodality is not required, our surprising finding is that we do not need probabilistic posterior predictives: deterministic models are on par, in fact they consistently (although non-significantly) outperform their probabilistic counterparts. We also found that heteroscedasticity at training time, perhaps acting as a regularizer, improves predictions at longer horizons. At the methodological side, we design metrics and an experimental protocol which can be used to evaluate the various models, predicting their asymptotic performance when using them on the control problem. Using this framework, we improve the state-of-the-art sample complexity of MBRL on Acrobot by two to four folds, using an aggressive training schedule which is outside of the hyperparameter interval usually considered.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Crucial model properties for model-based reinforcement learning: multi-modal posterior predictives and heteroscedasticity.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="y06VOYLcQXa" data-number="2890">
      <h4>
        <a href="https://openreview.net/forum?id=y06VOYLcQXa">
            Private Image Reconstruction from System Side Channels Using Generative Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=y06VOYLcQXa" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yuanyuan_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuanyuan_Yuan1">Yuanyuan Yuan</a>, <a href="https://openreview.net/profile?id=~Shuai_Wang7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuai_Wang7">Shuai Wang</a>, <a href="https://openreview.net/profile?id=~Junping_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junping_Zhang2">Junping Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 04 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#y06VOYLcQXa-details-3" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="y06VOYLcQXa-details-3"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">side channel analysis</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">System side channels denote effects imposed on the underlying system and hardware when running a program, such as its accessed CPU cache lines. Side channel analysis (SCA) allows attackers to infer program secrets based on observed side channel signals. Given the ever-growing adoption of machine learning as a service (MLaaS), image analysis software on cloud platforms has been exploited by reconstructing private user images from system side channels. Nevertheless, to date, SCA is still highly challenging, requiring technical knowledge of victim software's internal operations. For existing SCA attacks, comprehending such internal operations requires heavyweight program analysis or manual efforts.
      
      This research proposes an attack framework to reconstruct private user images processed by media software via system side channels. The framework forms an effective workflow by incorporating convolutional networks, variational autoencoders, and generative adversarial networks. Our evaluation of two popular side channels shows that the reconstructed images consistently match user inputs, making privacy leakage attacks more practical. We also show surprising results that even one-bit data read/write pattern side channels, which are deemed minimally informative, can be used to reconstruct quality images using our framework.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present the first generative model-based side channel analysis (SCA) to reconstruct private user images.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="zx_uX-BO7CH" data-number="2729">
      <h4>
        <a href="https://openreview.net/forum?id=zx_uX-BO7CH">
            Contextual Transformation Networks for Online Continual Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=zx_uX-BO7CH" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Quang_Pham1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Quang_Pham1">Quang Pham</a>, <a href="https://openreview.net/profile?id=~Chenghao_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chenghao_Liu1">Chenghao Liu</a>, <a href="https://openreview.net/profile?id=~Doyen_Sahoo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Doyen_Sahoo1">Doyen Sahoo</a>, <a href="https://openreview.net/profile?id=~Steven_HOI1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_HOI1">Steven HOI</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#zx_uX-BO7CH-details-267" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zx_uX-BO7CH-details-267"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Continual Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Continual learning methods with fixed architectures rely on a single network to learn models that can perform well on all tasks.
      As a result, they often only accommodate common features of those tasks but neglect each task's specific features. On the other hand, dynamic architecture methods can have a separate network for each task, but they are too expensive to train and not scalable in practice, especially in online settings.
      To address this problem, we propose a novel online continual learning method named ``Contextual Transformation Networks” (CTN) to efficiently model the \emph{task-specific features} while enjoying neglectable complexity overhead compared to other fixed architecture methods. 
      Moreover, inspired by the Complementary Learning Systems (CLS) theory, we propose a novel dual memory design and an objective to train CTN that can address both catastrophic forgetting and knowledge transfer simultaneously. 
      Our extensive experiments show that CTN is competitive with a large scale dynamic architecture network and consistently outperforms other fixed architecture methods under the same standard backbone. Our implementation can be found at \url{https://github.com/phquang/Contextual-Transformation-Network}.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper develops a novel method that can model task-specific features with minimal complexity overhead.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="X76iqnUbBjz" data-number="273">
      <h4>
        <a href="https://openreview.net/forum?id=X76iqnUbBjz">
            A Unified Approach to Interpreting and Boosting Adversarial Transferability
        </a>
      
        
          <a href="https://openreview.net/pdf?id=X76iqnUbBjz" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xin_Wang25" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xin_Wang25">Xin Wang</a>, <a href="https://openreview.net/profile?id=~Jie_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jie_Ren1">Jie Ren</a>, <a href="https://openreview.net/profile?id=~Shuyun_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuyun_Lin1">Shuyun Lin</a>, <a href="https://openreview.net/profile?id=~Xiangming_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiangming_Zhu2">Xiangming Zhu</a>, <a href="https://openreview.net/profile?id=~Yisen_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yisen_Wang1">Yisen Wang</a>, <a href="https://openreview.net/profile?id=~Quanshi_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Quanshi_Zhang1">Quanshi Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#X76iqnUbBjz-details-83" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="X76iqnUbBjz-details-83"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Adversarial Learning, Interpretability, Adversarial Transferability</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we use the interaction inside adversarial perturbations to explain and boost the adversarial transferability. We discover and prove the negative correlation between the adversarial transferability and the interaction inside adversarial perturbations. The negative correlation is further verified through different DNNs with various inputs. Moreover, this negative correlation can be regarded as a unified perspective to understand current transferability-boosting methods. To this end, we prove that some classic methods of enhancing the transferability essentially decease interactions inside adversarial perturbations. Based on this, we propose to directly penalize interactions during the attacking process, which significantly improves the adversarial transferability. We will release the code when the paper is accepted.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We prove the close relationship between the interaction and adversarial transferability, provide a unified explanation for previous transferability-boosting methods, and develop a loss to improve adversarial transferability.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="krz7T0xU9Z_" data-number="3072">
      <h4>
        <a href="https://openreview.net/forum?id=krz7T0xU9Z_">
            The inductive bias of ReLU networks on orthogonally separable data
        </a>
      
        
          <a href="https://openreview.net/pdf?id=krz7T0xU9Z_" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mary_Phuong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mary_Phuong1">Mary Phuong</a>, <a href="https://openreview.net/profile?id=~Christoph_H_Lampert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christoph_H_Lampert1">Christoph H Lampert</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#krz7T0xU9Z_-details-332" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="krz7T0xU9Z_-details-332"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">inductive bias, implicit bias, gradient descent, ReLU networks, max-margin, extremal sector</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the inductive bias of two-layer ReLU networks trained by gradient flow. We identify a class of easy-to-learn (`orthogonally separable') datasets, and characterise the solution that ReLU networks trained on such datasets converge to. Irrespective of network width, the solution turns out to be a combination of two max-margin classifiers: one corresponding to the positive data subset and one corresponding to the negative data subset.
      The proof is based on the recently introduced concept of extremal sectors, for which we prove a number of properties in the context of orthogonal separability. In particular, we prove stationarity of activation patterns from some time <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="162" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>T</mi></math></mjx-assistive-mml></mjx-container> onwards, which enables a reduction of the ReLU network to an ensemble of linear subnetworks.
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We characterise the function learnt by two-layer ReLU nets trained on orthogonally separable data.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Rd138pWXMvG" data-number="3112">
      <h4>
        <a href="https://openreview.net/forum?id=Rd138pWXMvG">
            A statistical theory of cold posteriors in deep neural networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Rd138pWXMvG" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Laurence_Aitchison1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Laurence_Aitchison1">Laurence Aitchison</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Rd138pWXMvG-details-114" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Rd138pWXMvG-details-114"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Bayesian inference, cold posteriors, sgld</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">To get Bayesian neural networks to perform comparably to standard neural networks it is usually necessary to artificially reduce uncertainty using a tempered or cold posterior. This is extremely concerning: if the prior is accurate, Bayes inference/decision theory is optimal, and any artificial changes to the posterior should harm performance. While this suggests that the prior may be at fault, here we argue that in fact, BNNs for image classification use the wrong likelihood. In particular, standard image benchmark datasets such as CIFAR-10 are carefully curated. We develop a generative model describing curation which gives a principled Bayesian account of cold posteriors, because the likelihood under this new generative model closely matches the tempered likelihoods used in past work.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We develop a generative model of dataset curation that explains the cold-posterior effect</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ipUPfYxWZvM" data-number="1853">
      <h4>
        <a href="https://openreview.net/forum?id=ipUPfYxWZvM">
            IOT: Instance-wise Layer Reordering for Transformer Structures
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ipUPfYxWZvM" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jinhua_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinhua_Zhu1">Jinhua Zhu</a>, <a href="https://openreview.net/profile?id=~Lijun_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lijun_Wu1">Lijun Wu</a>, <a href="https://openreview.net/profile?id=~Yingce_Xia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yingce_Xia1">Yingce Xia</a>, <a href="https://openreview.net/profile?id=~Shufang_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shufang_Xie1">Shufang Xie</a>, <a href="https://openreview.net/profile?id=~Tao_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Qin1">Tao Qin</a>, <a href="https://openreview.net/profile?id=~Wengang_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wengang_Zhou1">Wengang Zhou</a>, <a href="https://openreview.net/profile?id=~Houqiang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Houqiang_Li1">Houqiang Li</a>, <a href="https://openreview.net/profile?id=~Tie-Yan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tie-Yan_Liu1">Tie-Yan Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ipUPfYxWZvM-details-137" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ipUPfYxWZvM-details-137"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Layer order, Transformers, Instance-wise Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">With sequentially stacked self-attention, (optional) encoder-decoder attention, and feed-forward layers, Transformer achieves big success in natural language processing (NLP), and many variants have been proposed. Currently, almost all these models assume that the \emph{layer order} is fixed and kept the same across data samples. We observe that different data samples actually favor different orders of the layers. Based on this observation, in this work, we break the assumption of the fixed layer order in Transformer and introduce instance-wise layer reordering into model structure. Our Instance-wise Ordered Transformer (IOT) can model variant functions by reordered layers, which enables each sample to select the better one to improve the model performance under the constraint of almost same number of parameters. To achieve this, we introduce a light predictor with negligible parameter and inference cost to decide the most capable and favorable layer order for any input sequence. Experiments on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="163" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn></math></mjx-assistive-mml></mjx-container> tasks (neural machine translation, abstractive summarization, and code generation) and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="164" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c39"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>9</mn></math></mjx-assistive-mml></mjx-container> datasets demonstrate consistent improvements of our method. We further show that our method can also be applied to other architectures beyond Transformer. Our code is released at Github\footnote{\url{https://github.com/instance-wise-ordered-transformer/IOT}}.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="BXewfAYMmJw" data-number="233">
      <h4>
        <a href="https://openreview.net/forum?id=BXewfAYMmJw">
            Counterfactual Generative Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=BXewfAYMmJw" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Axel_Sauer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Axel_Sauer1">Axel Sauer</a>, <a href="https://openreview.net/profile?id=~Andreas_Geiger3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_Geiger3">Andreas Geiger</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#BXewfAYMmJw-details-194" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="BXewfAYMmJw-details-194"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Causality, Counterfactuals, Generative Models, Robustness, Image Classification, Data Augmentation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Neural networks are prone to learning shortcuts -- they often model simple correlations, ignoring more complex ones that potentially generalize better. Prior works on image classification show that instead of learning a connection to object shape, deep classifiers tend to exploit spurious correlations with low-level texture or the background for solving the classification task. In this work, we take a step towards more robust and interpretable classifiers that explicitly expose the task's causal structure. Building on current advances in deep generative modeling, we propose to decompose the image generation process into independent causal mechanisms that we train without direct supervision. By exploiting appropriate inductive biases, these mechanisms disentangle object shape, object texture, and background; hence, they allow for generating counterfactual images. We demonstrate the ability of our model to generate such images on MNIST and ImageNet. Further, we show that the counterfactual images can improve out-of-distribution robustness with a marginal drop in performance on the original classification task, despite being synthetic. Lastly, our generative model can be trained efficiently on a single GPU, exploiting common pre-trained models as inductive biases.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A generative model structured into independent causal mechanisms produces images for training invariant classifiers.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="de11dbHzAMF" data-number="367">
      <h4>
        <a href="https://openreview.net/forum?id=de11dbHzAMF">
            Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters &amp; Less Data
        </a>
      
        
          <a href="https://openreview.net/pdf?id=de11dbHzAMF" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jonathan_Pilault1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Pilault1">Jonathan Pilault</a>, <a href="https://openreview.net/profile?id=~Amine_El_hattami1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amine_El_hattami1">Amine El hattami</a>, <a href="https://openreview.net/profile?id=~Christopher_Pal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Pal1">Christopher Pal</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#de11dbHzAMF-details-384" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="de11dbHzAMF-details-384"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multi-Task Learning, Adaptive Learning, Transfer Learning, Natural Language Processing</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Multi-Task Learning (MTL) networks have emerged as a promising method for transferring learned knowledge across different tasks. However, MTL must deal with challenges such as: overfitting to low resource tasks, catastrophic forgetting, and negative task transfer, or learning interference. Often, in Natural Language Processing (NLP), a separate model per task is needed to obtain the best performance. However, many fine-tuning approaches are both parameter inefficient, i.e., potentially involving one new model per task, and highly susceptible to losing knowledge acquired during pretraining. We propose a novel Transformer based Adapter consisting of a new conditional attention mechanism as well as a set of task-conditioned modules that facilitate weight sharing. Through this construction, we achieve more efficient parameter sharing and mitigate forgetting by keeping half of the weights of a pretrained model fixed. We also use a new multi-task data sampling strategy to mitigate the negative effects of data imbalance across tasks. Using this approach, we are able to surpass single task fine-tuning methods while being parameter and data efficient (using around 66% of the data). Compared to other BERT Large methods on GLUE, our 8-task model surpasses other Adapter methods by 2.8% and our 24-task model outperforms by 0.7-1.0% models that use MTL and single task fine-tuning. We show that a larger variant of our single multi-task model approach performs competitively across 26 NLP tasks and yields state-of-the-art results on a number of test and development sets.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Can multi-task outperform single task fine-tuning? CA-MTL is a new method that shows that it is possible with task conditioned model adaption and uncertainty sampling.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=de11dbHzAMF&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="IMPnRXEWpvr" data-number="331">
      <h4>
        <a href="https://openreview.net/forum?id=IMPnRXEWpvr">
            Towards Impartial Multi-task Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=IMPnRXEWpvr" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Liyang_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liyang_Liu1">Liyang Liu</a>, <a href="https://openreview.net/profile?id=~Yi_Li15" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Li15">Yi Li</a>, <a href="https://openreview.net/profile?id=~Zhanghui_Kuang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhanghui_Kuang4">Zhanghui Kuang</a>, <a href="https://openreview.net/profile?id=~Jing-Hao_Xue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jing-Hao_Xue1">Jing-Hao Xue</a>, <a href="https://openreview.net/profile?id=~Yimin_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yimin_Chen1">Yimin Chen</a>, <a href="https://openreview.net/profile?id=~Wenming_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenming_Yang1">Wenming Yang</a>, <a href="https://openreview.net/profile?id=~Qingmin_Liao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qingmin_Liao1">Qingmin Liao</a>, <a href="https://openreview.net/profile?id=~Wayne_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wayne_Zhang2">Wayne Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#IMPnRXEWpvr-details-552" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IMPnRXEWpvr-details-552"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multi-task Learning, Impartial Learning, Scene Understanding</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Multi-task learning (MTL) has been widely used in representation learning. However, naively training all tasks simultaneously may lead to the partial training issue, where specific tasks are trained more adequately than others. In this paper, we propose to learn multiple tasks impartially. Specifically, for the task-shared parameters, we optimize the scaling factors via a closed-form solution, such that the aggregated gradient (sum of raw gradients weighted by the scaling factors) has equal projections onto individual tasks. For the task-specific parameters, we dynamically weigh the task losses so that all of them are kept at a comparable scale. Further, we find the above gradient balance and loss balance are complementary and thus propose a hybrid balance method to further improve the performance. Our impartial multi-task learning (IMTL) can be end-to-end trained without any heuristic hyper-parameter tuning, and is general to be applied on all kinds of losses without any distribution assumption. Moreover, our IMTL can converge to similar results even when the task losses are designed to have different scales, and thus it is scale-invariant. We extensively evaluate our IMTL on the standard MTL benchmarks including Cityscapes, NYUv2 and CelebA. It outperforms existing loss weighting methods under the same experimental settings.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an impartial multi-task learning method that treats all tasks equally without bias towards any task.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="SZ3wtsXfzQR" data-number="3384">
      <h4>
        <a href="https://openreview.net/forum?id=SZ3wtsXfzQR">
            Theoretical bounds on estimation error for meta-learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=SZ3wtsXfzQR" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~James_Lucas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_Lucas1">James Lucas</a>, <a href="https://openreview.net/profile?id=~Mengye_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mengye_Ren1">Mengye Ren</a>, <a href="https://openreview.net/profile?id=~Irene_Raissa_KAMENI_KAMENI1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Irene_Raissa_KAMENI_KAMENI1">Irene Raissa KAMENI KAMENI</a>, <a href="https://openreview.net/profile?id=~Toniann_Pitassi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Toniann_Pitassi1">Toniann Pitassi</a>, <a href="https://openreview.net/profile?id=~Richard_Zemel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Richard_Zemel1">Richard Zemel</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>43 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#SZ3wtsXfzQR-details-326" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SZ3wtsXfzQR-details-326"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">meta learning, few-shot, minimax risk, lower bounds, learning theory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Machine learning models have traditionally been developed under the assumption that the training and test distributions match exactly. However, recent success in few-shot learning and related problems are encouraging signs that these models can be adapted to more realistic settings where train and test distributions differ. Unfortunately, there is severely limited theoretical support for these algorithms and little is known about the difficulty of these problems. In this work, we provide novel information-theoretic lower-bounds on minimax rates of convergence for algorithms that are trained on data from multiple sources and tested on novel data. Our bounds depend intuitively on the information shared between sources of data, and characterize the difficulty of learning in this setting for arbitrary algorithms. We demonstrate these bounds on a hierarchical Bayesian model of meta-learning, computing both upper and lower bounds on parameter estimation via maximum-a-posteriori inference.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We prove novel minimax risk lower bounds and upper bounds for meta learners</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=SZ3wtsXfzQR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="QubpWYfdNry" data-number="1735">
      <h4>
        <a href="https://openreview.net/forum?id=QubpWYfdNry">
            Domain-Robust Visual Imitation Learning with Mutual Information Constraints
        </a>
      
        
          <a href="https://openreview.net/pdf?id=QubpWYfdNry" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Edoardo_Cetin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edoardo_Cetin1">Edoardo Cetin</a>, <a href="https://openreview.net/profile?id=~Oya_Celiktutan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Oya_Celiktutan2">Oya Celiktutan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#QubpWYfdNry-details-627" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QubpWYfdNry-details-627"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Imitation Learning, Reinforcement Learning, Observational Imitation, Third-Person Imitation, Mutual Information, Domain Adaption, Machine Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Human beings are able to understand objectives and learn by simply observing others perform a task. Imitation learning methods aim to replicate such capabilities, however, they generally depend on access to a full set of optimal states and actions taken with the agent's actuators and from the agent's point of view. In this paper, we introduce a new algorithm - called Disentangling Generative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of bypassing such constraints. Our algorithm enables autonomous agents to learn directly from high dimensional observations of an expert performing a task, by making use of adversarial learning with a latent representation inside the discriminator network. Such latent representation is regularized through mutual information constraints to incentivize learning only features that encode information about the completion levels of the task being demonstrated. This allows to obtain a shared feature space to successfully perform imitation while disregarding the differences between the expert's and the agent's domains. Empirically, our algorithm is able to efficiently imitate in a diverse range of control problems including balancing, manipulation and locomotive tasks, while being robust to various domain differences in terms of both environment appearance and agent embodiment.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Imitation of visual expert demonstrations robust to appearance and embodiment mismatch, working for high dimensional control problems.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=QubpWYfdNry&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="8qDwejCuCN" data-number="2425">
      <h4>
        <a href="https://openreview.net/forum?id=8qDwejCuCN">
            Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding
        </a>
      
        
          <a href="https://openreview.net/pdf?id=8qDwejCuCN" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sana_Tonekaboni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sana_Tonekaboni1">Sana Tonekaboni</a>, <a href="https://openreview.net/profile?email=biliary.colic%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="biliary.colic@gmail.com">Danny Eytan</a>, <a href="https://openreview.net/profile?id=~Anna_Goldenberg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anna_Goldenberg1">Anna Goldenberg</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#8qDwejCuCN-details-322" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8qDwejCuCN-details-322"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Time series are often complex and rich in information but sparsely labeled and therefore challenging to model. In this paper, we propose a self-supervised framework for learning robust and generalizable representations for time series. Our approach, called Temporal Neighborhood Coding (TNC), takes advantage of the local smoothness of a signal's generative process to define neighborhoods in time with stationary properties. Using a debiased contrastive objective, our framework learns time series representations by ensuring that in the encoding space, the distribution of signals from within a neighborhood is distinguishable from the distribution of non-neighboring signals. Our motivation stems from the medical field, where the ability to model the dynamic nature of time series data is especially valuable for identifying, tracking, and predicting the underlying patients' latent states in settings where labeling data is practically impossible. We compare our method to recently developed unsupervised representation learning approaches and demonstrate superior performance on clustering and classification tasks for multiple datasets.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">An unsupervised representation learning framework for high-dimensional non-stationary time series </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=8qDwejCuCN&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="5lhWG3Hj2By" data-number="1007">
      <h4>
        <a href="https://openreview.net/forum?id=5lhWG3Hj2By">
            Enforcing robust control guarantees within neural network policies
        </a>
      
        
          <a href="https://openreview.net/pdf?id=5lhWG3Hj2By" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Priya_L._Donti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Priya_L._Donti1">Priya L. Donti</a>, <a href="https://openreview.net/profile?email=mroderick%40cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mroderick@cmu.edu">Melrose Roderick</a>, <a href="https://openreview.net/profile?email=mahyarfa%40seas.upenn.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mahyarfa@seas.upenn.edu">Mahyar Fazlyab</a>, <a href="https://openreview.net/profile?id=~J_Zico_Kolter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~J_Zico_Kolter1">J Zico Kolter</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 22 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#5lhWG3Hj2By-details-31" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5lhWG3Hj2By-details-31"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">robust control, reinforcement learning, differentiable optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">When designing controllers for safety-critical systems, practitioners often face a challenging tradeoff between robustness and performance. While robust control methods provide rigorous guarantees on system stability under certain worst-case disturbances, they often yield simple controllers that perform poorly in the average (non-worst) case. In contrast, nonlinear control methods trained using deep learning have achieved state-of-the-art performance on many control tasks, but often lack robustness guarantees. In this paper, we propose a technique that combines the strengths of these two approaches: constructing a generic nonlinear control policy class, parameterized by neural networks, that nonetheless enforces the same provable robustness criteria as robust control. Specifically, our approach entails integrating custom convex-optimization-based projection layers into a neural network-based policy. We demonstrate the power of this approach on several domains, improving in average-case performance over existing robust control methods and in worst-case stability over (non-robust) deep RL methods.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We develop a generic nonlinear control policy class, parameterized by neural networks, that nonetheless enforces the same provable robustness criteria as robust control.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="OMizHuea_HB" data-number="249">
      <h4>
        <a href="https://openreview.net/forum?id=OMizHuea_HB">
            Active Contrastive Learning of Audio-Visual Video Representations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=OMizHuea_HB" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Shuang_Ma3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuang_Ma3">Shuang Ma</a>, <a href="https://openreview.net/profile?id=~Zhaoyang_Zeng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhaoyang_Zeng1">Zhaoyang Zeng</a>, <a href="https://openreview.net/profile?id=~Daniel_McDuff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_McDuff1">Daniel McDuff</a>, <a href="https://openreview.net/profile?id=~Yale_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yale_Song1">Yale Song</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#OMizHuea_HB-details-661" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OMizHuea_HB-details-661"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">self-supervised learning, contrastive representation learning, active learning, audio-visual representation, video recognition</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Contrastive learning has been shown to produce generalizable representations of audio and visual data by maximizing the lower bound on the mutual information (MI) between different views of an instance. However, obtaining a tight lower bound requires a sample size exponential in MI and thus a large set of negative samples. We can incorporate more samples by building a large queue-based dictionary, but there are theoretical limits to performance improvements even with a large number of negative samples. We hypothesize that random negative sampling leads to a highly redundant dictionary that results in suboptimal representations for downstream tasks. In this paper, we propose an active contrastive learning approach that builds an actively sampled dictionary with diverse and informative items, which improves the quality of negative samples and improves performances on tasks where there is high mutual information in the data, e.g., video classification. Our model achieves state-of-the-art performance on challenging audio and visual downstream benchmarks including UCF101, HMDB51 and ESC50. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an active learning approach to improve negative sampling for contrastive learning and demonstrate it on learning audio-visual representations from videos.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="6UdQLhqJyFD" data-number="1253">
      <h4>
        <a href="https://openreview.net/forum?id=6UdQLhqJyFD">
            Parameter Efficient Multimodal Transformers for Video Representation Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=6UdQLhqJyFD" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sangho_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sangho_Lee1">Sangho Lee</a>, <a href="https://openreview.net/profile?id=~Youngjae_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youngjae_Yu1">Youngjae Yu</a>, <a href="https://openreview.net/profile?id=~Gunhee_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gunhee_Kim1">Gunhee Kim</a>, <a href="https://openreview.net/profile?id=~Thomas_Breuel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Breuel1">Thomas Breuel</a>, <a href="https://openreview.net/profile?id=~Jan_Kautz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jan_Kautz1">Jan Kautz</a>, <a href="https://openreview.net/profile?id=~Yale_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yale_Song1">Yale Song</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#6UdQLhqJyFD-details-675" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6UdQLhqJyFD-details-675"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Self-supervised learning, audio-visual representation learning, video representation learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The recent success of Transformers in the language domain has motivated adapting it to a multimodal setting, where a new visual model is trained in tandem with an already pretrained language model. However, due to the excessive memory requirements from Transformers, existing work typically fixes the language model and train only the vision module, which limits its ability to learn cross-modal information in an end-to-end manner. In this work, we focus on reducing the parameters of multimodal Transformers in the context of audio-visual video representation learning. We alleviate the high memory requirement by sharing the parameters of Transformers across layers and modalities; we decompose the Transformer into modality-specific and modality-shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. We show that our approach reduces parameters of the Transformers up to 97%, allowing us to train our model end-to-end from scratch. We also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns together with the Transformers. To demonstrate our approach, we pretrain our model on 30-second clips (480 frames) from Kinetics-700 and transfer it to audio-visual classification tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a technique to reduce the number of parameters in multimodal BERT models up to 97% (from 128 million to 4 million parameters).</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="vXj_ucZQ4hA" data-number="2155">
      <h4>
        <a href="https://openreview.net/forum?id=vXj_ucZQ4hA">
            Robust Pruning at Initialization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=vXj_ucZQ4hA" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Soufiane_Hayou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soufiane_Hayou1">Soufiane Hayou</a>, <a href="https://openreview.net/profile?id=~Jean-Francois_Ton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jean-Francois_Ton1">Jean-Francois Ton</a>, <a href="https://openreview.net/profile?id=~Arnaud_Doucet2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arnaud_Doucet2">Arnaud Doucet</a>, <a href="https://openreview.net/profile?id=~Yee_Whye_Teh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yee_Whye_Teh1">Yee Whye Teh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#vXj_ucZQ4hA-details-455" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vXj_ucZQ4hA-details-455"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Pruning, Initialization, Compression</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Overparameterized Neural Networks (NN) display state-of-the-art performance. However, there is a growing need for smaller, energy-efficient, neural networks to be able to use machine learning applications on devices with limited computational resources. A popular approach consists of using pruning techniques. While these techniques have traditionally focused on pruning pre-trained NN (LeCun et al.,1990; Hassibi et al., 1993), recent work by Lee et al. (2018) has shown promising results when pruning at initialization. However, for Deep NNs, such procedures remain unsatisfactory as the resulting pruned networks can be difficult to train and, for instance, they do not prevent one layer from being fully pruned. In this paper, we provide a comprehensive theoretical analysis of Magnitude and Gradient based pruning at initialization and training of sparse architectures.  This allows us to propose novel principled approaches which we validate experimentally on a variety of NN architectures.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Making pruning at initialization robust to Gradient vanishing/exploding</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="OHgnfSrn2jv" data-number="1830">
      <h4>
        <a href="https://openreview.net/forum?id=OHgnfSrn2jv">
            Efficient Wasserstein Natural Gradients for Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=OHgnfSrn2jv" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ted_Moskovitz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ted_Moskovitz1">Ted Moskovitz</a>, <a href="https://openreview.net/profile?id=~Michael_Arbel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Arbel1">Michael Arbel</a>, <a href="https://openreview.net/profile?id=~Ferenc_Huszar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ferenc_Huszar1">Ferenc Huszar</a>, <a href="https://openreview.net/profile?id=~Arthur_Gretton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arthur_Gretton1">Arthur Gretton</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#OHgnfSrn2jv-details-610" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OHgnfSrn2jv-details-610"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A novel optimization approach is proposed for application to policy gradient methods and evolution strategies for reinforcement learning (RL). The procedure uses a computationally efficient \emph{Wasserstein natural gradient} (WNG) descent that takes advantage of the geometry induced by a Wasserstein penalty to speed optimization. This method follows the recent theme in RL of including divergence penalties in the objective to establish trust regions. Experiments on challenging tasks demonstrate improvements in both computational cost and performance over advanced baselines. 
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We develop novel, efficient estimators for the Wasserstein natural gradient applied to reinforcement learning that improve the efficiency and performance of advanced baselines.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=OHgnfSrn2jv&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="17VnwXYZyhH" data-number="807">
      <h4>
        <a href="https://openreview.net/forum?id=17VnwXYZyhH">
            Probing BERT in Hyperbolic Spaces
        </a>
      
        
          <a href="https://openreview.net/pdf?id=17VnwXYZyhH" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Boli_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boli_Chen1">Boli Chen</a>, <a href="https://openreview.net/profile?id=~Yao_Fu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yao_Fu3">Yao Fu</a>, <a href="https://openreview.net/profile?email=kunka.xgw%40taobao.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="kunka.xgw@taobao.com">Guangwei Xu</a>, <a href="https://openreview.net/profile?email=chengchen.xpj%40taobao.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="chengchen.xpj@taobao.com">Pengjun Xie</a>, <a href="https://openreview.net/profile?email=chuanqi.tcq%40alibaba-inc.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="chuanqi.tcq@alibaba-inc.com">Chuanqi Tan</a>, <a href="https://openreview.net/profile?email=chenmosha.cms%40alibaba-inc.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="chenmosha.cms@alibaba-inc.com">Mosha Chen</a>, <a href="https://openreview.net/profile?id=~Liping_Jing3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liping_Jing3">Liping Jing</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#17VnwXYZyhH-details-259" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="17VnwXYZyhH-details-259"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Hyperbolic, BERT, Probe, Syntax, Sentiment</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recently, a variety of probing tasks are proposed to discover linguistic properties learned in contextualized word embeddings. Many of these works implicitly assume these embeddings lay in certain metric spaces, typically the Euclidean space. This work considers a family of geometrically special spaces, the hyperbolic spaces, that exhibit better inductive biases for hierarchical structures and may better reveal linguistic hierarchies encoded in contextualized representations. We introduce a <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="165" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D450 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-utext variant="italic" style="font-size: 88.4%; padding: 0.848em 0px 0.226em; font-family: MJXZERO, serif; font-style: italic;">é</mjx-utext><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D45D TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D44F TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">Poincaré probe</mtext></math></mjx-assistive-mml></mjx-container>, a structural probe projecting these embeddings into a Poincaré subspace with explicitly defined hierarchies. We focus on two probing objectives: (a) dependency trees where the hierarchy is defined as head-dependent structures; (b) lexical sentiments where the hierarchy is defined as the polarity of words (positivity and negativity). We argue that a key desideratum of a probe is its sensitivity to the existence of linguistic structures. We apply our probes on BERT, a typical contextualized embedding model. In a syntactic subspace, our probe better recovers tree structures than Euclidean probes, revealing the possibility that the geometry of BERT syntax may not necessarily be Euclidean. In a sentiment subspace, we reveal two possible meta-embeddings for positive and negative sentiments and show how lexically-controlled contextualization would change the geometric localization of embeddings. We demonstrate the findings with our Poincaré probe via extensive experiments and visualization. Our results can be reproduced at https://github.com/FranxYao/PoincareProbe</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a Poincaré probe for finding syntax and sentiments from BERT in hyperbolic spaces.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=17VnwXYZyhH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="o81ZyBCojoA" data-number="2301">
      <h4>
        <a href="https://openreview.net/forum?id=o81ZyBCojoA">
            On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=o81ZyBCojoA" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ren_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ren_Wang1">Ren Wang</a>, <a href="https://openreview.net/profile?id=~Kaidi_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaidi_Xu1">Kaidi Xu</a>, <a href="https://openreview.net/profile?id=~Sijia_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sijia_Liu1">Sijia Liu</a>, <a href="https://openreview.net/profile?id=~Pin-Yu_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pin-Yu_Chen1">Pin-Yu Chen</a>, <a href="https://openreview.net/profile?id=~Tsui-Wei_Weng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tsui-Wei_Weng1">Tsui-Wei Weng</a>, <a href="https://openreview.net/profile?id=~Chuang_Gan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuang_Gan1">Chuang Gan</a>, <a href="https://openreview.net/profile?id=~Meng_Wang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Meng_Wang4">Meng Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#o81ZyBCojoA-details-319" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="o81ZyBCojoA-details-319"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="166" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c2D TEX-MI"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D467 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">meta-initialization</mtext></math></mjx-assistive-mml></mjx-container> of model parameters (that we call <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="167" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c2D TEX-MI"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">meta-model</mtext></math></mjx-assistive-mml></mjx-container>) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="168" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c><mjx-c class="mjx-c1D463 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D44F TEX-I"></mjx-c><mjx-c class="mjx-c1D462 TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">adversarial robustness</mtext></math></mjx-assistive-mml></mjx-container> can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we first study <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="169" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c><mjx-c class="mjx-c210E TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">when</mtext></math></mjx-assistive-mml></mjx-container> a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufficient to make robustness adapted to the task-specific fine-tuning stage even if the latter uses a standard training protocol. We also make additional justification on the acquired robustness adaptation by peering into the interpretability of neurons' activation maps. Furthermore, we investigate <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="170" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">how</mtext></math></mjx-assistive-mml></mjx-container> robust regularization can <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="171" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D453 TEX-I"></mjx-c><mjx-c class="mjx-c1D453 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D450 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">efficiently</mtext></math></mjx-assistive-mml></mjx-container> be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light fine-tuning. In particular, we for the first time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="LhY8QdUGSuw" data-number="2304">
      <h4>
        <a href="https://openreview.net/forum?id=LhY8QdUGSuw">
            Anatomy of Catastrophic Forgetting: Hidden Representations and Task Semantics
        </a>
      
        
          <a href="https://openreview.net/pdf?id=LhY8QdUGSuw" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Vinay_Venkatesh_Ramasesh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vinay_Venkatesh_Ramasesh1">Vinay Venkatesh Ramasesh</a>, <a href="https://openreview.net/profile?id=~Ethan_Dyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ethan_Dyer1">Ethan Dyer</a>, <a href="https://openreview.net/profile?id=~Maithra_Raghu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maithra_Raghu1">Maithra Raghu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#LhY8QdUGSuw-details-40" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LhY8QdUGSuw-details-40"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Catastrophic forgetting, continual learning, representation analysis, representation learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Catastrophic forgetting is a recurring challenge to developing versatile deep learning models. Despite its ubiquity, there is limited understanding of its connections to neural network (hidden) representations and task semantics. In this paper, we address this important knowledge gap. Through quantitative analysis of neural representations, we find that deeper layers are disproportionately responsible for forgetting, with sequential training resulting in an erasure of earlier task representational subspaces. Methods to mitigate forgetting stabilize these deeper layers, but show diversity on precise effects, with some increasing feature reuse while others store task representations orthogonally, preventing interference. These insights also enable the development of an analytic argument and empirical picture relating forgetting to task semantic similarity, where we find that maximal forgetting occurs for task sequences with intermediate similarity.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We study the layerwise change in representations due to catastrophic forgetting, and use our understanding to study how task similarity influences forgetting.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=LhY8QdUGSuw&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="OOsR8BzCnl5" data-number="591">
      <h4>
        <a href="https://openreview.net/forum?id=OOsR8BzCnl5">
            Trusted Multi-View Classification
        </a>
      
        
          <a href="https://openreview.net/pdf?id=OOsR8BzCnl5" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zongbo_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zongbo_Han1">Zongbo Han</a>, <a href="https://openreview.net/profile?id=~Changqing_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changqing_Zhang1">Changqing Zhang</a>, <a href="https://openreview.net/profile?id=~Huazhu_Fu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huazhu_Fu4">Huazhu Fu</a>, <a href="https://openreview.net/profile?id=~Joey_Tianyi_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joey_Tianyi_Zhou1">Joey Tianyi Zhou</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 05 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#OOsR8BzCnl5-details-21" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OOsR8BzCnl5-details-21"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Multi-view classification (MVC) generally focuses on improving classification accuracy by using information from different views, typically integrating them into a unified comprehensive representation for downstream tasks. However, it is also crucial to dynamically assess the quality of a view for different samples in order to provide reliable uncertainty estimations, which indicate whether predictions can be trusted. To this end, we propose a novel multi-view classification method, termed trusted multi-view classification, which provides a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The algorithm jointly utilizes multiple views to promote both classification reliability (uncertainty estimation during testing) and robustness (out-of-distribution-awareness during training) by integrating evidence from each view. To achieve this, the Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness for out-of-distribution samples. Extensive experimental results validate the effectiveness of the proposed model in accuracy, reliability and robustness.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=OOsR8BzCnl5&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="T6AxtOaWydQ" data-number="782">
      <h4>
        <a href="https://openreview.net/forum?id=T6AxtOaWydQ">
            <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="172" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi></math></mjx-assistive-mml></mjx-container>-Mix: A Domain-Agnostic Strategy for Contrastive Representation Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=T6AxtOaWydQ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kibok_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kibok_Lee1">Kibok Lee</a>, <a href="https://openreview.net/profile?email=yianz%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yianz@umich.edu">Yian Zhu</a>, <a href="https://openreview.net/profile?id=~Kihyuk_Sohn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kihyuk_Sohn1">Kihyuk Sohn</a>, <a href="https://openreview.net/profile?id=~Chun-Liang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chun-Liang_Li1">Chun-Liang Li</a>, <a href="https://openreview.net/profile?id=~Jinwoo_Shin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinwoo_Shin1">Jinwoo Shin</a>, <a href="https://openreview.net/profile?id=~Honglak_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Honglak_Lee2">Honglak Lee</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#T6AxtOaWydQ-details-885" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="T6AxtOaWydQ-details-885"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">self-supervised learning, unsupervised representation learning, contrastive representation learning, data augmentation, MixUp</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Contrastive representation learning has shown to be effective to learn representations from unlabeled data. However, much progress has been made in vision domains relying on data augmentations carefully designed using domain knowledge. In this work, we propose i-Mix, a simple yet effective domain-agnostic regularization strategy for improving contrastive representation learning. We cast contrastive learning as training a non-parametric classifier by assigning a unique virtual class to each data in a batch. Then, data instances are mixed in both the input and virtual label spaces, providing more augmented data during training. In experiments, we demonstrate that i-Mix consistently improves the quality of learned representations across domains, including image, speech, and tabular data. Furthermore, we confirm its regularization effect via extensive ablation studies across model and dataset sizes. The code is available at https://github.com/kibok90/imix.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose i-Mix, a simple yet effective domain-agnostic regularization strategy for improving contrastive representation learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="KTlJT1nof6d" data-number="1021">
      <h4>
        <a href="https://openreview.net/forum?id=KTlJT1nof6d">
            Initialization and Regularization of Factorized Neural Layers
        </a>
      
        
          <a href="https://openreview.net/pdf?id=KTlJT1nof6d" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mikhail_Khodak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mikhail_Khodak1">Mikhail Khodak</a>, <a href="https://openreview.net/profile?id=~Neil_A._Tenenholtz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neil_A._Tenenholtz1">Neil A. Tenenholtz</a>, <a href="https://openreview.net/profile?id=~Lester_Mackey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lester_Mackey1">Lester Mackey</a>, <a href="https://openreview.net/profile?id=~Nicolo_Fusi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicolo_Fusi1">Nicolo Fusi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#KTlJT1nof6d-details-70" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KTlJT1nof6d-details-70"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">model compression, knowledge distillation, multi-head attention, matrix factorization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Factorized layers—operations parameterized by products of two or more matrices—occur in a variety of deep learning contexts, including compressed model training, certain types of knowledge distillation, and multi-head self-attention architectures. We study how to initialize and regularize deep nets containing such layers, examining two simple, understudied schemes, spectral initialization and Frobenius decay, for improving their performance. The guiding insight is to design optimization routines for these networks that are as close as possible to that of their well-tuned, non-decomposed counterparts; we back this intuition with an analysis of how the initialization and regularization schemes impact training with gradient descent, drawing on modern attempts to understand the interplay of weight-decay and batch-normalization. Empirically, we highlight the benefits of spectral initialization and Frobenius decay across a variety of settings. In model compression, we show that they enable low-rank methods to significantly outperform both unstructured sparsity and tensor methods on the task of training low-memory residual networks; analogs of the schemes also improve the performance of tensor decomposition techniques. For knowledge distillation, Frobenius decay enables a simple, overcomplete baseline that yields a compact model from over-parameterized training without requiring retraining with or pruning a teacher network. Finally, we show how both schemes applied to multi-head attention lead to improved performance on both translation and unsupervised pre-training.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Principled initialization and regularization of factorized neural layers leads to strong performance in compression, knowledge distillation, and language modeling tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=KTlJT1nof6d&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="rABUmU3ulQh" data-number="151">
      <h4>
        <a href="https://openreview.net/forum?id=rABUmU3ulQh">
            Learning to Generate 3D Shapes with Generative Cellular Automata
        </a>
      
        
          <a href="https://openreview.net/pdf?id=rABUmU3ulQh" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Dongsu_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dongsu_Zhang1">Dongsu Zhang</a>, <a href="https://openreview.net/profile?id=~Changwoon_Choi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changwoon_Choi1">Changwoon Choi</a>, <a href="https://openreview.net/profile?email=whitealex95%40snu.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="whitealex95@snu.ac.kr">Jeonghwan Kim</a>, <a href="https://openreview.net/profile?id=~Young_Min_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Young_Min_Kim1">Young Min Kim</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 05 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#rABUmU3ulQh-details-272" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rABUmU3ulQh-details-272"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">3D generation, generative models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this work, we present a probabilistic 3D generative model, named Generative Cellular Automata, which is able to produce diverse and high quality shapes. We formulate the shape generation process as sampling from the transition kernel of a Markov chain, where the sampling chain eventually evolves to the full shape of the learned distribution. The transition kernel employs the local update rules of cellular automata, effectively reducing the search space in a high-resolution 3D grid space by exploiting the connectivity and sparsity of 3D shapes. Our progressive generation only focuses on the sparse set of occupied voxels and their neighborhood, thus enables the utilization of an expressive sparse convolutional network. We propose an effective training scheme to obtain the local homogeneous rule of generative cellular automata with sequences that are slightly different from the sampling chain but converge to the full shapes in the training data. Extensive experiments on probabilistic shape completion and shape generation demonstrate that our method achieves competitive performance against recent methods.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a Markov chain based 3D generative model named Generative Cellular Automata (GCA), that progressively evolves to a shape of the learned distribution using the local update rules of cellular automata.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=rABUmU3ulQh&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="jMPcEkJpdD" data-number="1222">
      <h4>
        <a href="https://openreview.net/forum?id=jMPcEkJpdD">
            Self-Supervised Learning of Compressed Video Representations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=jMPcEkJpdD" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Youngjae_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youngjae_Yu1">Youngjae Yu</a>, <a href="https://openreview.net/profile?id=~Sangho_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sangho_Lee1">Sangho Lee</a>, <a href="https://openreview.net/profile?id=~Gunhee_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gunhee_Kim1">Gunhee Kim</a>, <a href="https://openreview.net/profile?id=~Yale_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yale_Song1">Yale Song</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#jMPcEkJpdD-details-74" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jMPcEkJpdD-details-74"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Compressed videos, self-supervised learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Self-supervised learning of video representations has received great attention. Existing methods typically require frames to be decoded before being processed, which increases compute and storage requirements and ultimately hinders large-scale training. In this work, we propose an efficient self-supervised approach to learn video representations by eliminating the expensive decoding step. We use a three-stream video architecture that encodes I-frames and P-frames of a compressed video. Unlike existing approaches that encode I-frames and P-frames individually, we propose to jointly encode them by establishing bidirectional dynamic connections across streams. To enable self-supervised learning, we propose two pretext tasks that leverage the multimodal nature (RGB, motion vector, residuals) and the internal GOP structure of compressed videos. The first task asks our network to predict zeroth-order motion statistics in a spatio-temporal pyramid; the second task asks correspondence types between I-frames and P-frames after applying temporal transformations. We show that our approach achieves competitive performance on compressed video recognition both in supervised and self-supervised regimes. 
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a self-supervised approach to learning compressed video representations.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="VVdmjgu7pKM" data-number="2322">
      <h4>
        <a href="https://openreview.net/forum?id=VVdmjgu7pKM">
            Factorizing Declarative and Procedural Knowledge in Structured, Dynamical Environments
        </a>
      
        
          <a href="https://openreview.net/pdf?id=VVdmjgu7pKM" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Anirudh_Goyal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anirudh_Goyal1">Anirudh Goyal</a>, <a href="https://openreview.net/profile?id=~Alex_Lamb1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_Lamb1">Alex Lamb</a>, <a href="https://openreview.net/profile?email=gampa.phanideep.mat15%40itbhu.ac.in" class="profile-link" data-toggle="tooltip" data-placement="top" title="gampa.phanideep.mat15@itbhu.ac.in">Phanideep Gampa</a>, <a href="https://openreview.net/profile?id=~Philippe_Beaudoin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philippe_Beaudoin1">Philippe Beaudoin</a>, <a href="https://openreview.net/profile?id=~Charles_Blundell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Charles_Blundell1">Charles Blundell</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>, <a href="https://openreview.net/profile?id=~Yoshua_Bengio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoshua_Bengio1">Yoshua Bengio</a>, <a href="https://openreview.net/profile?id=~Michael_Curtis_Mozer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Curtis_Mozer1">Michael Curtis Mozer</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#VVdmjgu7pKM-details-321" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="VVdmjgu7pKM-details-321"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">procedural knowledge, declarative knowledge, Systematicity</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Modeling a structured, dynamic environment like a video game requires keeping track of the objects and their states (declarative knowledge) as well as predicting how objects behave (procedural knowledge). Black-box models with a monolithic hidden state often fail to apply procedural knowledge consistently and uniformly, i.e., they lack systematicity. For example, in a video game, correct prediction of one enemy's trajectory does not ensure correct prediction of another's. We address this issue via an architecture that factorizes declarative and procedural knowledge and that imposes modularity within each form of knowledge. The architecture consists of active modules called object files that maintain the state of a single object and invoke passive external knowledge sources called schemata that prescribe state updates. To use a video game as an illustration, two enemies of the same type will share schemata but will have separate object files to encode their distinct state (e.g., health, position). We propose to use attention to determine which object files to update, the selection of schemata, and the propagation of information between object files. The resulting architecture is a drop-in replacement conforming to the same input-output interface as normal recurrent networks (e.g., LSTM, GRU) yet achieves substantially better generalization on environments that have multiple object tokens of the same type, including a challenging intuitive physics benchmark.
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We explore separate factorization of procedural and declarative knowledge in modular recurrent neural networks.  </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=VVdmjgu7pKM&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="bjkX6Kzb5H" data-number="2529">
      <h4>
        <a href="https://openreview.net/forum?id=bjkX6Kzb5H">
            Cut out the annotator, keep the cutout: better segmentation with weak supervision
        </a>
      
        
          <a href="https://openreview.net/pdf?id=bjkX6Kzb5H" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sarah_Hooper1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sarah_Hooper1">Sarah Hooper</a>, <a href="https://openreview.net/profile?email=mwornow%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mwornow@stanford.edu">Michael Wornow</a>, <a href="https://openreview.net/profile?email=yinghang%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yinghang@stanford.edu">Ying Hang Seah</a>, <a href="https://openreview.net/profile?email=kellmanp%40nhlbi.nih.gov" class="profile-link" data-toggle="tooltip" data-placement="top" title="kellmanp@nhlbi.nih.gov">Peter Kellman</a>, <a href="https://openreview.net/profile?email=hui.xue%40nih.gov" class="profile-link" data-toggle="tooltip" data-placement="top" title="hui.xue@nih.gov">Hui Xue</a>, <a href="https://openreview.net/profile?id=~Frederic_Sala1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frederic_Sala1">Frederic Sala</a>, <a href="https://openreview.net/profile?id=~Curtis_Langlotz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Curtis_Langlotz1">Curtis Langlotz</a>, <a href="https://openreview.net/profile?id=~Christopher_Re1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Re1">Christopher Re</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>19 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#bjkX6Kzb5H-details-937" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bjkX6Kzb5H-details-937"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Weak supervision, segmentation, CNN, latent variable, medical imaging</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Constructing large, labeled training datasets for segmentation models is an expensive and labor-intensive process. This is a common challenge in machine learning, addressed by methods that require few or no labeled data points such as few-shot learning (FSL) and weakly-supervised learning (WS). Such techniques, however, have limitations when applied to image segmentation---FSL methods often produce noisy results and are strongly dependent on which few datapoints are labeled, while WS models struggle to fully exploit rich image information. We propose a framework that fuses FSL and WS for segmentation tasks, enabling users to train high-performing segmentation networks with very few hand-labeled training points. We use FSL models as weak sources in a WS framework, requiring a very small set of reference labeled images, and introduce a new WS model that focuses on key areas---areas with contention among noisy labels---of the image to fuse these weak sources. Empirically, we evaluate our proposed approach over seven well-motivated segmentation tasks. We show that our methods can achieve within 1.4 Dice points compared to fully supervised networks while only requiring five hand-labeled training points. Compared to existing FSL methods, our approach improves performance by a mean 3.6 Dice points over the next-best method. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">In this work, we present a weak supervision approach for segmentation tasks, allowing users to train high-performing segmentation CNNs with very few hand-labeled training points.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="piLPYqxtWuA" data-number="1673">
      <h4>
        <a href="https://openreview.net/forum?id=piLPYqxtWuA">
            FastSpeech 2: Fast and High-Quality End-to-End Text to Speech
        </a>
      
        
          <a href="https://openreview.net/pdf?id=piLPYqxtWuA" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yi_Ren2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Ren2">Yi Ren</a>, <a href="https://openreview.net/profile?id=~Chenxu_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chenxu_Hu1">Chenxu Hu</a>, <a href="https://openreview.net/profile?id=~Xu_Tan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xu_Tan1">Xu Tan</a>, <a href="https://openreview.net/profile?id=~Tao_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Qin1">Tao Qin</a>, <a href="https://openreview.net/profile?email=sheng.zhao%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sheng.zhao@microsoft.com">Sheng Zhao</a>, <a href="https://openreview.net/profile?id=~Zhou_Zhao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhou_Zhao2">Zhou Zhao</a>, <a href="https://openreview.net/profile?id=~Tie-Yan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tie-Yan_Liu1">Tie-Yan Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 04 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#piLPYqxtWuA-details-149" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="piLPYqxtWuA-details-149"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">text to speech, speech synthesis, non-autoregressive generation, one-to-many mapping, end-to-end</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a non-autoregressive TTS model named FastSpeech 2 to better solve the one-to-many mapping problem in TTS and surpass autoregressive models in voice quality.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=piLPYqxtWuA&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Uu1Nw-eeTxJ" data-number="330">
      <h4>
        <a href="https://openreview.net/forum?id=Uu1Nw-eeTxJ">
            On Learning Universal Representations Across Languages
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Uu1Nw-eeTxJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xiangpeng_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiangpeng_Wei1">Xiangpeng Wei</a>, <a href="https://openreview.net/profile?email=wengrx%40alibaba-inc.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="wengrx@alibaba-inc.com">Rongxiang Weng</a>, <a href="https://openreview.net/profile?email=huyue%40iie.ac.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="huyue@iie.ac.cn">Yue Hu</a>, <a href="https://openreview.net/profile?email=xingluxi%40iie.ac.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="xingluxi@iie.ac.cn">Luxi Xing</a>, <a href="https://openreview.net/profile?email=yuheng.yh%40alibaba-inc.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yuheng.yh@alibaba-inc.com">Heng Yu</a>, <a href="https://openreview.net/profile?email=weihua.luowh%40alibaba-inc.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="weihua.luowh@alibaba-inc.com">Weihua Luo</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Uu1Nw-eeTxJ-details-938" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Uu1Nw-eeTxJ-details-938"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">universal representation learning, cross-lingual pretraining, hierarchical contrastive learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent studies have demonstrated the overwhelming advantage of cross-lingual pre-trained models (PTMs), such as multilingual BERT and XLM, on cross-lingual NLP tasks. However, existing approaches essentially capture the co-occurrence among tokens through involving the masked language model (MLM) objective with token-level cross entropy. In this work, we extend these approaches to learn sentence-level representations and show the effectiveness on cross-lingual understanding and generation. Specifically, we propose a Hierarchical Contrastive Learning (HiCTL) method to (1) learn universal representations for parallel sentences distributed in one or multiple languages and (2) distinguish the semantically-related words from a shared cross-lingual vocabulary for each sentence. We conduct evaluations on two challenging cross-lingual tasks, XTREME and machine translation. Experimental results show that the HiCTL outperforms the state-of-the-art XLM-R by an absolute gain of 4.2% accuracy on the XTREME benchmark as well as achieves substantial improvements on both of the high resource and low-resource English<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="173" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">→</mo></math></mjx-assistive-mml></mjx-container>X translation tasks over strong baselines.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">In this work, we extend pre-trained language models to learn universal representations among multiple languages, and show the effectiveness on cross-lingual understanding and generation.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="jxdXSW9Doc" data-number="1362">
      <h4>
        <a href="https://openreview.net/forum?id=jxdXSW9Doc">
            Effective Distributed Learning with Random Features: Improved Bounds and Algorithms
        </a>
      
        
          <a href="https://openreview.net/pdf?id=jxdXSW9Doc" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=liuyonggsai%40ruc.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="liuyonggsai@ruc.edu.cn">Yong Liu</a>, <a href="https://openreview.net/profile?email=liujiankun%40iie.ac.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="liujiankun@iie.ac.cn">Jiankun Liu</a>, <a href="https://openreview.net/profile?email=sq.wang%40siat.ac.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="sq.wang@siat.ac.cn">Shuqiang Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 20 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#jxdXSW9Doc-details-210" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jxdXSW9Doc-details-210"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Risk bound, statistical learning theory, kernel methods</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we study the statistical properties of distributed kernel ridge regression together with random features (DKRR-RF), and obtain optimal generalization bounds under the basic setting, which can substantially relax the restriction on the number of local machines in the existing state-of-art bounds. Specifically, we first show that the simple combination of divide-and-conquer technique and random features can achieve the same statistical accuracy as the exact KRR in expectation requiring only <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="174" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c44 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mo data-mjx-texclass="ORD" stretchy="false">|</mo><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">D</mi></mrow><mo data-mjx-texclass="ORD" stretchy="false">|</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> memory and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="175" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c44 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c7C"></mjx-c></mjx-mo><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mo data-mjx-texclass="ORD" stretchy="false">|</mo><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">D</mi></mrow><msup><mo data-mjx-texclass="ORD" stretchy="false">|</mo><mrow><mn>1.5</mn></mrow></msup><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> time. Then, beyond the generalization bounds in expectation that demonstrate the average information for multiple trails, we derive generalization bounds in probability to capture the learning performance for a single trail. Finally, we propose an effective communication strategy to further improve the performance of DKRR-RF, and validate the theoretical bounds via numerical experiments.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This papaer focuses on the studies of  the statistical properties of distributed KRR together with random features</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="AICNpd8ke-m" data-number="600">
      <h4>
        <a href="https://openreview.net/forum?id=AICNpd8ke-m">
            Multi-Class Uncertainty Calibration via Mutual Information Maximization-based Binning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=AICNpd8ke-m" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kanil_Patel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kanil_Patel1">Kanil Patel</a>, <a href="https://openreview.net/profile?id=~William_H._Beluch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_H._Beluch1">William H. Beluch</a>, <a href="https://openreview.net/profile?id=~Bin_Yang5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bin_Yang5">Bin Yang</a>, <a href="https://openreview.net/profile?id=~Michael_Pfeiffer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Pfeiffer1">Michael Pfeiffer</a>, <a href="https://openreview.net/profile?id=~Dan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Zhang1">Dan Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 08 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#AICNpd8ke-m-details-54" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AICNpd8ke-m-details-54"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">uncertainty calibration, post-hoc calibration, histogram binning, mutual information, deep neural networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Post-hoc multi-class calibration is a common approach for providing high-quality confidence estimates of deep neural network predictions. Recent work has shown that widely used scaling methods underestimate their calibration error, while alternative Histogram Binning (HB) methods often fail to preserve classification accuracy. When classes have small prior probabilities, HB also faces the issue of severe sample-inefficiency after the conversion into K one-vs-rest class-wise calibration problems. The goal of this paper is to resolve the identified issues of HB in order to provide calibrated confidence estimates using only a small holdout calibration dataset for bin optimization while preserving multi-class ranking accuracy. From an information-theoretic perspective, we derive the I-Max concept for binning, which maximizes the mutual information between labels and quantized logits. This concept mitigates potential loss in ranking performance due to lossy quantization, and by disentangling the optimization of bin edges and representatives allows simultaneous improvement of ranking and calibration performance. To improve the sample efficiency and estimates from a small calibration set, we propose a shared class-wise (sCW) calibration strategy, sharing one calibrator among similar classes (e.g., with similar class priors) so that the training sets of their class-wise calibration problems can be merged to train the single calibrator. The combination of sCW and I-Max binning outperforms the state of the art calibration methods on various evaluation metrics across different benchmark datasets and models, using a small calibration set (e.g., 1k samples for ImageNet).</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose I-Max binning, a novel method for multi-class calibration, improving over previous methods in terms of various ECE measures</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="27acGyyI1BY" data-number="3579">
      <h4>
        <a href="https://openreview.net/forum?id=27acGyyI1BY">
            Neural ODE Processes
        </a>
      
        
          <a href="https://openreview.net/pdf?id=27acGyyI1BY" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=alex.norcliffe98%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="alex.norcliffe98@gmail.com">Alexander Norcliffe</a>, <a href="https://openreview.net/profile?id=~Cristian_Bodnar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cristian_Bodnar1">Cristian Bodnar</a>, <a href="https://openreview.net/profile?id=~Ben_Day1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ben_Day1">Ben Day</a>, <a href="https://openreview.net/profile?email=jm2311%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="jm2311@cam.ac.uk">Jacob Moss</a>, <a href="https://openreview.net/profile?id=~Pietro_Li%C3%B21" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pietro_Liò1">Pietro Liò</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#27acGyyI1BY-details-772" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="27acGyyI1BY-details-772"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">differential equations, neural processes, dynamics, deep learning, neural ode</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Neural Ordinary Differential Equations (NODEs) use a neural network to model the instantaneous rate of change in the state of a system. However, despite their apparent suitability for dynamics-governed time-series, NODEs present a few disadvantages. First, they are unable to adapt to incoming data-points, a fundamental requirement for real-time applications imposed by the natural direction of time. Second, time-series are often composed of a sparse set of measurements that could be explained by many possible underlying dynamics. NODEs do not capture this uncertainty. In contrast, Neural Processes (NPs) are a new class of stochastic processes providing uncertainty estimation and fast data-adaptation, but lack an explicit treatment of the flow of time. To address these problems, we introduce Neural ODE Processes (NDPs), a new class of stochastic processes determined by a distribution over Neural ODEs. By maintaining an adaptive data-dependent distribution over the underlying ODE, we show that our model can successfully capture the dynamics of low-dimensional systems from just a few data-points. At the same time, we demonstrate that NDPs scale up to challenging high-dimensional time-series with unknown latent dynamics such as rotating MNIST digits. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Neural Processes with time-awareness</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="q-cnWaaoUTH" data-number="636">
      <h4>
        <a href="https://openreview.net/forum?id=q-cnWaaoUTH">
            Conformation-Guided Molecular Representation with Hamiltonian Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=q-cnWaaoUTH" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ziyao_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziyao_Li1">Ziyao Li</a>, <a href="https://openreview.net/profile?email=swyang%40pku.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="swyang@pku.edu.cn">Shuwen Yang</a>, <a href="https://openreview.net/profile?id=~Guojie_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guojie_Song1">Guojie Song</a>, <a href="https://openreview.net/profile?email=cailingsheng%40pku.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="cailingsheng@pku.edu.cn">Lingsheng Cai</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#q-cnWaaoUTH-details-717" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="q-cnWaaoUTH-details-717"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Molecular Representation, Neural Physics Engines, Molecular Dynamics, Graph Neural Networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Well-designed molecular representations (fingerprints) are vital to combine medical chemistry and deep learning. Whereas incorporating 3D geometry of molecules (i.e. conformations) in their representations seems beneficial, current 3D algorithms are still in infancy. In this paper, we propose a novel molecular representation algorithm which preserves 3D conformations of molecules with a Molecular Hamiltonian Network (HamNet). In HamNet, implicit positions and momentums of atoms in a molecule interact in the Hamiltonian Engine following the discretized Hamiltonian equations. These implicit coordinations are supervised with real conformations with translation- &amp; rotation-invariant losses, and further used as inputs to the Fingerprint Generator, a message-passing neural network. Experiments show that the Hamiltonian Engine can well preserve molecular conformations, and that the fingerprints generated by HamNet achieve state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a molecular representation algorithm, which preserves molecular conformations with a neural physics engine and generates fingerprints with an MPNN.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=q-cnWaaoUTH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tIjRAiFmU3y" data-number="1455">
      <h4>
        <a href="https://openreview.net/forum?id=tIjRAiFmU3y">
            An Unsupervised Deep Learning Approach for Real-World Image Denoising
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tIjRAiFmU3y" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Dihan_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dihan_Zheng1">Dihan Zheng</a>, <a href="https://openreview.net/profile?id=~Sia_Huat_Tan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sia_Huat_Tan1">Sia Huat Tan</a>, <a href="https://openreview.net/profile?id=~Xiaowen_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaowen_Zhang2">Xiaowen Zhang</a>, <a href="https://openreview.net/profile?id=~Zuoqiang_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zuoqiang_Shi1">Zuoqiang Shi</a>, <a href="https://openreview.net/profile?id=~Kaisheng_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaisheng_Ma1">Kaisheng Ma</a>, <a href="https://openreview.net/profile?id=~Chenglong_Bao3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chenglong_Bao3">Chenglong Bao</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 07 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tIjRAiFmU3y-details-541" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tIjRAiFmU3y-details-541"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Real-world image denoising, unsupervised image denoising</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Designing an unsupervised image denoising approach in practical applications is a challenging task due to the complicated data acquisition process. In the real-world case, the noise distribution is so complex that the simplified additive white Gaussian (AWGN) assumption rarely holds, which significantly deteriorates the Gaussian denoisers' performance. To address this problem, we apply a deep neural network that maps the noisy image into a latent space in which the AWGN assumption holds, and thus any existing Gaussian denoiser is applicable. More specifically, the proposed neural network consists of the encoder-decoder structure and approximates the likelihood term in the Bayesian framework. Together with a Gaussian denoiser, the neural network can be trained with the input image itself and does not require any pre-training in other datasets. Extensive experiments on real-world noisy image datasets have shown that the combination of neural networks and Gaussian denoisers improves the performance of the original Gaussian denoisers by a large margin. In particular, the neural network+BM3D method significantly outperforms other unsupervised denoising approaches and is competitive with supervised networks such as DnCNN, FFDNet, and CBDNet.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an unsupervised real-world image denoising approach that combines DNNs with classical MAP approaches.  </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=tIjRAiFmU3y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="1Jv6b0Zq3qi" data-number="1680">
      <h4>
        <a href="https://openreview.net/forum?id=1Jv6b0Zq3qi">
            Uncertainty in Gradient Boosting via Ensembles
        </a>
      
        
          <a href="https://openreview.net/pdf?id=1Jv6b0Zq3qi" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Andrey_Malinin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrey_Malinin1">Andrey Malinin</a>, <a href="https://openreview.net/profile?id=~Liudmila_Prokhorenkova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liudmila_Prokhorenkova1">Liudmila Prokhorenkova</a>, <a href="https://openreview.net/profile?email=austimenko%40yandex-team.ru" class="profile-link" data-toggle="tooltip" data-placement="top" title="austimenko@yandex-team.ru">Aleksei Ustimenko</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 24 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#1Jv6b0Zq3qi-details-323" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1Jv6b0Zq3qi-details-323"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">uncertainty, ensembles, gradient boosting, decision trees, knowledge uncertainty</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">For many practical, high-risk applications, it is essential to quantify uncertainty in a model's predictions to avoid costly mistakes. While predictive uncertainty is widely studied for neural networks, the topic seems to be under-explored for models based on gradient boosting. However, gradient boosting often achieves state-of-the-art results on tabular data. This work examines a probabilistic ensemble-based framework for deriving uncertainty estimates in the predictions of gradient boosting classification and regression models. We conducted experiments on a range of synthetic and real datasets and investigated the applicability of ensemble approaches to gradient boosting models that are themselves ensembles of decision trees. Our analysis shows that ensembles of gradient boosting models successfully detect anomalous inputs while having limited ability to improve the predicted total uncertainty. Importantly, we also propose a concept of a virtual ensemble to get the benefits of an ensemble via only one gradient boosting model, which significantly reduces complexity. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Propose and analyze an ensemble-based framework for deriving uncertainty estimates in GBDT models.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="oxnp2q-PGL4" data-number="3256">
      <h4>
        <a href="https://openreview.net/forum?id=oxnp2q-PGL4">
            Lossless Compression of Structured Convolutional Models via Lifting
        </a>
      
        
          <a href="https://openreview.net/pdf?id=oxnp2q-PGL4" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Gustav_Sourek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gustav_Sourek1">Gustav Sourek</a>, <a href="https://openreview.net/profile?email=zelezny%40fel.cvut.cz" class="profile-link" data-toggle="tooltip" data-placement="top" title="zelezny@fel.cvut.cz">Filip Zelezny</a>, <a href="https://openreview.net/profile?id=~Ondrej_Kuzelka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ondrej_Kuzelka1">Ondrej Kuzelka</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 13 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#oxnp2q-PGL4-details-984" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oxnp2q-PGL4-details-984"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">weight sharing, graph neural networks, lifted inference, relational learning, dynamic computation graphs, convolutional models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Lifting is an efficient technique to scale up graphical models generalized to relational domains by exploiting the underlying symmetries. Concurrently, neural models are continuously expanding from grid-like tensor data into structured representations, such as various attributed graphs and relational databases. To address the irregular structure of the data, the models typically extrapolate on the idea of convolution, effectively introducing parameter sharing in their, dynamically unfolded, computation graphs. The computation graphs themselves then reflect the symmetries of the underlying data, similarly to the lifted graphical models. Inspired by lifting, we introduce a simple and efficient technique to detect the symmetries and compress the neural models without loss of any information. We demonstrate through experiments that such compression can lead to significant speedups of structured convolutional models, such as various Graph Neural Networks, across various tasks, such as molecule classification and knowledge-base completion.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Speeding up weight-sharing dynamic neural computation graphs, such as GNNs, with lifted inference.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=oxnp2q-PGL4&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="C0qJUx5dxFb" data-number="3621">
      <h4>
        <a href="https://openreview.net/forum?id=C0qJUx5dxFb">
            Neural networks with late-phase weights
        </a>
      
        
          <a href="https://openreview.net/pdf?id=C0qJUx5dxFb" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Johannes_Von_Oswald1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Johannes_Von_Oswald1">Johannes Von Oswald</a>, <a href="https://openreview.net/profile?email=seijink%40ethz.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="seijink@ethz.ch">Seijin Kobayashi</a>, <a href="https://openreview.net/profile?id=~Joao_Sacramento1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joao_Sacramento1">Joao Sacramento</a>, <a href="https://openreview.net/profile?id=~Alexander_Meulemans1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Meulemans1">Alexander Meulemans</a>, <a href="https://openreview.net/profile?id=~Christian_Henning1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christian_Henning1">Christian Henning</a>, <a href="https://openreview.net/profile?id=~Benjamin_F_Grewe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_F_Grewe1">Benjamin F Grewe</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#C0qJUx5dxFb-details-622" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="C0qJUx5dxFb-details-622"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The largely successful method of training neural networks is to learn their weights using some variant of stochastic gradient descent (SGD). Here, we show that the solutions found by SGD can be further improved by ensembling a subset of the weights in late stages of learning. At the end of learning, we obtain back a single model by taking a spatial average in weight space. To avoid incurring increased computational costs, we investigate a family of low-dimensional late-phase weight models which interact multiplicatively with the remaining parameters. Our results show that augmenting standard models with late-phase weights improves generalization in established benchmarks such as CIFAR-10/100, ImageNet and enwik8. These findings are complemented with a theoretical analysis of a noisy quadratic problem which provides a simplified picture of the late phases of neural network learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=C0qJUx5dxFb&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="K5j7D81ABvt" data-number="3399">
      <h4>
        <a href="https://openreview.net/forum?id=K5j7D81ABvt">
            Disambiguating Symbolic Expressions in Informal Documents
        </a>
      
        
          <a href="https://openreview.net/pdf?id=K5j7D81ABvt" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Dennis_M%C3%BCller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dennis_Müller1">Dennis Müller</a>, <a href="https://openreview.net/profile?id=~Cezary_Kaliszyk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cezary_Kaliszyk1">Cezary Kaliszyk</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#K5j7D81ABvt-details-570" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="K5j7D81ABvt-details-570"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose the task of \emph{disambiguating} symbolic expressions in informal STEM documents in the form of \LaTeX files -- that is, determining their precise semantics and abstract syntax tree -- as a neural machine translation task. We discuss the distinct challenges involved and present a dataset with roughly 33,000 entries. We evaluated several baseline models on this dataset, which failed to yield even syntactically valid \LaTeX before overfitting. Consequently, we describe a methodology using a \emph{transformer} language model pre-trained on sources obtained from \url{arxiv.org}, which yields promising results despite the small size of the dataset. We evaluate our model using a plurality of dedicated techniques, taking syntax and semantics of symbolic expressions into account.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=K5j7D81ABvt&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="0OlrLvrsHwQ" data-number="1178">
      <h4>
        <a href="https://openreview.net/forum?id=0OlrLvrsHwQ">
            Learning Parametrised Graph Shift Operators
        </a>
      
        
          <a href="https://openreview.net/pdf?id=0OlrLvrsHwQ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~George_Dasoulas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~George_Dasoulas1">George Dasoulas</a>, <a href="https://openreview.net/profile?id=~Johannes_F._Lutzeyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Johannes_F._Lutzeyer1">Johannes F. Lutzeyer</a>, <a href="https://openreview.net/profile?id=~Michalis_Vazirgiannis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michalis_Vazirgiannis1">Michalis Vazirgiannis</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 08 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#0OlrLvrsHwQ-details-172" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0OlrLvrsHwQ-details-172"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">graph neural networks, graph shift operators, graph classification, node classification, graph representation learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In many domains data is currently represented as graphs and therefore, the graph representation of this data becomes increasingly important in machine learning. Network data is, implicitly or explicitly, always represented using a graph shift operator (GSO) with the most common choices being the adjacency, Laplacian matrices and their normalisations. In this paper, a novel parametrised GSO (PGSO) is proposed, where specific parameter values result in the most commonly used GSOs and message-passing operators in graph neural network (GNN) frameworks. The PGSO is suggested as a replacement of the standard GSOs that are used in state-of-the-art GNN architectures and the optimisation of the PGSO parameters is seamlessly included in the model training. It is proved that the PGSO has real eigenvalues and a set of real eigenvectors independent of the parameter values and spectral bounds on the PGSO are derived. PGSO parameters are shown to adapt to the sparsity of the graph structure in a study on stochastic blockmodel networks, where they are found to automatically replicate the GSO regularisation found in the literature. On several real-world datasets the accuracy of state-of-the-art GNN architectures is improved by the inclusion of the PGSO in both node- and graph-classification tasks. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a parametrised graph shift operator (PGSO) to encode graph structure, providing a unified view of the most common GSOs, and improve GNN performance by incorporating the PGSO into the model training in an end-to-end manner.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tnSo6VRLmT" data-number="1527">
      <h4>
        <a href="https://openreview.net/forum?id=tnSo6VRLmT">
            Efficient Conformal Prediction via Cascaded Inference with Expanded Admission
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tnSo6VRLmT" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Adam_Fisch2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adam_Fisch2">Adam Fisch</a>, <a href="https://openreview.net/profile?id=~Tal_Schuster1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tal_Schuster1">Tal Schuster</a>, <a href="https://openreview.net/profile?id=~Tommi_S._Jaakkola1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tommi_S._Jaakkola1">Tommi S. Jaakkola</a>, <a href="https://openreview.net/profile?id=~Regina_Barzilay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Regina_Barzilay1">Regina Barzilay</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tnSo6VRLmT-details-507" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tnSo6VRLmT-details-507"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">conformal prediction, uncertainty estimation, efficient inference methods, natural language processing, chemistry</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we present a novel approach for conformal prediction (CP), in which we aim to identify a set of promising prediction candidates---in place of a single prediction. This set is guaranteed to contain a correct answer with high probability, and is well-suited for many open-ended classification tasks. In the standard CP paradigm, the predicted set can often be unusably large and also costly to obtain. This is particularly pervasive in settings where the correct answer is not unique, and the number of total possible answers is high. We first expand the CP correctness criterion to allow for additional, inferred "admissible" answers, which can substantially reduce the size of the predicted set while still providing valid performance guarantees. Second, we amortize costs by conformalizing prediction cascades, in which we aggressively prune implausible labels early on by using progressively stronger classifiers---again, while still providing valid performance guarantees. We demonstrate the empirical effectiveness of our approach for multiple applications in natural language processing and computational chemistry for drug discovery.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This work proposes two complementary techniques for improving the efficiency of conformal prediction in large-scale domains---while still retaining performance guarantees.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="1AoMhc_9jER" data-number="2565">
      <h4>
        <a href="https://openreview.net/forum?id=1AoMhc_9jER">
            GANs Can Play Lottery Tickets Too
        </a>
      
        
          <a href="https://openreview.net/pdf?id=1AoMhc_9jER" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xuxi_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuxi_Chen1">Xuxi Chen</a>, <a href="https://openreview.net/profile?id=~Zhenyu_Zhang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhenyu_Zhang4">Zhenyu Zhang</a>, <a href="https://openreview.net/profile?id=~Yongduo_Sui1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yongduo_Sui1">Yongduo Sui</a>, <a href="https://openreview.net/profile?id=~Tianlong_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianlong_Chen1">Tianlong Chen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#1AoMhc_9jER-details-236" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1AoMhc_9jER-details-236"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">lottery tickets, GAN compression, generative adversarial networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep generative adversarial networks (GANs) have gained growing popularity in numerous scenarios, while usually suffer from high parameter complexities for resource-constrained real-world applications. However, the compression of GANs has less been explored. A few works show that heuristically applying compression techniques normally leads to unsatisfactory results, due to the notorious training instability of GANs. In parallel, the lottery ticket hypothesis shows prevailing success on discriminative models, in locating sparse matching subnetworks capable of training in isolation to full model performance. In this work, we for the first time study the existence of such trainable matching subnetworks in deep GANs. For a range of GANs, we certainly find matching subnetworks at <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="176" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c37"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>67</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container>-<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="177" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>74</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> sparsity. We observe that with or without pruning discriminator has a minor effect on the existence and quality of matching subnetworks, while the initialization weights used in the discriminator plays a significant role. We then show the powerful transferability of these subnetworks to unseen tasks. Furthermore, extensive experimental results demonstrate that our found subnetworks substantially outperform previous state-of-the-art GAN compression approaches in both image generation (e.g. SNGAN) and image-to-image translation GANs (e.g. CycleGAN). Codes available at https://github.com/VITA-Group/GAN-LTH.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Winning tickets exist in the deep generative adversarial networks, which substantially outperform previous state-of-the-art compressed GAN</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=1AoMhc_9jER&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="HxzSxSxLOJZ" data-number="3615">
      <h4>
        <a href="https://openreview.net/forum?id=HxzSxSxLOJZ">
            ResNet After All: Neural ODEs and Their Numerical Solution
        </a>
      
        
          <a href="https://openreview.net/pdf?id=HxzSxSxLOJZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Katharina_Ott1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Katharina_Ott1">Katharina Ott</a>, <a href="https://openreview.net/profile?email=prateek.katiyar%40de.bosch.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="prateek.katiyar@de.bosch.com">Prateek Katiyar</a>, <a href="https://openreview.net/profile?id=~Philipp_Hennig1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philipp_Hennig1">Philipp Hennig</a>, <a href="https://openreview.net/profile?id=~Michael_Tiemann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Tiemann1">Michael Tiemann</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>23 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#HxzSxSxLOJZ-details-298" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HxzSxSxLOJZ-details-298"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A key appeal of the recently proposed Neural Ordinary Differential Equation (ODE) framework is that it seems to provide a continuous-time extension of discrete residual neural networks. 
      As we show herein, though, trained Neural ODE models actually depend on the specific numerical method used during training.
      If the trained model is supposed to be a flow generated from an ODE, it should be possible to choose another numerical solver with equal or smaller numerical error without loss of performance.
      We observe that if training relies on a solver with overly coarse discretization, then testing with another solver of equal or smaller numerical error results in a sharp drop in accuracy. 
      In such cases, the combination of vector field and numerical method cannot be interpreted as a flow generated from an ODE, which arguably poses a fatal breakdown of the Neural ODE concept.
      We observe, however, that there exists a critical step size beyond which the training yields a valid ODE vector field. 
      We propose a method that monitors the behavior of the ODE solver during training to adapt its step size, aiming to ensure a valid ODE without unnecessarily increasing computational cost.
      We verify this adaption algorithm on a common bench mark dataset as well as a synthetic dataset. 
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We explain why some Neural ODE models do not permit a continuous-depth interpretation after training and how to fix it.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Ov_sMNau-PF" data-number="2757">
      <h4>
        <a href="https://openreview.net/forum?id=Ov_sMNau-PF">
            Semantic Re-tuning with Contrastive Tension
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Ov_sMNau-PF" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Fredrik_Carlsson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fredrik_Carlsson1">Fredrik Carlsson</a>, <a href="https://openreview.net/profile?id=~Amaru_Cuba_Gyllensten2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amaru_Cuba_Gyllensten2">Amaru Cuba Gyllensten</a>, <a href="https://openreview.net/profile?id=~Evangelia_Gogoulou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Evangelia_Gogoulou1">Evangelia Gogoulou</a>, <a href="https://openreview.net/profile?id=~Erik_Ylip%C3%A4%C3%A4_Hellqvist1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Erik_Ylipää_Hellqvist1">Erik Ylipää Hellqvist</a>, <a href="https://openreview.net/profile?id=~Magnus_Sahlgren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Magnus_Sahlgren1">Magnus Sahlgren</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Ov_sMNau-PF-details-461" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ov_sMNau-PF-details-461"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Semantic Textual Similarity, Transformers, Language Modelling, Sentence Embeddings, Sentence Representations, Pre-training, Fine-tuning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Extracting semantically useful natural language sentence representations from pre-trained deep neural networks such as Transformers remains a challenge. We first demonstrate that pre-training objectives impose a significant task bias onto the final layers of models with a layer-wise survey of the Semantic Textual Similarity (STS) correlations for multiple common Transformer language models. We then propose a new self-supervised method called Contrastive Tension (CT) to counter such biases. CT frames the training objective as a noise-contrastive task between the final layer representations of two independent models, in turn making the final layer representations suitable for feature extraction. Results from multiple common unsupervised and supervised STS tasks indicate that CT outperforms previous State Of The Art (SOTA), and when combining CT with supervised data we improve upon previous SOTA results with large margins. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A self-supervised method for learning STS related sentence embedding using pre-trained language models, setting a new SOTA for STS related embedding tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tYxG_OMs9WE" data-number="2614">
      <h4>
        <a href="https://openreview.net/forum?id=tYxG_OMs9WE">
            Property Controllable Variational Autoencoder via Invertible Mutual Dependence
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tYxG_OMs9WE" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xiaojie_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaojie_Guo1">Xiaojie Guo</a>, <a href="https://openreview.net/profile?id=~Yuanqi_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuanqi_Du1">Yuanqi Du</a>, <a href="https://openreview.net/profile?id=~Liang_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liang_Zhao1">Liang Zhao</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>21 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tYxG_OMs9WE-details-142" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tYxG_OMs9WE-details-142"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep generative models, interpretable latent representation, disentangled representation learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep generative models have made important progress towards modeling complex, high dimensional data via learning latent representations. Their usefulness is nevertheless often limited by a lack of control over the generative process or a poor understanding of the latent representation. To overcome these issues, attention is now focused on discovering latent variables correlated to the data properties and ways to manipulate these properties. This paper presents the new Property controllable VAE (PCVAE), where a new Bayesian model is proposed to inductively bias the latent representation using explicit data properties via novel group-wise and property-wise disentanglement. Each data property corresponds seamlessly to a latent variable, by innovatively enforcing invertible mutual dependence between them. This allows us to move along the learned latent dimensions to control specific properties of the generated data with great precision. Quantitative and qualitative evaluations confirm that the PCVAE outperforms the existing models by up to 28% in capturing and 65% in manipulating the desired properties.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A novel generative model for learning interpretable latent representation for generating data with desired properties.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=tYxG_OMs9WE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="4TSiOTkKe5P" data-number="3016">
      <h4>
        <a href="https://openreview.net/forum?id=4TSiOTkKe5P">
            Latent Convergent Cross Mapping
        </a>
      
        
          <a href="https://openreview.net/pdf?id=4TSiOTkKe5P" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Edward_De_Brouwer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edward_De_Brouwer1">Edward De Brouwer</a>, <a href="https://openreview.net/profile?id=~Adam_Arany1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adam_Arany1">Adam Arany</a>, <a href="https://openreview.net/profile?id=~Jaak_Simm1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaak_Simm1">Jaak Simm</a>, <a href="https://openreview.net/profile?id=~Yves_Moreau2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yves_Moreau2">Yves Moreau</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#4TSiOTkKe5P-details-674" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4TSiOTkKe5P-details-674"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Causality, Time Series, Chaos, Neural ODE, Missing Values</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Discovering causal structures of temporal processes is a major tool of scientific inquiry because it helps us better understand and explain the mechanisms driving a phenomenon of interest, thereby facilitating analysis, reasoning, and synthesis for such systems. 
      However, accurately inferring causal structures within a phenomenon based on observational data only is still an open problem. Indeed, this type of data usually consists in short time series with missing or noisy values for which causal inference is increasingly difficult. In this work, we propose a method to uncover causal relations in chaotic dynamical systems from short, noisy and sporadic time series (that is, incomplete observations at infrequent and irregular intervals) where the classical convergent cross mapping (CCM) fails. Our method works by learning a Neural ODE latent process modeling the state-space dynamics of the time series and by checking the existence of a continuous map between the resulting processes. We provide theoretical analysis and show empirically that Latent-CCM can reliably uncover the true causal pattern, unlike traditional methods.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Latent CCM uses reconstruction between latent processes of dynamical systems to infer causality between short and sporadic time series.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=4TSiOTkKe5P&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="n6jl7fLxrP" data-number="663">
      <h4>
        <a href="https://openreview.net/forum?id=n6jl7fLxrP">
            Adaptive Universal Generalized PageRank Graph Neural Network
        </a>
      
        
          <a href="https://openreview.net/pdf?id=n6jl7fLxrP" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Eli_Chien1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eli_Chien1">Eli Chien</a>, <a href="https://openreview.net/profile?id=~Jianhao_Peng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianhao_Peng1">Jianhao Peng</a>, <a href="https://openreview.net/profile?id=~Pan_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pan_Li2">Pan Li</a>, <a href="https://openreview.net/profile?id=~Olgica_Milenkovic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Olgica_Milenkovic1">Olgica Milenkovic</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#n6jl7fLxrP-details-5" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="n6jl7fLxrP-details-5"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Graph Neural Networks, Generalized PageRank, Heterophily, Homophily, Over-smoothing</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In many important graph data processing applications the acquired information includes both node features and observations of the graph topology. Graph neural networks (GNNs) are designed to exploit both sources of evidence but they do not optimally trade-off their utility and integrate them in a manner that is also universal. Here, universality refers to independence on homophily or heterophily graph assumptions. We address these issues by introducing a new Generalized PageRank (GPR) GNN architecture that adaptively learns the GPR weights so as to jointly optimize node feature and topological information extraction, regardless of the extent to which the node labels are homophilic or heterophilic. Learned GPR weights automatically adjust to the node label pattern, irrelevant on the type of initialization, and thereby guarantee excellent learning performance for label patterns that are usually hard to handle. Furthermore, they allow one to avoid feature over-smoothing, a process which renders feature information nondiscriminative, without requiring the network to be shallow. Our accompanying theoretical analysis of the GPR-GNN method is facilitated by novel synthetic benchmark datasets generated by the so-called contextual stochastic block model. We also compare the performance of our GNN architecture with that of several state-of-the-art GNNs on the problem of node-classification, using well-known benchmark homophilic and heterophilic datasets. The results demonstrate that GPR-GNN offers significant performance improvement compared to existing techniques on both synthetic and benchmark data.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We combine generalized PageRank with GNNs to adapt universal node label patterns and the over-smoothing problem.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=n6jl7fLxrP&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ATp1nW2FuZL" data-number="478">
      <h4>
        <a href="https://openreview.net/forum?id=ATp1nW2FuZL">
            Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ATp1nW2FuZL" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yatin_Nandwani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yatin_Nandwani1">Yatin Nandwani</a>, <a href="https://openreview.net/profile?email=deepanshujindal.99%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="deepanshujindal.99@gmail.com">Deepanshu Jindal</a>, <a href="https://openreview.net/profile?id=~Mausam_.1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mausam_.1">Mausam .</a>, <a href="https://openreview.net/profile?id=~Parag_Singla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Parag_Singla1">Parag Singla</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ATp1nW2FuZL-details-569" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ATp1nW2FuZL-details-569"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neuro symbolic, constraint satisfaction, reasoning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent research has proposed neural architectures for solving combinatorial problems in structured output spaces. In many such problems, there may exist multiple solutions for a given input, e.g. a partially filled Sudoku puzzle may have many completions satisfying all constraints. Further, we are often interested in finding any "one" of the possible solutions, without any preference between them. Existing approaches completely ignore this solution multiplicity. In this paper, we argue that being oblivious to the presence of multiple solutions can severely hamper their training ability. Our contribution is two-fold. First, we formally define the task of learning one-of-many solutions for combinatorial problems in structured output spaces, which is applicable for solving several problems of interest such as N-Queens, and Sudoku. Second, we present a generic learning framework that adapts an existing prediction network for a combinatorial problem to handle solution multiplicity. Our framework uses a selection module, whose goal is to dynamically determine, for every input, the solution that is most effective for training the network parameters in any given learning iteration. We propose an RL based approach to jointly train the selection module with the prediction network. Experiments on three different domains, and using two different prediction networks,  demonstrate that our framework significantly improves the accuracy in our setting, obtaining up to 21 pt gain over the baselines.
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This work identifies and proposes a solution for handling solution multiplicity while learning neural methods for combinatorial problems in structured output spaces.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="N3zUDGN5lO" data-number="1292">
      <h4>
        <a href="https://openreview.net/forum?id=N3zUDGN5lO">
            My Body is a Cage: the Role of Morphology in Graph-Based Incompatible Control
        </a>
      
        
          <a href="https://openreview.net/pdf?id=N3zUDGN5lO" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Vitaly_Kurin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vitaly_Kurin1">Vitaly Kurin</a>, <a href="https://openreview.net/profile?id=~Maximilian_Igl1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maximilian_Igl1">Maximilian Igl</a>, <a href="https://openreview.net/profile?id=~Tim_Rockt%C3%A4schel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tim_Rocktäschel1">Tim Rocktäschel</a>, <a href="https://openreview.net/profile?id=~Wendelin_Boehmer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wendelin_Boehmer1">Wendelin Boehmer</a>, <a href="https://openreview.net/profile?id=~Shimon_Whiteson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shimon_Whiteson1">Shimon Whiteson</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 14 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#N3zUDGN5lO-details-450" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="N3zUDGN5lO-details-450"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep Reinforcement Learning, Multitask Reinforcement Learning, Graph Neural Networks, Continuous Control, Incompatible Environments</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Multitask Reinforcement Learning is a promising way to obtain models with better performance, generalisation, data efficiency, and robustness. Most existing work is limited to compatible settings, where the state and action space dimensions are the same across tasks. Graph Neural Networks (GNN) are one way to address incompatible environments, because they can process graphs of arbitrary size. They also allow practitioners to inject biases encoded in the structure of the input graph. Existing work in graph-based continuous control uses the physical morphology of the agent to construct the input graph, i.e., encoding limb features as node labels and using edges to connect the nodes if their corresponded limbs are physically connected.
      In this work, we present a series of ablations on existing methods that show that morphological information encoded in the graph does not improve their performance. Motivated by the hypothesis that any benefits GNNs extract from the graph structure are outweighed by difficulties they create for message passing, we also propose Amorpheus, a transformer-based approach. Further results show that, while Amorpheus ignores the morphological information that GNNs encode, it nonetheless substantially outperforms GNN-based methods.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Transformer-based approach to multitask incompatible continuous control inspired by a hypothesis that any benefits GNNs extract from the graph structure are outweighed by difficulties they create for message passing.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="dgtpE6gKjHn" data-number="2078">
      <h4>
        <a href="https://openreview.net/forum?id=dgtpE6gKjHn">
            FedBE: Making Bayesian Model Ensemble Applicable to Federated Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=dgtpE6gKjHn" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hong-You_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hong-You_Chen1">Hong-You Chen</a>, <a href="https://openreview.net/profile?id=~Wei-Lun_Chao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei-Lun_Chao1">Wei-Lun Chao</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 21 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#dgtpE6gKjHn-details-300" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dgtpE6gKjHn-details-300"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Federated learning aims to collaboratively train a strong global model by accessing users' locally trained models but not their own data. A crucial step is therefore to aggregate local models into a global model, which has been shown challenging when users have non-i.i.d. data. In this paper, we propose a novel aggregation algorithm named FedBE, which takes a Bayesian inference perspective by sampling higher-quality global models and combining them via Bayesian model Ensemble, leading to much robust aggregation. We show that an effective model distribution can be constructed by simply fitting a Gaussian or Dirichlet distribution to the local models. Our empirical studies validate FedBE's superior performance, especially when users' data are not i.i.d. and when the neural networks go deeper. Moreover, FedBE is compatible with recent efforts in regularizing users' model training, making it an easily applicable module: you only need to replace the aggregation method but leave other parts of your federated learning algorithm intact.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="blfSjHeFM_e" data-number="170">
      <h4>
        <a href="https://openreview.net/forum?id=blfSjHeFM_e">
            MALI: A memory efficient and reverse accurate integrator for Neural ODEs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=blfSjHeFM_e" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Juntang_Zhuang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Juntang_Zhuang1">Juntang Zhuang</a>, <a href="https://openreview.net/profile?id=~Nicha_C_Dvornek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicha_C_Dvornek1">Nicha C Dvornek</a>, <a href="https://openreview.net/profile?id=~sekhar_tatikonda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~sekhar_tatikonda1">sekhar tatikonda</a>, <a href="https://openreview.net/profile?id=~James_s_Duncan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_s_Duncan1">James s Duncan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 04 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#blfSjHeFM_e-details-672" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="blfSjHeFM_e-details-672"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">neural ode, memory efficient, reverse accuracy, gradient estimation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Neural ordinary differential equations (Neural ODEs) are a new family of deep-learning models with continuous depth. However, the numerical estimation of the gradient in the continuous case is not well solved: existing implementations of the adjoint method suffer from inaccuracy in reverse-time trajectory, while the naive method and the adaptive checkpoint adjoint method (ACA) have a memory cost that grows with integration time. In this project, based on the asynchronous leapfrog (ALF) solver, we propose the Memory-efficient ALF Integrator (MALI), which has a constant memory cost <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="178" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D464 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>w</mi><mo>.</mo><mi>r</mi><mo>.</mo><mi>t</mi></math></mjx-assistive-mml></mjx-container> integration time similar to the adjoint method, and guarantees accuracy in reverse-time trajectory (hence accuracy in gradient estimation). We validate MALI in various tasks: on image recognition tasks, to our knowledge, MALI is the first to enable feasible training of a Neural ODE on ImageNet and outperform a well-tuned ResNet, while existing methods fail due to either heavy memory burden or inaccuracy; for time series modeling, MALI significantly outperforms the adjoint method; and for continuous generative models, MALI achieves new state-of-the-art performance. We provide a pypi package: https://jzkay12.github.io/TorchDiffEqPack</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A solver for ODE that guarantees accuracy in reverse-time trajectory  at a constant memory cost</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="sTeoJiB4uR" data-number="1523">
      <h4>
        <a href="https://openreview.net/forum?id=sTeoJiB4uR">
            Reducing the Computational Cost of Deep Generative Models with Binary Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=sTeoJiB4uR" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Thomas_Bird1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Bird1">Thomas Bird</a>, <a href="https://openreview.net/profile?email=fhkingma%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="fhkingma@gmail.com">Friso Kingma</a>, <a href="https://openreview.net/profile?id=~David_Barber1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Barber1">David Barber</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#sTeoJiB4uR-details-153" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sTeoJiB4uR-details-153"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">binary, generative, optimization, compression</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep generative models provide a powerful set of tools to understand real-world data. But as these models improve, they increase in size and complexity, so their computational cost in memory and execution time grows. Using binary weights in neural networks is one method which has shown promise in reducing this cost. However, whether binary neural networks can be used in generative models is an open problem. In this work we show, for the first time, that we can successfully train generative models which utilize binary neural networks. This reduces the computational cost of the models massively. We develop a new class of binary weight normalization, and provide insights for architecture designs of these binarized generative models. We demonstrate that two state-of-the-art deep generative models, the ResNet VAE and Flow++ models, can be binarized effectively using these techniques. We train binary models that achieve loss values close to those of the regular models but are 90%-94% smaller in size, and also allow significant speed-ups in execution time.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We demonstrate that deep generative models can be effectively trained using binary weights and/or activations</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=sTeoJiB4uR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="jznizqvr15J" data-number="2940">
      <h4>
        <a href="https://openreview.net/forum?id=jznizqvr15J">
            In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness
        </a>
      
        
          <a href="https://openreview.net/pdf?id=jznizqvr15J" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sang_Michael_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sang_Michael_Xie1">Sang Michael Xie</a>, <a href="https://openreview.net/profile?id=~Ananya_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ananya_Kumar1">Ananya Kumar</a>, <a href="https://openreview.net/profile?email=rmjones%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="rmjones@stanford.edu">Robbie Jones</a>, <a href="https://openreview.net/profile?id=~Fereshte_Khani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fereshte_Khani1">Fereshte Khani</a>, <a href="https://openreview.net/profile?id=~Tengyu_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tengyu_Ma1">Tengyu Ma</a>, <a href="https://openreview.net/profile?id=~Percy_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Percy_Liang1">Percy Liang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#jznizqvr15J-details-271" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jznizqvr15J-details-271"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">pre-training, self-training theory, robustness, out-of-distribution, unlabeled data, auxiliary information, multi-task learning theory, distribution shift</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Consider a prediction setting with few in-distribution labeled examples and many unlabeled examples both in- and out-of-distribution (OOD). The goal is to learn a model which performs well both in-distribution and OOD. In these settings, auxiliary information is often cheaply available for every input. How should we best leverage this auxiliary information for the prediction task? Empirically across three image and time-series datasets, and theoretically in a multi-task linear regression setting, we show that (i) using auxiliary information as input features improves in-distribution error but can hurt OOD error; but (ii) using auxiliary information as outputs of auxiliary pre-training tasks improves OOD error. To get the best of both worlds, we introduce In-N-Out, which first trains a model with auxiliary inputs and uses it to pseudolabel all the in-distribution inputs, then pre-trains a model on OOD auxiliary outputs and fine-tunes this model with the pseudolabels (self-training). We show both theoretically and empirically that In-N-Out outperforms auxiliary inputs or outputs alone on both in-distribution and OOD error.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Using auxiliary information as inputs hurts OOD, but using auxiliary information by pretraining and self-training improves in-distribution and OOD accuracies on real-world datasets, with theoretical guarantees in a linear multi-task setting.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=jznizqvr15J&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="3SV-ZePhnZM" data-number="1287">
      <h4>
        <a href="https://openreview.net/forum?id=3SV-ZePhnZM">
            Incremental few-shot learning via vector quantization in deep embedded space
        </a>
      
        
          <a href="https://openreview.net/pdf?id=3SV-ZePhnZM" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kuilin_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kuilin_Chen1">Kuilin Chen</a>, <a href="https://openreview.net/profile?id=~Chi-Guhn_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chi-Guhn_Lee1">Chi-Guhn Lee</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#3SV-ZePhnZM-details-276" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3SV-ZePhnZM-details-276"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">incremental learning, few-shot, vector quantization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The capability of incrementally learning new tasks without forgetting old ones is a challenging problem due to catastrophic forgetting. This challenge becomes greater when novel tasks contain very few labelled training samples. Currently, most methods are dedicated to class-incremental learning and rely on sufficient training data to learn additional weights for newly added classes. Those methods cannot be easily extended to incremental regression tasks and could suffer from severe overfitting when learning few-shot novel tasks. In this study, we propose a nonparametric method in deep embedded space to tackle incremental few-shot learning problems. The knowledge about the learned tasks are compressed into a small number of quantized reference vectors. The proposed method learns new tasks sequentially by adding more reference vectors to the model using few-shot samples in each novel task. For classification problems, we employ the nearest neighbor scheme to make classification on sparsely available data and incorporate intra-class variation, less forgetting regularization and calibration of reference vectors to mitigate catastrophic forgetting. In addition, the proposed learning vector quantization (LVQ) in deep embedded space can be customized as a kernel smoother to handle incremental few-shot regression tasks. Experimental results demonstrate that the proposed method outperforms other state-of-the-art methods in incremental learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="F8whUO8HNbP" data-number="2403">
      <h4>
        <a href="https://openreview.net/forum?id=F8whUO8HNbP">
            Contrastive Syn-to-Real Generalization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=F8whUO8HNbP" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Wuyang_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wuyang_Chen1">Wuyang Chen</a>, <a href="https://openreview.net/profile?id=~Zhiding_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiding_Yu1">Zhiding Yu</a>, <a href="https://openreview.net/profile?id=~Shalini_De_Mello1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shalini_De_Mello1">Shalini De Mello</a>, <a href="https://openreview.net/profile?id=~Sifei_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sifei_Liu2">Sifei Liu</a>, <a href="https://openreview.net/profile?id=~Jose_M._Alvarez2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jose_M._Alvarez2">Jose M. Alvarez</a>, <a href="https://openreview.net/profile?id=~Zhangyang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhangyang_Wang1">Zhangyang Wang</a>, <a href="https://openreview.net/profile?id=~Anima_Anandkumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anima_Anandkumar1">Anima Anandkumar</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#F8whUO8HNbP-details-809" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="F8whUO8HNbP-details-809"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">synthetic-to-real generalization, domain generalization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Training on synthetic data can be beneficial for label or data-scarce scenarios. However, synthetically trained models often suffer from poor generalization in real domains due to domain gaps. In this work, we make a key observation that the diversity of the learned feature embeddings plays an important role in the generalization performance. To this end, we propose contrastive synthetic-to-real generalization (CSG), a novel framework that leverage the pre-trained ImageNet knowledge to prevent overfitting to the synthetic domain, while promoting the diversity of feature embeddings as an inductive bias to improve generalization. In addition, we enhance the proposed CSG framework with attentional pooling (A-pool) to let the model focus on semantically important regions and further improve its generalization. We demonstrate the effectiveness of CSG on various synthetic training tasks, exhibiting state-of-the-art performance on zero-shot domain generalization.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a new contrastive synthetic-to-real generalization framework that achieves state-of-the-art performance on synthetic-to-real generalization problems.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tHgJoMfy6nI" data-number="864">
      <h4>
        <a href="https://openreview.net/forum?id=tHgJoMfy6nI">
            Remembering for the Right Reasons: Explanations Reduce Catastrophic Forgetting
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tHgJoMfy6nI" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sayna_Ebrahimi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sayna_Ebrahimi1">Sayna Ebrahimi</a>, <a href="https://openreview.net/profile?id=~Suzanne_Petryk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Suzanne_Petryk1">Suzanne Petryk</a>, <a href="https://openreview.net/profile?email=akashgokul%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="akashgokul@berkeley.edu">Akash Gokul</a>, <a href="https://openreview.net/profile?email=wjgan%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="wjgan@berkeley.edu">William Gan</a>, <a href="https://openreview.net/profile?id=~Joseph_E._Gonzalez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joseph_E._Gonzalez1">Joseph E. Gonzalez</a>, <a href="https://openreview.net/profile?id=~Marcus_Rohrbach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marcus_Rohrbach1">Marcus Rohrbach</a>, <a href="https://openreview.net/profile?id=~trevor_darrell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~trevor_darrell1">trevor darrell</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tHgJoMfy6nI-details-323" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tHgJoMfy6nI-details-323"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Continual Learning, Lifelong Learning, Catastrophic Forgetting, XAI, Explainability</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The goal of continual learning (CL) is to learn a sequence of tasks without suffering from the phenomenon of catastrophic forgetting. Previous work has shown that leveraging memory in the form of a replay buffer can reduce performance degradation on prior tasks. We hypothesize that forgetting can be further reduced when the model is encouraged to remember the \textit{evidence} for previously made decisions. As a first step towards exploring this hypothesis, we propose a simple novel training paradigm, called Remembering for the Right Reasons (RRR), that additionally stores visual model explanations for each example in the buffer and ensures the model has ``the right reasons'' for its predictions by encouraging its explanations to remain consistent with those used to make decisions at training time. Without this constraint, there is a drift in explanations and increase in forgetting as conventional continual learning algorithms learn new tasks. We demonstrate how RRR can be easily added to any memory or regularization-based approach and results in reduced forgetting, and more importantly, improved model explanations. We have evaluated our approach in the standard and few-shot settings and observed a consistent improvement across various CL approaches using different architectures and techniques to generate model explanations and demonstrated our approach showing a promising connection between explainability and continual learning. Our code is available at \url{https://github.com/SaynaEbrahimi/Remembering-for-the-Right-Reasons}.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Introducing a connection between continual learning and model explainability by regularizing saliency maps to avoid forgetting and showing its effect on memory and regularization-based continual learning approaches. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="zElset1Klrp" data-number="2083">
      <h4>
        <a href="https://openreview.net/forum?id=zElset1Klrp">
            Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online
        </a>
      
        
          <a href="https://openreview.net/pdf?id=zElset1Klrp" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yangchen_Pan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yangchen_Pan2">Yangchen Pan</a>, <a href="https://openreview.net/profile?email=kdbanman%40ualberta.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="kdbanman@ualberta.ca">Kirby Banman</a>, <a href="https://openreview.net/profile?id=~Martha_White1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martha_White1">Martha White</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#zElset1Klrp-details-565" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zElset1Klrp-details-565"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement learning, natural sparsity, sparse representation, fuzzy tiling activation function</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent work has shown that sparse representations---where only a small percentage of units are active---can significantly reduce interference. Those works, however, relied on relatively complex regularization or meta-learning approaches, that have only been used offline in a pre-training phase. In this work, we pursue a direction that achieves sparsity by design, rather than by learning. Specifically, we design an activation function that produces sparse representations deterministically by construction, and so is more amenable to online training. The idea relies on the simple approach of binning, but overcomes the two key limitations of binning: zero gradients for the flat regions almost everywhere,  and lost precision---reduced discrimination---due to coarse aggregation. We introduce a Fuzzy Tiling Activation (FTA) that provides non-negligible gradients and produces overlap between bins that improves discrimination. We first show that FTA is robust under covariate shift in a synthetic online supervised learning problem, where we can vary the level of correlation and drift. Then we move to the deep reinforcement learning setting and investigate both value-based and policy gradient algorithms that use neural networks with FTAs, in classic discrete control and Mujoco continuous control environments. We show that algorithms equipped with FTAs are able to learn a stable policy faster without needing target networks on most domains. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A simple and efficient way to learn sparse feature online in deep learning setting. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=zElset1Klrp&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="MxaY4FzOTa" data-number="690">
      <h4>
        <a href="https://openreview.net/forum?id=MxaY4FzOTa">
            High-Capacity Expert Binary Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=MxaY4FzOTa" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Adrian_Bulat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adrian_Bulat1">Adrian Bulat</a>, <a href="https://openreview.net/profile?id=~Brais_Martinez3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brais_Martinez3">Brais Martinez</a>, <a href="https://openreview.net/profile?id=~Georgios_Tzimiropoulos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Georgios_Tzimiropoulos1">Georgios Tzimiropoulos</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 20 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>24 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#MxaY4FzOTa-details-320" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MxaY4FzOTa-details-320"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Network binarization is a promising hardware-aware direction for creating efficient deep models. Despite its memory and computational advantages, reducing the accuracy gap between binary models and their real-valued counterparts remains an unsolved challenging research problem. To this end, we make the following 3 contributions: (a) To increase model capacity, we propose Expert Binary Convolution, which, for the first time, tailors conditional computing to binary networks by learning to select one data-specific expert binary filter at a time conditioned on input features. (b) To increase representation capacity, we propose to address the inherent information bottleneck in binary networks by introducing an efficient width expansion mechanism which keeps the binary operations within the same budget. (c) To improve network design, we propose a principled binary network growth mechanism that unveils a set of network topologies of favorable properties. Overall, our method improves upon prior work, with no increase in computational cost, by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="179" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c36"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∼</mo><mn>6</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container>, reaching a groundbreaking <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="180" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>∼</mo><mn>71</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> on ImageNet classification. Code will be made available <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="181" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><a href="https://www.adrianbulat.com/binary-networks"><mjx-mrow><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi></mjx-mrow></a></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow href="https://www.adrianbulat.com/binary-networks"><mi>h</mi><mi>e</mi><mi>r</mi><mi>e</mi></mrow></math></mjx-assistive-mml></mjx-container>.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="kBVJ2NtiY-" data-number="1398">
      <h4>
        <a href="https://openreview.net/forum?id=kBVJ2NtiY-">
            Learning What To Do by Simulating the Past
        </a>
      
        
          <a href="https://openreview.net/pdf?id=kBVJ2NtiY-" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~David_Lindner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Lindner1">David Lindner</a>, <a href="https://openreview.net/profile?id=~Rohin_Shah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rohin_Shah1">Rohin Shah</a>, <a href="https://openreview.net/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="https://openreview.net/profile?id=~Anca_Dragan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anca_Dragan1">Anca Dragan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#kBVJ2NtiY--details-747" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kBVJ2NtiY--details-747"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">imitation learning, reward learning, reinforcement learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state. Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a specific skill in MuJoCo environments given a single state sampled from the optimal policy for that skill.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Imitating policies given just a single state sampled from a rollout from an expert.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9GsFOUyUPi" data-number="1729">
      <h4>
        <a href="https://openreview.net/forum?id=9GsFOUyUPi">
            Progressive Skeletonization: Trimming more fat from a network at initialization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9GsFOUyUPi" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Pau_de_Jorge1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pau_de_Jorge1">Pau de Jorge</a>, <a href="https://openreview.net/profile?id=~Amartya_Sanyal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amartya_Sanyal1">Amartya Sanyal</a>, <a href="https://openreview.net/profile?id=~Harkirat_Behl1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Harkirat_Behl1">Harkirat Behl</a>, <a href="https://openreview.net/profile?id=~Philip_Torr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philip_Torr1">Philip Torr</a>, <a href="https://openreview.net/profile?id=~Gr%C3%A9gory_Rogez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Grégory_Rogez1">Grégory Rogez</a>, <a href="https://openreview.net/profile?id=~Puneet_K._Dokania1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Puneet_K._Dokania1">Puneet K. Dokania</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9GsFOUyUPi-details-19" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9GsFOUyUPi-details-19"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Pruning, Pruning at initialization, Sparsity</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent studies have shown that skeletonization (pruning parameters) of networks at initialization provides all the practical benefits of sparsity both at inference and training time, while only marginally degrading their performance. However, we observe that beyond a certain level of sparsity (approx 95%), these approaches fail to preserve the network performance, and to our surprise, in many cases perform even worse than trivial random pruning. To this end, we propose an objective to find a skeletonized network with maximum foresight connection sensitivity (FORCE) whereby the trainability, in terms of connection sensitivity, of a pruned network is taken into consideration. We then propose two approximate procedures to maximize our objective (1) Iterative SNIP: allows parameters that were unimportant at earlier stages of skeletonization to become important at later stages; and (2) FORCE: iterative process that allows exploration by allowing already pruned parameters to resurrect at later stages of skeletonization. Empirical analysis on a large suite of experiments show that our approach, while providing at least as good performance as other recent approaches on moderate pruning levels, provide remarkably improved performance on high pruning levels (could remove up to 99.5% parameters while keeping the networks trainable).</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We find performance of current methods for pruning at initialization plummets at high sparsity levels, we study the possible reasons and present a more robust method overall.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="A2gNouoXE7" data-number="2212">
      <h4>
        <a href="https://openreview.net/forum?id=A2gNouoXE7">
            Filtered Inner Product Projection for Crosslingual Embedding Alignment
        </a>
      
        
          <a href="https://openreview.net/pdf?id=A2gNouoXE7" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Vin_Sachidananda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vin_Sachidananda1">Vin Sachidananda</a>, <a href="https://openreview.net/profile?id=~Ziyi_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziyi_Yang1">Ziyi Yang</a>, <a href="https://openreview.net/profile?id=~Chenguang_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chenguang_Zhu1">Chenguang Zhu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#A2gNouoXE7-details-447" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="A2gNouoXE7-details-447"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">multilingual representations, word embeddings, natural language processing</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Due to widespread interest in machine translation and transfer learning, there are numerous algorithms for mapping multiple embeddings to a shared representation space. Recently, these algorithms have been studied in the setting of bilingual lexicon induction where one seeks to align the embeddings of a source and a target language such that translated word pairs lie close to one another in a common representation space. In this paper, we propose a method, Filtered Inner Product Projection (FIPP), for mapping embeddings to a common representation space. As semantic shifts are pervasive across languages and domains, FIPP first identifies the common geometric structure in both embeddings and then, only on the common structure, aligns the Gram matrices of these embeddings. FIPP is applicable even when the source and target embeddings are of differing dimensionalities. Additionally, FIPP provides computational benefits in ease of implementation and is faster to compute than current approaches. Following the baselines in Glavas et al. 2019, we evaluate FIPP both in the context of bilingual lexicon induction and downstream language tasks. We show that FIPP outperforms existing methods on the XLING BLI dataset for most language pairs while also providing robust performance across downstream tasks. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Gu5WqN9J3Fn" data-number="685">
      <h4>
        <a href="https://openreview.net/forum?id=Gu5WqN9J3Fn">
            Learning Manifold Patch-Based Representations of Man-Made Shapes
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Gu5WqN9J3Fn" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Dmitriy_Smirnov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dmitriy_Smirnov1">Dmitriy Smirnov</a>, <a href="https://openreview.net/profile?id=~Mikhail_Bessmeltsev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mikhail_Bessmeltsev1">Mikhail Bessmeltsev</a>, <a href="https://openreview.net/profile?id=~Justin_Solomon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Justin_Solomon1">Justin Solomon</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Gu5WqN9J3Fn-details-409" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Gu5WqN9J3Fn-details-409"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">3D shape representations, CAD modeling, sketch-based modeling, computer graphics, computer vision, deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Choosing the right representation for geometry is crucial for making 3D models compatible with existing applications. Focusing on piecewise-smooth man-made shapes, we propose a new representation that is usable in conventional CAD modeling pipelines and can also be learned by deep neural networks. We demonstrate its benefits by applying it to the task of sketch-based modeling. Given a raster image, our system infers a set of parametric surfaces that realize the input in 3D. To capture piecewise smooth geometry, we learn a special shape representation: a deformable parametric template composed of Coons patches. Naively training such a system, however, is hampered by non-manifold artifacts in the parametric shapes and by a lack of data. To address this, we introduce loss functions that bias the network to output non-self-intersecting shapes and implement them as part of a fully self-supervised system, automatically generating both shape templates and synthetic training data. We develop a testbed for sketch-based modeling, demonstrate shape interpolation, and provide comparison to related work.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a parametrically defined patch-based 3D shape representation that is compatible both with traditional CAD modeling tools and modern deep learning pipelines.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="dNy_RKzJacY" data-number="2039">
      <h4>
        <a href="https://openreview.net/forum?id=dNy_RKzJacY">
            Aligning AI With Shared Human Values
        </a>
      
        
          <a href="https://openreview.net/pdf?id=dNy_RKzJacY" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Dan_Hendrycks1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Hendrycks1">Dan Hendrycks</a>, <a href="https://openreview.net/profile?email=collin.burns%40columbia.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="collin.burns@columbia.edu">Collin Burns</a>, <a href="https://openreview.net/profile?id=~Steven_Basart1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_Basart1">Steven Basart</a>, <a href="https://openreview.net/profile?id=~Andrew_Critch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_Critch1">Andrew Critch</a>, <a href="https://openreview.net/profile?id=~Jerry_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jerry_Li1">Jerry Li</a>, <a href="https://openreview.net/profile?id=~Dawn_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dawn_Song1">Dawn Song</a>, <a href="https://openreview.net/profile?id=~Jacob_Steinhardt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jacob_Steinhardt1">Jacob Steinhardt</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 05 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#dNy_RKzJacY-details-281" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dNy_RKzJacY-details-281"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">value learning, human preferences, alignment</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We approach a longstanding problem in machine ethics provide evidence that it is soluble.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="QoWatN-b8T" data-number="1802">
      <h4>
        <a href="https://openreview.net/forum?id=QoWatN-b8T">
            Kanerva++: Extending the Kanerva Machine With Differentiable, Locally Block Allocated Latent Memory
        </a>
      
        
          <a href="https://openreview.net/pdf?id=QoWatN-b8T" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jason_Ramapuram1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jason_Ramapuram1">Jason Ramapuram</a>, <a href="https://openreview.net/profile?id=~Yan_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yan_Wu1">Yan Wu</a>, <a href="https://openreview.net/profile?id=~Alexandros_Kalousis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexandros_Kalousis1">Alexandros Kalousis</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#QoWatN-b8T-details-139" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QoWatN-b8T-details-139"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">memory, generative model, latent variable, heap allocation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Episodic and semantic memory are critical components of the human memory model. The theory of complementary learning systems (McClelland et al., 1995) suggests that the compressed representation produced by a serial event (episodic memory) is later restructured to build a more generalized form of reusable knowledge (semantic memory). In this work, we develop a new principled Bayesian memory allocation scheme that bridges the gap between episodic and semantic memory via a hierarchical latent variable model. We take inspiration from traditional heap allocation and extend the idea of locally contiguous memory to the Kanerva Machine, enabling a novel differentiable block allocated latent memory. In contrast to the Kanerva Machine, we simplify the process of memory writing by treating it as a fully feed forward deterministic process, relying on the stochasticity of the read key distribution to disperse information within the memory. We demonstrate that this allocation scheme improves performance in memory conditional image generation, resulting in new state-of-the-art conditional likelihood values on binarized MNIST (≤41.58 nats/image) , binarized Omniglot (≤66.24 nats/image), as well as presenting competitive performance on CIFAR10, DMLab Mazes, Celeb-A and ImageNet32×32.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Differentiable block allocated latent memory model for generative modeling.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=QoWatN-b8T&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="d7KBjmI3GmQ" data-number="2049">
      <h4>
        <a href="https://openreview.net/forum?id=d7KBjmI3GmQ">
            Measuring Massive Multitask Language Understanding
        </a>
      
        
          <a href="https://openreview.net/pdf?id=d7KBjmI3GmQ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Dan_Hendrycks1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Hendrycks1">Dan Hendrycks</a>, <a href="https://openreview.net/profile?email=collin.burns%40columbia.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="collin.burns@columbia.edu">Collin Burns</a>, <a href="https://openreview.net/profile?id=~Steven_Basart1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_Basart1">Steven Basart</a>, <a href="https://openreview.net/profile?email=andyzou_jiaming%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="andyzou_jiaming@berkeley.edu">Andy Zou</a>, <a href="https://openreview.net/profile?id=~Mantas_Mazeika3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mantas_Mazeika3">Mantas Mazeika</a>, <a href="https://openreview.net/profile?id=~Dawn_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dawn_Song1">Dawn Song</a>, <a href="https://openreview.net/profile?id=~Jacob_Steinhardt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jacob_Steinhardt1">Jacob Steinhardt</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#d7KBjmI3GmQ-details-934" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="d7KBjmI3GmQ-details-934"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">multitask, few-shot</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We test language models on 57 different multiple-choice tasks.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="2AL06y9cDE-" data-number="2276">
      <h4>
        <a href="https://openreview.net/forum?id=2AL06y9cDE-">
            Towards Robust Neural Networks via Close-loop Control
        </a>
      
        
          <a href="https://openreview.net/pdf?id=2AL06y9cDE-" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhuotong_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhuotong_Chen1">Zhuotong Chen</a>, <a href="https://openreview.net/profile?id=~Qianxiao_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qianxiao_Li1">Qianxiao Li</a>, <a href="https://openreview.net/profile?id=~Zheng_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zheng_Zhang2">Zheng Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#2AL06y9cDE--details-978" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2AL06y9cDE--details-978"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">neural network robustness, optimal control, dynamical system</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Despite their success in massive engineering applications, deep neural networks are vulnerable to various perturbations due to their black-box nature. Recent study has shown that a deep neural network can misclassify the data even if the input data is perturbed by an imperceptible amount. In this paper, we address the robustness issue of neural networks by a novel close-loop control method from the perspective of dynamic systems. Instead of modifying the parameters in a fixed neural network architecture, a close-loop control process is added to generate control signals adaptively for the perturbed or corrupted data. We connect the robustness of neural networks with optimal control using the geometrical information of underlying data to design the control objective. The detailed analysis shows how the embedding manifolds of state trajectory affect error estimation of the proposed method. Our approach can simultaneously maintain the performance on clean data and improve the robustness against many types of data perturbations. It can also further improve the performance of robustly trained neural networks against different perturbations. To the best of our knowledge, this is the first work that improves the robustness of neural networks with close-loop control.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a close-loop control framework to improve the robustness of neural networks under various data perturbations.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="z9k8BWL-_2u" data-number="1665">
      <h4>
        <a href="https://openreview.net/forum?id=z9k8BWL-_2u">
            Statistical inference for individual fairness
        </a>
      
        
          <a href="https://openreview.net/pdf?id=z9k8BWL-_2u" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Subha_Maity1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Subha_Maity1">Subha Maity</a>, <a href="https://openreview.net/profile?email=sxue%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sxue@umich.edu">Songkai Xue</a>, <a href="https://openreview.net/profile?id=~Mikhail_Yurochkin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mikhail_Yurochkin1">Mikhail Yurochkin</a>, <a href="https://openreview.net/profile?id=~Yuekai_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuekai_Sun1">Yuekai Sun</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#z9k8BWL-_2u-details-827" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="z9k8BWL-_2u-details-827"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">As we rely on machine learning (ML) models to make more consequential decisions, the issue of ML models perpetuating unwanted social biases has come to the fore of the public's and the research community's attention. In this paper, we focus on the problem of detecting violations of individual fairness in ML models. We formalize the problem as measuring the susceptibility of ML models against a form of adversarial attack and develop a suite of inference tools for the adversarial loss. The tools allow practitioners to assess the individual fairness of ML models in a statistically-principled way: form confidence intervals for the adversarial loss and test hypotheses of model fairness with (asymptotic) non-coverage/Type I error rate control. We demonstrate the utility of our tools in a real-world case study.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=z9k8BWL-_2u&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="hiq1rHO8pNT" data-number="1622">
      <h4>
        <a href="https://openreview.net/forum?id=hiq1rHO8pNT">
            HyperGrid Transformers: Towards A Single Model for Multiple Tasks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=hiq1rHO8pNT" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yi_Tay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Tay1">Yi Tay</a>, <a href="https://openreview.net/profile?id=~Zhe_Zhao3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhe_Zhao3">Zhe Zhao</a>, <a href="https://openreview.net/profile?id=~Dara_Bahri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dara_Bahri1">Dara Bahri</a>, <a href="https://openreview.net/profile?email=metzler%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="metzler@google.com">Donald Metzler</a>, <a href="https://openreview.net/profile?id=~Da-Cheng_Juan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Da-Cheng_Juan1">Da-Cheng Juan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 14 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#hiq1rHO8pNT-details-959" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hiq1rHO8pNT-details-959"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Transformers, Multi-Task Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Achieving state-of-the-art performance on natural language understanding tasks typically relies on fine-tuning a fresh model for every task. Consequently, this approach leads to a higher overall parameter cost, along with higher technical maintenance for serving multiple models. Learning a single multi-task model that is able to do well for all the tasks has been a challenging and yet attractive proposition. In this paper, we propose HyperGrid Transformers, a new Transformer architecture that leverages task-conditioned hyper networks for controlling its feed-forward layers. Specifically, we propose a decomposable hypernetwork that learns grid-wise projections that help to specialize regions in weight matrices for different tasks. In order to construct the proposed hypernetwork, our method learns the interactions and composition between a global (task-agnostic) state and a local task-specific state. We conduct an extensive set of experiments on GLUE/SuperGLUE. On the SuperGLUE test set, we match the performance of the state-of-the-art while being <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="182" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>16</mn></math></mjx-assistive-mml></mjx-container> times more parameter efficient. Our method helps bridge the gap between fine-tuning and multi-task learning approaches.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">State-of-the-art multi-task NLU performance with only a single model</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="6t_dLShIUyZ" data-number="725">
      <h4>
        <a href="https://openreview.net/forum?id=6t_dLShIUyZ">
            Greedy-GQ with Variance Reduction: Finite-time Analysis and Improved Complexity
        </a>
      
        
          <a href="https://openreview.net/pdf?id=6t_dLShIUyZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Shaocong_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shaocong_Ma1">Shaocong Ma</a>, <a href="https://openreview.net/profile?id=~Ziyi_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziyi_Chen2">Ziyi Chen</a>, <a href="https://openreview.net/profile?id=~Yi_Zhou2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Zhou2">Yi Zhou</a>, <a href="https://openreview.net/profile?id=~Shaofeng_Zou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shaofeng_Zou1">Shaofeng Zou</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#6t_dLShIUyZ-details-823" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6t_dLShIUyZ-details-823"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Optimization, Reinforcement Learning, Machine Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Greedy-GQ is a value-based reinforcement learning (RL) algorithm for optimal control. Recently, the finite-time analysis of Greedy-GQ has been developed under linear function approximation and Markovian sampling, and the algorithm is shown to achieve an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="183" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi></math></mjx-assistive-mml></mjx-container>-stationary point with a sample complexity in the order of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="184" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><msup><mi>ϵ</mi><mrow><mo>−</mo><mn>3</mn></mrow></msup><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>. Such a high sample complexity is due to the large variance induced by the Markovian samples. In this paper, we propose a variance-reduced Greedy-GQ (VR-Greedy-GQ) algorithm for off-policy optimal control. In particular, the algorithm applies the SVRG-based variance reduction scheme to reduce the stochastic variance of the two time-scale updates. We study the finite-time convergence of VR-Greedy-GQ under linear function approximation and Markovian sampling and show that the algorithm achieves a much smaller bias and variance error than the original Greedy-GQ. In particular, we prove that VR-Greedy-GQ achieves an improved sample complexity that is in the order of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="185" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><msup><mi>ϵ</mi><mrow><mo>−</mo><mn>2</mn></mrow></msup><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>. We further compare the performance of VR-Greedy-GQ with that of Greedy-GQ in various RL experiments to corroborate our theoretical findings.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=6t_dLShIUyZ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="AhElGnhU2BV" data-number="1359">
      <h4>
        <a href="https://openreview.net/forum?id=AhElGnhU2BV">
            On InstaHide, Phase Retrieval, and Sparse Matrix Factorization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=AhElGnhU2BV" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sitan_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sitan_Chen1">Sitan Chen</a>, <a href="https://openreview.net/profile?id=~Xiaoxiao_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoxiao_Li1">Xiaoxiao Li</a>, <a href="https://openreview.net/profile?id=~Zhao_Song3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhao_Song3">Zhao Song</a>, <a href="https://openreview.net/profile?id=~Danyang_Zhuo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Danyang_Zhuo1">Danyang Zhuo</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#AhElGnhU2BV-details-203" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AhElGnhU2BV-details-203"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Distributed learning, InstaHide, phase retrieval, matrix factorization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this work, we examine the security of InstaHide, a scheme recently proposed by \cite{hsla20} for preserving the security of private datasets in the context of distributed learning. To generate a synthetic training example to be shared among the distributed learners, InstaHide takes a convex combination of private feature vectors and randomly flips the sign of each entry of the resulting vector with probability 1/2. A salient question is whether this scheme is secure in any provable sense, perhaps under a plausible complexity-theoretic assumption. 
      
      The answer to this turns out to be quite subtle and closely related to the average-case complexity of a multi-task, missing-data version of the classic problem of phase retrieval that is interesting in its own right. Motivated by this connection, under the standard distributional assumption that the public/private feature vectors are isotropic Gaussian, we design an algorithm that can actually recover a private vector using only the public vectors and a sequence of synthetic vectors generated by InstaHide.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We examine the security of InstaHide, a recently proposed framework for private distributed learning, through the lens of phase retrieval and give an attack when the underlying datasets are Gaussian.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=AhElGnhU2BV&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="g21u6nlbPzn" data-number="948">
      <h4>
        <a href="https://openreview.net/forum?id=g21u6nlbPzn">
            VA-RED<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="186" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-n"></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>2</mn></msup></math></mjx-assistive-mml></mjx-container>: Video Adaptive Redundancy Reduction
        </a>
      
        
          <a href="https://openreview.net/pdf?id=g21u6nlbPzn" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Bowen_Pan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bowen_Pan2">Bowen Pan</a>, <a href="https://openreview.net/profile?id=~Rameswar_Panda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rameswar_Panda1">Rameswar Panda</a>, <a href="https://openreview.net/profile?id=~Camilo_Luciano_Fosco1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Camilo_Luciano_Fosco1">Camilo Luciano Fosco</a>, <a href="https://openreview.net/profile?id=~Chung-Ching_Lin2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chung-Ching_Lin2">Chung-Ching Lin</a>, <a href="https://openreview.net/profile?id=~Alex_J_Andonian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_J_Andonian1">Alex J Andonian</a>, <a href="https://openreview.net/profile?id=~Yue_Meng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yue_Meng1">Yue Meng</a>, <a href="https://openreview.net/profile?id=~Kate_Saenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kate_Saenko1">Kate Saenko</a>, <a href="https://openreview.net/profile?id=~Aude_Oliva1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aude_Oliva1">Aude Oliva</a>, <a href="https://openreview.net/profile?id=~Rogerio_Feris1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rogerio_Feris1">Rogerio Feris</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#g21u6nlbPzn-details-961" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="g21u6nlbPzn-details-961"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Performing inference on deep learning models for videos remains a challenge due to the large amount of computational resources required to achieve robust recognition. An inherent property of real-world videos is the high correlation of information across frames which can translate into redundancy in either temporal or spatial feature maps of the models, or both. The type of redundant features depends on the dynamics and type of events in the video: static videos have more temporal redundancy while videos focusing on objects tend to have more channel redundancy. Here we present a redundancy reduction framework, termed VA-RED<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="187" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-n"></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>2</mn></msup></math></mjx-assistive-mml></mjx-container>, which is input-dependent. Specifically, our VA-RED<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="188" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-n"></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>2</mn></msup></math></mjx-assistive-mml></mjx-container> framework uses an input-dependent policy to decide how many features need to be computed for temporal and channel dimensions. To keep the capacity of the original model, after fully computing the necessary features, we reconstruct the remaining redundant features from those using cheap linear operations. We learn the adaptive policy jointly with the network weights in a differentiable way with a shared-weight mechanism, making it highly efficient. Extensive experiments on multiple video datasets and different visual tasks show that our framework achieves <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="189" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>20</mn><mi mathvariant="normal">%</mi><mo>−</mo><mn>40</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> reduction in computation (FLOPs) when compared to state-of-the-art methods without any performance loss. Project page: http://people.csail.mit.edu/bpan/va-red/.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="XLfdzwNKzch" data-number="1127">
      <h4>
        <a href="https://openreview.net/forum?id=XLfdzwNKzch">
            SEDONA: Search for Decoupled Neural Networks toward Greedy Block-wise Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=XLfdzwNKzch" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Myeongjang_Pyeon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Myeongjang_Pyeon1">Myeongjang Pyeon</a>, <a href="https://openreview.net/profile?id=~Jihwan_Moon2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jihwan_Moon2">Jihwan Moon</a>, <a href="https://openreview.net/profile?id=~Taeyoung_Hahn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Taeyoung_Hahn1">Taeyoung Hahn</a>, <a href="https://openreview.net/profile?id=~Gunhee_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gunhee_Kim1">Gunhee Kim</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#XLfdzwNKzch-details-521" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XLfdzwNKzch-details-521"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">AutoML, Neural Architecture Search, Greedy Learning, Deep Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Backward locking and update locking are well-known sources of inefficiency in backpropagation that prevent from concurrently updating layers. Several works have recently suggested using local error signals to train network blocks asynchronously to overcome these limitations. However, they often require numerous iterations of trial-and-error to find the best configuration for local training, including how to decouple network blocks and which auxiliary networks to use for each block. In this work, we propose a differentiable search algorithm named SEDONA to automate this process. Experimental results show that our algorithm can consistently discover transferable decoupled architectures for VGG and ResNet variants, and significantly outperforms the ones trained with end-to-end backpropagation and other state-of-the-art greedy-leaning methods in CIFAR-10, Tiny-ImageNet and ImageNet.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Our approach is the first attempt to automate decoupling neural networks for greedy block-wise learning and outperforms both end-to-end backprop and state-of-the-art greedy-learning methods on CIFAR-10, Tiny-ImageNet and ImageNet classification.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="0IOX0YcCdTn" data-number="1426">
      <h4>
        <a href="https://openreview.net/forum?id=0IOX0YcCdTn">
            ALFWorld: Aligning Text and Embodied Environments for Interactive Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=0IOX0YcCdTn" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mohit_Shridhar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohit_Shridhar1">Mohit Shridhar</a>, <a href="https://openreview.net/profile?id=~Xingdi_Yuan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xingdi_Yuan2">Xingdi Yuan</a>, <a href="https://openreview.net/profile?id=~Marc-Alexandre_Cote1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marc-Alexandre_Cote1">Marc-Alexandre Cote</a>, <a href="https://openreview.net/profile?id=~Yonatan_Bisk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yonatan_Bisk1">Yonatan Bisk</a>, <a href="https://openreview.net/profile?id=~Adam_Trischler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adam_Trischler1">Adam Trischler</a>, <a href="https://openreview.net/profile?id=~Matthew_Hausknecht1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthew_Hausknecht1">Matthew Hausknecht</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#0IOX0YcCdTn-details-698" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0IOX0YcCdTn-details-698"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Textworld, Text-based Games, Embodied Agents, Language Grounding, Generalization, Imitation Learning, ALFRED</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Given a simple request like Put a washed apple in the kitchen fridge, humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text-based policies in TextWorld (Côté et al., 2018) and then execute goals from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER’s simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, and visual scene understanding).</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Pre-training in text-based environments allows agents to learn priors and policies that help solve embodied tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=0IOX0YcCdTn&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="vcopnwZ7bC" data-number="2633">
      <h4>
        <a href="https://openreview.net/forum?id=vcopnwZ7bC">
            Learning Task Decomposition with Ordered Memory Policy Network
        </a>
      
        
          <a href="https://openreview.net/pdf?id=vcopnwZ7bC" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yuchen_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuchen_Lu1">Yuchen Lu</a>, <a href="https://openreview.net/profile?id=~Yikang_Shen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yikang_Shen1">Yikang Shen</a>, <a href="https://openreview.net/profile?id=~Siyuan_Zhou2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siyuan_Zhou2">Siyuan Zhou</a>, <a href="https://openreview.net/profile?id=~Aaron_Courville3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aaron_Courville3">Aaron Courville</a>, <a href="https://openreview.net/profile?id=~Joshua_B._Tenenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_B._Tenenbaum1">Joshua B. Tenenbaum</a>, <a href="https://openreview.net/profile?id=~Chuang_Gan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuang_Gan1">Chuang Gan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>26 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#vcopnwZ7bC-details-374" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vcopnwZ7bC-details-374"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Task Segmentation, Hierarchical Imitation Learning, Network Inductive Bias</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Many complex real-world tasks are composed of several levels of subtasks. Humans leverage these hierarchical structures to accelerate the learning process and achieve better generalization. In this work, we study the inductive bias and propose Ordered Memory Policy Network (OMPN) to discover subtask hierarchy by learning from demonstration. The discovered subtask hierarchy could be used to perform task decomposition, recovering the subtask boundaries in an unstructured demonstration. Experiments on Craft and Dial demonstrate that our model can achieve higher task decomposition performance under both unsupervised and weakly supervised settings, comparing with strong baselines. OMPN can also be directly applied to partially observable environments and still achieve higher task decomposition performance. Our visualization further confirms that the subtask hierarchy can emerge in our model 1.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce an Ordered Memory Policy Network (OMPN) to discover task decomposition by imitation learning from demonstration.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=vcopnwZ7bC&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ijJZbomCJIm" data-number="2366">
      <h4>
        <a href="https://openreview.net/forum?id=ijJZbomCJIm">
            Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ijJZbomCJIm" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Francisco_Utrera1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Francisco_Utrera1">Francisco Utrera</a>, <a href="https://openreview.net/profile?email=kravitz%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kravitz@berkeley.edu">Evan Kravitz</a>, <a href="https://openreview.net/profile?id=~N._Benjamin_Erichson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~N._Benjamin_Erichson1">N. Benjamin Erichson</a>, <a href="https://openreview.net/profile?id=~Rajiv_Khanna1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rajiv_Khanna1">Rajiv Khanna</a>, <a href="https://openreview.net/profile?id=~Michael_W._Mahoney1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_W._Mahoney1">Michael W. Mahoney</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ijJZbomCJIm-details-207" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ijJZbomCJIm-details-207"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">transfer learning, adversarial training, influence functions, limited data</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks on image recognition tasks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labeled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better than non-adversarially-trained models, especially if only limited data are available for the new domain task. Further, we observe that adversarial training biases the learnt representations to retaining shapes, as opposed to textures, which impacts the transferability of the source models. Finally, through the lens of influence functions, we discover that transferred adversarially-trained models contain more human-identifiable semantic information, which explains -- at least partly -- why adversarially-trained models transfer better.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We demonstrate that adversarially-trained models transfer better to new domains than naturally-trained models, especially when only limited training data is available in the target domain. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=ijJZbomCJIm&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="BM---bH_RSh" data-number="1960">
      <h4>
        <a href="https://openreview.net/forum?id=BM---bH_RSh">
            UMEC: Unified model and embedding compression for efficient recommendation systems
        </a>
      
        
          <a href="https://openreview.net/pdf?id=BM---bH_RSh" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jiayi_Shen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiayi_Shen1">Jiayi Shen</a>, <a href="https://openreview.net/profile?id=~Haotao_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haotao_Wang1">Haotao Wang</a>, <a href="https://openreview.net/profile?id=~Shupeng_Gui1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shupeng_Gui1">Shupeng Gui</a>, <a href="https://openreview.net/profile?id=~Jianchao_Tan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianchao_Tan1">Jianchao Tan</a>, <a href="https://openreview.net/profile?id=~Zhangyang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhangyang_Wang1">Zhangyang Wang</a>, <a href="https://openreview.net/profile?id=~Ji_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ji_Liu1">Ji Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 02 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#BM---bH_RSh-details-580" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="BM---bH_RSh-details-580"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">recommendation system, model compression, ADMM, resource constrained</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The recommendation system (RS) plays an important role in the content recommendation and retrieval scenarios. The core part of the system is the Ranking neural network, which is usually a bottleneck of whole system performance during online inference.  In this work, we propose a unified model and embedding compression (UMEC) framework to hammer an efficient neural network-based recommendation system.  Our framework jointly learns input feature selection and neural network compression together, and solve them as an end-to-end resource-constrained optimization problem using ADMM.  Our method outperforms other baselines in terms of neural network Flops, sparse embedding feature size and the number of sparse embedding features.  We evaluate our method on the public benchmark of DLRM, trained over the Kaggle Criteo dataset. The codes can be found at https://github.com/VITA-Group/UMEC.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a unified model and embedding compression (UMEC) framework to hammer an efficient neural network-based recommendation system.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="OqtLIabPTit" data-number="1154">
      <h4>
        <a href="https://openreview.net/forum?id=OqtLIabPTit">
            Exploring Balanced Feature Spaces for Representation Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=OqtLIabPTit" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Bingyi_Kang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bingyi_Kang1">Bingyi Kang</a>, <a href="https://openreview.net/profile?id=~Yu_Li7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Li7">Yu Li</a>, <a href="https://openreview.net/profile?id=~Sa_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sa_Xie1">Sa Xie</a>, <a href="https://openreview.net/profile?id=~Zehuan_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zehuan_Yuan1">Zehuan Yuan</a>, <a href="https://openreview.net/profile?id=~Jiashi_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiashi_Feng1">Jiashi Feng</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#OqtLIabPTit-details-767" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OqtLIabPTit-details-767"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Representation Learning, Contrastive Learning, Long-Tailed Recognition</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Existing self-supervised learning (SSL) methods are mostly applied for training representation models from artificially balanced datasets (e.g., ImageNet). It is unclear how well they will perform in the practical scenarios where datasets are often imbalanced w.r.t. the classes. Motivated by this question, we conduct a series of studies on the performance of self-supervised contrastive learning and supervised learning methods over multiple datasets where training instance distributions vary from a balanced one to a long-tailed one. Our findings are quite intriguing. Different from supervised methods with large performance drop, the self-supervised contrastive learning methods perform stably well even when the datasets are heavily imbalanced. This motivates us to explore the balanced feature spaces learned by contrastive learning, where the feature representations present similar linear separability w.r.t. all the classes. Our further experiments reveal that a representation model generating a balanced feature space can generalize better than that yielding an imbalanced one across multiple settings. Inspired by these insights, we develop a novel representation learning method, called <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="190" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>-positive contrastive learning. It effectively combines strengths of the supervised method and the contrastive learning method to learn representations that are both discriminative and balanced. Extensive experiments demonstrate its superiority on multiple recognition tasks. Remarkably, it achieves new state-of-the-art on challenging long-tailed recognition benchmarks. Code and models will be released.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="eQe8DEWNN2W" data-number="812">
      <h4>
        <a href="https://openreview.net/forum?id=eQe8DEWNN2W">
            Calibration of Neural Networks using Splines
        </a>
      
        
          <a href="https://openreview.net/pdf?id=eQe8DEWNN2W" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kartik_Gupta2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kartik_Gupta2">Kartik Gupta</a>, <a href="https://openreview.net/profile?id=~Amir_Rahimi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amir_Rahimi1">Amir Rahimi</a>, <a href="https://openreview.net/profile?id=~Thalaiyasingam_Ajanthan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thalaiyasingam_Ajanthan1">Thalaiyasingam Ajanthan</a>, <a href="https://openreview.net/profile?id=~Thomas_Mensink1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Mensink1">Thomas Mensink</a>, <a href="https://openreview.net/profile?id=~Cristian_Sminchisescu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cristian_Sminchisescu1">Cristian Sminchisescu</a>, <a href="https://openreview.net/profile?id=~Richard_Hartley1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Richard_Hartley1">Richard Hartley</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#eQe8DEWNN2W-details-44" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eQe8DEWNN2W-details-44"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">neural network calibration, uncertainty, calibration measure</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Calibrating neural networks is of utmost importance when employing them in safety-critical applications where the downstream decision making depends on the predicted probabilities. Measuring calibration error amounts to comparing two empirical distributions. In this work, we introduce a binning-free calibration measure inspired by the classical Kolmogorov-Smirnov (KS) statistical test in which the main idea is to compare the respective cumulative probability distributions. From this, by approximating the empirical cumulative distribution using a differentiable function via splines, we obtain a recalibration function, which maps the network outputs to actual (calibrated) class assignment probabilities. The spline-fitting is performed using a held-out calibration set and the obtained recalibration function is evaluated on an unseen test set. We tested our method against existing calibration approaches on various image classification datasets and our spline-based recalibration approach consistently outperforms existing methods on KS error as well as other commonly used calibration measures. Code is available online at https://github.com/kartikgupta-at-anu/spline-calibration.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a binning-free calibration measure inspired by the classical Kolmogorov-Smirnov statistical test and obtain a recalibration function by approximating the empirical cumulative distribution using a differentiable function via splines.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=eQe8DEWNN2W&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="DiQD7FWL233" data-number="2560">
      <h4>
        <a href="https://openreview.net/forum?id=DiQD7FWL233">
            Improving Relational Regularized Autoencoders with Spherical Sliced Fused Gromov Wasserstein
        </a>
      
        
          <a href="https://openreview.net/pdf?id=DiQD7FWL233" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Khai_Nguyen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Khai_Nguyen1">Khai Nguyen</a>, <a href="https://openreview.net/profile?email=v.sonnv27%40vinai.io" class="profile-link" data-toggle="tooltip" data-placement="top" title="v.sonnv27@vinai.io">Son Nguyen</a>, <a href="https://openreview.net/profile?id=~Nhat_Ho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nhat_Ho1">Nhat Ho</a>, <a href="https://openreview.net/profile?email=v.tungph4%40vinai.io" class="profile-link" data-toggle="tooltip" data-placement="top" title="v.tungph4@vinai.io">Tung Pham</a>, <a href="https://openreview.net/profile?id=~Hung_Bui1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hung_Bui1">Hung Bui</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#DiQD7FWL233-details-380" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DiQD7FWL233-details-380"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Relational regularized autoencoder, deep generative model, sliced fused Gromov Wasserstein, spherical distributions</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Relational regularized autoencoder (RAE) is a framework to learn the distribution of data by minimizing a reconstruction loss together with a relational regularization on the prior of latent space. A recent attempt to reduce the inner discrepancy between the prior and aggregated posterior distributions is to incorporate sliced fused Gromov-Wasserstein (SFG) between these distributions. That approach has a weakness since it treats every slicing direction similarly, meanwhile several directions are not useful for the discriminative task. To improve the discrepancy and consequently the relational regularization, we propose a new relational discrepancy, named spherical sliced fused Gromov Wasserstein (SSFG), that can find an important area of projections characterized by a von Mises-Fisher distribution. Then, we introduce two variants of SSFG to improve its performance. The first variant, named mixture spherical sliced fused Gromov Wasserstein (MSSFG), replaces the vMF distribution by a mixture of von Mises-Fisher distributions to capture multiple important areas of directions that are far from each other. The second variant, named power spherical sliced fused Gromov Wasserstein (PSSFG), replaces the vMF distribution by a power spherical distribution to improve the sampling time of the vMF distribution in high dimension settings. We then apply the new discrepancies to the RAE framework to achieve its new variants. Finally, we conduct extensive experiments to show that the new autoencoders have favorable performance in learning latent manifold structure, image generation, and reconstruction.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Improving relational regularized autoencoder by introducing new sliced optimal transport discrepancies between the prior and aggregated posterior distributions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=DiQD7FWL233&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="09-528y2Fgf" data-number="772">
      <h4>
        <a href="https://openreview.net/forum?id=09-528y2Fgf">
            Rethinking Positional Encoding in Language Pre-training
        </a>
      
        
          <a href="https://openreview.net/pdf?id=09-528y2Fgf" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Guolin_Ke3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guolin_Ke3">Guolin Ke</a>, <a href="https://openreview.net/profile?id=~Di_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Di_He1">Di He</a>, <a href="https://openreview.net/profile?id=~Tie-Yan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tie-Yan_Liu1">Tie-Yan Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#09-528y2Fgf-details-545" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="09-528y2Fgf-details-545"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Natural Language Processing, Pre-training</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this work, we investigate the positional encoding methods used in language pre-training (e.g., BERT) and identify several problems in the existing formulations. First, we show that in the absolute positional encoding, the addition operation applied on positional embeddings and word embeddings brings mixed correlations between the two heterogeneous information resources. It may bring unnecessary randomness in the attention and further limit the expressiveness of the model.  Second, we question whether treating the position of the symbol \texttt{[CLS]} the same as other words is a reasonable design, considering its special role (the representation of the entire sentence) in the downstream tasks. Motivated from above analysis, we propose a new positional encoding method called \textbf{T}ransformer with \textbf{U}ntied \textbf{P}ositional \textbf{E}ncoding (TUPE). In the self-attention module, TUPE computes the word contextual correlation and positional correlation separately with different parameterizations and then adds them together. This design removes the mixed and noisy correlations over heterogeneous embeddings and offers more expressiveness by using different projection matrices. Furthermore, TUPE unties the \texttt{[CLS]} symbol from other positions, making it easier to capture information from all positions. Extensive experiments and ablation studies on GLUE benchmark demonstrate the effectiveness of the proposed method. Codes and models are released at \url{https://github.com/guolinke/TUPE}.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A novel and better positional encoding method for Transformer-based language pre-training models.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="jP1vTH3inC" data-number="2461">
      <h4>
        <a href="https://openreview.net/forum?id=jP1vTH3inC">
            Discovering Non-monotonic Autoregressive Orderings with Variational Inference
        </a>
      
        
          <a href="https://openreview.net/pdf?id=jP1vTH3inC" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xuanlin_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuanlin_Li1">Xuanlin Li</a>, <a href="https://openreview.net/profile?id=~Brandon_Trabucco1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brandon_Trabucco1">Brandon Trabucco</a>, <a href="https://openreview.net/profile?id=~Dong_Huk_Park2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dong_Huk_Park2">Dong Huk Park</a>, <a href="https://openreview.net/profile?id=~Michael_Luo2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Luo2">Michael Luo</a>, <a href="https://openreview.net/profile?id=~Sheng_Shen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sheng_Shen2">Sheng Shen</a>, <a href="https://openreview.net/profile?id=~Trevor_Darrell2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Trevor_Darrell2">Trevor Darrell</a>, <a href="https://openreview.net/profile?id=~Yang_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Gao1">Yang Gao</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#jP1vTH3inC-details-226" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jP1vTH3inC-details-226"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">variational inference, unsupervised learning, computer vision, natural language processing, optimization, reinforcement learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The predominant approach for language modeling is to encode a sequence of tokens from left to right, but this eliminates a source of information: the order by which the sequence was naturally generated. One strategy to recover this information is to decode both the content and ordering of tokens. Some prior work supervises content and ordering with hand-designed loss functions to encourage specific orders or bootstraps from a predefined ordering. These approaches require domain-specific insight. Other prior work searches over valid insertion operations that lead to ground truth sequences during training, which has high time complexity and cannot be efficiently parallelized. We address these limitations with an unsupervised learner that can be trained in a fully-parallelizable manner to discover high-quality autoregressive orders in a data driven way without a domain-specific prior. The learner is a neural network that performs variational inference with the autoregressive ordering as a latent variable. Since the corresponding variational lower bound is not differentiable, we develop a practical algorithm for end-to-end optimization using policy gradients. Strong empirical results with our solution on sequence modeling tasks suggest that our algorithm is capable of discovering various autoregressive orders for different sequences that are competitive with or even better than fixed orders.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">The paper proposes an unsupervised learner that discovers non-monotonic autoregressive orders for sequence generation through fully-parallelizable end-to-end training without domain-specific prior.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=jP1vTH3inC&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="qYZD-AO1Vn" data-number="3480">
      <h4>
        <a href="https://openreview.net/forum?id=qYZD-AO1Vn">
            Differentiable Trust Region Layers for Deep Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=qYZD-AO1Vn" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Fabian_Otto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fabian_Otto1">Fabian Otto</a>, <a href="https://openreview.net/profile?id=~Philipp_Becker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philipp_Becker1">Philipp Becker</a>, <a href="https://openreview.net/profile?id=~Vien_Anh_Ngo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vien_Anh_Ngo1">Vien Anh Ngo</a>, <a href="https://openreview.net/profile?id=~Hanna_Carolin_Maria_Ziesche1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hanna_Carolin_Maria_Ziesche1">Hanna Carolin Maria Ziesche</a>, <a href="https://openreview.net/profile?id=~Gerhard_Neumann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gerhard_Neumann1">Gerhard Neumann</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#qYZD-AO1Vn-details-999" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qYZD-AO1Vn-details-999"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, trust region, policy gradient, projection, Wasserstein distance, Kullback-Leibler divergence, Frobenius norm</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Trust region methods are a popular tool in reinforcement learning as they yield robust policy updates in continuous and discrete action spaces. However, enforcing such trust regions in deep reinforcement learning is difficult. Hence, many approaches, such as Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), are based on approximations. Due to those approximations, they violate the constraints or fail to find the optimal solution within the trust region. Moreover, they are difficult to implement, often lack sufficient exploration, and have been shown to depend on seemingly unrelated implementation choices. In this work, we propose differentiable neural network layers to enforce trust regions for deep Gaussian policies via closed-form projections. Unlike existing methods, those layers formalize trust regions for each state individually and can complement existing reinforcement learning algorithms. We derive trust region projections based on the Kullback-Leibler divergence, the Wasserstein L2 distance, and the Frobenius norm for Gaussian distributions. We empirically demonstrate that those projection layers achieve similar or better results than existing methods while being almost agnostic to specific implementation choices. The code is available at https://git.io/Jthb0.
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-M0QkvBGTTq" data-number="2486">
      <h4>
        <a href="https://openreview.net/forum?id=-M0QkvBGTTq">
            SaliencyMix: A Saliency Guided Data Augmentation Strategy for Better Regularization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-M0QkvBGTTq" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~A_F_M_Shahab_Uddin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~A_F_M_Shahab_Uddin1">A F M Shahab Uddin</a>, <a href="https://openreview.net/profile?id=~Mst._Sirazam_Monira1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mst._Sirazam_Monira1">Mst. Sirazam Monira</a>, <a href="https://openreview.net/profile?email=wheemi%40khu.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="wheemi@khu.ac.kr">Wheemyung Shin</a>, <a href="https://openreview.net/profile?email=tcchung%40khu.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="tcchung@khu.ac.kr">TaeChoong Chung</a>, <a href="https://openreview.net/profile?id=~Sung-Ho_Bae1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sung-Ho_Bae1">Sung-Ho Bae</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-M0QkvBGTTq-details-789" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-M0QkvBGTTq-details-789"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">SaliencyMix, Saliency Guided Data Augmentation, Data Augmentation, Regularization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Advanced data augmentation strategies have widely been studied to improve the generalization ability of deep learning models. Regional dropout is one of the popular solutions that guides the model to focus on less discriminative parts by randomly removing image regions, resulting in improved regularization. However, such information removal is undesirable. On the other hand, recent strategies suggest to randomly cut and mix patches and their labels among training images, to enjoy the advantages of regional dropout without having any pointless pixel in the augmented images. We argue that such random selection strategies of the patches may not necessarily represent sufficient information about the corresponding object and thereby mixing the labels according to that uninformative patch enables the model to learn unexpected feature representation. Therefore, we propose SaliencyMix that carefully selects a representative image patch with the help of a saliency map and mixes this indicative patch with the target image, thus leading the model to learn more appropriate feature representation. SaliencyMix achieves the best known top-1 error of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="191" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>21.26</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="192" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c39"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>20.09</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> for ResNet-50 and ResNet-101 architectures on ImageNet classification, respectively, and also improves the model robustness against adversarial perturbations. Furthermore, models that are trained with SaliencyMix, help to improve the object detection performance.  Source code is available at \url{https://github.com/SaliencyMix/SaliencyMix}.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">The proposed method carefully selects a representative image patch with the help of a saliency map and mixes that indicative patch with the target image that leads the model to learn more appropriate feature representation</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=-M0QkvBGTTq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="CGQ6ENUMX6" data-number="1368">
      <h4>
        <a href="https://openreview.net/forum?id=CGQ6ENUMX6">
            Task-Agnostic Morphology Evolution
        </a>
      
        
          <a href="https://openreview.net/pdf?id=CGQ6ENUMX6" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Donald_Joseph_Hejna_III1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Donald_Joseph_Hejna_III1">Donald Joseph Hejna III</a>, <a href="https://openreview.net/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="https://openreview.net/profile?id=~Lerrel_Pinto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lerrel_Pinto1">Lerrel Pinto</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 08 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#CGQ6ENUMX6-details-174" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CGQ6ENUMX6-details-174"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">morphology, unsupervised, evolution, information theory, empowerment</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep reinforcement learning primarily focuses on learning behavior, usually overlooking the fact that an agent's function is largely determined by form. So, how should one go about finding a morphology fit for solving tasks in a given environment? Current approaches that co-adapt morphology and behavior use a specific task's reward as a signal for morphology optimization. However, this often requires expensive policy optimization and results in task-dependent morphologies that are not built to generalize. In this work, we propose a new approach, Task-Agnostic Morphology Evolution (TAME), to alleviate both of these issues. Without any task or reward specification, TAME evolves morphologies by only applying randomly sampled action primitives on a population of agents. This is accomplished using an information-theoretic objective that efficiently ranks agents by their ability to reach diverse states in the environment and the causality of their actions. Finally, we empirically demonstrate that across 2D, 3D, and manipulation environments TAME can evolve morphologies that match the multi-task performance of those learned with task supervised algorithms. Our code and videos can be found at https://sites.google.com/view/task-agnostic-evolution .
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce TAME, a novel method for optimizing agent morphology using only  randomly sampled action primitives and no task driven reward signals.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=CGQ6ENUMX6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="TuK6agbdt27" data-number="3467">
      <h4>
        <a href="https://openreview.net/forum?id=TuK6agbdt27">
            Learning Associative Inference Using Fast Weight Memory
        </a>
      
        
          <a href="https://openreview.net/pdf?id=TuK6agbdt27" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Imanol_Schlag3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Imanol_Schlag3">Imanol Schlag</a>, <a href="https://openreview.net/profile?id=~Tsendsuren_Munkhdalai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tsendsuren_Munkhdalai1">Tsendsuren Munkhdalai</a>, <a href="https://openreview.net/profile?id=~J%C3%BCrgen_Schmidhuber1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jürgen_Schmidhuber1">Jürgen Schmidhuber</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#TuK6agbdt27-details-448" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TuK6agbdt27-details-448"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">memory-augmented neural networks, tensor product, fast weights</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Humans can quickly associate stimuli to solve problems in novel contexts. Our novel neural network model learns state representations of facts that can be composed to perform such associative inference. To this end, we augment the LSTM model with an associative memory, dubbed \textit{Fast Weight Memory} (FWM). Through differentiable operations at every step of a given input sequence, the LSTM \textit{updates and maintains} compositional associations stored in the rapidly changing FWM weights. Our model is trained end-to-end by gradient descent and yields excellent performance on compositional language reasoning problems, meta-reinforcement-learning for POMDPs, and small-scale word-level language modelling.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a Recurrent Neural Network model which is augmented with an associative memory to generalise in a more systematically</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=TuK6agbdt27&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ebS5NUfoMKL" data-number="3104">
      <h4>
        <a href="https://openreview.net/forum?id=ebS5NUfoMKL">
            Boost then Convolve: Gradient Boosting Meets Graph Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ebS5NUfoMKL" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sergei_Ivanov2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergei_Ivanov2">Sergei Ivanov</a>, <a href="https://openreview.net/profile?id=~Liudmila_Prokhorenkova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liudmila_Prokhorenkova1">Liudmila Prokhorenkova</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ebS5NUfoMKL-details-605" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ebS5NUfoMKL-details-605"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">GNN, GBDT, graphs, tabular data, heterogeneous data</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Graph neural networks (GNNs) are powerful models that have been successful in various graph representation learning tasks. Whereas gradient boosted decision trees (GBDT) often outperform other machine learning methods when faced with heterogeneous tabular data. But what approach should be used for graphs with tabular node features? Previous GNN models have mostly focused on networks with homogeneous sparse features and, as we show, are suboptimal in the heterogeneous setting. In this work, we propose a novel architecture that trains GBDT and GNN jointly to get the best of both worlds: the GBDT model deals with heterogeneous features, while GNN accounts for the graph structure. Our model benefits from end-to-end optimization by allowing new trees to fit the gradient updates of GNN. With an extensive experimental comparison to the leading GBDT and GNN models, we demonstrate a significant increase in performance on a variety of graphs with tabular features. The code is available: https://github.com/nd7141/bgnn.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A novel strong architecture that combines advantages of GBDT and GNN for node-level prediction problems on graphs with tabular data.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=ebS5NUfoMKL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="NSBrFgJAHg" data-number="2013">
      <h4>
        <a href="https://openreview.net/forum?id=NSBrFgJAHg">
            Degree-Quant: Quantization-Aware Training for Graph Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=NSBrFgJAHg" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Shyam_Anil_Tailor1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shyam_Anil_Tailor1">Shyam Anil Tailor</a>, <a href="https://openreview.net/profile?id=~Javier_Fernandez-Marques1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Javier_Fernandez-Marques1">Javier Fernandez-Marques</a>, <a href="https://openreview.net/profile?id=~Nicholas_Donald_Lane1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_Donald_Lane1">Nicholas Donald Lane</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#NSBrFgJAHg-details-738" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NSBrFgJAHg-details-738"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Graph neural networks, quantization, benchmark</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Graph neural networks (GNNs) have demonstrated strong performance on a wide variety of tasks due to their ability to model non-uniform structured data. Despite their promise, there exists little research exploring methods to make them more efficient at inference time. In this work, we explore the viability of training quantized GNNs, enabling the usage of low precision integer arithmetic during inference. For GNNs seemingly unimportant choices in quantization implementation cause dramatic changes in performance. We identify the sources of error that uniquely arise when attempting to quantize GNNs, and propose an architecturally-agnostic and stable method, Degree-Quant, to improve performance over existing quantization-aware training baselines commonly used on other architectures, such as CNNs. We validate our method on six datasets and show, unlike previous quantization attempts, that models generalize to unseen graphs. Models trained with Degree-Quant for INT8 quantization perform as well as FP32 models in most cases; for INT4 models, we obtain up to 26% gains over the baselines. Our work enables up to 4.7x speedups on CPU when using INT8 arithmetic.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We provide a training technique that enables graph neural networks to use low precision integer arithmetic at inference time, yielding up to 4.7x latency improvements on CPU</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=NSBrFgJAHg&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Cb54AMqHQFP" data-number="69">
      <h4>
        <a href="https://openreview.net/forum?id=Cb54AMqHQFP">
            Network Pruning That Matters:  A Case Study on Retraining Variants
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Cb54AMqHQFP" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Duong_Hoang_Le2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Duong_Hoang_Le2">Duong Hoang Le</a>, <a href="https://openreview.net/profile?id=~Binh-Son_Hua1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Binh-Son_Hua1">Binh-Son Hua</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Cb54AMqHQFP-details-148" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Cb54AMqHQFP-details-148"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Network Pruning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Network pruning is an effective method to reduce the computational expense of over-parameterized neural networks for deployment on low-resource systems. Recent state-of-the-art techniques for retraining pruned networks such as weight rewinding and learning rate rewinding have been shown to outperform the traditional fine-tuning technique in recovering the lost accuracy (Renda et al., 2020), but so far it is unclear what accounts for such performance. In this work, we conduct extensive experiments to verify and analyze the uncanny effectiveness of learning rate rewinding. We find that the reason behind the success of learning rate rewinding is the usage of a large learning rate. Similar phenomenon can be observed in other learning rate schedules that involve large learning rates, e.g., the 1-cycle learning rate schedule (Smith et al., 2019). By leveraging the right learning rate schedule in retraining, we demonstrate a counter-intuitive phenomenon in that randomly pruned networks could even achieve better performance than methodically pruned networks (fine-tuned with the conventional approach). Our results emphasize the cruciality of the learning rate schedule in pruned network retraining - a detail often overlooked by practitioners during the implementation of network pruning. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We study the effective of different retraining mechanisms while doing pruning</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="MJAqnaC2vO1" data-number="1042">
      <h4>
        <a href="https://openreview.net/forum?id=MJAqnaC2vO1">
            Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=MJAqnaC2vO1" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=haoli%40link.cuhk.edu.hk" class="profile-link" data-toggle="tooltip" data-placement="top" title="haoli@link.cuhk.edu.hk">Hao Li</a>, <a href="https://openreview.net/profile?id=~Chenxin_Tao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chenxin_Tao2">Chenxin Tao</a>, <a href="https://openreview.net/profile?id=~Xizhou_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xizhou_Zhu1">Xizhou Zhu</a>, <a href="https://openreview.net/profile?id=~Xiaogang_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaogang_Wang2">Xiaogang Wang</a>, <a href="https://openreview.net/profile?id=~Gao_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gao_Huang1">Gao Huang</a>, <a href="https://openreview.net/profile?id=~Jifeng_Dai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jifeng_Dai1">Jifeng Dai</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#MJAqnaC2vO1-details-674" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MJAqnaC2vO1-details-674"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Loss Function Search, Metric Surrogate, Semantic Segmentation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Designing proper loss functions is essential in training deep networks. Especially in the field of semantic segmentation, various evaluation metrics have been proposed for diverse scenarios. Despite the success of the widely adopted cross-entropy loss and its variants, the mis-alignment between the loss functions and evaluation metrics degrades the network performance. Meanwhile, manually designing loss functions for each specific metric requires expertise and significant manpower. In this paper, we propose to automate the design of metric-specific loss functions by searching differentiable surrogate losses for each metric. We substitute the non-differentiable operations in the metrics with parameterized functions, and conduct parameter search to optimize the shape of loss surfaces. Two constraints are introduced to regularize the search space and make the search efficient. Extensive experiments on PASCAL VOC and Cityscapes demonstrate that the searched surrogate losses outperform the manually designed loss functions consistently. The searched losses can generalize well to other datasets and networks. Code shall be released at https://github.com/fundamentalvision/Auto-Seg-Loss.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Auto Seg-Loss is the first general framework for searching surrogate losses for mainstream semantic segmentation metrics. </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=MJAqnaC2vO1&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="4T489T4yav" data-number="3185">
      <h4>
        <a href="https://openreview.net/forum?id=4T489T4yav">
            Differentiable Segmentation of Sequences
        </a>
      
        
          <a href="https://openreview.net/pdf?id=4T489T4yav" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Erik_Scharw%C3%A4chter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Erik_Scharwächter1">Erik Scharwächter</a>, <a href="https://openreview.net/profile?email=jlen%40uni-bonn.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="jlen@uni-bonn.de">Jonathan Lennartz</a>, <a href="https://openreview.net/profile?email=emmanuel.mueller%40cs.tu-dortmund.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="emmanuel.mueller@cs.tu-dortmund.de">Emmanuel Müller</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#4T489T4yav-details-838" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4T489T4yav-details-838"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">segmented models, segmentation, change point detection, concept drift, warping functions, gradient descent</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Segmented models are widely used to describe non-stationary sequential data with discrete change points. Their estimation usually requires solving a mixed discrete-continuous optimization problem, where the segmentation is the discrete part and all other model parameters are continuous. A number of estimation algorithms have been developed that are highly specialized for their specific model assumptions. The dependence on non-standard algorithms makes it hard to integrate segmented models in state-of-the-art deep learning architectures that critically depend on gradient-based optimization techniques. In this work, we formulate a relaxed variant of segmented models that enables joint estimation of all model parameters, including the segmentation, with gradient descent. We build on recent advances in learning continuous warping functions and propose a novel family of warping functions based on the two-sided power (TSP) distribution. TSP-based warping functions are differentiable, have simple closed-form expressions, and can represent segmentation functions exactly. Our formulation includes the important class of segmented generalized linear models as a special case, which makes it highly versatile. We use our approach to model the spread of COVID-19 with Poisson regression, apply it on a change point detection task, and learn classification models with concept drift. The experiments show that our approach effectively learns all these tasks with standard algorithms for gradient descent.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an architecture for effective gradient-based learning of segmented models for sequential data.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=4T489T4yav&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="bhCDO_cEGCz" data-number="2607">
      <h4>
        <a href="https://openreview.net/forum?id=bhCDO_cEGCz">
            Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=bhCDO_cEGCz" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhenfang_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhenfang_Chen1">Zhenfang Chen</a>, <a href="https://openreview.net/profile?id=~Jiayuan_Mao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiayuan_Mao1">Jiayuan Mao</a>, <a href="https://openreview.net/profile?id=~Jiajun_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiajun_Wu1">Jiajun Wu</a>, <a href="https://openreview.net/profile?id=~Kwan-Yee_Kenneth_Wong2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kwan-Yee_Kenneth_Wong2">Kwan-Yee Kenneth Wong</a>, <a href="https://openreview.net/profile?id=~Joshua_B._Tenenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_B._Tenenbaum1">Joshua B. Tenenbaum</a>, <a href="https://openreview.net/profile?id=~Chuang_Gan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuang_Gan1">Chuang Gan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Apr 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#bhCDO_cEGCz-details-958" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bhCDO_cEGCz-details-958"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Concept Learning, Neuro-Symbolic Learning, Video Reasoning, Visual Reasoning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the problem of dynamic visual reasoning on raw videos. This is a challenging problem; currently, state-of-the-art models often require dense supervision on physical object properties and events from simulation, which are impractical to obtain in real life. In this paper, we present the Dynamic Concept Learner (DCL), a unified framework that grounds physical objects and events from video and language. DCL first adopts a trajectory extractor to track each object over time and to represent it as a latent, object-centric feature vector. Building upon this object-centric representation, DCL learns to approximate the dynamic interaction among objects using graph networks. DCL further incorporates a semantic parser to parse question into semantic programs and, finally, a program executor to run the program to answer the question, levering the learned dynamics model. After training, DCL can detect and associate objects across the frames, ground visual properties and physical events, understand the causal relationship between events, make future and counterfactual predictions, and leverage these extracted presentations for answering queries. DCL achieves state-of-the-art performance on CLEVRER, a challenging causal video reasoning dataset, even without using ground-truth attributes and collision labels from simulations for training. We further test DCL on a newly proposed video-retrieval and event localization dataset derived from CLEVRER, showing its strong generalization capacity.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a neural-symbolic framework to learn physical concepts of objects and events via causal reasoning on videos.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="sy4Kg_ZQmS7" data-number="1670">
      <h4>
        <a href="https://openreview.net/forum?id=sy4Kg_ZQmS7">
            Learning Deep Features in Instrumental Variable Regression
        </a>
      
        
          <a href="https://openreview.net/pdf?id=sy4Kg_ZQmS7" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Liyuan_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liyuan_Xu1">Liyuan Xu</a>, <a href="https://openreview.net/profile?id=~Yutian_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yutian_Chen1">Yutian Chen</a>, <a href="https://openreview.net/profile?email=sidsrini%40cs.washington.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sidsrini@cs.washington.edu">Siddarth Srinivasan</a>, <a href="https://openreview.net/profile?id=~Nando_de_Freitas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nando_de_Freitas1">Nando de Freitas</a>, <a href="https://openreview.net/profile?id=~Arnaud_Doucet2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arnaud_Doucet2">Arnaud Doucet</a>, <a href="https://openreview.net/profile?id=~Arthur_Gretton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arthur_Gretton1">Arthur Gretton</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#sy4Kg_ZQmS7-details-834" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sy4Kg_ZQmS7-details-834"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Causal Inference, Instrumental Variable Regression, Deep Learning, Reinforcement Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Instrumental variable (IV) regression is a standard strategy for learning causal relationships between confounded treatment and outcome variables from observational data by using an instrumental variable, which affects the outcome only through the treatment. In classical IV regression, learning proceeds in two stages: stage 1 performs linear regression from the instrument to the treatment; and stage 2 performs linear regression from the treatment to the outcome, conditioned on the instrument. We propose a novel method, deep feature instrumental variable regression (DFIV), to address the case where relations between instruments, treatments, and outcomes may be nonlinear. In this case, deep neural nets are trained to define informative nonlinear features on the instruments and treatments. We propose an alternating training regime for these features to ensure good end-to-end performance when composing stages 1 and 2, thus obtaining highly flexible feature maps in a computationally efficient manner.
      DFIV outperforms recent state-of-the-art methods on challenging IV benchmarks, including settings involving high dimensional image data. DFIV also exhibits competitive performance in off-policy policy evaluation for reinforcement learning, which can be understood as an IV regression task.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Propose a novel deep learning based method for instrumental variable regression</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=sy4Kg_ZQmS7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="_i3ASPp12WS" data-number="2704">
      <h4>
        <a href="https://openreview.net/forum?id=_i3ASPp12WS">
            Online Adversarial Purification based on Self-supervised Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=_i3ASPp12WS" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Changhao_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changhao_Shi1">Changhao Shi</a>, <a href="https://openreview.net/profile?id=~Chester_Holtz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chester_Holtz1">Chester Holtz</a>, <a href="https://openreview.net/profile?id=~Gal_Mishne1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gal_Mishne1">Gal Mishne</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 14 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#_i3ASPp12WS-details-742" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_i3ASPp12WS-details-742"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Adversarial Robustness, Self-Supervised Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep neural networks are known to be vulnerable to adversarial examples, where a perturbation in the input space leads to an amplified shift in the latent network representation. In this paper, we combine canonical supervised learning with self-supervised representation learning, and present Self-supervised Online Adversarial Purification (SOAP), a novel defense strategy that uses a self-supervised loss to purify adversarial examples at test-time. Our approach leverages the label-independent nature of self-supervised signals and counters the adversarial perturbation with respect to the self-supervised tasks. SOAP yields competitive robust accuracy against state-of-the-art adversarial training and purification methods, with considerably less training complexity. In addition, our approach is robust even when adversaries are given the knowledge of the purification defense strategy. To the best of our knowledge, our paper is the first that generalizes the idea of using self-supervised signals to perform online test-time purification.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="bM4Iqfg8M2k" data-number="779">
      <h4>
        <a href="https://openreview.net/forum?id=bM4Iqfg8M2k">
            Graph Information Bottleneck for Subgraph Recognition
        </a>
      
        
          <a href="https://openreview.net/pdf?id=bM4Iqfg8M2k" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Junchi_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junchi_Yu1">Junchi Yu</a>, <a href="https://openreview.net/profile?id=~Tingyang_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tingyang_Xu1">Tingyang Xu</a>, <a href="https://openreview.net/profile?id=~Yu_Rong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Rong1">Yu Rong</a>, <a href="https://openreview.net/profile?id=~Yatao_Bian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yatao_Bian1">Yatao Bian</a>, <a href="https://openreview.net/profile?id=~Junzhou_Huang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junzhou_Huang2">Junzhou Huang</a>, <a href="https://openreview.net/profile?id=~Ran_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ran_He1">Ran He</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#bM4Iqfg8M2k-details-845" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bM4Iqfg8M2k-details-845"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Given the input graph and its label/property, several key problems  of graph learning, such as finding interpretable subgraphs, graph denoising and graph compression,  can be  attributed to the fundamental problem of recognizing a subgraph of the original one.  This subgraph shall be as informative as possible, yet contains less redundant and noisy structure. This problem setting is closely related to the well-known information bottleneck (IB) principle, which, however, has less been studied for the irregular graph data and graph neural networks (GNNs). In this paper, we propose a framework of Graph Information Bottleneck (GIB) for the subgraph recognition problem in deep graph learning. Under this framework, one can recognize the maximally informative yet compressive subgraph, named IB-subgraph.  However, the GIB objective is notoriously hard to optimize, mostly due to the intractability of the mutual information of irregular graph data and the unstable optimization process. In order to tackle these challenges, we propose:  i) a GIB objective based-on a mutual information estimator for the irregular graph data; ii) a bi-level optimization scheme to maximize the GIB objective; iii) a connectivity loss to stabilize the optimization process. We evaluate the properties of the IB-subgraph in three application scenarios: improvement of graph classification, graph interpretation and graph denoising. Extensive experiments demonstrate that the information-theoretic  IB-subgraph  enjoys superior graph properties. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="lQdXeXDoWtI" data-number="1580">
      <h4>
        <a href="https://openreview.net/forum?id=lQdXeXDoWtI">
            In Search of Lost Domain Generalization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=lQdXeXDoWtI" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ishaan_Gulrajani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ishaan_Gulrajani1">Ishaan Gulrajani</a>, <a href="https://openreview.net/profile?id=~David_Lopez-Paz2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Lopez-Paz2">David Lopez-Paz</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#lQdXeXDoWtI-details-886" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lQdXeXDoWtI-details-886"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">domain generalization, reproducible research</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The goal of domain generalization algorithms is to predict well on distributions different from those seen during training.
      While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions---datasets, network architectures, and model selection criteria---render fair comparisons difficult.
      The goal of this paper is to understand how useful domain generalization algorithms are in realistic settings.
      As a first step, we realize that model selection is non-trivial for domain generalization tasks, and we argue that algorithms without a model selection criterion remain incomplete.
      Next we implement DomainBed, a testbed for domain generalization including seven benchmarks, fourteen algorithms, and three model selection criteria.
      When conducting extensive experiments using DomainBed we find that when carefully implemented and tuned, ERM outperforms the state-of-the-art in terms of average performance.
      Furthermore, no algorithm included in DomainBed outperforms ERM by more than one point when evaluated under the same experimental conditions.
      We hope that the release of DomainBed, alongside contributions from fellow researchers, will streamline reproducible and rigorous advances in domain generalization.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Our ERM baseline achieves state-of-the-art performance across many domain generalization benchmarks</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=lQdXeXDoWtI&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="lmTWnm3coJJ" data-number="3222">
      <h4>
        <a href="https://openreview.net/forum?id=lmTWnm3coJJ">
            Robust Curriculum Learning: from clean label detection to noisy label self-correction
        </a>
      
        
          <a href="https://openreview.net/pdf?id=lmTWnm3coJJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tianyi_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianyi_Zhou1">Tianyi Zhou</a>, <a href="https://openreview.net/profile?id=~Shengjie_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shengjie_Wang1">Shengjie Wang</a>, <a href="https://openreview.net/profile?id=~Jeff_Bilmes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jeff_Bilmes1">Jeff Bilmes</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#lmTWnm3coJJ-details-593" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lmTWnm3coJJ-details-593"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">curriculum learning, noisy label, robust learning, training dynamics, neural networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Neural network training can easily overfit noisy labels resulting in poor generalization performance. Existing methods address this problem by (1) filtering out the noisy data and only using the clean data for training or (2) relabeling the noisy data by the model during training or by another model trained only on a clean dataset. However, the former does not leverage the features' information of wrongly-labeled data, while the latter may produce wrong pseudo-labels for some data and introduce extra noises. In this paper, we propose a smooth transition and interplay between these two strategies as a curriculum that selects training samples dynamically. In particular, we start with learning from clean data and then gradually move to learn noisy-labeled data with pseudo labels produced by a time-ensemble of the model and data augmentations. Instead of using the instantaneous loss computed at the current step, our data selection is based on the dynamics of both the loss and output consistency for each sample across historical steps and different data augmentations, resulting in more precise detection of both clean labels and correct pseudo labels. On multiple benchmarks of noisy labels, we show that our curriculum learning strategy can significantly improve the test accuracy without any auxiliary model or extra clean data.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">RoCL improves noisy label learning by periodical transitions from supervised learning of clean labeled data to self-supervision of wrongly-labeled data, where the data are selected according to training dynamics.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=lmTWnm3coJJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="hWr3e3r-oH5" data-number="2362">
      <h4>
        <a href="https://openreview.net/forum?id=hWr3e3r-oH5">
            Cross-Attentional Audio-Visual Fusion for Weakly-Supervised Action Localization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=hWr3e3r-oH5" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jun-Tae_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jun-Tae_Lee1">Jun-Tae Lee</a>, <a href="https://openreview.net/profile?id=~Mihir_Jain1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihir_Jain1">Mihir Jain</a>, <a href="https://openreview.net/profile?email=hwoopark%40qti.qualcomm.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="hwoopark@qti.qualcomm.com">Hyoungwoo Park</a>, <a href="https://openreview.net/profile?email=sungrack%40qti.qualcomm.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sungrack@qti.qualcomm.com">Sungrack Yun</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#hWr3e3r-oH5-details-621" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hWr3e3r-oH5-details-621"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Audio-Visual, Multimodal Attention, Action localization, Event localization, Weak-supervision</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Temporally localizing actions in videos is one of the key components for video understanding. Learning from weakly-labeled data is seen as a potential solution towards avoiding expensive frame-level annotations. Different from other works which only depend on visual-modality, we propose to learn richer audiovisual representation for weakly-supervised action localization. First, we propose a multi-stage cross-attention mechanism to collaboratively fuse audio and visual features, which preserves the intra-modal characteristics. Second, to model both foreground and background frames, we construct an open-max classifier that treats the background class as an open-set. Third, for precise action localization, we design consistency losses to enforce temporal continuity for the action class prediction, and also help with foreground-prediction reliability. Extensive experiments on two publicly available video-datasets (AVE and ActivityNet1.2) show that the proposed method effectively fuses audio and visual modalities, and achieves the state-of-the-art results for weakly-supervised action localization.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">An audio-visual fusion technique, called "multi-stage cross-attention", is developed to exploit the multi-modal representation in weakly-supervised action or event localization in untrimmed videos.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="VD_ozqvBy4W" data-number="3544">
      <h4>
        <a href="https://openreview.net/forum?id=VD_ozqvBy4W">
            CoCon: A Self-Supervised Approach for Controlled Text Generation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=VD_ozqvBy4W" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alvin_Chan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alvin_Chan1">Alvin Chan</a>, <a href="https://openreview.net/profile?id=~Yew-Soon_Ong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yew-Soon_Ong1">Yew-Soon Ong</a>, <a href="https://openreview.net/profile?email=pung0013%40e.ntu.edu.sg" class="profile-link" data-toggle="tooltip" data-placement="top" title="pung0013@e.ntu.edu.sg">Bill Pung</a>, <a href="https://openreview.net/profile?id=~Aston_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aston_Zhang2">Aston Zhang</a>, <a href="https://openreview.net/profile?id=~Jie_Fu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jie_Fu2">Jie Fu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#VD_ozqvBy4W-details-317" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="VD_ozqvBy4W-details-317"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Language modeling, text generation, controlled generation, self-supervised learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Pretrained Transformer-based language models (LMs) display remarkable natural language generation capabilities. With their immense potential, controlling text generation of such LMs is getting attention. While there are studies that seek to control high-level attributes (such as sentiment and topic) of generated text, there is still a lack of more precise control over its content at the word- and phrase-level. Here, we propose Content-Conditioner (CoCon) to control an LM's output text with a content input, at a fine-grained level. In our self-supervised approach, the CoCon block learns to help the LM complete a partially-observed text sequence by conditioning with content inputs that are withheld from the LM. Through experiments, we show that CoCon can naturally incorporate target content into generated texts and control high-level text attributes in a zero-shot manner.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose CoCon to control the content of text generation from LMs by conditioning on content inputs at an interleave layer.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="rgFNuJHHXv" data-number="1928">
      <h4>
        <a href="https://openreview.net/forum?id=rgFNuJHHXv">
            Group Equivariant Generative Adversarial Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=rgFNuJHHXv" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Neel_Dey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neel_Dey1">Neel Dey</a>, <a href="https://openreview.net/profile?id=~Antong_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Antong_Chen2">Antong Chen</a>, <a href="https://openreview.net/profile?id=~Soheil_Ghafurian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soheil_Ghafurian1">Soheil Ghafurian</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#rgFNuJHHXv-details-476" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rgFNuJHHXv-details-476"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Group Equivariance, Geometric Deep Learning, Generative Adversarial Networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent improvements in generative adversarial visual synthesis incorporate real and fake image transformation in a self-supervised setting, leading to increased stability and perceptual fidelity. However, these approaches typically involve image augmentations via additional regularizers in the GAN objective and thus spend valuable network capacity towards approximating transformation equivariance instead of their desired task. In this work, we explicitly incorporate inductive symmetry priors into the network architectures via group-equivariant convolutional networks. Group-convolutions have higher expressive power with fewer samples and lead to better gradient feedback between generator and discriminator. We show that group-equivariance integrates seamlessly with recent techniques for GAN training across regularizers, architectures, and loss functions. We demonstrate the utility of our methods for conditional synthesis by improving generation in the limited data regime across symmetric imaging datasets and even find benefits for natural images with preferred orientation.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Equivariance to symmetry groups improves the generative adversarial synthesis of symmetric images.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="YmA86Zo-P_t" data-number="878">
      <h4>
        <a href="https://openreview.net/forum?id=YmA86Zo-P_t">
            What they do when in doubt: a study of inductive biases in seq2seq learners
        </a>
      
        
          <a href="https://openreview.net/pdf?id=YmA86Zo-P_t" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Eugene_Kharitonov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eugene_Kharitonov1">Eugene Kharitonov</a>, <a href="https://openreview.net/profile?id=~Rahma_Chaabouni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rahma_Chaabouni1">Rahma Chaabouni</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>19 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#YmA86Zo-P_t-details-459" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YmA86Zo-P_t-details-459"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">inductive biases, description length, sequence-to-sequence models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Sequence-to-sequence (seq2seq) learners are widely used, but we still have only limited knowledge about what inductive biases shape the way they generalize. We address that by investigating how popular seq2seq learners generalize in tasks that have high ambiguity in the training data. We use four new tasks  to study learners' preferences for memorization, arithmetic, hierarchical, and compositional reasoning. Further, we connect to Solomonoff's theory of induction and propose to use description length as a principled and sensitive measure of inductive biases. In our experimental study, we find that LSTM-based learners can learn to perform counting, addition, and multiplication by a constant from a single training example. Furthermore, Transformer and LSTM-based learners show a bias toward the hierarchical induction over the linear one, while CNN-based learners prefer the opposite. The latter also show a bias toward a compositional generalization over memorization. Finally, across all our experiments, description length proved to be a sensitive measure of inductive biases.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Standard seq2seq models can infer perfectly different rules when presented with as little as one training example, showing strikingly different inductive biases that can be studied via description length</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ECuvULjFQia" data-number="3758">
      <h4>
        <a href="https://openreview.net/forum?id=ECuvULjFQia">
            A teacher-student framework to distill future trajectories
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ECuvULjFQia" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alexander_Neitz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Neitz1">Alexander Neitz</a>, <a href="https://openreview.net/profile?id=~Giambattista_Parascandolo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Giambattista_Parascandolo1">Giambattista Parascandolo</a>, <a href="https://openreview.net/profile?id=~Bernhard_Sch%C3%B6lkopf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernhard_Schölkopf1">Bernhard Schölkopf</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 20 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ECuvULjFQia-details-331" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ECuvULjFQia-details-331"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">meta-learning, privileged information</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">By learning to predict trajectories of dynamical systems, model-based methods can make extensive use of all observations from past experience. However, due to partial observability, stochasticity, compounding errors, and irrelevant dynamics, training to predict observations explicitly often results in poor models. Model-free techniques try to side-step the problem by learning to predict values directly. While breaking the explicit dependency on future observations can result in strong performance, this usually comes at the cost of low sample efficiency, as the abundant information about the dynamics contained in future observations goes unused. Here we take a step back from both approaches: Instead of hand-designing how trajectories should be incorporated, a teacher network learns to interpret the trajectories and to provide target activations which guide a student model that can only observe the present. The teacher is trained with meta-gradients to maximize the student's performance on a validation set. We show that our approach performs well on tasks that are difficult for model-free and model-based methods, and we study the role of every component through ablation studies.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We explore meta-learning a teacher network to efficiently incorporate privileged information such as trajectories.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="90JprVrJBO" data-number="1866">
      <h4>
        <a href="https://openreview.net/forum?id=90JprVrJBO">
            Learning a Latent Search Space for Routing Problems using Variational Autoencoders
        </a>
      
        
          <a href="https://openreview.net/pdf?id=90JprVrJBO" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Andr%C3%A9_Hottung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~André_Hottung1">André Hottung</a>, <a href="https://openreview.net/profile?email=bhanubhandar%40cs.umass.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="bhanubhandar@cs.umass.edu">Bhanu Bhandari</a>, <a href="https://openreview.net/profile?id=~Kevin_Tierney1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kevin_Tierney1">Kevin Tierney</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#90JprVrJBO-details-162" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="90JprVrJBO-details-162"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">heuristic search, variational autoencoders, learning to optimize, routing problems, traveling salesperson problem, vehicle routing problem, combinatorial optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Methods for automatically learning to solve routing problems are rapidly improving in performance. While most of these methods excel at generating solutions quickly, they are unable to effectively utilize longer run times because they lack a sophisticated search component. We present a learning-based optimization approach that allows a guided search in the distribution of high-quality solutions for a problem instance. More precisely, our method uses a conditional variational autoencoder that learns to map points in a continuous (latent) search space to high-quality, instance-specific routing problem solutions. The learned space can then be searched by any unconstrained continuous optimization method. We show that even using a standard differential evolution search strategy our approach is able to outperform existing purely machine learning based approaches. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We use conditional variational autoencoders to learn continuous search spaces for routing problems that can be searched with any unconstrained continuous optimizer.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-IXhmY16R3M" data-number="683">
      <h4>
        <a href="https://openreview.net/forum?id=-IXhmY16R3M">
            Universal approximation power of deep residual neural networks via nonlinear control theory
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-IXhmY16R3M" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Paulo_Tabuada1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paulo_Tabuada1">Paulo Tabuada</a>, <a href="https://openreview.net/profile?email=bahman.gharesifard%40queensu.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="bahman.gharesifard@queensu.ca">Bahman Gharesifard</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 20 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-IXhmY16R3M-details-558" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-IXhmY16R3M-details-558"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep residual neural networks, universal approximation, nonlinear control theory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we explain the universal approximation capabilities of deep residual neural networks through geometric nonlinear control. Inspired by recent work establishing links between residual networks and control systems, we provide a general sufficient condition for a residual network to have the power of universal approximation by asking the activation function, or one of its derivatives, to satisfy a quadratic differential equation. Many activation functions used in practice satisfy this assumption, exactly or approximately, and we show this property to be sufficient for an adequately deep neural network with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="193" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>+</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> neurons per
      layer to approximate arbitrarily well, on a compact set and with respect to the supremum norm, any continuous function from <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="194" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.41em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>n</mi></msup></math></mjx-assistive-mml></mjx-container> to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="195" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.41em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>n</mi></msup></math></mjx-assistive-mml></mjx-container>. We further show this result to hold for very simple architectures for which the weights only need to assume two values. The first key technical contribution consists of relating the universal approximation problem to controllability of an ensemble of control systems corresponding to a residual network and to leverage classical Lie algebraic techniques to characterize controllability. The second technical contribution is to identify monotonicity as the bridge between controllability of finite ensembles and uniform approximability on compact sets.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Using nonlinear control theory, it is shown that deep residual neural networks have the power of universal approximation with respect to the supremum norm.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="6NFBvWlRXaG" data-number="481">
      <h4>
        <a href="https://openreview.net/forum?id=6NFBvWlRXaG">
            On the Universality of Rotation Equivariant Point Cloud Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=6NFBvWlRXaG" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=nadavdym%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="nadavdym@gmail.com">Nadav Dym</a>, <a href="https://openreview.net/profile?id=~Haggai_Maron1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haggai_Maron1">Haggai Maron</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#6NFBvWlRXaG-details-888" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6NFBvWlRXaG-details-888"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">3D deep learning, Rotation invariance, Invariant and equivariant deep networks, Universal approximation, Point clouds</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Learning functions on point clouds has applications in many fields, including computer vision, computer graphics, physics, and chemistry. Recently, there has been a growing interest in neural architectures that are invariant or equivariant to all three shape-preserving transformations of point clouds: translation, rotation, and permutation. In this paper, we present a first study of the approximation power of these architectures. We first derive two sufficient conditions for an equivariant architecture to have the universal approximation property, based on a novel characterization of the space of equivariant polynomials. We then use these conditions to show that two recently suggested models, Tensor field Networks and SE3-Transformers, are universal, and for devising two other novel universal architectures.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We provide sufficient conditions for universality of rotation equivariant point cloud networks and use these conditions to show that current models are universal as well as for devising new universal architectures.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="UoaQUQREMOs" data-number="1771">
      <h4>
        <a href="https://openreview.net/forum?id=UoaQUQREMOs">
            CT-Net: Channel Tensorization Network for Video Classification
        </a>
      
        
          <a href="https://openreview.net/pdf?id=UoaQUQREMOs" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kunchang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kunchang_Li1">Kunchang Li</a>, <a href="https://openreview.net/profile?id=~Xianhang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xianhang_Li1">Xianhang Li</a>, <a href="https://openreview.net/profile?id=~Yali_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yali_Wang1">Yali Wang</a>, <a href="https://openreview.net/profile?id=~Jun_Wang7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jun_Wang7">Jun Wang</a>, <a href="https://openreview.net/profile?id=~Yu_Qiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Qiao1">Yu Qiao</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#UoaQUQREMOs-details-754" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UoaQUQREMOs-details-754"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Video Classification, 3D Convolution, Channel Tensorization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">3D convolution is powerful for video classification but often computationally expensive, recent studies mainly focus on decomposing it on spatial-temporal and/or channel dimensions.  Unfortunately,  most approaches fail to achieve a preferable balance between convolutional efficiency and feature-interaction sufficiency.  For this reason,  we propose a concise and novel Channel Tensorization Network (CT-Net),  by treating the channel dimension of input feature as a multiplication of K sub-dimensions. On one hand,  it naturally factorizes convolution in a multiple dimension way,  leading to a light computation burden.  On the other hand, it can effectively enhance feature interaction from different channels,  and progressively enlarge the 3D receptive field of such interaction to boost classification accuracy.  Furthermore, we equip our CT-Module with a Tensor Excitation (TE) mechanism. It can learn to exploit spatial, temporal and channel attention in a high-dimensional manner, to improve the cooperative power of all the feature dimensions in our CT-Module. Finally, we flexibly adapt ResNet as our CT-Net. Extensive experiments are conducted on several challenging video benchmarks, e.g., Kinetics-400, Something-Something V1 and V2. Our CT-Net outperforms a number of recent SOTA approaches, in terms of accuracy and/or efficiency.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">To achieve convolution efficiency and feature-interaction sufficiency,  we propose a Channel Tensorization Network (CT-Net), by treating the channel dimension of input feature as a multiplication of K sub-dimensions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="eU776ZYxEpz" data-number="3565">
      <h4>
        <a href="https://openreview.net/forum?id=eU776ZYxEpz">
            Learning to live with Dale's principle: ANNs with separate excitatory and inhibitory units
        </a>
      
        
          <a href="https://openreview.net/pdf?id=eU776ZYxEpz" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jonathan_Cornford1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Cornford1">Jonathan Cornford</a>, <a href="https://openreview.net/profile?email=damjank7354%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="damjank7354@gmail.com">Damjan Kalajdzievski</a>, <a href="https://openreview.net/profile?email=marco.leite.11%40ucl.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="marco.leite.11@ucl.ac.uk">Marco Leite</a>, <a href="https://openreview.net/profile?email=al858%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="al858@cam.ac.uk">Amélie Lamarquette</a>, <a href="https://openreview.net/profile?email=d.kullmann%40ucl.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="d.kullmann@ucl.ac.uk">Dimitri Michael Kullmann</a>, <a href="https://openreview.net/profile?id=~Blake_Aaron_Richards1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Blake_Aaron_Richards1">Blake Aaron Richards</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#eU776ZYxEpz-details-222" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eU776ZYxEpz-details-222"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value "> The units in artificial neural networks (ANNs) can be thought of as abstractions of biological neurons, and ANNs are increasingly used in neuroscience research. However, there are many important differences between ANN units and real neurons. One of the most notable is the absence of Dale's principle, which ensures that biological neurons are either exclusively excitatory or inhibitory. Dale's principle is typically left out of ANNs because its inclusion impairs learning. This is problematic, because one of the great advantages of ANNs for neuroscience research is their ability to learn complicated, realistic tasks. Here, by taking inspiration from feedforward inhibitory interneurons in the brain we show that we can develop ANNs with separate populations of excitatory and inhibitory units that learn just as well as standard ANNs. We call these networks Dale's ANNs (DANNs). We present two insights that enable DANNs to learn well: (1) DANNs are related to normalization schemes, and can be initialized such that the inhibition centres and standardizes the excitatory activity, (2) updates to inhibitory neuron parameters should be scaled using corrections based on the Fisher Information matrix. These results demonstrate how ANNs that respect Dale's principle can be built without sacrificing learning performance, which is important for future work using ANNs as models of the brain. The results may also have interesting implications for how inhibitory plasticity in the real brain operates.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="jN5y-zb5Q7m" data-number="2052">
      <h4>
        <a href="https://openreview.net/forum?id=jN5y-zb5Q7m">
            Uncertainty Estimation in Autoregressive Structured Prediction
        </a>
      
        
          <a href="https://openreview.net/pdf?id=jN5y-zb5Q7m" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Andrey_Malinin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrey_Malinin1">Andrey Malinin</a>, <a href="https://openreview.net/profile?id=~Mark_Gales1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mark_Gales1">Mark Gales</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 11 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>19 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#jN5y-zb5Q7m-details-722" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jN5y-zb5Q7m-details-722"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">ensembles, structures prediction, uncertainty estimation, knowledge uncertainty, autoregressive models, information theory, machine translation, speech recognition.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Uncertainty estimation is important for ensuring safety and robustness of AI systems.  While most research in the area has focused on un-structured prediction tasks, limited work has investigated general uncertainty estimation approaches for structured prediction. Thus, this work aims to investigate uncertainty estimation for structured prediction tasks within a single unified and interpretable probabilistic ensemble-based framework.  We consider: uncertainty estimation for sequence data at the token-level and complete sequence-level; interpretations for, and applications of, various measures of uncertainty; and discuss both the theoretical and practical challenges associated with obtaining them. This work also provides baselines for token-level and sequence-level error detection, and sequence-level out-of-domain input detection on the WMT’14 English-French and WMT’17 English-German translation and LibriSpeech speech recognition datasets.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A Deep Investigation of Ensemble-based Uncertainty Estimation for Autoregressive ASR and NMT models.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="fylclEqgvgd" data-number="3581">
      <h4>
        <a href="https://openreview.net/forum?id=fylclEqgvgd">
            Transformer protein language models are unsupervised structure learners
        </a>
      
        
          <a href="https://openreview.net/pdf?id=fylclEqgvgd" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Roshan_Rao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roshan_Rao1">Roshan Rao</a>, <a href="https://openreview.net/profile?id=~Joshua_Meier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_Meier1">Joshua Meier</a>, <a href="https://openreview.net/profile?id=~Tom_Sercu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tom_Sercu1">Tom Sercu</a>, <a href="https://openreview.net/profile?id=~Sergey_Ovchinnikov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Ovchinnikov1">Sergey Ovchinnikov</a>, <a href="https://openreview.net/profile?id=~Alexander_Rives1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Rives1">Alexander Rives</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#fylclEqgvgd-details-577" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fylclEqgvgd-details-577"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">proteins, language modeling, structure prediction, unsupervised learning, explainable</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="7I12hXRi8F" data-number="1394">
      <h4>
        <a href="https://openreview.net/forum?id=7I12hXRi8F">
            ANOCE: Analysis of Causal Effects with Multiple Mediators via Constrained Structural Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=7I12hXRi8F" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hengrui_Cai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hengrui_Cai1">Hengrui Cai</a>, <a href="https://openreview.net/profile?id=~Rui_Song2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rui_Song2">Rui Song</a>, <a href="https://openreview.net/profile?email=wlu4%40ncsu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="wlu4@ncsu.edu">Wenbin Lu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#7I12hXRi8F-details-982" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7I12hXRi8F-details-982"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Causal network, Constrained optimization, COVID-19, Individual mediation effects, Structure learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In the era of causal revolution, identifying the causal effect of an exposure on the outcome of interest is an important problem in many areas, such as epidemics, medicine, genetics, and economics. Under a general causal graph, the exposure may have a direct effect on the outcome and also an indirect effect regulated by a set of mediators. An analysis of causal effects that interprets the causal mechanism contributed through mediators is hence challenging but on demand. To the best of our knowledge, there are no feasible algorithms that give an exact decomposition of the indirect effect on the level of individual mediators, due to common interaction among mediators in the complex graph. In this paper, we establish a new statistical framework to comprehensively characterize causal effects with multiple mediators, namely, ANalysis Of Causal Effects (ANOCE), with a newly introduced definition of the mediator effect, under the linear structure equation model. We further propose a constrained causal structure learning method by incorporating a novel identification constraint that specifies the temporal causal relationship of variables. The proposed algorithm is applied to investigate the causal effects of 2020 Hubei lockdowns on reducing the spread of the coronavirus in Chinese major cities out of Hubei. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Analysis of causal effects on the level of individual mediators via constrained structural learning, with application to the COVID-19 Spread in China.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=7I12hXRi8F&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="w2Z2OwVNeK" data-number="3201">
      <h4>
        <a href="https://openreview.net/forum?id=w2Z2OwVNeK">
            Plan-Based Relaxed Reward Shaping for Goal-Directed Tasks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=w2Z2OwVNeK" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ingmar_Schubert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ingmar_Schubert1">Ingmar Schubert</a>, <a href="https://openreview.net/profile?id=~Ozgur_S_Oguz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ozgur_S_Oguz1">Ozgur S Oguz</a>, <a href="https://openreview.net/profile?id=~Marc_Toussaint1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marc_Toussaint1">Marc Toussaint</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>21 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#w2Z2OwVNeK-details-129" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="w2Z2OwVNeK-details-129"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, reward shaping, plan-based reward shaping, robotics, robotic manipulation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In high-dimensional state spaces, the usefulness of Reinforcement Learning (RL) is limited by the problem of exploration. This issue has been addressed using potential-based reward shaping (PB-RS) previously. In the present work, we introduce Final-Volume-Preserving Reward Shaping (FV-RS). FV-RS relaxes the strict optimality guarantees of PB-RS to a guarantee of preserved long-term behavior. Being less restrictive, FV-RS allows for reward shaping functions that are even better suited for improving the sample efficiency of RL algorithms. In particular, we consider settings in which the agent has access to an approximate plan. Here, we use examples of simulated robotic manipulation tasks to demonstrate that plan-based FV-RS can indeed significantly improve the sample efficiency of RL over plan-based PB-RS.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce Final-Volume-Preserving Reward Shaping, and show in a plan-based setting that it significantly increases the sample efficiency of reinforcement learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=w2Z2OwVNeK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="PrzjugOsDeE" data-number="1395">
      <h4>
        <a href="https://openreview.net/forum?id=PrzjugOsDeE">
            CcGAN: Continuous Conditional Generative Adversarial Networks for Image Generation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=PrzjugOsDeE" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xin_Ding2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xin_Ding2">Xin Ding</a>, <a href="https://openreview.net/profile?id=~Yongwei_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yongwei_Wang1">Yongwei Wang</a>, <a href="https://openreview.net/profile?id=~Zuheng_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zuheng_Xu1">Zuheng Xu</a>, <a href="https://openreview.net/profile?id=~William_J_Welch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_J_Welch1">William J Welch</a>, <a href="https://openreview.net/profile?id=~Z._Jane_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Z._Jane_Wang1">Z. Jane Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#PrzjugOsDeE-details-353" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PrzjugOsDeE-details-353"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Conditional generative adversarial networks, image generation, continuous and scalar conditions</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This work proposes the continuous conditional generative adversarial network (CcGAN), the first generative model for image generation conditional on continuous, scalar conditions (termed regression labels). Existing conditional GANs (cGANs) are mainly designed for categorical conditions (e.g., class labels); conditioning on a continuous label is mathematically distinct and raises two fundamental problems: (P1) Since there may be very few (even zero) real images for some regression labels, minimizing existing empirical versions of cGAN losses (a.k.a. empirical cGAN losses) often fails in practice; (P2) Since regression labels are scalar and infinitely many, conventional label input methods (e.g., combining a hidden map of the generator/discriminator with a one-hot encoded label) are not applicable. The proposed CcGAN solves the above problems, respectively, by (S1) reformulating existing empirical cGAN losses to be appropriate for the continuous scenario; and (S2) proposing a novel method to incorporate regression labels into the generator and the discriminator. The reformulation in (S1) leads to two novel empirical discriminator losses, termed the hard vicinal discriminator loss (HVDL) and the soft vicinal discriminator loss (SVDL) respectively, and a novel empirical generator loss. The error bounds of a discriminator trained with HVDL and SVDL are derived under mild assumptions in this work. A new benchmark dataset, RC-49, is also proposed for generative image modeling conditional on regression labels. Our experiments on the Circular 2-D Gaussians, RC-49, and UTKFace datasets show that CcGAN is able to generate diverse, high-quality samples from the image distribution conditional on a given regression label. Moreover, in these experiments, CcGAN substantially outperforms cGAN both visually and quantitatively.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This work proposes the continuous conditional generative adversarial network (CcGAN), the first generative model for image generation conditional on continuous, scalar conditions (termed as regression labels). </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=PrzjugOsDeE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="CHLhSw9pSw8" data-number="2148">
      <h4>
        <a href="https://openreview.net/forum?id=CHLhSw9pSw8">
            Single-Photon Image Classification
        </a>
      
        
          <a href="https://openreview.net/pdf?id=CHLhSw9pSw8" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Thomas_Fischbacher1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Fischbacher1">Thomas Fischbacher</a>, <a href="https://openreview.net/profile?id=~Luciano_Sbaiz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luciano_Sbaiz1">Luciano Sbaiz</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#CHLhSw9pSw8-details-353" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CHLhSw9pSw8-details-353"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">quantum mechanics, image classification, quantum machine learning, theoretical limits</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Quantum Computing based Machine Learning mainly focuses on quantum computing hardware that is experimentally challenging to realize due to requiring quantum gates that operate at very low temperature. We demonstrate the existence of a "quantum computing toy model" that illustrates key aspects of quantum information processing while being experimentally accessible with room temperature optics. Pondering the question of the theoretical classification accuracy performance limit for MNIST (respectively "Fashion-MNIST") classifiers, subject to the constraint that a decision has to be made after detection of the very first photon that passed through an image-filter, we show that a machine learning system that is permitted to use quantum interference on the photon's state can substantially outperform any machine learning system that can not.  Specifically, we prove that a "classical" MNIST (respectively "Fashion-MNIST") classifier cannot achieve an accuracy of better than <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="196" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>21.28</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> (respectively <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="197" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c38"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>18.28</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> for "Fashion-MNIST") if it must make a decision after seeing a single photon falling on one of the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="198" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>28</mn><mo>×</mo><mn>28</mn></math></mjx-assistive-mml></mjx-container> image pixels of a detector array.  We further demonstrate that a classifier that is permitted to employ quantum interference by optically transforming the photon state prior to detection can achieve a classification accuracy of at least <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="199" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c37"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>41.27</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> for MNIST (respectively <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="200" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>36.14</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> for "Fashion-MNIST"). We show in detail how to train the corresponding quantum state transformation with TensorFlow and also explain how this example can serve as a teaching tool for the measurement process in quantum mechanics.
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Mathematical proof that the classical accuracy limit for single-photon image classification can be exceeded very substantially by employing a problem-tailored quantum transformation on the photon state.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=CHLhSw9pSw8&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="bgQek2O63w" data-number="3713">
      <h4>
        <a href="https://openreview.net/forum?id=bgQek2O63w">
            Self-supervised Adversarial Robustness for the Low-label, High-data Regime
        </a>
      
        
          <a href="https://openreview.net/pdf?id=bgQek2O63w" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sven_Gowal2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sven_Gowal2">Sven Gowal</a>, <a href="https://openreview.net/profile?id=~Po-Sen_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Po-Sen_Huang1">Po-Sen Huang</a>, <a href="https://openreview.net/profile?id=~Aaron_van_den_Oord2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aaron_van_den_Oord2">Aaron van den Oord</a>, <a href="https://openreview.net/profile?id=~Timothy_Mann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Timothy_Mann1">Timothy Mann</a>, <a href="https://openreview.net/profile?id=~Pushmeet_Kohli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pushmeet_Kohli1">Pushmeet Kohli</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#bgQek2O63w-details-740" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bgQek2O63w-details-740"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">self-supervised, adversarial training, robustness</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent work discovered that training models to be invariant to adversarial perturbations requires substantially larger datasets than those required for standard classification. Perhaps more surprisingly, these larger datasets can be "mostly" unlabeled. Pseudo-labeling, a technique simultaneously pioneered by four separate and simultaneous works in 2019, has been proposed as a competitive alternative to labeled data for training adversarially robust models. However, when the amount of labeled data decreases, the performance of pseudo-labeling catastrophically drops, thus questioning the theoretical insights put forward by Uesato et al. (2019), which suggest that the sample complexity for learning an adversarially robust model from unlabeled data should match the fully supervised case. We introduce Bootstrap Your Own Robust Latents (BYORL), a self-supervised learning technique based on BYOL for training adversarially robust models. Our method enables us to train robust representations without any labels (reconciling practice with theory). Most notably, this robust representation can be leveraged by a linear classifier to train adversarially robust models, even when the linear classifier is not trained adversarially. We evaluate BYORL and pseudo-labeling on CIFAR-10 and ImageNet and demonstrate that BYORL achieves significantly higher robustness (i.e., models resulting from BYORL are up to two times more accurate). Experiments on CIFAR-10 against <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="201" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="202" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-n" size="s"><mjx-c class="mjx-c221E"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi mathvariant="normal">∞</mi></msub></math></mjx-assistive-mml></mjx-container> norm-bounded perturbations demonstrate that BYORL achieves near state-of-the-art robustness with as little as 500 labeled examples. We also note that against <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="203" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> norm-bounded perturbations of size <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="204" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi><mo>=</mo><mn>128</mn><mrow><mo>/</mo></mrow><mn>255</mn></math></mjx-assistive-mml></mjx-container>, BYORL surpasses the known state-of-the-art with an accuracy under attack of 77.61% (against 72.91% for the prior art).</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper builds of Bootstrap Your Own Latents and proposes a self-supervised learning technique that can learn robust representations that are competitive with fully-supervised techniques.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Mu2ZxFctAI" data-number="3616">
      <h4>
        <a href="https://openreview.net/forum?id=Mu2ZxFctAI">
            Uncertainty-aware Active Learning for Optimal Bayesian Classifier
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Mu2ZxFctAI" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Guang_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guang_Zhao1">Guang Zhao</a>, <a href="https://openreview.net/profile?id=~Edward_Dougherty1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edward_Dougherty1">Edward Dougherty</a>, <a href="https://openreview.net/profile?id=~Byung-Jun_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Byung-Jun_Yoon1">Byung-Jun Yoon</a>, <a href="https://openreview.net/profile?email=falexander%40bnl.gov" class="profile-link" data-toggle="tooltip" data-placement="top" title="falexander@bnl.gov">Francis Alexander</a>, <a href="https://openreview.net/profile?id=~Xiaoning_Qian2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoning_Qian2">Xiaoning Qian</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Mu2ZxFctAI-details-143" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Mu2ZxFctAI-details-143"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Active learning, Bayesian classification</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">For pool-based active learning, in each iteration a candidate training sample is chosen for labeling by optimizing an acquisition function. In Bayesian classification, expected Loss Reduction~(ELR) methods maximize the expected reduction in the classification error given a new labeled candidate based on a one-step-look-ahead strategy. ELR is the optimal strategy with a single query; however, since such myopic strategies cannot identify the long-term effect of a query on the classification error, ELR may get stuck before reaching the optimal classifier.  In this paper, inspired by the mean objective cost of uncertainty (MOCU), a metric quantifying the uncertainty directly affecting the classification error, we propose an acquisition function based on a weighted form of MOCU. Similar to ELR, the proposed method focuses on the reduction of the uncertainty that pertains to the classification error. But unlike any other existing scheme, it provides the critical advantage that the resulting Bayesian active learning algorithm guarantees convergence to the optimal classifier of the true model. We demonstrate its performance with both synthetic and real-world datasets.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We focus on pool-based Bayesian active learning, for which we have proposed a new weighted MOCU method and analyzed its theoretical properties to demonstrate the better convergence properties than existing methods in this category.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Mu2ZxFctAI&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="jXe91kq3jAq" data-number="1425">
      <h4>
        <a href="https://openreview.net/forum?id=jXe91kq3jAq">
            Latent Skill Planning for Exploration and Transfer
        </a>
      
        
          <a href="https://openreview.net/pdf?id=jXe91kq3jAq" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kevin_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kevin_Xie1">Kevin Xie</a>, <a href="https://openreview.net/profile?id=~Homanga_Bharadhwaj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Homanga_Bharadhwaj1">Homanga Bharadhwaj</a>, <a href="https://openreview.net/profile?id=~Danijar_Hafner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Danijar_Hafner1">Danijar Hafner</a>, <a href="https://openreview.net/profile?id=~Animesh_Garg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Animesh_Garg1">Animesh Garg</a>, <a href="https://openreview.net/profile?id=~Florian_Shkurti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Shkurti1">Florian Shkurti</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#jXe91kq3jAq-details-59" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jXe91kq3jAq-details-59"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Model-Based Reinforcement Learning, World Models, Skill Discovery, Mutual Information, Planning, Model Predictive Control, Partial Amortization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">To quickly solve new tasks in complex environments, intelligent agents need to build up reusable knowledge. For example, a learned world model captures knowledge about the environment that applies to new tasks. Similarly, skills capture general behaviors that can apply to new tasks. In this paper, we investigate how these two approaches can be integrated into a single reinforcement learning agent. Specifically, we leverage the idea of partial amortization for fast adaptation at test time. For this, actions are produced by a policy that is learned over time while the skills it conditions on are chosen using online planning. We demonstrate the benefits of our design decisions across a suite of challenging locomotion tasks and demonstrate improved sample efficiency in single tasks as well as in transfer from one task to another, as compared to competitive baselines. Videos are available at: https://sites.google.com/view/latent-skill-planning/</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Partially amortized planning through hierarchy helps learn skills for complex control tasks</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="aUX5Plaq7Oy" data-number="3532">
      <h4>
        <a href="https://openreview.net/forum?id=aUX5Plaq7Oy">
            Learning continuous-time PDEs from sparse data with graph neural networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=aUX5Plaq7Oy" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Valerii_Iakovlev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Valerii_Iakovlev1">Valerii Iakovlev</a>, <a href="https://openreview.net/profile?id=~Markus_Heinonen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Markus_Heinonen1">Markus Heinonen</a>, <a href="https://openreview.net/profile?id=~Harri_L%C3%A4hdesm%C3%A4ki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Harri_Lähdesmäki1">Harri Lähdesmäki</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 30 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#aUX5Plaq7Oy-details-687" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="aUX5Plaq7Oy-details-687"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">dynamical systems, partial differential equations, PDEs, graph neural networks, continuous time</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The behavior of many dynamical systems follow complex, yet still unknown partial differential equations (PDEs). While several machine learning methods have been proposed to learn PDEs directly from data, previous methods are limited to discrete-time approximations or make the limiting assumption of the observations arriving at regular grids. We propose a general continuous-time differential model for dynamical systems whose governing equations are parameterized by message passing graph neural networks. The model admits arbitrary space and time discretizations, which removes constraints on the locations of observation points and time intervals between the observations. The model is trained with continuous-time adjoint method enabling efficient neural PDE inference. We demonstrate the model's ability to work with unstructured grids, arbitrary time steps, and noisy observations. We compare our method with existing approaches on several well-known physical systems that involve first and higher-order PDEs with state-of-the-art predictive performance.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">The paper introduces a method for learning partial differential equations on arbitrary spatial and temporal grids.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="IX3Nnir2omJ" data-number="3152">
      <h4>
        <a href="https://openreview.net/forum?id=IX3Nnir2omJ">
            Characterizing signal propagation to close the performance gap in unnormalized ResNets
        </a>
      
        
          <a href="https://openreview.net/pdf?id=IX3Nnir2omJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Andrew_Brock1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_Brock1">Andrew Brock</a>, <a href="https://openreview.net/profile?id=~Soham_De2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soham_De2">Soham De</a>, <a href="https://openreview.net/profile?id=~Samuel_L_Smith1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samuel_L_Smith1">Samuel L Smith</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#IX3Nnir2omJ-details-531" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IX3Nnir2omJ-details-531"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">normalizers, signal propagation, deep learning, neural networks, ResNets, EfficientNets, ImageNet, CNNs, ConvNets</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Batch Normalization is a key component in almost all state-of-the-art image classifiers, but it also introduces practical challenges: it breaks the independence between training examples within a batch, can incur compute and memory overhead, and often results in unexpected bugs. Building on recent theoretical analyses of deep ResNets at initialization, we propose a simple set of analysis tools to characterize signal propagation on the forward pass, and leverage these tools to design highly performant ResNets without activation normalization layers.  Crucial to our success is an adapted version of the recently proposed Weight Standardization.  Our analysis tools show how this technique preserves the signal in ReLU networks by ensuring that the per-channel activation means do not grow with depth. Across a range of FLOP budgets, our networks attain performance competitive with state-of-the-art EfficientNets on ImageNet.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show how to train ResNets completely without normalization, and attain performance competitive with batch-normalized EfficientNets.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="qZzy5urZw9" data-number="2638">
      <h4>
        <a href="https://openreview.net/forum?id=qZzy5urZw9">
            Robust Overfitting may be mitigated by properly learned smoothening
        </a>
      
        
          <a href="https://openreview.net/pdf?id=qZzy5urZw9" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tianlong_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianlong_Chen1">Tianlong Chen</a>, <a href="https://openreview.net/profile?id=~Zhenyu_Zhang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhenyu_Zhang4">Zhenyu Zhang</a>, <a href="https://openreview.net/profile?id=~Sijia_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sijia_Liu1">Sijia Liu</a>, <a href="https://openreview.net/profile?id=~Shiyu_Chang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shiyu_Chang2">Shiyu Chang</a>, <a href="https://openreview.net/profile?id=~Zhangyang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhangyang_Wang1">Zhangyang Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#qZzy5urZw9-details-962" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qZzy5urZw9-details-962"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Robust Overfitting, Adversarial Training, Adversarial Robustness</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A recent study (Rice et al.,  2020) revealed overfitting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overfitting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overfitting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="205" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3.72</mn><mi mathvariant="normal">%</mi><mo>∼</mo><mn>6.68</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> and robust accuracy by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="206" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mn class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.22</mn><mi mathvariant="normal">%</mi><mo>∼</mo><mn>2</mn><mn>.03</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container>, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="207" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c221E"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mrow><mi mathvariant="normal">∞</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="208" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container>), and robustified methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models' robustness against transfer attacks. Codes are available at https://github.com/VITA-Group/Alleviate-Robust-Overfitting.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Mitigate robust overfitting by properly learned smoothening, establishing the new state-of-the-art bar in adversarial training</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=qZzy5urZw9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="LXMSvPmsm0g" data-number="2110">
      <h4>
        <a href="https://openreview.net/forum?id=LXMSvPmsm0g">
            Long Live the Lottery: The Existence of Winning Tickets in Lifelong Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=LXMSvPmsm0g" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tianlong_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianlong_Chen1">Tianlong Chen</a>, <a href="https://openreview.net/profile?id=~Zhenyu_Zhang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhenyu_Zhang4">Zhenyu Zhang</a>, <a href="https://openreview.net/profile?id=~Sijia_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sijia_Liu1">Sijia Liu</a>, <a href="https://openreview.net/profile?id=~Shiyu_Chang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shiyu_Chang2">Shiyu Chang</a>, <a href="https://openreview.net/profile?id=~Zhangyang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhangyang_Wang1">Zhangyang Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#LXMSvPmsm0g-details-978" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LXMSvPmsm0g-details-978"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">lottery tickets, winning tickets, lifelong learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The lottery ticket hypothesis states that a highly sparsified sub-network can be trained in isolation, given the appropriate weight initialization. This paper extends that hypothesis from one-shot task learning, and demonstrates for the first time that such extremely compact and independently trainable sub-networks can be also identified in the lifelong learning scenario, which we call lifelong tickets. We show that the resulting lifelong ticket can further be leveraged to improve the performance of learning over continual tasks. However, it is highly non-trivial to conduct network pruning in the lifelong setting. Two critical roadblocks arise: i) As many tasks now arrive sequentially, finding tickets in a greedy weight pruning fashion will inevitably suffer from the intrinsic bias, that the earlier emerging tasks impact more; ii) As lifelong learning is consistently challenged by catastrophic forgetting, the compact network capacity of tickets might amplify the risk of forgetting. In view of those, we introduce two pruning options, e.g., top-down and bottom-up, for finding lifelong tickets. Compared to the top-down pruning that extends vanilla (iterative) pruning over sequential tasks, we show that the bottom-up one, which can dynamically shrink and (re-)expand model capacity, effectively avoids the undesirable excessive pruning in the early stage. We additionally introduce lottery teaching that further overcomes forgetting via knowledge distillation aided by external unlabeled data. Unifying those ingredients, we demonstrate the existence of very competitive lifelong tickets, e.g., achieving 3-8% of the dense model size with even higher accuracy, compared to strong class-incremental learning baselines on CIFAR-10/CIFAR-100/Tiny-ImageNet datasets. Codes available at https://github.com/VITA-Group/Lifelong-Learning-LTH.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Proposed novel bottom-up lifelong pruning effectively identify the winning tickets, which significantly improve the performance of learning over continual tasks</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=LXMSvPmsm0g&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="jEYKjPE1xYN" data-number="1710">
      <h4>
        <a href="https://openreview.net/forum?id=jEYKjPE1xYN">
            Symmetry-Aware Actor-Critic for 3D Molecular Design
        </a>
      
        
          <a href="https://openreview.net/pdf?id=jEYKjPE1xYN" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Gregor_N._C._Simm1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gregor_N._C._Simm1">Gregor N. C. Simm</a>, <a href="https://openreview.net/profile?id=~Robert_Pinsler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Robert_Pinsler1">Robert Pinsler</a>, <a href="https://openreview.net/profile?email=gc121%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="gc121@cam.ac.uk">Gábor Csányi</a>, <a href="https://openreview.net/profile?id=~Jos%C3%A9_Miguel_Hern%C3%A1ndez-Lobato1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~José_Miguel_Hernández-Lobato1">José Miguel Hernández-Lobato</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#jEYKjPE1xYN-details-807" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jEYKjPE1xYN-details-807"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep reinforcement learning, molecular design, covariant neural networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Automating molecular design using deep reinforcement learning (RL) has the potential to greatly accelerate the search for novel materials. Despite recent progress on leveraging graph representations to design molecules, such methods are fundamentally limited by the lack of three-dimensional (3D) information. In light of this, we propose a novel actor-critic architecture for 3D molecular design that can generate molecular structures unattainable with previous approaches. This is achieved by exploiting the symmetries of the design process through a rotationally covariant state-action representation based on a spherical harmonics series expansion. We demonstrate the benefits of our approach on several 3D molecular design tasks, where we find that building in such symmetries significantly improves generalization and the quality of generated molecules.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Covariant actor-critic based on spherical harmonics that exploits symmetries to design molecules in 3D</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-TwO99rbVRu" data-number="243">
      <h4>
        <a href="https://openreview.net/forum?id=-TwO99rbVRu">
            PseudoSeg: Designing Pseudo Labels for Semantic Segmentation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-TwO99rbVRu" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yuliang_Zou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuliang_Zou1">Yuliang Zou</a>, <a href="https://openreview.net/profile?id=~Zizhao_Zhang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zizhao_Zhang3">Zizhao Zhang</a>, <a href="https://openreview.net/profile?id=~Han_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Han_Zhang1">Han Zhang</a>, <a href="https://openreview.net/profile?id=~Chun-Liang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chun-Liang_Li1">Chun-Liang Li</a>, <a href="https://openreview.net/profile?id=~Xiao_Bian3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiao_Bian3">Xiao Bian</a>, <a href="https://openreview.net/profile?id=~Jia-Bin_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jia-Bin_Huang1">Jia-Bin Huang</a>, <a href="https://openreview.net/profile?id=~Tomas_Pfister1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tomas_Pfister1">Tomas Pfister</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-TwO99rbVRu-details-25" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-TwO99rbVRu-details-25"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">pseudo-labeling, semi-supervised, semantic-segmentation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent advances in semi-supervised learning (SSL) demonstrate that a combination of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classification, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to generate well-calibrated structured pseudo labels for training with unlabeled or weakly-labeled data. Our proposed pseudo-labeling strategy is network structure agnostic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and high-data regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for segmentation. The source code will be released.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper presents a new method that first demonstrates how well-calibrated soft pseudo labels obtained through wise fusion of predictions from diverse sources greatly improve consistency training for semantic segmentation.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="CU0APx9LMaL" data-number="3528">
      <h4>
        <a href="https://openreview.net/forum?id=CU0APx9LMaL">
            NAS-Bench-ASR: Reproducible Neural Architecture Search for Speech Recognition
        </a>
      
        
          <a href="https://openreview.net/pdf?id=CU0APx9LMaL" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Abhinav_Mehrotra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abhinav_Mehrotra1">Abhinav Mehrotra</a>, <a href="https://openreview.net/profile?email=a.gilramos%40samsung.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="a.gilramos@samsung.com">Alberto Gil C. P. Ramos</a>, <a href="https://openreview.net/profile?id=~Sourav_Bhattacharya1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sourav_Bhattacharya1">Sourav Bhattacharya</a>, <a href="https://openreview.net/profile?id=~%C5%81ukasz_Dudziak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Łukasz_Dudziak1">Łukasz Dudziak</a>, <a href="https://openreview.net/profile?id=~Ravichander_Vipperla1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ravichander_Vipperla1">Ravichander Vipperla</a>, <a href="https://openreview.net/profile?email=thomas.chau%40samsung.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="thomas.chau@samsung.com">Thomas Chau</a>, <a href="https://openreview.net/profile?id=~Mohamed_S_Abdelfattah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohamed_S_Abdelfattah1">Mohamed S Abdelfattah</a>, <a href="https://openreview.net/profile?email=s.ishtiaq%40samsung.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="s.ishtiaq@samsung.com">Samin Ishtiaq</a>, <a href="https://openreview.net/profile?id=~Nicholas_Donald_Lane1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicholas_Donald_Lane1">Nicholas Donald Lane</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#CU0APx9LMaL-details-364" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CU0APx9LMaL-details-364"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">NAS, ASR, Benchmark</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Powered by innovations in novel architecture design, noise tolerance techniques and increasing model capacity, Automatic Speech Recognition (ASR) has made giant strides in reducing word-error-rate over the past decade. ASR models are often trained with tens of thousand hours of high quality speech data to produce state-of-the-art (SOTA) results. Industry-scale ASR model training thus remains computationally heavy and time-consuming, and consequently has attracted little attention in adopting automatic techniques. On the other hand, Neural Architecture Search (NAS) has gained a lot of interest in the recent years thanks to its successes in discovering efficient architectures, often outperforming handcrafted alternatives. However, by changing the standard training process into a bi-level optimisation problem, NAS approaches often require significantly more time and computational power compared to single-model training, and at the same time increase complexity of the overall process. As a result, NAS has been predominately applied to problems which do not require as extensive training as ASR, and even then reproducibility of NAS algorithms is often problematic. Lately, a number of benchmark datasets has been introduced to address reproducibility issues by pro- viding NAS researchers with information about performance of different models obtained through exhaustive evaluation. However, these datasets focus mainly on computer vision and NLP tasks and thus suffer from limited coverage of application domains. In order to increase diversity in the existing NAS benchmarks, and at the same time provide systematic study of the effects of architectural choices for ASR, we release NAS-Bench-ASR – the first NAS benchmark for ASR models. The dataset consists of 8, 242 unique models trained on the TIMIT audio dataset for three different target epochs, and each starting from three different initializations. The dataset also includes runtime measurements of all the models on a diverse set of hardware platforms. Lastly, we show that identified good cell structures in our search space for TIMIT transfer well to a much larger LibriSpeech dataset.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">The first NAS benchmark for ASR comprising of 8,242 unique models trained on the TIMIT audio dataset.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="uQfOy7LrlTR" data-number="126">
      <h4>
        <a href="https://openreview.net/forum?id=uQfOy7LrlTR">
            Scaling the Convex Barrier with Active Sets
        </a>
      
        
          <a href="https://openreview.net/pdf?id=uQfOy7LrlTR" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alessandro_De_Palma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alessandro_De_Palma1">Alessandro De Palma</a>, <a href="https://openreview.net/profile?id=~Harkirat_Behl1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Harkirat_Behl1">Harkirat Behl</a>, <a href="https://openreview.net/profile?id=~Rudy_R_Bunel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rudy_R_Bunel1">Rudy R Bunel</a>, <a href="https://openreview.net/profile?id=~Philip_Torr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philip_Torr1">Philip Torr</a>, <a href="https://openreview.net/profile?id=~M._Pawan_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~M._Pawan_Kumar1">M. Pawan Kumar</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#uQfOy7LrlTR-details-104" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uQfOy7LrlTR-details-104"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural Network Verification, Neural Network Bounding, Optimisation for Deep Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Tight and efficient neural network bounding is of critical importance for the scaling of neural network verification systems. A number of efficient specialised dual solvers for neural network bounds have been presented recently, but they are often too loose to verify more challenging properties. This lack of tightness is linked to the weakness of the employed relaxation, which is usually a linear program of size linear in the number of neurons. While a tighter linear relaxation for piecewise linear activations exists, it comes at the cost of exponentially many constraints and thus currently lacks an efficient customised solver. We alleviate this deficiency via a novel dual algorithm that realises the full potential of the new relaxation by operating on a small active set of dual variables. Our method recovers the strengths of the new relaxation in the dual space: tightness and a linear separation oracle. At the same time, it shares the benefits of previous dual approaches for weaker relaxations: massive parallelism, GPU implementation, low cost per iteration and valid bounds at any time. As a consequence, we obtain better bounds than off-the-shelf solvers in only a fraction of their running time and recover the speed-accuracy trade-offs of looser dual solvers if the computational budget is small. We demonstrate that this results in significant formal verification speed-ups.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a specialised dual solver for a tight ReLU convex relaxation and show that it speeds up formal network verification.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=uQfOy7LrlTR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="AWOSz_mMAPx" data-number="1497">
      <h4>
        <a href="https://openreview.net/forum?id=AWOSz_mMAPx">
            Local Convergence Analysis of Gradient Descent Ascent with Finite Timescale Separation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=AWOSz_mMAPx" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tanner_Fiez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tanner_Fiez1">Tanner Fiez</a>, <a href="https://openreview.net/profile?id=~Lillian_J_Ratliff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lillian_J_Ratliff1">Lillian J Ratliff</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>57 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#AWOSz_mMAPx-details-201" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AWOSz_mMAPx-details-201"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">game theory, continuous games, generative adversarial networks, theory, gradient descent-ascent, equilibrium, convergence</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the role that a finite timescale separation parameter <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="209" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>τ</mi></math></mjx-assistive-mml></mjx-container> has on gradient descent-ascent in non-convex, non-concave zero-sum games where the learning rate of player 1 is denoted by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="210" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D6FE TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>γ</mi><mn>1</mn></msub></math></mjx-assistive-mml></mjx-container> and the learning rate of player 2 is defined to be <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="211" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D6FE TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D70F TEX-I"></mjx-c></mjx-mi><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D6FE TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>γ</mi><mn>2</mn></msub><mo>=</mo><mi>τ</mi><msub><mi>γ</mi><mn>1</mn></msub></math></mjx-assistive-mml></mjx-container>. We provide a non-asymptotic construction of the finite timescale separation parameter <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="212" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.054em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>τ</mi><mrow><mo>∗</mo></mrow></msup></math></mjx-assistive-mml></mjx-container> such that gradient descent-ascent locally converges to  <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="213" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>x</mi><mrow><mo>∗</mo></mrow></msup></math></mjx-assistive-mml></mjx-container> for all <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="214" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70F TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.054em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-n" space="2"><mjx-c class="mjx-c221E"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>τ</mi><mo>∈</mo><mo stretchy="false">(</mo><msup><mi>τ</mi><mrow><mo>∗</mo></mrow></msup><mo>,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> if and only if it is a strict local minmax equilibrium. Moreover, we provide explicit local convergence rates given the finite timescale separation. The convergence results we present are complemented by a non-convergence result: given a critical point <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="215" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>x</mi><mrow><mo>∗</mo></mrow></msup></math></mjx-assistive-mml></mjx-container> that is not a strict local minmax equilibrium, we present a non-asymptotic construction of a finite timescale separation <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="216" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D70F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>τ</mi><mrow><mn>0</mn></mrow></msub></math></mjx-assistive-mml></mjx-container> such that gradient descent-ascent with timescale separation <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="217" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70F TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D70F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-n" space="2"><mjx-c class="mjx-c221E"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>τ</mi><mo>∈</mo><mo stretchy="false">(</mo><msub><mi>τ</mi><mn>0</mn></msub><mo>,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> does not converge to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="218" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>x</mi><mrow><mo>∗</mo></mrow></msup></math></mjx-assistive-mml></mjx-container>. Finally, we extend the results to gradient penalty regularization methods for generative adversarial networks and empirically demonstrate on CIFAR-10 and CelebA the significant impact timescale separation has on training performance. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that there exists a range of finite learning ratios which we construct such that gradient descent-ascent converges to a critical point if and only if it is a strict local minmax equilibrium</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=AWOSz_mMAPx&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="UvBPbpvHRj-" data-number="2992">
      <h4>
        <a href="https://openreview.net/forum?id=UvBPbpvHRj-">
            Activation-level uncertainty in deep neural networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=UvBPbpvHRj-" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Pablo_Morales-Alvarez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pablo_Morales-Alvarez1">Pablo Morales-Alvarez</a>, <a href="https://openreview.net/profile?id=~Daniel_Hern%C3%A1ndez-Lobato1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Hernández-Lobato1">Daniel Hernández-Lobato</a>, <a href="https://openreview.net/profile?email=rms%40decsai.ugr.es" class="profile-link" data-toggle="tooltip" data-placement="top" title="rms@decsai.ugr.es">Rafael Molina</a>, <a href="https://openreview.net/profile?id=~Jos%C3%A9_Miguel_Hern%C3%A1ndez-Lobato1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~José_Miguel_Hernández-Lobato1">José Miguel Hernández-Lobato</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#UvBPbpvHRj--details-974" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UvBPbpvHRj--details-974"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Gaussian Processes, Uncertainty estimation, Deep Gaussian Processes, Bayesian Neural Networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Current approaches for uncertainty estimation in deep learning often produce too confident results. Bayesian Neural Networks (BNNs) model uncertainty in the space of weights, which is usually high-dimensional and limits the quality of variational approximations. The more recent functional BNNs (fBNNs) address this only partially because, although the prior is specified in the space of functions, the posterior approximation is still defined in terms of stochastic weights. In this work we propose to move uncertainty from the weights (which are deterministic) to the activation function. Specifically, the activations are modelled with simple 1D Gaussian Processes (GP), for which a triangular kernel inspired by the ReLu non-linearity is explored. Our experiments show that activation-level stochasticity provides more reliable uncertainty estimates than BNN and fBNN, whereas it performs competitively in standard prediction tasks. We also study the connection with deep GPs, both theoretically and empirically. More precisely, we show that activation-level uncertainty requires fewer inducing points and is better suited for deep architectures.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We use 1D Gaussian Processes to introduce activation-level uncertainty in neural networks, which overcomes known limitations of (functional) Bayesian neural nets and obtains better results than the related deep Gaussian Processes.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=UvBPbpvHRj-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="EKV158tSfwv" data-number="483">
      <h4>
        <a href="https://openreview.net/forum?id=EKV158tSfwv">
            Efficient Continual Learning with Modular Networks and Task-Driven Priors
        </a>
      
        
          <a href="https://openreview.net/pdf?id=EKV158tSfwv" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tom_Veniat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tom_Veniat1">Tom Veniat</a>, <a href="https://openreview.net/profile?id=~Ludovic_Denoyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ludovic_Denoyer1">Ludovic Denoyer</a>, <a href="https://openreview.net/profile?id=~MarcAurelio_Ranzato1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~MarcAurelio_Ranzato1">MarcAurelio Ranzato</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#EKV158tSfwv-details-452" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EKV158tSfwv-details-452"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Continual learning, Lifelong learning, Benchmark, Modular network, Neural Network</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Existing literature in Continual Learning (CL) has focused on overcoming catastrophic forgetting, the inability of the learner to recall how to perform tasks observed in the past. 
      There are however other desirable properties of a CL system, such as the ability to transfer knowledge from previous tasks and to scale memory and compute sub-linearly with the number of tasks. Since most current benchmarks focus only on forgetting using short streams of tasks, we first propose a new suite of benchmarks to probe CL algorithms across these new axes. 
      Finally, we introduce a new modular architecture, whose modules represent atomic skills that can be composed to perform a certain task. Learning a task reduces to figuring out which past modules to re-use, and which new modules to instantiate to solve the current task. Our learning algorithm leverages a task-driven prior over the exponential search space of all possible ways to combine modules, enabling efficient learning on long streams of tasks. 
      Our experiments show that this modular architecture and learning algorithm perform competitively on widely used CL benchmarks while yielding superior performance on the more challenging benchmarks we introduce in this work. The Benchmark is publicly available at https://github.com/facebookresearch/CTrLBenchmark.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a new benchmark allowing a detailed analysis of the properties of continual learning alogrithms and a new modular neural network leveraging task-based priors to efficiently learn in the CL setting.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="193sEnKY1ij" data-number="1873">
      <h4>
        <a href="https://openreview.net/forum?id=193sEnKY1ij">
            No Cost Likelihood Manipulation at Test Time for Making Better Mistakes in Deep Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=193sEnKY1ij" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Shyamgopal_Karthik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shyamgopal_Karthik1">Shyamgopal Karthik</a>, <a href="https://openreview.net/profile?id=~Ameya_Prabhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ameya_Prabhu1">Ameya Prabhu</a>, <a href="https://openreview.net/profile?id=~Puneet_K._Dokania1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Puneet_K._Dokania1">Puneet K. Dokania</a>, <a href="https://openreview.net/profile?id=~Vineet_Gandhi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vineet_Gandhi2">Vineet Gandhi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#193sEnKY1ij-details-740" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="193sEnKY1ij-details-740"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Hierarchy-Aware Classification, Conditional Risk Minimization, Post-Hoc Correction</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">There has been increasing interest in building deep hierarchy-aware classifiers that aim to quantify and reduce the severity of mistakes, and not just reduce the number of errors. The idea is to exploit the label hierarchy (e.g., the WordNet ontology) and consider graph distances as a proxy for mistake severity. Surprisingly, on examining mistake-severity distributions of the top-1 prediction, we find that current state-of-the-art hierarchy-aware deep classifiers do not always show practical improvement over the standard cross-entropy baseline in making better mistakes. The reason for the reduction in average mistake-severity can be attributed to the increase in low-severity mistakes, which may also explain the noticeable drop in their accuracy. To this end, we use the classical Conditional Risk Minimization (CRM) framework for hierarchy-aware classification. Given a cost matrix and a reliable estimate of likelihoods (obtained from a trained network), CRM simply amends mistakes at inference time; it needs no extra hyperparameters and requires adding just a few lines of code to the standard cross-entropy baseline. It significantly outperforms the state-of-the-art and consistently obtains large reductions in the average hierarchical distance of top-<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="219" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container> predictions across datasets, with very little loss in accuracy. CRM, because of its simplicity, can be used with any off-the-shelf trained model that provides reliable likelihood estimates.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Conditional risk framework exploiting the label hierarchy outperforms state of the art and makes a strong baseline for future explorations.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="TaYhv-q1Xit" data-number="3561">
      <h4>
        <a href="https://openreview.net/forum?id=TaYhv-q1Xit">
            Ringing ReLUs: Harmonic Distortion Analysis of Nonlinear Feedforward Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=TaYhv-q1Xit" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Christian_H.X._Ali_Mehmeti-G%C3%B6pel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christian_H.X._Ali_Mehmeti-Göpel1">Christian H.X. Ali Mehmeti-Göpel</a>, <a href="https://openreview.net/profile?id=~David_Hartmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Hartmann1">David Hartmann</a>, <a href="https://openreview.net/profile?email=michael.wand%40uni-mainz.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="michael.wand@uni-mainz.de">Michael Wand</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#TaYhv-q1Xit-details-598" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TaYhv-q1Xit-details-598"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning theory, loss landscape, harmonic distortion analysis, network trainability</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we apply harmonic distortion analysis to understand the effect of nonlinearities in the spectral domain. Each nonlinear layer creates higher-frequency harmonics, which we call "blueshift", whose magnitude increases with network depth, thereby increasing the “roughness” of the output landscape. Unlike differential models (such as vanishing gradients, sharpness), this provides a more global view of how network architectures behave across larger areas of their parameter domain. For example, the model predicts that residual connections are able to counter the effect by dampening corresponding higher frequency modes. We empirically verify the connection between blueshift and architectural choices, and provide evidence for a connection with trainability.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Nonlinearities create high-frequency distortions that affect network trainability.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=TaYhv-q1Xit&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="IFqrg1p5Bc" data-number="2216">
      <h4>
        <a href="https://openreview.net/forum?id=IFqrg1p5Bc">
            Distance-Based Regularisation of Deep Networks for Fine-Tuning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=IFqrg1p5Bc" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Henry_Gouk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Henry_Gouk1">Henry Gouk</a>, <a href="https://openreview.net/profile?id=~Timothy_Hospedales1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Timothy_Hospedales1">Timothy Hospedales</a>, <a href="https://openreview.net/profile?id=~massimiliano_pontil1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~massimiliano_pontil1">massimiliano pontil</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>23 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#IFqrg1p5Bc-details-464" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IFqrg1p5Bc-details-464"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep Learning, Transfer Learning, Statistical Learning Theory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We investigate approaches to regularisation during fine-tuning of deep neural networks. First we provide a neural network generalisation bound based on Rademacher complexity that uses the distance the weights have moved from their initial values. This bound has no direct dependence on the number of weights and compares favourably to other bounds when applied to convolutional networks. Our bound is highly relevant for fine-tuning, because providing a network with a good initialisation based on transfer learning means that learning can modify the weights less, and hence achieve tighter generalisation. Inspired by this, we develop a simple yet effective fine-tuning algorithm that constrains the hypothesis class to a small sphere centred on the initial pre-trained weights, thus obtaining provably better generalisation performance than conventional transfer learning. Empirical evaluation shows that our algorithm works well, corroborating our theoretical results. It outperforms both state of the art fine-tuning competitors, and penalty-based alternatives that we show do not directly constrain the radius of the search space.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We derive generalisation bounds applicable to fine-tuning, then demonstrate an algorithm that regularises these bounds improves fine-tuning performance.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=IFqrg1p5Bc&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="TGFO0DbD_pk" data-number="3101">
      <h4>
        <a href="https://openreview.net/forum?id=TGFO0DbD_pk">
            Genetic Soft Updates for Policy Evolution in Deep Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=TGFO0DbD_pk" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Enrico_Marchesini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Enrico_Marchesini1">Enrico Marchesini</a>, <a href="https://openreview.net/profile?email=davide.corsi%40univr.it" class="profile-link" data-toggle="tooltip" data-placement="top" title="davide.corsi@univr.it">Davide Corsi</a>, <a href="https://openreview.net/profile?id=~Alessandro_Farinelli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alessandro_Farinelli1">Alessandro Farinelli</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#TGFO0DbD_pk-details-592" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TGFO0DbD_pk-details-592"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep Reinforcement Learning, Evolutionary Algorithms, Formal Verification, Machine Learning for Robotics</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The combination of Evolutionary Algorithms (EAs) and Deep Reinforcement Learning (DRL) has been recently proposed to merge the benefits of both solutions. Existing mixed approaches, however, have been successfully applied only to actor-critic methods and present significant overhead. We address these issues by introducing a novel mixed framework that exploits a periodical genetic evaluation to soft update the weights of a DRL agent. The resulting approach is applicable with any DRL method and, in a worst-case scenario, it does not exhibit detrimental behaviours. Experiments in robotic applications and continuous control benchmarks demonstrate the versatility of our approach that significantly outperforms prior DRL, EAs, and mixed approaches. Finally, we employ formal verification to confirm the policy improvement, mitigating the inefficient exploration and hyper-parameter sensitivity of DRL.ment, mitigating the inefficient exploration and hyper-parameter sensitivity of DRL.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a novel mixed framework that combines the benefits of Evolutionary Algorithms and any DRL algorithms (including value-based ones); we support our claims on the beneficial policy improvement using recent formal verification tools.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="1Fqg133qRaI" data-number="306">
      <h4>
        <a href="https://openreview.net/forum?id=1Fqg133qRaI">
            Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis
        </a>
      
        
          <a href="https://openreview.net/pdf?id=1Fqg133qRaI" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Bingchen_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bingchen_Liu2">Bingchen Liu</a>, <a href="https://openreview.net/profile?id=~Yizhe_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yizhe_Zhu2">Yizhe Zhu</a>, <a href="https://openreview.net/profile?id=~Kunpeng_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kunpeng_Song1">Kunpeng Song</a>, <a href="https://openreview.net/profile?id=~Ahmed_Elgammal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ahmed_Elgammal1">Ahmed Elgammal</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 02 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>24 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#1Fqg133qRaI-details-862" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1Fqg133qRaI-details-862"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning, generative model, image synthesis, few-shot learning, generative adversarial network, self-supervised learning, unsupervised learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Training Generative Adversarial Networks (GAN) on high-fidelity images usually requires large-scale GPU-clusters and a vast number of training images. In this paper, we study the few-shot image synthesis task for GAN with minimum computing cost. We propose a light-weight GAN structure that gains superior quality on 1024^2 resolution. Notably, the model converges from scratch with just a few hours of training on a single RTX-2080 GPU, and has a consistent performance, even with less than 100 training samples. Two technique designs constitute our work, a skip-layer channel-wise excitation module and a self-supervised discriminator trained as a feature-encoder. With thirteen datasets covering a wide variety of image domains (The datasets and code are available at https://github.com/odegeasslbc/FastGAN-pytorch), we show our model's superior performance compared to the state-of-the-art StyleGAN2, when data and computing budget are limited.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A computational-efficient GAN for few-shot hi-fi image dataset (converge on single gpu with few hours' training, on 1024 resolution sub-hundred images).</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=1Fqg133qRaI&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Pzj6fzU6wkj" data-number="2324">
      <h4>
        <a href="https://openreview.net/forum?id=Pzj6fzU6wkj">
            IsarStep: a Benchmark for High-level Mathematical Reasoning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Pzj6fzU6wkj" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Wenda_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenda_Li1">Wenda Li</a>, <a href="https://openreview.net/profile?id=~Lei_Yu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lei_Yu4">Lei Yu</a>, <a href="https://openreview.net/profile?id=~Yuhuai_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuhuai_Wu1">Yuhuai Wu</a>, <a href="https://openreview.net/profile?email=lp15%40cam.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="lp15@cam.ac.uk">Lawrence C. Paulson</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Pzj6fzU6wkj-details-368" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Pzj6fzU6wkj-details-368"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">mathematical reasoning, dataset, benchmark, reasoning, transformer</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A well-defined benchmark is essential for measuring and accelerating research progress of machine learning models. In this paper, we present a benchmark for high-level mathematical reasoning and study the reasoning capabilities of neural sequence-to-sequence models. We build a non-synthetic dataset from the largest repository of proofs written by human experts in a theorem prover. The dataset has a broad coverage of undergraduate and research-level mathematical and computer science theorems. In our defined task, a model is required to fill in a missing intermediate proposition given surrounding proofs. This task provides a starting point for the long-term goal of having machines generate human-readable proofs automatically. Our experiments and analysis reveal that while the task is challenging, neural models can capture non-trivial mathematical reasoning. We further design a hierarchical transformer that outperforms the transformer baseline. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a benchmark for high-level mathematical reasoning and study the reasoning capabilities of neural sequence-to-sequence models. </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Pzj6fzU6wkj&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="U_mat0b9iv" data-number="738">
      <h4>
        <a href="https://openreview.net/forum?id=U_mat0b9iv">
            Multi-Prize Lottery Ticket Hypothesis: Finding Accurate Binary Neural Networks by Pruning A Randomly Weighted Network
        </a>
      
        
          <a href="https://openreview.net/pdf?id=U_mat0b9iv" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~James_Diffenderfer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_Diffenderfer1">James Diffenderfer</a>, <a href="https://openreview.net/profile?id=~Bhavya_Kailkhura1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bhavya_Kailkhura1">Bhavya Kailkhura</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>24 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#U_mat0b9iv-details-868" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="U_mat0b9iv-details-868"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Binary Neural Networks, Pruning, Lottery Ticket Hypothesis</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recently, Frankle &amp; Carbin (2019) demonstrated that randomly-initialized dense networks contain subnetworks that once found can be trained to reach test accuracy comparable to the trained dense network. However, finding these high performing trainable subnetworks is expensive, requiring iterative process of training and pruning weights. In this paper, we propose (and prove) a stronger Multi-Prize Lottery Ticket Hypothesis:
      
      A sufficiently over-parameterized neural network with random weights contains several subnetworks (winning tickets) that (a) have comparable accuracy to a dense target network with learned weights (prize 1), (b) do not require any further training to achieve prize 1 (prize 2), and (c) is robust to extreme forms of quantization (i.e., binary weights and/or activation) (prize 3).
      
      This provides a new paradigm for learning compact yet highly accurate binary neural networks simply by pruning and quantizing randomly weighted full precision neural networks. We also propose an algorithm for finding multi-prize tickets (MPTs) and test it by performing a series of experiments on CIFAR-10 and ImageNet datasets. Empirical results indicate that as models grow deeper and wider, multi-prize tickets start to reach similar (and sometimes even higher) test accuracy compared to their significantly larger and full-precision counterparts that have been weight-trained. Without ever updating the weight values, our MPTs-1/32 not only set new binary weight network state-of-the-art (SOTA) Top-1 accuracy -- 94.8% on CIFAR-10 and 74.03% on ImageNet -- but also outperform their full-precision counterparts by 1.78% and 0.76%, respectively. Further, our MPT-1/1 achieves SOTA Top-1 accuracy (91.9%) for binary neural networks on CIFAR-10. Code and pre-trained models are available at: https://github.com/chrundle/biprop.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A new paradigm for learning compact yet accurate binary neural networks by pruning and quantizing randomly weighted full precision DNNs</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="H0syOoy3Ash" data-number="540">
      <h4>
        <a href="https://openreview.net/forum?id=H0syOoy3Ash">
            Average-case Acceleration for Bilinear Games and Normal Matrices
        </a>
      
        
          <a href="https://openreview.net/pdf?id=H0syOoy3Ash" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Carles_Domingo-Enrich1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Carles_Domingo-Enrich1">Carles Domingo-Enrich</a>, <a href="https://openreview.net/profile?id=~Fabian_Pedregosa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fabian_Pedregosa1">Fabian Pedregosa</a>, <a href="https://openreview.net/profile?id=~Damien_Scieur3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Damien_Scieur3">Damien Scieur</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#H0syOoy3Ash-details-347" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="H0syOoy3Ash-details-347"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Smooth games, First-order Methods, Acceleration, Bilinear games, Average-case Analysis, Orthogonal Polynomials</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Advances in generative modeling and adversarial learning have given rise to renewed interest in smooth games. However, the absence of symmetry in the matrix of second derivatives poses challenges that are not present in the classical minimization framework. While a rich theory of average-case analysis has been developed for minimization problems, little is known in the context of smooth games. In this work we take a first step towards closing this gap by developing average-case optimal first-order methods for a subset of smooth games. 
      We make the following three main contributions. First, we show that for zero-sum bilinear games the average-case optimal method is the optimal method for the minimization of the Hamiltonian. Second, we provide an explicit expression for the optimal method corresponding to normal matrices, potentially non-symmetric. Finally, we specialize it to matrices with eigenvalues located in a disk and show a provable speed-up compared to worst-case optimal algorithms. We illustrate our findings through benchmarks with a varying degree of mismatch with our assumptions.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We extend the framework of average-case optimal first-order methods to problems with non-symmetric matrices, which naturally arise in equilibrium finding for games.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=H0syOoy3Ash&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="VbLH04pRA3" data-number="2652">
      <h4>
        <a href="https://openreview.net/forum?id=VbLH04pRA3">
            ECONOMIC HYPERPARAMETER OPTIMIZATION WITH BLENDED SEARCH STRATEGY
        </a>
      
        
          <a href="https://openreview.net/pdf?id=VbLH04pRA3" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Chi_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chi_Wang3">Chi Wang</a>, <a href="https://openreview.net/profile?email=qingyun.wu%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="qingyun.wu@microsoft.com">Qingyun Wu</a>, <a href="https://openreview.net/profile?email=silu.huang%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="silu.huang@microsoft.com">Silu Huang</a>, <a href="https://openreview.net/profile?email=amin.saied%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="amin.saied@microsoft.com">Amin Saied</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Apr 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#VbLH04pRA3-details-196" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="VbLH04pRA3-details-196"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">HYPERPARAMETER OPTIMIZATION, COST</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the problem of using low cost to search for hyperparameter configurations in a large search space with heterogeneous evaluation cost and model quality.
      We propose a blended search strategy to combine the strengths of global and local search, and prioritize them on the fly with the goal of minimizing the total cost spent in finding good configurations. Our approach demonstrates robust performance for tuning both tree-based models and deep neural networks on a large AutoML benchmark, as well as superior performance in model quality, time, and resource consumption for a production transformer-based NLP model fine-tuning task.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A low-cost hyperparameter optimization solution by blending global and local search methods with cost-based prioritization</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=VbLH04pRA3&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="TiXl51SCNw8" data-number="110">
      <h4>
        <a href="https://openreview.net/forum?id=TiXl51SCNw8">
            BSQ: Exploring Bit-Level Sparsity for Mixed-Precision Neural Network Quantization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=TiXl51SCNw8" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Huanrui_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huanrui_Yang1">Huanrui Yang</a>, <a href="https://openreview.net/profile?email=ld213%40duke.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ld213@duke.edu">Lin Duan</a>, <a href="https://openreview.net/profile?id=~Yiran_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yiran_Chen1">Yiran Chen</a>, <a href="https://openreview.net/profile?id=~Hai_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hai_Li1">Hai Li</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>19 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#TiXl51SCNw8-details-327" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TiXl51SCNw8-details-327"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Mixed-precision quantization, bit-level sparsity, DNN compression</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Mixed-precision quantization can potentially achieve the optimal tradeoff between performance and compression rate of deep neural networks, and thus, have been widely investigated. However, it lacks a systematic method to determine the exact quantization scheme. Previous methods either examine only a small manually-designed search space or utilize a cumbersome neural architecture search to explore the vast search space. These approaches cannot lead to an optimal quantization scheme efficiently. This work proposes bit-level sparsity quantization (BSQ) to tackle the mixed-precision quantization from a new angle of inducing bit-level sparsity. We consider each bit of quantized weights as an independent trainable variable and introduce a differentiable bit-sparsity regularizer. BSQ can induce all-zero bits across a group of weight elements and realize the dynamic precision reduction, leading to a mixed-precision quantization scheme of the original model. Our method enables the exploration of the full mixed-precision space with a single gradient-based optimization process, with only one hyperparameter to tradeoff the performance and compression. BSQ achieves both higher accuracy and higher bit reduction on various model architectures on the CIFAR-10 and ImageNet datasets comparing to previous methods.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose bit-level sparsity inducing regularizer to induce mixed-percision quantization scheme in DNN with gradient-based training.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=TiXl51SCNw8&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="SlrqM9_lyju" data-number="1900">
      <h4>
        <a href="https://openreview.net/forum?id=SlrqM9_lyju">
            AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly
        </a>
      
        
          <a href="https://openreview.net/pdf?id=SlrqM9_lyju" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yuchen_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuchen_Jin1">Yuchen Jin</a>, <a href="https://openreview.net/profile?id=~Tianyi_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianyi_Zhou1">Tianyi Zhou</a>, <a href="https://openreview.net/profile?email=liangyu%40cs.washington.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="liangyu@cs.washington.edu">Liangyu Zhao</a>, <a href="https://openreview.net/profile?email=zhuyibo%40bytedance.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhuyibo@bytedance.com">Yibo Zhu</a>, <a href="https://openreview.net/profile?email=guochuanxiong%40bytedance.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="guochuanxiong@bytedance.com">Chuanxiong Guo</a>, <a href="https://openreview.net/profile?id=~Marco_Canini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marco_Canini1">Marco Canini</a>, <a href="https://openreview.net/profile?id=~Arvind_Krishnamurthy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arvind_Krishnamurthy1">Arvind Krishnamurthy</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#SlrqM9_lyju-details-619" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SlrqM9_lyju-details-619"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The learning rate (LR) schedule is one of the most important hyper-parameters needing careful tuning in training DNNs. However, it is also one of the least automated parts of machine learning systems and usually costs significant manual effort and computing. Though there are pre-defined LR schedules and optimizers with adaptive LR, they introduce new hyperparameters that need to be tuned separately for different tasks/datasets. In this paper, we consider the question: Can we automatically tune the LR over the course of training without human involvement? We propose an efficient method, AutoLRS, which automatically optimizes the LR for each training stage by modeling training dynamics. AutoLRS aims to find an LR that minimizes the validation loss, every <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="220" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>τ</mi></math></mjx-assistive-mml></mjx-container> steps. We formulate it as black-box optimization and solve it by Bayesian optimization (BO). However, collecting training instances for BO requires a system to evaluate each LR queried by BO's acquisition function for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="221" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>τ</mi></math></mjx-assistive-mml></mjx-container> steps, which is prohibitively expensive in practice. Instead, we apply each candidate LR for only <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="222" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em; margin-left: 0.054em;"><mjx-mo class="mjx-n" size="s"><mjx-c class="mjx-c2032"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c226A"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D70F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>τ</mi><mo>′</mo></msup><mo>≪</mo><mi>τ</mi></math></mjx-assistive-mml></mjx-container> steps and train an exponential model to predict the validation loss after <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="223" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D70F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>τ</mi></math></mjx-assistive-mml></mjx-container> steps. This mutual-training process between BO and the exponential model allows us to bound the number of training steps invested in the BO search. We demonstrate the advantages and the generality of AutoLRS through extensive experiments of training DNNs from diverse domains and using different optimizers. The LR schedules auto-generated by AutoLRS leads to a speedup of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="224" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1.22</mn><mo>×</mo></math></mjx-assistive-mml></mjx-container>, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="225" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1.43</mn><mo>×</mo></math></mjx-assistive-mml></mjx-container>, and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="226" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1.5</mn><mo>×</mo></math></mjx-assistive-mml></mjx-container> when training ResNet-50, Transformer, and BERT, respectively, compared to the LR schedules in their original papers, and an average speedup of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="227" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1.31</mn><mo>×</mo></math></mjx-assistive-mml></mjx-container> over state-of-the-art highly tuned LR schedules.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="YWtLZvLmud7" data-number="947">
      <h4>
        <a href="https://openreview.net/forum?id=YWtLZvLmud7">
            BERTology Meets Biology: Interpreting Attention in Protein Language Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=YWtLZvLmud7" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jesse_Vig1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jesse_Vig1">Jesse Vig</a>, <a href="https://openreview.net/profile?email=madani%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="madani@berkeley.edu">Ali Madani</a>, <a href="https://openreview.net/profile?id=~Lav_R._Varshney1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lav_R._Varshney1">Lav R. Varshney</a>, <a href="https://openreview.net/profile?id=~Caiming_Xiong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Caiming_Xiong1">Caiming Xiong</a>, <a href="https://openreview.net/profile?id=~richard_socher1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~richard_socher1">richard socher</a>, <a href="https://openreview.net/profile?id=~Nazneen_Rajani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nazneen_Rajani1">Nazneen Rajani</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#YWtLZvLmud7-details-831" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YWtLZvLmud7-details-831"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">interpretability, black box, computational biology, representation learning, attention, transformers, visualization, natural language processing</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Transformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. In this work, we demonstrate a set of methods for analyzing protein Transformer models through the lens of attention. We show that attention: (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We find this behavior to be consistent across three Transformer architectures (BERT, ALBERT, XLNet) and two distinct protein datasets. We also present a three-dimensional visualization of the interaction between attention and protein structure. Code for visualization and analysis is available at https://github.com/salesforce/provis.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We analyze the internal representations of protein language models, and show that attention targets structural and functional properties of protein sequences.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=YWtLZvLmud7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="qzBUIzq5XR2" data-number="535">
      <h4>
        <a href="https://openreview.net/forum?id=qzBUIzq5XR2">
            Learning Task-General Representations with Generative Neuro-Symbolic Modeling
        </a>
      
        
          <a href="https://openreview.net/pdf?id=qzBUIzq5XR2" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Reuben_Feinman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Reuben_Feinman1">Reuben Feinman</a>, <a href="https://openreview.net/profile?id=~Brenden_M._Lake1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brenden_M._Lake1">Brenden M. Lake</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#qzBUIzq5XR2-details-864" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qzBUIzq5XR2-details-864"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">few-shot concept learning, neuro-symbolic models, probabilistic programs, generative models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">People can learn rich, general-purpose conceptual representations from only raw perceptual inputs. Current machine learning approaches fall well short of these human standards, although different modeling traditions often have complementary strengths. Symbolic models can capture the compositional and causal knowledge that enables flexible generalization, but they struggle to learn from raw inputs, relying on strong abstractions and simplifying assumptions. Neural network models can learn directly from raw data, but they struggle to capture compositional and causal structure and typically must retrain to tackle new tasks. We bring together these two traditions to learn generative models of concepts that capture rich compositional and causal structure, while learning from raw data. We develop a generative neuro-symbolic (GNS) model of handwritten character concepts that uses the control flow of a probabilistic program, coupled with symbolic stroke primitives and a symbolic image renderer, to represent the causal and compositional processes by which characters are formed. The distributions of parts (strokes), and correlations between parts, are modeled with neural network subroutines, allowing the model to learn directly from raw data and express nonparametric statistical relationships. We apply our model to the Omniglot challenge of human-level concept learning, using a background set of alphabets to learn an expressive prior distribution over character drawings. In a subsequent evaluation, our GNS model uses probabilistic inference to learn rich conceptual representations from a single training image that generalize to 4 unique tasks, succeeding where previous work has fallen short.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">few-shot concept learning with generative neuro-symbolic models</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=qzBUIzq5XR2&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="8wqCDnBmnrT" data-number="951">
      <h4>
        <a href="https://openreview.net/forum?id=8wqCDnBmnrT">
            Zero-shot Synthesis with Group-Supervised Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=8wqCDnBmnrT" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yunhao_Ge1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yunhao_Ge1">Yunhao Ge</a>, <a href="https://openreview.net/profile?id=~Sami_Abu-El-Haija1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sami_Abu-El-Haija1">Sami Abu-El-Haija</a>, <a href="https://openreview.net/profile?id=~Gan_Xin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gan_Xin1">Gan Xin</a>, <a href="https://openreview.net/profile?id=~Laurent_Itti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Laurent_Itti1">Laurent Itti</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#8wqCDnBmnrT-details-559" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8wqCDnBmnrT-details-559"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Disentangled representation learning, Group-supervised learning, Zero-shot synthesis, Knowledge factorization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Visual cognition of primates is superior to that of artificial neural networks in its ability to “envision” a visual object, even a newly-introduced one, in different attributes including pose, position, color, texture, etc.  To aid neural networks to envision objects with different attributes,  we propose a family of objective functions, expressed on groups of examples, as a novel learning framework that we term Group-Supervised Learning (GSL). GSL allows us to decompose inputs into a disentangled representation with swappable components, that can be recombined to synthesize new samples.  For instance, images of red boats &amp; blue cars can be decomposed and recombined to synthesize novel images of red cars.   We propose an implementation based on auto-encoder, termed group-supervised zero-shot synthesis network (GZS-Net) trained with our learning framework, that can produce a high-quality red car even if no such example is witnessed during training. We test our model and learning framework on existing benchmarks, in addition to a new dataset that we open-source. We qualitatively and quantitatively demonstrate that GZS-Net trained with GSL outperforms state-of-the-art methods</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">To aid neural networks to envision objects with different attributes,  we propose GSL which allows us to decompose inputs into a disentangled representation with swappable components, that can be recombined to synthesize new samples. </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=8wqCDnBmnrT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="N0M_4BkQ05i" data-number="2955">
      <h4>
        <a href="https://openreview.net/forum?id=N0M_4BkQ05i">
            Selective Classification Can Magnify Disparities Across Groups
        </a>
      
        
          <a href="https://openreview.net/pdf?id=N0M_4BkQ05i" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Erik_Jones3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Erik_Jones3">Erik Jones</a>, <a href="https://openreview.net/profile?id=~Shiori_Sagawa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shiori_Sagawa1">Shiori Sagawa</a>, <a href="https://openreview.net/profile?id=~Pang_Wei_Koh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pang_Wei_Koh1">Pang Wei Koh</a>, <a href="https://openreview.net/profile?id=~Ananya_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ananya_Kumar1">Ananya Kumar</a>, <a href="https://openreview.net/profile?id=~Percy_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Percy_Liang1">Percy Liang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#N0M_4BkQ05i-details-858" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="N0M_4BkQ05i-details-858"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">selective classification, group disparities, log-concavity, robustness</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Selective classification, in which models can abstain on uncertain predictions, is a natural approach to improving accuracy in settings where errors are costly but abstentions are manageable. In this paper, we find that while selective classification can improve average accuracies, it can simultaneously magnify existing accuracy disparities between various groups within a population, especially in the presence of spurious correlations. We observe this behavior consistently across five vision and NLP datasets. Surprisingly, increasing abstentions can even decrease accuracies on some groups. To better understand this phenomenon, we study the margin distribution, which captures the model’s confidences over all predictions. For symmetric margin distributions, we prove that whether selective classification monotonically improves or worsens accuracy is fully determined by the accuracy at full coverage (i.e., without any abstentions) and whether the distribution satisfies a property we call left-log-concavity. Our analysis also shows that selective classification tends to magnify full-coverage accuracy disparities. Motivated by our analysis, we train distributionally-robust models that achieve similar full-coverage accuracies across groups and show that selective classification uniformly improves each group on these models. Altogether, our results suggest that selective classification should be used with care and underscore the importance of training models to perform equally well across groups at full coverage.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="OQ08SN70M1V" data-number="260">
      <h4>
        <a href="https://openreview.net/forum?id=OQ08SN70M1V">
            Better Fine-Tuning by Reducing Representational Collapse
        </a>
      
        
          <a href="https://openreview.net/pdf?id=OQ08SN70M1V" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Armen_Aghajanyan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Armen_Aghajanyan1">Armen Aghajanyan</a>, <a href="https://openreview.net/profile?email=akshats%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="akshats@fb.com">Akshat Shrivastava</a>, <a href="https://openreview.net/profile?email=anchit%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="anchit@fb.com">Anchit Gupta</a>, <a href="https://openreview.net/profile?id=~Naman_Goyal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Naman_Goyal1">Naman Goyal</a>, <a href="https://openreview.net/profile?id=~Luke_Zettlemoyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luke_Zettlemoyer1">Luke Zettlemoyer</a>, <a href="https://openreview.net/profile?email=sonalgupta%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sonalgupta@fb.com">Sonal Gupta</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#OQ08SN70M1V-details-777" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OQ08SN70M1V-details-777"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">finetuning, nlp, representational learning, glue</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Although widely adopted, existing approaches for fine-tuning pre-trained language models have been shown to be unstable across hyper-parameter settings, motivating recent work on trust region methods. In this paper, we present a simplified and efficient method rooted in trust region theory that replaces previously used adversarial objectives with parametric noise (sampling from either a normal or uniform distribution), thereby discouraging representation change during fine-tuning when possible without hurting performance. We also introduce a new analysis to motivate the use of trust region methods more generally, by studying representational collapse; the degradation of generalizable representations from pre-trained models as they are fine-tuned for a specific end task. Extensive experiments show that our fine-tuning method matches or exceeds the performance of previous trust region methods on a range of understanding and generation tasks (including DailyMail/CNN, Gigaword, Reddit TIFU, and the GLUE benchmark), while also being much faster. We also show that it is less prone to representation collapse; the pre-trained models maintain more generalizable representations every time they are fine-tuned.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a lightweight augmentation to standard fine-tuning which outperforms previous methods across the board (i.e. SOTA on 3 summarization tasks, XNLI, RoBERTa on GLUE) while being computationally cheaper than other fine-tuning approaches.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="OGg9XnKxFAH" data-number="3498">
      <h4>
        <a href="https://openreview.net/forum?id=OGg9XnKxFAH">
            Training independent subnetworks for robust prediction
        </a>
      
        
          <a href="https://openreview.net/pdf?id=OGg9XnKxFAH" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Marton_Havasi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marton_Havasi1">Marton Havasi</a>, <a href="https://openreview.net/profile?id=~Rodolphe_Jenatton3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rodolphe_Jenatton3">Rodolphe Jenatton</a>, <a href="https://openreview.net/profile?id=~Stanislav_Fort1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stanislav_Fort1">Stanislav Fort</a>, <a href="https://openreview.net/profile?id=~Jeremiah_Zhe_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jeremiah_Zhe_Liu1">Jeremiah Zhe Liu</a>, <a href="https://openreview.net/profile?id=~Jasper_Snoek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jasper_Snoek1">Jasper Snoek</a>, <a href="https://openreview.net/profile?id=~Balaji_Lakshminarayanan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Balaji_Lakshminarayanan1">Balaji Lakshminarayanan</a>, <a href="https://openreview.net/profile?id=~Andrew_Mingbo_Dai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_Mingbo_Dai1">Andrew Mingbo Dai</a>, <a href="https://openreview.net/profile?id=~Dustin_Tran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dustin_Tran1">Dustin Tran</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#OGg9XnKxFAH-details-240" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OGg9XnKxFAH-details-240"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Efficient ensembles, robustness</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent approaches to efficiently ensemble neural networks have shown that strong robustness and uncertainty performance  can be achieved with a negligible gain in parameters over the original network. However, these methods still require multiple forward passes for prediction, leading to a significant runtime cost. In this work, we show a surprising result:
      the benefits of using multiple predictions can be achieved 'for free' under a single model's forward pass. In particular, we show that, using a multi-input multi-output (MIMO) configuration, one can utilize a single model's capacity to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the subnetworks, we improve model robustness without increasing compute. We observe a significant improvement in negative log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100,  ImageNet, and their out-of-distribution variants compared to previous methods.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that a deep neural network can be trained to give multiple independent predictions simultaneously, which results in a computationally efficient ensemble model.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=OGg9XnKxFAH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="--gvHfE3Xf5" data-number="1648">
      <h4>
        <a href="https://openreview.net/forum?id=--gvHfE3Xf5">
            Meta-Learning of Structured Task Distributions in Humans and Machines
        </a>
      
        
          <a href="https://openreview.net/pdf?id=--gvHfE3Xf5" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sreejan_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sreejan_Kumar1">Sreejan Kumar</a>, <a href="https://openreview.net/profile?id=~Ishita_Dasgupta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ishita_Dasgupta1">Ishita Dasgupta</a>, <a href="https://openreview.net/profile?id=~Jonathan_Cohen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Cohen1">Jonathan Cohen</a>, <a href="https://openreview.net/profile?id=~Nathaniel_Daw1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nathaniel_Daw1">Nathaniel Daw</a>, <a href="https://openreview.net/profile?id=~Thomas_Griffiths1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Griffiths1">Thomas Griffiths</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>23 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#--gvHfE3Xf5-details-349" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="--gvHfE3Xf5-details-349"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">meta-learning, human cognition, reinforcement learning, compositionality</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In recent years, meta-learning, in which a model is trained on a family of tasks (i.e. a task distribution), has emerged as an approach to training neural networks to perform tasks that were previously assumed to require structured representations, making strides toward closing the gap between humans and machines. However, we argue that evaluating meta-learning remains a challenge, and can miss whether meta-learning actually uses the structure embedded within the tasks. These meta-learners might therefore still be significantly different from humans learners. To demonstrate this difference, we first define a new meta-reinforcement learning task in which a structured task distribution is generated using a compositional grammar. We then introduce a novel approach to constructing a "null task distribution" with the same statistical complexity as this structured task distribution but without the explicit rule-based structure used to generate the structured task. We train a standard meta-learning agent, a recurrent network trained with model-free reinforcement learning, and compare it with human performance across the two task distributions. We find a double dissociation in which humans do better in the structured task distribution whereas agents do better in the null task distribution -- despite comparable statistical complexity. This work highlights that multiple strategies can achieve reasonable meta-test performance, and that careful construction of control task distributions is a valuable way to understand which strategies meta-learners acquire, and how they might differ from humans. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We developed a novel meta-learning task with a structured task distribution and statistically equivalent "null" task distribution to show humans are more adept at the former whereas current meta-learning agents are more adept at the latter. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=--gvHfE3Xf5&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9QLRCVysdlO" data-number="28">
      <h4>
        <a href="https://openreview.net/forum?id=9QLRCVysdlO">
            BiPointNet: Binary Neural Network for Point Clouds
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9QLRCVysdlO" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=qinhaotong%40buaa.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="qinhaotong@buaa.edu.cn">Haotong Qin</a>, <a href="https://openreview.net/profile?id=~Zhongang_Cai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhongang_Cai1">Zhongang Cai</a>, <a href="https://openreview.net/profile?id=~Mingyuan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingyuan_Zhang1">Mingyuan Zhang</a>, <a href="https://openreview.net/profile?id=~Yifu_Ding2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yifu_Ding2">Yifu Ding</a>, <a href="https://openreview.net/profile?id=~Haiyu_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haiyu_Zhao1">Haiyu Zhao</a>, <a href="https://openreview.net/profile?id=~Shuai_Yi3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuai_Yi3">Shuai Yi</a>, <a href="https://openreview.net/profile?id=~Xianglong_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xianglong_Liu2">Xianglong Liu</a>, <a href="https://openreview.net/profile?id=~Hao_Su1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Su1">Hao Su</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 13 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9QLRCVysdlO-details-145" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9QLRCVysdlO-details-145"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">point clouds, efficient deep learning, binary neural networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">To alleviate the resource constraint for real-time point cloud applications that run on edge devices, in this paper we present BiPointNet, the first model binarization approach for efficient deep learning on point clouds. We discover that the immense performance drop of binarized models for point clouds mainly stems from two challenges: aggregation-induced feature homogenization that leads to a degradation of information entropy, and scale distortion that hinders optimization and invalidates scale-sensitive structures. With theoretical justifications and in-depth analysis, our BiPointNet introduces Entropy-Maximizing Aggregation (EMA) to modulate the distribution before aggregation for the maximum information entropy, and Layer-wise Scale Recovery (LSR) to efficiently restore feature representation capacity. Extensive experiments show that BiPointNet outperforms existing binarization methods by convincing margins, at the level even comparable with the full precision counterpart. We highlight that our techniques are generic, guaranteeing significant improvements on various fundamental tasks and mainstream backbones. Moreover, BiPointNet gives an impressive 14.7× speedup and 18.9× storage saving on real-world resource-constrained devices.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present BiPointNet, the first model binarization approach to efficient deep learning on point clouds, targeting at extreme compression and acceleration.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="kWSeGEeHvF8" data-number="2401">
      <h4>
        <a href="https://openreview.net/forum?id=kWSeGEeHvF8">
            Benchmarks for Deep Off-Policy Evaluation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=kWSeGEeHvF8" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Justin_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Justin_Fu1">Justin Fu</a>, <a href="https://openreview.net/profile?id=~Mohammad_Norouzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammad_Norouzi1">Mohammad Norouzi</a>, <a href="https://openreview.net/profile?id=~Ofir_Nachum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ofir_Nachum1">Ofir Nachum</a>, <a href="https://openreview.net/profile?id=~George_Tucker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~George_Tucker1">George Tucker</a>, <a href="https://openreview.net/profile?id=~ziyu_wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~ziyu_wang1">ziyu wang</a>, <a href="https://openreview.net/profile?id=~Alexander_Novikov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Novikov1">Alexander Novikov</a>, <a href="https://openreview.net/profile?id=~Mengjiao_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mengjiao_Yang1">Mengjiao Yang</a>, <a href="https://openreview.net/profile?id=~Michael_R_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_R_Zhang1">Michael R Zhang</a>, <a href="https://openreview.net/profile?id=~Yutian_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yutian_Chen1">Yutian Chen</a>, <a href="https://openreview.net/profile?id=~Aviral_Kumar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aviral_Kumar2">Aviral Kumar</a>, <a href="https://openreview.net/profile?id=~Cosmin_Paduraru1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cosmin_Paduraru1">Cosmin Paduraru</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>, <a href="https://openreview.net/profile?id=~Thomas_Paine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Paine1">Thomas Paine</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#kWSeGEeHvF8-details-966" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kWSeGEeHvF8-details-966"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, off-policy evaluation, benchmarks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area.    </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A benchmark proposal for off-policy evaluation and policy selection.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=kWSeGEeHvF8&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="V6BjBgku7Ro" data-number="2661">
      <h4>
        <a href="https://openreview.net/forum?id=V6BjBgku7Ro">
            Planning from Pixels using Inverse Dynamics Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=V6BjBgku7Ro" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Keiran_Paster1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Keiran_Paster1">Keiran Paster</a>, <a href="https://openreview.net/profile?id=~Sheila_A._McIlraith1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sheila_A._McIlraith1">Sheila A. McIlraith</a>, <a href="https://openreview.net/profile?id=~Jimmy_Ba1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jimmy_Ba1">Jimmy Ba</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#V6BjBgku7Ro-details-206" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="V6BjBgku7Ro-details-206"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">model based reinforcement learning, deep reinforcement learning, multi-task learning, deep learning, goal-conditioned reinforcement learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Learning dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn models in a latent space by learning to predict sequences of future actions conditioned on task completion. These models track task-relevant environment dynamics over a distribution of tasks, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">GLAMOR learns a latent world model by learning to predict action sequences conditioned on task completion.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="rsogjAnYs4z" data-number="1569">
      <h4>
        <a href="https://openreview.net/forum?id=rsogjAnYs4z">
            Understanding the effects of data parallelism and sparsity on neural network training
        </a>
      
        
          <a href="https://openreview.net/pdf?id=rsogjAnYs4z" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Namhoon_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Namhoon_Lee1">Namhoon Lee</a>, <a href="https://openreview.net/profile?id=~Thalaiyasingam_Ajanthan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thalaiyasingam_Ajanthan1">Thalaiyasingam Ajanthan</a>, <a href="https://openreview.net/profile?id=~Philip_Torr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philip_Torr1">Philip Torr</a>, <a href="https://openreview.net/profile?id=~Martin_Jaggi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Jaggi1">Martin Jaggi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#rsogjAnYs4z-details-317" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rsogjAnYs4z-details-317"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">data parallelism, sparsity, neural network training</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study two factors in neural network training: data parallelism and sparsity; here, data parallelism means processing training data in parallel using distributed systems (or equivalently increasing batch size), so that training can be accelerated; for sparsity, we refer to pruning parameters in a neural network model, so as to reduce computational and memory cost. Despite their promising benefits, however, understanding of their effects on neural network training remains elusive. In this work, we first measure these effects rigorously by conducting extensive experiments while tuning all metaparameters involved in the optimization. As a result, we find across various workloads of data set, network model, and optimization algorithm that there exists a general scaling trend between batch size and number of training steps to convergence for the effect of data parallelism, and further, difficulty of training under sparsity. Then, we develop a theoretical analysis based on the convergence properties of stochastic gradient methods and smoothness of the optimization landscape, which illustrates the observed phenomena precisely and generally, establishing a better account of the effects of data parallelism and sparsity on neural network training.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We accurately measure the effects of data parallelism and sparsity on neural network training and develop a theoretical analysis to precisely account for their effects.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Iw4ZGwenbXf" data-number="2254">
      <h4>
        <a href="https://openreview.net/forum?id=Iw4ZGwenbXf">
            NOVAS: Non-convex Optimization via Adaptive Stochastic Search for End-to-end Learning and Control
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Iw4ZGwenbXf" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ioannis_Exarchos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ioannis_Exarchos1">Ioannis Exarchos</a>, <a href="https://openreview.net/profile?id=~Marcus_Aloysius_Pereira1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marcus_Aloysius_Pereira1">Marcus Aloysius Pereira</a>, <a href="https://openreview.net/profile?id=~Ziyi_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziyi_Wang1">Ziyi Wang</a>, <a href="https://openreview.net/profile?id=~Evangelos_Theodorou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Evangelos_Theodorou1">Evangelos Theodorou</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Iw4ZGwenbXf-details-255" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Iw4ZGwenbXf-details-255"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep neural networks, nested optimization, stochastic control, deep FBSDEs</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this work we propose the use of adaptive stochastic search as a building block for general, non-convex optimization operations within deep neural network architectures. Specifically, for an objective function located at some layer in the network and parameterized by some network parameters, we employ adaptive stochastic search to perform optimization over its output. This operation is differentiable and does not obstruct the passing of gradients during backpropagation, thus enabling us to incorporate it as a component in end-to-end learning. We study the proposed optimization module's properties and benchmark it against two existing alternatives on a synthetic energy-based structured prediction task, and further showcase its use in stochastic optimal control applications.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Iw4ZGwenbXf&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="8e6BrwU6AjQ" data-number="190">
      <h4>
        <a href="https://openreview.net/forum?id=8e6BrwU6AjQ">
            MoVie: Revisiting Modulated Convolutions for Visual Counting and Beyond
        </a>
      
        
          <a href="https://openreview.net/pdf?id=8e6BrwU6AjQ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Duy_Kien_Nguyen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Duy_Kien_Nguyen1">Duy Kien Nguyen</a>, <a href="https://openreview.net/profile?id=~Vedanuj_Goswami1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vedanuj_Goswami1">Vedanuj Goswami</a>, <a href="https://openreview.net/profile?id=~Xinlei_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinlei_Chen1">Xinlei Chen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#8e6BrwU6AjQ-details-968" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8e6BrwU6AjQ-details-968"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">visual counting, visual question answering, common object counting, visual reasoning, modulated convolution</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper focuses on visual counting, which aims to predict the number of occurrences given a natural image and a query (e.g. a question or a category). Unlike most prior works that use explicit, symbolic models which can be computationally expensive and limited in generalization, we propose a simple and effective alternative by revisiting modulated convolutions that fuse the query and the image locally. Following the design of residual bottleneck, we call our method MoVie, short for Modulated conVolutional bottlenecks. Notably, MoVie reasons implicitly and holistically and only needs a single forward-pass during inference. Nevertheless, MoVie showcases strong performance for counting: 1) advancing the state-of-the-art on counting-specific VQA tasks while being more efficient; 2) outperforming prior-art on difficult benchmarks like COCO for common object counting; 3) helped us secure the first place of 2020 VQA challenge when integrated as a module for ‘number’ related questions in generic VQA models. Finally, we show evidence that modulated convolutions such as MoVie can serve as a general mechanism for reasoning tasks beyond counting.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">2020 VQA challenge winner; state-of-the-art performance on three counting benchmarks; can work beyond counting towards general visual reasoning</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=8e6BrwU6AjQ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="pmj131uIL9H" data-number="1030">
      <h4>
        <a href="https://openreview.net/forum?id=pmj131uIL9H">
            NeMo: Neural Mesh Models of Contrastive Features for Robust 3D Pose Estimation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=pmj131uIL9H" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Angtian_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Angtian_Wang2">Angtian Wang</a>, <a href="https://openreview.net/profile?id=~Adam_Kortylewski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adam_Kortylewski1">Adam Kortylewski</a>, <a href="https://openreview.net/profile?id=~Alan_Yuille1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alan_Yuille1">Alan Yuille</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#pmj131uIL9H-details-535" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pmj131uIL9H-details-535"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Pose Estimation, Robust Deep Learning, Contrastive Learning, Render-and-Compare</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">3D pose estimation is a challenging but important task in computer vision. In this work, we show that standard deep learning approaches to 3D pose estimation are not robust to partial occlusion. Inspired by the robustness of generative vision models to partial occlusion, we propose to integrate deep neural networks with 3D generative representations of objects into a unified neural architecture that we term NeMo. In particular, NeMo learns a generative model of neural feature activations at each vertex on a dense 3D mesh. Using differentiable rendering we estimate the 3D object pose by minimizing the reconstruction error between NeMo and the feature representation of the target image. To avoid local optima in the reconstruction loss, we train the feature extractor to maximize the distance between the individual feature representations on the mesh using contrastive learning. Our extensive experiments on PASCAL3D+, occluded-PASCAL3D+ and ObjectNet3D show that NeMo is much more robust to partial occlusion compared to standard deep networks, while retaining competitive performance on non-occluded data. Interestingly, our experiments also show that NeMo performs reasonably well even when the mesh representation only crudely approximates the true object geometry with a cuboid, hence revealing that the detailed 3D geometry is not needed for accurate 3D pose estimation.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce NeMo, a rendering-based approach to 3D pose estimation that models objects in terms of neural feature activations, instead of image intensities.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=pmj131uIL9H&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tiqI7w64JG2" data-number="2846">
      <h4>
        <a href="https://openreview.net/forum?id=tiqI7w64JG2">
            On Graph Neural Networks versus Graph-Augmented MLPs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tiqI7w64JG2" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Lei_Chen4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lei_Chen4">Lei Chen</a>, <a href="https://openreview.net/profile?id=~Zhengdao_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhengdao_Chen1">Zhengdao Chen</a>, <a href="https://openreview.net/profile?id=~Joan_Bruna1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joan_Bruna1">Joan Bruna</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tiqI7w64JG2-details-157" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tiqI7w64JG2-details-157"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Graph Neural Networks, expressive power, feature propagation, rooted graphs, attributed walks, community detection, depth separation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">From the perspectives of expressive power and learning, this work compares multi-layer Graph Neural Networks (GNNs) with a simplified alternative that we call Graph-Augmented Multi-Layer Perceptrons (GA-MLPs), which first augments node features with certain multi-hop operators on the graph and then applies learnable node-wise functions. From the perspective of graph isomorphism testing, we show both theoretically and numerically that GA-MLPs with suitable operators can distinguish almost all non-isomorphic graphs, just like the Weisfeiler-Lehman (WL) test and GNNs. However, by viewing them as node-level functions and examining the equivalence classes they induce on rooted graphs, we prove a separation in expressive power between GA-MLPs and GNNs that grows exponentially in depth. In particular, unlike GNNs, GA-MLPs are unable to count the number of attributed walks. We also demonstrate via community detection experiments that GA-MLPs can be limited by their choice of operator family, whereas GNNs have higher flexibility in learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We establish a separation in expressive power and flexibility of learning between GNNs and Graph-Augmented MLPs.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Pz_dcqfcKW8" data-number="103">
      <h4>
        <a href="https://openreview.net/forum?id=Pz_dcqfcKW8">
            Dual-mode ASR: Unify and Improve Streaming ASR with Full-context Modeling
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Pz_dcqfcKW8" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jiahui_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiahui_Yu1">Jiahui Yu</a>, <a href="https://openreview.net/profile?id=~Wei_Han3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Han3">Wei Han</a>, <a href="https://openreview.net/profile?id=~Anmol_Gulati1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anmol_Gulati1">Anmol Gulati</a>, <a href="https://openreview.net/profile?id=~Chung-Cheng_Chiu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chung-Cheng_Chiu1">Chung-Cheng Chiu</a>, <a href="https://openreview.net/profile?id=~Bo_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Li1">Bo Li</a>, <a href="https://openreview.net/profile?id=~Tara_N_Sainath1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tara_N_Sainath1">Tara N Sainath</a>, <a href="https://openreview.net/profile?id=~Yonghui_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yonghui_Wu1">Yonghui Wu</a>, <a href="https://openreview.net/profile?id=~Ruoming_Pang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruoming_Pang1">Ruoming Pang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Pz_dcqfcKW8-details-525" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Pz_dcqfcKW8-details-525"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Speech Recognition, Streaming ASR, Low-latency ASR, Dual-mode ASR</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Streaming automatic speech recognition (ASR) aims to emit each hypothesized word as quickly and accurately as possible, while full-context ASR waits for the completion of a full speech utterance before emitting completed hypotheses. In this work, we propose a unified framework, Dual-mode ASR, to train a single end-to-end ASR model with shared weights for both streaming and full-context speech recognition. We show that the latency and accuracy of streaming ASR significantly benefit from weight sharing and joint training of full-context ASR, especially with inplace knowledge distillation during the training. The Dual-mode ASR framework can be applied to recent state-of-the-art convolution-based and transformer-based ASR networks. We present extensive experiments with two state-of-the-art ASR networks, ContextNet and Conformer, on two datasets, a widely used public dataset LibriSpeech and a large-scale dataset MultiDomain. Experiments and ablation studies demonstrate that Dual-mode ASR not only simplifies the workflow of training and deploying streaming and full-context ASR models, but also significantly improves both emission latency and recognition accuracy of streaming ASR. With Dual-mode ASR, we achieve new state-of-the-art streaming ASR results on both LibriSpeech and MultiDomain in terms of accuracy and latency.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Dual-mode ASR unifies and improves Streaming ASR with full-context modeling, simplifying the development and deployment workflow and improving both latency and accuracy.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="EQfpYwF3-b" data-number="2517">
      <h4>
        <a href="https://openreview.net/forum?id=EQfpYwF3-b">
            Deep Learning meets Projective Clustering
        </a>
      
        
          <a href="https://openreview.net/pdf?id=EQfpYwF3-b" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alaa_Maalouf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alaa_Maalouf1">Alaa Maalouf</a>, <a href="https://openreview.net/profile?id=~Harry_Lang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Harry_Lang1">Harry Lang</a>, <a href="https://openreview.net/profile?id=~Daniela_Rus1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniela_Rus1">Daniela Rus</a>, <a href="https://openreview.net/profile?id=~Dan_Feldman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Feldman1">Dan Feldman</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#EQfpYwF3-b-details-877" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EQfpYwF3-b-details-877"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Compressing Deep Networks, NLP, Matrix Factorization, SVD</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A common approach for compressing Natural Language Processing (NLP) networks is to encode the embedding layer as a matrix <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="228" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.41em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>n</mi><mo>×</mo><mi>d</mi></mrow></msup></math></mjx-assistive-mml></mjx-container>, compute its rank-<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="229" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>j</mi></math></mjx-assistive-mml></mjx-container> approximation <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="230" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>A</mi><mi>j</mi></msub></math></mjx-assistive-mml></mjx-container> via SVD (Singular Value Decomposition), and then factor <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="231" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>A</mi><mi>j</mi></msub></math></mjx-assistive-mml></mjx-container> into a pair of matrices that correspond to smaller fully-connected layers to replace the original embedding layer. Geometrically, the rows of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="232" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></mjx-assistive-mml></mjx-container> represent points in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="233" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.41em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>d</mi></msup></math></mjx-assistive-mml></mjx-container>, and the rows of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="234" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>A</mi><mi>j</mi></msub></math></mjx-assistive-mml></mjx-container> represent their projections onto the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="235" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>j</mi></math></mjx-assistive-mml></mjx-container>-dimensional subspace that minimizes the sum of squared distances (``errors'') to the points. 
      In practice, these rows of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="236" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></mjx-assistive-mml></mjx-container> may be spread around <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="237" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3E"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi><mo>&gt;</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> subspaces, so factoring <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="238" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></mjx-assistive-mml></mjx-container> based on a single subspace may lead to large errors that turn into large drops in accuracy.
      
      Inspired by \emph{projective clustering} from computational geometry,  we suggest replacing this subspace by a set of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="239" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container> subspaces, each of dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="240" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D457 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>j</mi></math></mjx-assistive-mml></mjx-container>, that minimizes the sum of squared distances over every point (row in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="241" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></mjx-assistive-mml></mjx-container>) to its \emph{closest} subspace. Based on this approach, we provide a novel architecture that replaces the original embedding layer by a set of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="242" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container> small layers that operate in parallel and are then recombined with a single fully-connected layer. 
      
      Extensive experimental results on the GLUE benchmark yield networks that are both more accurate and smaller compared to the standard matrix factorization (SVD). For example, we further compress DistilBERT by reducing the size of the embedding layer by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="243" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>40</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> while incurring only a <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="244" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.5</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> average drop in accuracy over all nine GLUE tasks, compared to a <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="245" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2.8</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> drop using the existing SVD approach.
      On RoBERTa we achieve <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="246" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>43</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> compression of the embedding layer with less than a <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="247" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.8</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> average drop in accuracy as compared to a <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="248" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> drop previously.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We suggest a novel technique for compressing a fully connected layer (or an embedding layer).</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=EQfpYwF3-b&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="QFYnKlBJYR" data-number="2259">
      <h4>
        <a href="https://openreview.net/forum?id=QFYnKlBJYR">
            Reinforcement Learning with Random Delays
        </a>
      
        
          <a href="https://openreview.net/pdf?id=QFYnKlBJYR" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yann_Bouteiller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yann_Bouteiller1">Yann Bouteiller</a>, <a href="https://openreview.net/profile?id=~Simon_Ramstedt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Ramstedt1">Simon Ramstedt</a>, <a href="https://openreview.net/profile?email=giovanni.beltrame%40polymtl.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="giovanni.beltrame@polymtl.ca">Giovanni Beltrame</a>, <a href="https://openreview.net/profile?id=~Christopher_Pal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Pal1">Christopher Pal</a>, <a href="https://openreview.net/profile?id=~Jonathan_Binas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Binas1">Jonathan Binas</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>22 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#QFYnKlBJYR-details-969" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QFYnKlBJYR-details-969"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement Learning, Deep Reinforcement Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Action and observation delays commonly occur in many Reinforcement Learning applications, such as remote control scenarios. We study the anatomy of randomly delayed environments, and show that partially resampling trajectory fragments in hindsight allows for off-policy multi-step value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor-Critic with significantly better performance in environments with delays. This is shown theoretically and also demonstrated practically on a delay-augmented version of the MuJoCo continuous control benchmark.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a framework for Reinforcement Learning with random action and observation delays.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xYGNO86OWDH" data-number="328">
      <h4>
        <a href="https://openreview.net/forum?id=xYGNO86OWDH">
            Isotropy in the Contextual Embedding Space: Clusters and Manifolds
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xYGNO86OWDH" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xingyu_Cai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xingyu_Cai1">Xingyu Cai</a>, <a href="https://openreview.net/profile?id=~Jiaji_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiaji_Huang1">Jiaji Huang</a>, <a href="https://openreview.net/profile?id=~Yuchen_Bian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuchen_Bian1">Yuchen Bian</a>, <a href="https://openreview.net/profile?id=~Kenneth_Church1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kenneth_Church1">Kenneth Church</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xYGNO86OWDH-details-547" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xYGNO86OWDH-details-547"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Contextual embedding space, Isotropy, Clusters, Manifolds</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The geometric properties of contextual embedding spaces for deep language models such as BERT and ERNIE, have attracted considerable attention in recent years. Investigations on the contextual embeddings demonstrate a strong anisotropic space such that most of the vectors fall within a narrow cone, leading to high cosine similarities.  It is surprising that these LMs are as successful as they are, given that most of their embedding vectors are as similar to one another as they are. In this paper, we argue that the isotropy indeed exists in the space, from a different but more constructive perspective. We identify isolated clusters and low dimensional manifolds in the contextual embedding space, and introduce tools to both qualitatively and quantitatively analyze them. We hope the study in this paper could provide insights towards a better understanding of the deep language models.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper reveals isotropy in the clustered contextual embedding space, and found low-dimensional manifolds in there.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=xYGNO86OWDH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="CF-ZIuSMXRz" data-number="1432">
      <h4>
        <a href="https://openreview.net/forum?id=CF-ZIuSMXRz">
            Spatio-Temporal Graph Scattering Transform
        </a>
      
        
          <a href="https://openreview.net/pdf?id=CF-ZIuSMXRz" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Chao_Pan2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chao_Pan2">Chao Pan</a>, <a href="https://openreview.net/profile?id=~Siheng_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siheng_Chen1">Siheng Chen</a>, <a href="https://openreview.net/profile?id=~Antonio_Ortega1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Antonio_Ortega1">Antonio Ortega</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#CF-ZIuSMXRz-details-944" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CF-ZIuSMXRz-details-944"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">scattering transform, spatio-temporal graph, graph neural networks, skeleton-based action recognition</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Although spatio-temporal graph neural networks have achieved great empirical success in handling multiple correlated time series, they may be impractical in some real-world scenarios due to a lack of sufficient high-quality training data. Furthermore, spatio-temporal graph neural networks lack theoretical interpretation. To address these issues, we put forth a novel mathematically designed framework to analyze spatio-temporal data. Our proposed spatio-temporal graph scattering transform (ST-GST) extends traditional scattering transform to the spatio-temporal domain. It performs iterative applications of spatio-temporal graph wavelets and  nonlinear activation functions, which can be viewed as a forward pass of spatio-temporal graph convolutional networks without training. Since all the filter coefficients in ST-GST are mathematically designed, it is promising for the real-world scenarios with limited training data, and also allows for a theoretical analysis, which shows that  the proposed ST-GST is stable to small perturbations of input signals and structures. Finally, our experiments show that i) ST-GST outperforms spatio-temporal graph convolutional networks by an increase of 35% in accuracy for MSR Action3D dataset; ii) it is  better and computationally more efficient to design the transform based on separable  spatio-temporal graphs than the joint ones; and iii) nonlinearity in ST-GST is critical to empirical performance.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We put forth a novel mathematically designed framework "ST-GST" to analyze spatio-temporal data.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=CF-ZIuSMXRz&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="3hGNqpI4WS" data-number="1529">
      <h4>
        <a href="https://openreview.net/forum?id=3hGNqpI4WS">
            Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=3hGNqpI4WS" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tatsuya_Matsushima1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tatsuya_Matsushima1">Tatsuya Matsushima</a>, <a href="https://openreview.net/profile?id=~Hiroki_Furuta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hiroki_Furuta1">Hiroki Furuta</a>, <a href="https://openreview.net/profile?id=~Yutaka_Matsuo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yutaka_Matsuo1">Yutaka Matsuo</a>, <a href="https://openreview.net/profile?id=~Ofir_Nachum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ofir_Nachum1">Ofir Nachum</a>, <a href="https://openreview.net/profile?id=~Shixiang_Gu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shixiang_Gu1">Shixiang Gu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#3hGNqpI4WS-details-989" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3hGNqpI4WS-details-989"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement Learning, deployment-efficiency, offline RL, Model-based RL</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Most reinforcement learning (RL) algorithms assume online access to the environment, in which one may readily interleave updates to the policy with experience collection using that policy. However, in many real-world applications such as health, education, dialogue agents, and robotics, the cost or potential risk of deploying a new data-collection policy is high, to the point that it can become prohibitive to update the data-collection policy more than a few times during learning. With this view, we propose a novel concept of deployment efficiency, measuring the number of distinct data-collection policies that are used during policy learning. We observe that naïvely applying existing model-free offline RL algorithms recursively does not lead to a practical deployment-efficient and sample-efficient algorithm. We propose a novel model-based algorithm, Behavior-Regularized Model-ENsemble (BREMEN), that not only performs better than or comparably as the state-of-the-art dynamic-programming-based and concurrently-proposed model-based offline approaches on existing benchmarks, but can also effectively optimize a policy offline using 10-20 times fewer data than prior works. Furthermore, the recursive application of BREMEN achieves impressive deployment efficiency while maintaining the same or better sample efficiency, learning successful policies from scratch on simulated robotic environments with only 5-10 deployments, compared to typical values of hundreds to millions in standard RL baselines.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a novel method that achieves both high sample-efficiency in offline RL and "deployment-efficiency" in online RL.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=3hGNqpI4WS&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="c_E8kFWfhp0" data-number="2601">
      <h4>
        <a href="https://openreview.net/forum?id=c_E8kFWfhp0">
            gradSim: Differentiable simulation for system identification and visuomotor control
        </a>
      
        
          <a href="https://openreview.net/pdf?id=c_E8kFWfhp0" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~J._Krishna_Murthy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~J._Krishna_Murthy1">J. Krishna Murthy</a>, <a href="https://openreview.net/profile?id=~Miles_Macklin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Miles_Macklin1">Miles Macklin</a>, <a href="https://openreview.net/profile?id=~Florian_Golemo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Golemo1">Florian Golemo</a>, <a href="https://openreview.net/profile?id=~Vikram_Voleti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vikram_Voleti1">Vikram Voleti</a>, <a href="https://openreview.net/profile?id=~Linda_Petrini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Linda_Petrini1">Linda Petrini</a>, <a href="https://openreview.net/profile?id=~Martin_Weiss4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Weiss4">Martin Weiss</a>, <a href="https://openreview.net/profile?id=~Breandan_Considine2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Breandan_Considine2">Breandan Considine</a>, <a href="https://openreview.net/profile?id=~J%C3%A9r%C3%B4me_Parent-L%C3%A9vesque2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jérôme_Parent-Lévesque2">Jérôme Parent-Lévesque</a>, <a href="https://openreview.net/profile?email=kevincxie%40cs.toronto.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="kevincxie@cs.toronto.edu">Kevin Xie</a>, <a href="https://openreview.net/profile?email=kenny%40di.ku.dk" class="profile-link" data-toggle="tooltip" data-placement="top" title="kenny@di.ku.dk">Kenny Erleben</a>, <a href="https://openreview.net/profile?id=~Liam_Paull1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liam_Paull1">Liam Paull</a>, <a href="https://openreview.net/profile?id=~Florian_Shkurti1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Shkurti1">Florian Shkurti</a>, <a href="https://openreview.net/profile?id=~Derek_Nowrouzezahrai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Derek_Nowrouzezahrai1">Derek Nowrouzezahrai</a>, <a href="https://openreview.net/profile?id=~Sanja_Fidler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanja_Fidler1">Sanja Fidler</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#c_E8kFWfhp0-details-56" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="c_E8kFWfhp0-details-56"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Differentiable simulation, System identification, Physical parameter estimation, 3D scene understanding, 3D vision, Differentiable rendering, Differentiable physics</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we tackle the problem of estimating object physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current best solutions to the problem require precise 3D labels which are labor intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. In this work we present gradSim, a framework that overcomes the dependence on 3D supervision by combining differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This unique combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Furthermore, our unified computation graph across dynamics and rendering engines enables the learning of challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to/better than techniques that require precise 3D labels.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Differentiable models of time-varying dynamics and image formation pipelines result in highly accurate physical parameter estimation from video</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=c_E8kFWfhp0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="4dXmpCDGNp7" data-number="1959">
      <h4>
        <a href="https://openreview.net/forum?id=4dXmpCDGNp7">
            Evaluations and Methods for Explanation through Robustness Analysis
        </a>
      
        
          <a href="https://openreview.net/pdf?id=4dXmpCDGNp7" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Cheng-Yu_Hsieh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cheng-Yu_Hsieh1">Cheng-Yu Hsieh</a>, <a href="https://openreview.net/profile?id=~Chih-Kuan_Yeh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chih-Kuan_Yeh1">Chih-Kuan Yeh</a>, <a href="https://openreview.net/profile?id=~Xuanqing_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuanqing_Liu1">Xuanqing Liu</a>, <a href="https://openreview.net/profile?id=~Pradeep_Kumar_Ravikumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pradeep_Kumar_Ravikumar1">Pradeep Kumar Ravikumar</a>, <a href="https://openreview.net/profile?id=~Seungyeon_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seungyeon_Kim1">Seungyeon Kim</a>, <a href="https://openreview.net/profile?id=~Sanjiv_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanjiv_Kumar1">Sanjiv Kumar</a>, <a href="https://openreview.net/profile?id=~Cho-Jui_Hsieh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cho-Jui_Hsieh1">Cho-Jui Hsieh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#4dXmpCDGNp7-details-749" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4dXmpCDGNp7-details-749"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Interpretability, Explanations, Adversarial Robustness</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Feature based explanations, that provide importance of each feature towards the model prediction, is arguably one of the most intuitive ways to explain a model. In this paper, we establish a novel set of evaluation criteria for such feature based explanations by robustness analysis. In contrast to existing evaluations which require us to specify some way to "remove" features that could inevitably introduces biases and artifacts, we make use of the subtler notion of smaller adversarial perturbations. By optimizing towards our proposed evaluation criteria, we obtain new explanations that are loosely necessary and sufficient for a prediction. We further extend the explanation to extract the set of features that would move the current prediction to a target class by adopting targeted adversarial attack for the robustness analysis. Through experiments across multiple domains and a user study, we validate the usefulness of our evaluation criteria and our derived explanations.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a suite of objective measurements for evaluating feature based explanations by the notion of robustness analysis; we further derive new explanation that captures different characteristics of explanation comparing to existing methods.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=4dXmpCDGNp7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tGZu6DlbreV" data-number="2964">
      <h4>
        <a href="https://openreview.net/forum?id=tGZu6DlbreV">
            RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tGZu6DlbreV" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Meng_Qu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Meng_Qu2">Meng Qu</a>, <a href="https://openreview.net/profile?email=chenjk17%40mails.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="chenjk17@mails.tsinghua.edu.cn">Junkun Chen</a>, <a href="https://openreview.net/profile?email=xhonneul%40mila.quebec" class="profile-link" data-toggle="tooltip" data-placement="top" title="xhonneul@mila.quebec">Louis-Pascal Xhonneux</a>, <a href="https://openreview.net/profile?id=~Yoshua_Bengio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoshua_Bengio1">Yoshua Bengio</a>, <a href="https://openreview.net/profile?id=~Jian_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jian_Tang1">Jian Tang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tGZu6DlbreV-details-620" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tGZu6DlbreV-details-620"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Knowledge Graph Reasoning, Logic Rules, EM</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper studies learning logic rules for reasoning on knowledge graphs. Logic rules provide interpretable explanations when used for prediction as well as being able to generalize to other tasks, and hence are critical to learn. Existing methods either suffer from the problem of searching in a large search space (e.g., neural logic programming) or ineffective optimization due to sparse rewards (e.g., techniques based on reinforcement learning). To address these limitations, this paper proposes a probabilistic model called RNNLogic. RNNLogic treats logic rules as a latent variable, and simultaneously trains a rule generator as well as a reasoning predictor with logic rules. We develop an EM-based algorithm for optimization. In each iteration, the reasoning predictor is updated to explore some generated logic rules for reasoning. Then in the E-step, we select a set of high-quality rules from all generated rules with both the rule generator and reasoning predictor via posterior inference; and in the M-step, the rule generator is updated with the rules selected in the E-step. Experiments on four datasets prove the effectiveness of RNNLogic.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Learn Logic Rules for Reasoning on Knowledge Graphs.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=tGZu6DlbreV&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xfmSoxdxFCG" data-number="2145">
      <h4>
        <a href="https://openreview.net/forum?id=xfmSoxdxFCG">
            Can a Fruit Fly Learn Word Embeddings?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xfmSoxdxFCG" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=liangy7%40rpi.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="liangy7@rpi.edu">Yuchen Liang</a>, <a href="https://openreview.net/profile?id=~Chaitanya_Ryali1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chaitanya_Ryali1">Chaitanya Ryali</a>, <a href="https://openreview.net/profile?email=benjamin.hoover%40ibm.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="benjamin.hoover@ibm.com">Benjamin Hoover</a>, <a href="https://openreview.net/profile?email=leopoldgrinberg%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="leopoldgrinberg@gmail.com">Leopold Grinberg</a>, <a href="https://openreview.net/profile?email=navlakha%40cshl.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="navlakha@cshl.edu">Saket Navlakha</a>, <a href="https://openreview.net/profile?id=~Mohammed_J_Zaki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammed_J_Zaki1">Mohammed J Zaki</a>, <a href="https://openreview.net/profile?id=~Dmitry_Krotov2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dmitry_Krotov2">Dmitry Krotov</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 13 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xfmSoxdxFCG-details-73" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xfmSoxdxFCG-details-73"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">neurobiology, neuroscience, fruit fly, locality sensitive hashing, word embedding, sparse representations</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The mushroom body of the fruit fly brain is one of the best studied systems in neuroscience. At its core it consists of a population of Kenyon cells, which receive inputs from multiple sensory modalities. These cells are inhibited by the anterior paired lateral neuron, thus creating a sparse high dimensional representation of the inputs. In this work we study a mathematical formalization of this network motif and apply it to learning the correlational structure between words and their context in a corpus of unstructured text, a common natural language processing (NLP) task. We show that this network can learn semantic representations of words and can generate both static and context-dependent word embeddings. Unlike conventional methods (e.g., BERT, GloVe) that use dense representations for word embedding, our algorithm encodes semantic meaning of words and their context in the form of sparse binary hash codes. The quality of the learned representations is evaluated on word similarity analysis, word-sense disambiguation, and document classification. It is shown that not only can the fruit fly network motif achieve performance comparable to existing methods in NLP, but, additionally, it uses only a fraction of the computational resources (shorter training time and smaller memory footprint). </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that a network motif from the fruit fly brain can learn word embeddings.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=xfmSoxdxFCG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="snOgiCYZgJ7" data-number="2126">
      <h4>
        <a href="https://openreview.net/forum?id=snOgiCYZgJ7">
            Neural representation and generation for RNA secondary structures
        </a>
      
        
          <a href="https://openreview.net/pdf?id=snOgiCYZgJ7" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zichao_Yan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zichao_Yan1">Zichao Yan</a>, <a href="https://openreview.net/profile?id=~William_L._Hamilton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_L._Hamilton1">William L. Hamilton</a>, <a href="https://openreview.net/profile?id=~Mathieu_Blanchette1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mathieu_Blanchette1">Mathieu Blanchette</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 04 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#snOgiCYZgJ7-details-305" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="snOgiCYZgJ7-details-305"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Graph neural network, Deep generative modeling, Machine learning, Drug discovery, RNA structure, RNA structure embedding, RNA-protein interaction prediction</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We investigate a new direction in computational drug discovery for designing large scale and complex macromolecular structures known as the RNAs.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=snOgiCYZgJ7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="eEn8KTtJOx" data-number="2806">
      <h4>
        <a href="https://openreview.net/forum?id=eEn8KTtJOx">
            WaNet - Imperceptible Warping-based Backdoor Attack
        </a>
      
        
          <a href="https://openreview.net/pdf?id=eEn8KTtJOx" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tuan_Anh_Nguyen4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tuan_Anh_Nguyen4">Tuan Anh Nguyen</a>, <a href="https://openreview.net/profile?id=~Anh_Tuan_Tran2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anh_Tuan_Tran2">Anh Tuan Tran</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 22 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#eEn8KTtJOx-details-569" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eEn8KTtJOx-details-569"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">backdoor attack, image warping, wanet</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">With the thriving of deep learning and the widespread practice of using pre-trained networks, backdoor attacks have become an increasing security threat drawing many research interests in recent years. A third-party model can be poisoned in training to work well in normal conditions but behave maliciously when a trigger pattern appears. However, the existing backdoor attacks are all built on noise perturbation triggers, making them noticeable to humans. In this paper, we instead propose using warping-based triggers. The proposed backdoor outperforms the previous methods in a human inspection test by a wide margin, proving its stealthiness. To make such models undetectable by machine defenders, we propose a novel training mode, called the ``noise mode. The trained networks successfully attack and bypass the state-ofthe art defense methods on standard classification datasets, including MNIST, CIFAR-10, GTSRB, and CelebA. Behavior analyses show that our backdoors are transparent to network inspection, further proving this novel attack mechanism's efficiency.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an imperceptible backdoor attack based on image-warping, which can surpass both human and machine inspections.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=eEn8KTtJOx&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="hJmtwocEqzc" data-number="2316">
      <h4>
        <a href="https://openreview.net/forum?id=hJmtwocEqzc">
            LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition
        </a>
      
        
          <a href="https://openreview.net/pdf?id=hJmtwocEqzc" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Valeriia_Cherepanova1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Valeriia_Cherepanova1">Valeriia Cherepanova</a>, <a href="https://openreview.net/profile?id=~Micah_Goldblum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Micah_Goldblum1">Micah Goldblum</a>, <a href="https://openreview.net/profile?email=m211926%40usna.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="m211926@usna.edu">Harrison Foley</a>, <a href="https://openreview.net/profile?email=sduan1%40umd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sduan1@umd.edu">Shiyuan Duan</a>, <a href="https://openreview.net/profile?id=~John_P_Dickerson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_P_Dickerson1">John P Dickerson</a>, <a href="https://openreview.net/profile?id=~Gavin_Taylor1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gavin_Taylor1">Gavin Taylor</a>, <a href="https://openreview.net/profile?id=~Tom_Goldstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tom_Goldstein1">Tom Goldstein</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 08 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#hJmtwocEqzc-details-226" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hJmtwocEqzc-details-226"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">facial recognition, adversarial attacks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Facial recognition systems are increasingly deployed by private corporations, government agencies, and contractors for consumer services and mass surveillance programs alike.  These systems are typically built by scraping social media profiles for user images.  Adversarial perturbations have been proposed for bypassing facial recognition systems.  However, existing methods fail on full-scale systems and commercial APIs.  We develop our own adversarial filter that accounts for the entire image processing pipeline and is demonstrably effective against industrial-grade pipelines that include face detection and large scale databases.  Additionally, we release an easy-to-use webtool that significantly degrades the accuracy of Amazon Rekognition and the Microsoft Azure Face Recognition API, reducing the accuracy of each to below 1%.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We leverage adversarial attacks in our tool, LowKey, which protects social media users from invasive mass surveillance systems.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=hJmtwocEqzc&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Hf3qXoiNkR" data-number="3718">
      <h4>
        <a href="https://openreview.net/forum?id=Hf3qXoiNkR">
            Learning from others' mistakes: Avoiding dataset biases without modeling them
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Hf3qXoiNkR" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Victor_Sanh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Victor_Sanh1">Victor Sanh</a>, <a href="https://openreview.net/profile?id=~Thomas_Wolf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Wolf1">Thomas Wolf</a>, <a href="https://openreview.net/profile?id=~Yonatan_Belinkov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yonatan_Belinkov1">Yonatan Belinkov</a>, <a href="https://openreview.net/profile?id=~Alexander_M_Rush1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_M_Rush1">Alexander M Rush</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Hf3qXoiNkR-details-921" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Hf3qXoiNkR-details-921"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">dataset bias, product of experts, natural language processing</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Reducing a model's reliance on dataset biases by encouraging a robust model to learn from a weak learner's mistakes.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="KmykpuSrjcq" data-number="3143">
      <h4>
        <a href="https://openreview.net/forum?id=KmykpuSrjcq">
            Prototypical Contrastive Learning of Unsupervised Representations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=KmykpuSrjcq" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Junnan_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junnan_Li2">Junnan Li</a>, <a href="https://openreview.net/profile?id=~Pan_Zhou3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pan_Zhou3">Pan Zhou</a>, <a href="https://openreview.net/profile?id=~Caiming_Xiong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Caiming_Xiong1">Caiming Xiong</a>, <a href="https://openreview.net/profile?id=~Steven_Hoi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_Hoi2">Steven Hoi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#KmykpuSrjcq-details-535" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KmykpuSrjcq-details-535"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">self-supervised learning, unsupervised learning, representation learning, contrastive learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an unsupervised representation learning method that bridges contrastive learning with clustering in an EM framework.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=KmykpuSrjcq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Z4R1vxLbRLO" data-number="2552">
      <h4>
        <a href="https://openreview.net/forum?id=Z4R1vxLbRLO">
            Extreme Memorization via Scale of Initialization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Z4R1vxLbRLO" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Harsh_Mehta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Harsh_Mehta1">Harsh Mehta</a>, <a href="https://openreview.net/profile?id=~Ashok_Cutkosky1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ashok_Cutkosky1">Ashok Cutkosky</a>, <a href="https://openreview.net/profile?id=~Behnam_Neyshabur1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Behnam_Neyshabur1">Behnam Neyshabur</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Z4R1vxLbRLO-details-773" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Z4R1vxLbRLO-details-773"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Scale of initialization, Memorization, Overfitting, Generalization, Generalization Measure, Understanding Deep Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We construct an experimental setup in which changing the scale of initialization strongly impacts the implicit regularization induced by SGD, interpolating from good generalization performance to completely memorizing the training set while making little progress on the test set. Moreover, we find that the extent and manner in which generalization ability is affected depends on the activation and loss function used, with sin activation being the most extreme. In the case of the homogeneous ReLU activation, we show that this behavior can be attributed to the loss function. Our empirical investigation reveals that increasing the scale of initialization correlates with misalignment of representations and gradients across examples in the same class. This insight allows us to device an alignment measure over gradients and representations which can capture this phenomenon. We demonstrate that our alignment measure correlates with generalization of deep models trained on image classification tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Through studying the effect of scale of initialization on generalization, we come up with an alignment measure that correlations with generalization of deep models.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Z4R1vxLbRLO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="IDFQI9OY6K" data-number="193">
      <h4>
        <a href="https://openreview.net/forum?id=IDFQI9OY6K">
            Interactive Weak Supervision: Learning Useful Heuristics for Data Labeling
        </a>
      
        
          <a href="https://openreview.net/pdf?id=IDFQI9OY6K" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Benedikt_Boecking1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benedikt_Boecking1">Benedikt Boecking</a>, <a href="https://openreview.net/profile?id=~Willie_Neiswanger2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Willie_Neiswanger2">Willie Neiswanger</a>, <a href="https://openreview.net/profile?id=~Eric_Xing1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eric_Xing1">Eric Xing</a>, <a href="https://openreview.net/profile?id=~Artur_Dubrawski2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Artur_Dubrawski2">Artur Dubrawski</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#IDFQI9OY6K-details-588" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IDFQI9OY6K-details-588"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">weak supervision, data programming, data labeling, active learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Obtaining large annotated datasets is critical for training successful machine learning models and it is often a bottleneck in practice. Weak supervision offers a promising alternative for producing labeled datasets without ground truth annotations by generating probabilistic labels using multiple noisy heuristics. This process can scale to large datasets and has demonstrated state of the art performance in diverse domains such as healthcare and e-commerce. One practical issue with learning from user-generated heuristics is that their creation requires creativity, foresight, and domain expertise from those who hand-craft them, a process which can be tedious and subjective. We develop the first framework for interactive weak supervision in which a method proposes heuristics and learns from user feedback given on each proposed heuristic. Our experiments demonstrate that only a small number of feedback iterations are needed to train models that achieve highly competitive test set performance without access to ground truth training labels. We conduct user studies, which show that users are able to effectively provide feedback on heuristics and that test set results track the performance of simulated oracles.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a framework and method for training classifiers on datasets without ground truth annotation by interacting  with domain experts to discover good weak supervision sources. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="8xLkv08d70T" data-number="2855">
      <h4>
        <a href="https://openreview.net/forum?id=8xLkv08d70T">
            Adaptive Procedural Task Generation for Hard-Exploration Problems
        </a>
      
        
          <a href="https://openreview.net/pdf?id=8xLkv08d70T" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kuan_Fang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kuan_Fang3">Kuan Fang</a>, <a href="https://openreview.net/profile?id=~Yuke_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuke_Zhu1">Yuke Zhu</a>, <a href="https://openreview.net/profile?id=~Silvio_Savarese1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Silvio_Savarese1">Silvio Savarese</a>, <a href="https://openreview.net/profile?id=~Fei-Fei_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fei-Fei_Li1">Fei-Fei Li</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>22 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#8xLkv08d70T-details-15" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8xLkv08d70T-details-15"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, curriculum learning, procedural generation, task generation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We introduce Adaptive Procedural Task Generation (APT-Gen), an approach to progressively generate a sequence of tasks as curricula to facilitate reinforcement learning in hard-exploration problems. At the heart of our approach, a task generator learns to create tasks from a parameterized task space via a black-box procedural generation module. To enable curriculum learning in the absence of a direct indicator of learning progress, we propose to train the task generator by balancing the agent's performance in the generated tasks and the similarity to the target tasks. Through adversarial training, the task similarity is adaptively estimated by a task discriminator defined on the agent's experiences, allowing the generated tasks to approximate target tasks of unknown parameterization or outside of the predefined task space. Our experiments on the grid world and robotic manipulation task domains show that APT-Gen achieves substantially better performance than various existing baselines by generating suitable tasks of rich variations.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a framework which creates tasks as curricula via procedural generation to expedite reinforcement learning in hard-exploration problems.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9ITXiTrAoT" data-number="2792">
      <h4>
        <a href="https://openreview.net/forum?id=9ITXiTrAoT">
            Multi-timescale Representation Learning in LSTM Language Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9ITXiTrAoT" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=shivangi%40utexas.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="shivangi@utexas.edu">Shivangi Mahto</a>, <a href="https://openreview.net/profile?email=vy.vo%40intel.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="vy.vo@intel.com">Vy Ai Vo</a>, <a href="https://openreview.net/profile?id=~Javier_S._Turek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Javier_S._Turek1">Javier S. Turek</a>, <a href="https://openreview.net/profile?id=~Alexander_Huth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_Huth1">Alexander Huth</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9ITXiTrAoT-details-924" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9ITXiTrAoT-details-924"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Language Model, LSTM, timescales</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Language models must capture statistical dependencies between words at timescales ranging from very short to very long. Earlier work has demonstrated that dependencies in natural language tend to decay with distance between words according to a power law. However, it is unclear how this knowledge can be used for analyzing or designing neural network language models. In this work, we derived a theory for how the memory gating mechanism in long short-term memory (LSTM) language models can capture power law decay. We found that unit timescales within an LSTM, which are determined by the forget gate bias, should follow an Inverse Gamma distribution. Experiments then showed that LSTM language models trained on natural English text learn to approximate this theoretical distribution. Further, we found that explicitly imposing the theoretical distribution upon the model during training yielded better language model perplexity overall, with particular improvements for predicting low-frequency (rare) words. Moreover, the explicit multi-timescale model selectively routes information about different types of words through units with different timescales, potentially improving model interpretability. These results demonstrate the importance of careful, theoretically-motivated analysis of memory and timescale in language models.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This work presents a theoretically-motivated analysis of memory and timescale in LSTM language models.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="KpfasTaLUpq" data-number="579">
      <h4>
        <a href="https://openreview.net/forum?id=KpfasTaLUpq">
            Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=KpfasTaLUpq" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jungo_Kasai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jungo_Kasai1">Jungo Kasai</a>, <a href="https://openreview.net/profile?id=~Nikolaos_Pappas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nikolaos_Pappas1">Nikolaos Pappas</a>, <a href="https://openreview.net/profile?id=~Hao_Peng4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Peng4">Hao Peng</a>, <a href="https://openreview.net/profile?id=~James_Cross3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_Cross3">James Cross</a>, <a href="https://openreview.net/profile?id=~Noah_Smith1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noah_Smith1">Noah Smith</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#KpfasTaLUpq-details-338" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KpfasTaLUpq-details-338"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Machine Translation, Sequence Modeling, Natural Language Processing</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Much recent effort has been invested in non-autoregressive neural machine translation, which appears to be an efficient alternative to state-of-the-art autoregressive machine translation on modern GPUs.  In contrast to the latter, where generation is sequential, the former allows generation to be parallelized across target token positions. Some of the latest non-autoregressive models have achieved impressive translation quality-speed tradeoffs compared to autoregressive baselines. In this work, we reexamine this tradeoff and argue that autoregressive baselines can be substantially sped up without loss in accuracy. Specifically, we study autoregressive models with encoders and decoders of varied depths. Our extensive experiments show that given a sufficiently deep encoder, a single-layer autoregressive decoder can substantially outperform strong non-autoregressive models with comparable inference speed. We show that the speed disadvantage for autoregressive baselines compared to non-autoregressive methods has been overestimated in three aspects: suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation. Our results establish a new protocol for future research toward fast, accurate machine translation. Our code is available at https://github.com/jungokasai/deep-shallow.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that the speed disadvantage for autoregressive baselines to evaluate non-autoregressive machine translation is overestimated in three aspects: suboptimal layer allocation, insufficient speed measurement, and lack of knowledge distillation.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="5NsEIflpbSv" data-number="2910">
      <h4>
        <a href="https://openreview.net/forum?id=5NsEIflpbSv">
            Learning Better Structured Representations Using Low-rank Adaptive Label Smoothing
        </a>
      
        
          <a href="https://openreview.net/pdf?id=5NsEIflpbSv" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Asish_Ghoshal2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Asish_Ghoshal2">Asish Ghoshal</a>, <a href="https://openreview.net/profile?id=~Xilun_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xilun_Chen1">Xilun Chen</a>, <a href="https://openreview.net/profile?email=sonalgupta%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sonalgupta@fb.com">Sonal Gupta</a>, <a href="https://openreview.net/profile?id=~Luke_Zettlemoyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Luke_Zettlemoyer1">Luke Zettlemoyer</a>, <a href="https://openreview.net/profile?id=~Yashar_Mehdad2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yashar_Mehdad2">Yashar Mehdad</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 24 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#5NsEIflpbSv-details-943" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5NsEIflpbSv-details-943"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">label smoothing, calibration, semantic parsing, structured prediction</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Training with soft targets instead of hard targets has been shown to improve performance and calibration of deep neural networks. Label smoothing is a popular way of computing soft targets, where one-hot encoding of a class is smoothed with a uniform distribution. Owing to its simplicity, label smoothing has found wide-spread use for training deep neural networks on a wide variety of tasks, ranging from image and text classification to machine translation and semantic parsing. Complementing recent empirical justification for label smoothing, we obtain PAC-Bayesian generalization bounds for label smoothing and show that the generalization error depends on the choice of the noise (smoothing) distribution. Then we propose low-rank adaptive label smoothing (LORAS): a simple yet novel method for training with learned soft targets that generalizes label smoothing and adapts to the latent structure of the label space in structured prediction tasks. Specifically, we evaluate our method on semantic parsing tasks and show that training with appropriately smoothed soft targets can significantly improve accuracy and model calibration, especially in low-resource settings. Used in conjunction with pre-trained sequence-to-sequence models, our method achieves state of the art performance on four semantic parsing data sets. LORAS can be used with any model, improves performance and implicit model calibration  without increasing the number of model parameters, and can be scaled to problems with large label spaces containing tens of thousands of labels.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an extension of label smoothing which improves generalization performance by adapting to the structure present in label space of structured prediction tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="mNtmhaDkAr" data-number="3826">
      <h4>
        <a href="https://openreview.net/forum?id=mNtmhaDkAr">
            Predicting Inductive Biases of Pre-Trained Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=mNtmhaDkAr" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Charles_Lovering1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Charles_Lovering1">Charles Lovering</a>, <a href="https://openreview.net/profile?email=rohan_jha%40brown.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="rohan_jha@brown.edu">Rohan Jha</a>, <a href="https://openreview.net/profile?id=~Tal_Linzen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tal_Linzen1">Tal Linzen</a>, <a href="https://openreview.net/profile?id=~Ellie_Pavlick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ellie_Pavlick1">Ellie Pavlick</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#mNtmhaDkAr-details-380" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mNtmhaDkAr-details-380"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">information-theoretical probing, probing, challenge sets, natural language processing</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Most current NLP systems are based on a pre-train-then-fine-tune paradigm, in which a large neural network is first trained in a self-supervised way designed to encourage the network to extract broadly-useful linguistic features, and then fine-tuned for a specific task of interest. Recent work attempts to understand why this recipe works and explain when it fails. Currently, such analyses have produced two sets of apparently-contradictory results. Work that analyzes the representations that result from pre-training (via "probing classifiers") finds evidence that rich features of linguistic structure can be decoded with high accuracy, but work that analyzes model behavior after fine-tuning (via "challenge sets") indicates that decisions are often not based on such structure but rather on spurious heuristics specific to the training set. In this work, we test the hypothesis that the extent to which a feature influences a model's decisions can be predicted using a combination of two factors: The feature's "extractability" after pre-training (measured using information-theoretic probing techniques), and the "evidence" available during fine-tuning (defined as the feature's co-occurrence rate with the label). In experiments with both synthetic and natural language data, we find strong evidence (statistically significant correlations) supporting this hypothesis.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We find that feature extractability, measured by probing classifiers, can be viewed as an inductive bias: the more extractable a feature is after pre-training, the less statistical evidence needed during fine-tuning for the model to use the feature.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="jHefDGsorp5" data-number="2246">
      <h4>
        <a href="https://openreview.net/forum?id=jHefDGsorp5">
            Molecule Optimization by Explainable Evolution
        </a>
      
        
          <a href="https://openreview.net/pdf?id=jHefDGsorp5" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Binghong_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Binghong_Chen1">Binghong Chen</a>, <a href="https://openreview.net/profile?id=~Tianzhe_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianzhe_Wang1">Tianzhe Wang</a>, <a href="https://openreview.net/profile?id=~Chengtao_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chengtao_Li1">Chengtao Li</a>, <a href="https://openreview.net/profile?id=~Hanjun_Dai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hanjun_Dai1">Hanjun Dai</a>, <a href="https://openreview.net/profile?id=~Le_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Le_Song1">Le Song</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#jHefDGsorp5-details-220" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="jHefDGsorp5-details-220"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Molecule Design, Explainable Model, Evolutionary Algorithm, Reinforcement Learning, Graph Generative Model</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Optimizing molecules for desired properties is a fundamental yet challenging task in chemistry, material science, and drug discovery. This paper develops a novel algorithm for optimizing molecular properties via an Expectation-Maximization (EM) like explainable evolutionary process. The algorithm is designed to mimic human experts in the process of searching for desirable molecules and alternate between two stages: the first stage on explainable local search which identifies rationales, i.e., critical subgraph patterns accounting for desired molecular properties, and the second stage on molecule completion which explores the larger space of molecules containing good rationales. We test our approach against various baselines on a real-world multi-property optimization task where each method is given the same number of queries to the property oracle. We show that our evolution-by-explanation algorithm is 79% better than the best baseline in terms of a generic metric combining aspects such as success rate, novelty, and diversity. Human expert evaluation on optimized molecules shows that 60% of top molecules obtained from our methods are deemed successful. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a novel EM-like evolution-by-explanation algorithm alternating between an explainable graph model and a conditional generative model for molecule optimization.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Vd7lCMvtLqg" data-number="1353">
      <h4>
        <a href="https://openreview.net/forum?id=Vd7lCMvtLqg">
            Anchor &amp; Transform: Learning Sparse Embeddings for Large Vocabularies
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Vd7lCMvtLqg" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Paul_Pu_Liang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Pu_Liang1">Paul Pu Liang</a>, <a href="https://openreview.net/profile?id=~Manzil_Zaheer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manzil_Zaheer1">Manzil Zaheer</a>, <a href="https://openreview.net/profile?id=~Yuan_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuan_Wang1">Yuan Wang</a>, <a href="https://openreview.net/profile?id=~Amr_Ahmed1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amr_Ahmed1">Amr Ahmed</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 11 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Vd7lCMvtLqg-details-384" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Vd7lCMvtLqg-details-384"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">sparse embeddings, large vocabularies, text classification, language modeling, recommendation systems</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Learning continuous representations of discrete objects such as text, users, movies, and URLs lies at the heart of many applications including language and user modeling. When using discrete objects as input to neural networks, we often ignore the underlying structures (e.g., natural groupings and similarities) and embed the objects independently into individual vectors. As a result, existing methods do not scale to large vocabulary sizes. In this paper, we design a simple and efficient embedding algorithm that learns a small set of anchor embeddings and a sparse transformation matrix. We call our method Anchor &amp; Transform (ANT) as the embeddings of discrete objects are a sparse linear combination of the anchors, weighted according to the transformation matrix. ANT is scalable, flexible, and end-to-end trainable. We further provide a statistical interpretation of our algorithm as a Bayesian nonparametric prior for embeddings that encourages sparsity and leverages natural groupings among objects. By deriving an approximate inference algorithm based on Small Variance Asymptotics, we obtain a natural extension that automatically learns the optimal number of anchors instead of having to tune it as a hyperparameter. On text classification, language modeling, and movie recommendation benchmarks, we show that ANT is particularly suitable for large vocabulary sizes and demonstrates stronger performance with fewer parameters (up to 40x compression) as compared to existing compression baselines.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">End-to-end learning of sparse embeddings for large vocabularies with a Bayesian nonparametric interpretation that results in up to 40x smaller embedding tables.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Vd7lCMvtLqg&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="O3bqkf_Puys" data-number="75">
      <h4>
        <a href="https://openreview.net/forum?id=O3bqkf_Puys">
            PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences
        </a>
      
        
          <a href="https://openreview.net/pdf?id=O3bqkf_Puys" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hehe_Fan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hehe_Fan1">Hehe Fan</a>, <a href="https://openreview.net/profile?id=~Xin_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xin_Yu1">Xin Yu</a>, <a href="https://openreview.net/profile?id=~Yuhang_Ding1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuhang_Ding1">Yuhang Ding</a>, <a href="https://openreview.net/profile?id=~Yi_Yang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Yang4">Yi Yang</a>, <a href="https://openreview.net/profile?id=~Mohan_Kankanhalli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohan_Kankanhalli1">Mohan Kankanhalli</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 30 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#O3bqkf_Puys-details-101" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="O3bqkf_Puys-details-101"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Point cloud, spatio-temporal modeling, video analysis, action recognition, semantic segmentation, convolutional neural network</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Point cloud sequences are irregular and unordered in the spatial dimension while exhibiting regularities and order in the temporal dimension. Therefore, existing grid based convolutions for conventional video processing cannot be directly applied to spatio-temporal modeling of raw point cloud sequences. In this paper, we propose a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequences.  Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution is used to model the dynamics of the spatial regions along the time dimension.  Furthermore, we incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequences in a hierarchical manner.  Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequences.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper proposes a point spatio-temporal (PST) convolution, which decomposes space and time, to learn representations of raw point cloud sequences in a spatio-temporally hierarchical manner.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="e8W-hsu_q5" data-number="1535">
      <h4>
        <a href="https://openreview.net/forum?id=e8W-hsu_q5">
            Group Equivariant Conditional Neural Processes
        </a>
      
        
          <a href="https://openreview.net/pdf?id=e8W-hsu_q5" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Makoto_Kawano1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Makoto_Kawano1">Makoto Kawano</a>, <a href="https://openreview.net/profile?id=~Wataru_Kumagai2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wataru_Kumagai2">Wataru Kumagai</a>, <a href="https://openreview.net/profile?id=~Akiyoshi_Sannai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Akiyoshi_Sannai1">Akiyoshi Sannai</a>, <a href="https://openreview.net/profile?id=~Yusuke_Iwasawa1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yusuke_Iwasawa1">Yusuke Iwasawa</a>, <a href="https://openreview.net/profile?id=~Yutaka_Matsuo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yutaka_Matsuo1">Yutaka Matsuo</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#e8W-hsu_q5-details-790" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="e8W-hsu_q5-details-790"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural Processes, Conditional Neural Processes, Stochastic Processes, Regression, Group Equivariance, Symmetry</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present the group equivariant conditional neural process (EquivCNP), a meta-learning method with permutation invariance in a data set as in conventional conditional neural processes (CNPs), and it also has transformation equivariance in data space. Incorporating group equivariance, such as rotation and scaling equivariance, provides a way to consider the symmetry of real-world data. We give a decomposition theorem for permutation-invariant and group-equivariant maps, which leads us to construct EquivCNPs with an infinite-dimensional latent space to handle group symmetries. In this paper, we build architecture using Lie group convolutional layers for practical implementation. We show that EquivCNP with translation equivariance achieves comparable performance to conventional CNPs in a 1D regression task. Moreover, we demonstrate that incorporating an appropriate Lie group equivariance, EquivCNP is capable of zero-shot generalization for an image-completion task by selecting an appropriate Lie group equivariance.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A model for regression that learns conditional distributions of a stochastic process, by incorporating group equivariance into Conditional Neural Processes.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=e8W-hsu_q5&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="S724o4_WB3" data-number="755">
      <h4>
        <a href="https://openreview.net/forum?id=S724o4_WB3">
            When does preconditioning help or hurt generalization?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=S724o4_WB3" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Shun-ichi_Amari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shun-ichi_Amari1">Shun-ichi Amari</a>, <a href="https://openreview.net/profile?id=~Jimmy_Ba1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jimmy_Ba1">Jimmy Ba</a>, <a href="https://openreview.net/profile?id=~Roger_Baker_Grosse1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roger_Baker_Grosse1">Roger Baker Grosse</a>, <a href="https://openreview.net/profile?id=~Xuechen_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuechen_Li1">Xuechen Li</a>, <a href="https://openreview.net/profile?id=~Atsushi_Nitanda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Atsushi_Nitanda1">Atsushi Nitanda</a>, <a href="https://openreview.net/profile?id=~Taiji_Suzuki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Taiji_Suzuki1">Taiji Suzuki</a>, <a href="https://openreview.net/profile?id=~Denny_Wu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Denny_Wu2">Denny Wu</a>, <a href="https://openreview.net/profile?id=~Ji_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ji_Xu1">Ji Xu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 13 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#S724o4_WB3-details-606" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="S724o4_WB3-details-606"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">generalization, second-order optimization, natural gradient descent, high-dimensional asymptotics</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While second order optimizers such as natural gradient descent (NGD) often speed up optimization, their effect on generalization has been called into question. This work presents a more nuanced view on how the \textit{implicit bias} of optimizers affects the comparison of generalization properties. 
      We provide an exact asymptotic bias-variance decomposition of the generalization error of preconditioned ridgeless regression in the overparameterized regime, and consider the inverse population Fisher information matrix (used in NGD) as a particular example. We determine the optimal preconditioner <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="249" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-b mjx-i"><mjx-c class="mjx-c1D477 TEX-BI"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold-italic">P</mi></math></mjx-assistive-mml></mjx-container> for both the bias and variance, and find that the relative generalization performance of different optimizers depends on label noise and ``shape'' of the signal (true parameters): when the labels are noisy, the model is misspecified, or the signal is misaligned with the features, NGD can achieve lower risk; conversely, GD generalizes better under clean labels, a well-specified model, or aligned signal. 
      Based on this analysis, we discuss several approaches to manage the bias-variance tradeoff, and the potential benefit of interpolating between first- and second-order updates. We then extend our analysis to regression in the reproducing kernel Hilbert space and demonstrate that preconditioning can lead to more efficient decrease in the population risk. Lastly, we empirically compare the generalization error of first- and second-order optimizers in neural network experiments, and observe robust trends matching our theoretical analysis. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Characterized the generalization error of preconditioned least squares regression in the overparameterized regime and determined the optimal preconditioner.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="hPWj1qduVw8" data-number="2558">
      <h4>
        <a href="https://openreview.net/forum?id=hPWj1qduVw8">
            Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues
        </a>
      
        
          <a href="https://openreview.net/pdf?id=hPWj1qduVw8" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hung_Le2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hung_Le2">Hung Le</a>, <a href="https://openreview.net/profile?id=~Nancy_F._Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nancy_F._Chen1">Nancy F. Chen</a>, <a href="https://openreview.net/profile?id=~Steven_Hoi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_Hoi2">Steven Hoi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#hPWj1qduVw8-details-690" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hPWj1qduVw8-details-690"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">video-grounded dialogues, reasoning paths, semantic graphs</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce PDC, a novel approach to learn reasoning paths over semantic graphs which are built upon dialogue context at each turn, for video-grounded dialogues. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="aCgLmfhIy_f" data-number="1588">
      <h4>
        <a href="https://openreview.net/forum?id=aCgLmfhIy_f">
            Prototypical Representation Learning for Relation Extraction
        </a>
      
        
          <a href="https://openreview.net/pdf?id=aCgLmfhIy_f" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ning_Ding5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ning_Ding5">Ning Ding</a>, <a href="https://openreview.net/profile?id=~Xiaobin_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaobin_Wang1">Xiaobin Wang</a>, <a href="https://openreview.net/profile?id=~Yao_Fu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yao_Fu3">Yao Fu</a>, <a href="https://openreview.net/profile?id=~Guangwei_Xu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guangwei_Xu2">Guangwei Xu</a>, <a href="https://openreview.net/profile?id=~Rui_Wang16" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rui_Wang16">Rui Wang</a>, <a href="https://openreview.net/profile?id=~Pengjun_Xie2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pengjun_Xie2">Pengjun Xie</a>, <a href="https://openreview.net/profile?id=~Ying_Shen3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ying_Shen3">Ying Shen</a>, <a href="https://openreview.net/profile?id=~Fei_Huang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fei_Huang2">Fei Huang</a>, <a href="https://openreview.net/profile?id=~Hai-Tao_Zheng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hai-Tao_Zheng2">Hai-Tao Zheng</a>, <a href="https://openreview.net/profile?id=~Rui_Zhang11" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rui_Zhang11">Rui Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#aCgLmfhIy_f-details-283" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="aCgLmfhIy_f-details-283"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">NLP, Relation Extraction, Representation Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recognizing relations between entities is a pivotal task of relational learning.  
      Learning relation representations from distantly-labeled datasets is difficult because of the abundant label noise and complicated expressions in human language.  
      This paper aims to learn predictive, interpretable, and robust relation representations from distantly-labeled data that are effective in different settings, including supervised, distantly supervised, and few-shot learning. 
      Instead of solely relying on the supervision from noisy labels, we propose to learn prototypes for each relation from contextual information to best explore the intrinsic semantics of relations. 
      Prototypes are representations in the feature space abstracting the essential semantics of relations between entities in sentences.
      We learn prototypes based on objectives with clear geometric interpretation, where the prototypes are unit vectors uniformly dispersed in a unit ball, and statement embeddings are centered at the end of their corresponding prototype vectors on the surface of the ball. 
      This approach allows us to learn meaningful, interpretable prototypes for the final classification.
      Results on several relation learning tasks show that our model significantly outperforms the previous state-of-the-art models.
      We further demonstrate the robustness of the encoder and the interpretability of prototypes with extensive experiments.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="H6ATjJ0TKdf" data-number="2494">
      <h4>
        <a href="https://openreview.net/forum?id=H6ATjJ0TKdf">
            Layer-adaptive Sparsity for the Magnitude-based Pruning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=H6ATjJ0TKdf" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jaeho_Lee3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaeho_Lee3">Jaeho Lee</a>, <a href="https://openreview.net/profile?id=~Sejun_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sejun_Park1">Sejun Park</a>, <a href="https://openreview.net/profile?id=~Sangwoo_Mo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sangwoo_Mo1">Sangwoo Mo</a>, <a href="https://openreview.net/profile?id=~Sungsoo_Ahn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sungsoo_Ahn1">Sungsoo Ahn</a>, <a href="https://openreview.net/profile?id=~Jinwoo_Shin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinwoo_Shin1">Jinwoo Shin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#H6ATjJ0TKdf-details-863" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="H6ATjJ0TKdf-details-863"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">network pruning, layerwise sparsity, magnitude-based pruning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="250" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.
      Under various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.
      Furthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=H6ATjJ0TKdf&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Zbc-ue9p_rE" data-number="805">
      <h4>
        <a href="https://openreview.net/forum?id=Zbc-ue9p_rE">
            Refining Deep Generative Models via Discriminator Gradient Flow
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Zbc-ue9p_rE" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Abdul_Fatir_Ansari2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abdul_Fatir_Ansari2">Abdul Fatir Ansari</a>, <a href="https://openreview.net/profile?id=~Ming_Liang_Ang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ming_Liang_Ang4">Ming Liang Ang</a>, <a href="https://openreview.net/profile?id=~Harold_Soh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Harold_Soh1">Harold Soh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Zbc-ue9p_rE-details-980" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Zbc-ue9p_rE-details-980"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">gradient flows, generative models, GAN, VAE, Normalizing Flow</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep generative modeling has seen impressive advances in recent years, to the point where it is now commonplace to see simulated samples (e.g., images) that closely resemble real-world data. However, generation quality is generally inconsistent for any given model and can vary dramatically between samples. We introduce Discriminator Gradient <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="251" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></mjx-assistive-mml></mjx-container>low (DG<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="252" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></mjx-assistive-mml></mjx-container>low), a new technique that improves generated samples via the gradient flow of entropy-regularized <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="253" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></mjx-assistive-mml></mjx-container>-divergences between the real and the generated data distributions. The gradient flow takes the form of a non-linear Fokker-Plank equation, which can be easily simulated by sampling from the equivalent McKean-Vlasov process. By refining inferior samples, our technique avoids wasteful sample rejection used by previous methods (DRS &amp; MH-GAN). Compared to existing works that focus on specific GAN variants, we show our refinement approach can be applied to GANs with vector-valued critics and even other deep generative models such as VAEs and Normalizing Flows. Empirical results on multiple synthetic, image, and text datasets demonstrate that DG<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="254" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></mjx-assistive-mml></mjx-container>low leads to significant improvement in the quality of generated samples for a variety of generative models, outperforming the state-of-the-art Discriminator Optimal Transport (DOT) and Discriminator Driven Latent Sampling (DDLS) methods.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A method of refining samples from deep generative models using the discriminator gradient flow of f-divergences.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Zbc-ue9p_rE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="HHiiQKWsOcV" data-number="2791">
      <h4>
        <a href="https://openreview.net/forum?id=HHiiQKWsOcV">
            Explaining the Efficacy of Counterfactually Augmented Data
        </a>
      
        
          <a href="https://openreview.net/pdf?id=HHiiQKWsOcV" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Divyansh_Kaushik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Divyansh_Kaushik1">Divyansh Kaushik</a>, <a href="https://openreview.net/profile?id=~Amrith_Setlur1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amrith_Setlur1">Amrith Setlur</a>, <a href="https://openreview.net/profile?id=~Eduard_H_Hovy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eduard_H_Hovy1">Eduard H Hovy</a>, <a href="https://openreview.net/profile?id=~Zachary_Chase_Lipton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zachary_Chase_Lipton1">Zachary Chase Lipton</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 24 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#HHiiQKWsOcV-details-513" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HHiiQKWsOcV-details-513"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">humans in the loop, annotation artifacts, text classification, sentiment analysis, natural language inference</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In attempts to produce machine learning models less reliant on spurious patterns in NLP datasets, researchers have recently proposed curating counterfactually augmented data (CAD) via a human-in-the-loop process in which given some documents and their (initial) labels, humans must revise the text to make a counterfactual label applicable. Importantly, edits that are not necessary to flip the applicable label are prohibited. Models trained on the augmented (original and revised) data appear, empirically, to rely less on semantically irrelevant words and to generalize better out of domain. While this work draws loosely on causal thinking, the underlying causal model (even at an abstract level) and the principles underlying the observed out-of-domain improvements remain unclear. In this paper, we introduce a toy analog based on linear Gaussian models, observing interesting relationships between causal models, measurement noise, out-of-domain generalization, and reliance on spurious signals. Our analysis provides some insights that help to explain the efficacy of CAD. Moreover, we develop the hypothesis that while adding noise to causal features should degrade both in-domain and out-of-domain performance, adding noise to non-causal features should lead to relative improvements in out-of-domain performance. This idea inspires a speculative test for determining whether a feature attribution technique has identified the causal spans. If adding noise (e.g., by random word flips) to the highlighted spans degrades both in-domain and out-of-domain performance on a battery of challenge datasets, but adding noise to the complement gives improvements out-of-domain, this suggests we have identified causal spans. Thus, we present a large scale empirical study comparing spans edited to create CAD to those selected by attention and saliency maps. Across numerous challenge domains and models, we find that the hypothesized phenomenon is pronounced for CAD.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a framework for thinking about counterfactually augmented data and make strides towards understanding its benefits in out-of-domain generalization.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-N7PBXqOUJZ" data-number="3125">
      <h4>
        <a href="https://openreview.net/forum?id=-N7PBXqOUJZ">
            Lipschitz Recurrent Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-N7PBXqOUJZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~N._Benjamin_Erichson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~N._Benjamin_Erichson1">N. Benjamin Erichson</a>, <a href="https://openreview.net/profile?email=azencot%40bgu.ac.il" class="profile-link" data-toggle="tooltip" data-placement="top" title="azencot@bgu.ac.il">Omri Azencot</a>, <a href="https://openreview.net/profile?email=afq%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="afq@google.com">Alejandro Queiruga</a>, <a href="https://openreview.net/profile?id=~Liam_Hodgkinson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liam_Hodgkinson1">Liam Hodgkinson</a>, <a href="https://openreview.net/profile?id=~Michael_W._Mahoney1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_W._Mahoney1">Michael W. Mahoney</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-N7PBXqOUJZ-details-337" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-N7PBXqOUJZ-details-337"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">recurrent neural networks, dynamical systems, differential equations</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Viewing recurrent neural networks (RNNs) as continuous-time dynamical systems, we propose a recurrent unit that describes the hidden state's evolution with two parts: a well-understood linear component plus a Lipschitz nonlinearity. This particular functional form facilitates stability analysis of the long-term behavior of the recurrent unit using tools from nonlinear systems theory. In turn, this enables architectural design decisions before experimentation. Sufficient conditions for global stability of the recurrent unit are obtained, motivating a novel scheme for constructing hidden-to-hidden matrices. Our experiments demonstrate that the Lipschitz RNN can outperform existing recurrent units on a range of benchmark tasks, including computer vision, language modeling and speech prediction tasks. Finally, through Hessian-based analysis we demonstrate that our Lipschitz recurrent unit is more robust with respect to input and parameter perturbations as compared to other continuous-time RNNs.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We develop a provably stable parameterization for continuous-time Lipschitz Recurrent Neural Networks that can employ high order integration schemes and outperform existing RNNs in performance, robustness, and conditioning.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=-N7PBXqOUJZ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="yqPnIRhHtZv" data-number="967">
      <h4>
        <a href="https://openreview.net/forum?id=yqPnIRhHtZv">
            Learning Hyperbolic Representations of Topological Features
        </a>
      
        
          <a href="https://openreview.net/pdf?id=yqPnIRhHtZv" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Panagiotis_Kyriakis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Panagiotis_Kyriakis1">Panagiotis Kyriakis</a>, <a href="https://openreview.net/profile?id=~Iordanis_Fostiropoulos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Iordanis_Fostiropoulos1">Iordanis Fostiropoulos</a>, <a href="https://openreview.net/profile?id=~Paul_Bogdan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Bogdan1">Paul Bogdan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#yqPnIRhHtZv-details-659" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="yqPnIRhHtZv-details-659"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">representation learning, hyperbolic deep learning, persistent homology, persistence diagrams</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Learning task-specific representations of persistence diagrams is an important problem in topological data analysis and machine learning. However, current state of the art methods are restricted in terms of their expressivity as they are focused on Euclidean representations. Persistence diagrams often contain features of infinite persistence (i.e., essential features) and Euclidean spaces shrink their importance relative to non-essential features because they cannot assign infinite distance to finite points. To deal with this issue, we propose a method to learn representations of persistence diagrams on hyperbolic spaces, more specifically on the Poincare ball. By representing features of infinite persistence infinitesimally close to the boundary of the ball, their distance to non-essential features approaches infinity, thereby their relative importance is preserved. This is achieved without utilizing extremely high values for the learnable parameters, thus the representation can be fed into downstream optimization methods and trained efficiently in an end-to-end fashion. We present experimental results on graph and image classification tasks and show that the performance of our method is on par with or exceeds the performance of other state of the art methods.
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We develop a method to learn representations of topological features (i.e., persistence diagrams) on hyperbolic spaces.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=yqPnIRhHtZv&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="o3iritJHLfO" data-number="3537">
      <h4>
        <a href="https://openreview.net/forum?id=o3iritJHLfO">
            Bidirectional Variational Inference for Non-Autoregressive Text-to-Speech
        </a>
      
        
          <a href="https://openreview.net/pdf?id=o3iritJHLfO" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yoonhyung_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoonhyung_Lee2">Yoonhyung Lee</a>, <a href="https://openreview.net/profile?id=~Joongbo_Shin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joongbo_Shin1">Joongbo Shin</a>, <a href="https://openreview.net/profile?id=~Kyomin_Jung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyomin_Jung1">Kyomin Jung</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#o3iritJHLfO-details-783" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="o3iritJHLfO-details-783"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">text-to-speech, speech synthesis, non-autoregressive, VAE</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Although early text-to-speech (TTS) models such as Tacotron 2 have succeeded in generating human-like speech, their autoregressive architectures have several limitations: (1) They require a lot of time to generate a mel-spectrogram consisting of hundreds of steps. (2) The autoregressive speech generation shows a lack of robustness due to its error propagation property. In this paper, we propose a novel non-autoregressive TTS model called BVAE-TTS, which eliminates the architectural limitations and generates a mel-spectrogram in parallel. BVAE-TTS adopts a bidirectional-inference variational autoencoder (BVAE) that learns hierarchical latent representations using both bottom-up and top-down paths to increase its expressiveness. To apply BVAE to TTS, we design our model to utilize text information via an attention mechanism. By using attention maps that BVAE-TTS generates, we train a duration predictor so that the model uses the predicted duration of each phoneme at inference. In experiments conducted on LJSpeech dataset, we show that our model generates a mel-spectrogram 27 times faster than Tacotron 2 with similar speech quality. Furthermore, our BVAE-TTS outperforms Glow-TTS, which is one of the state-of-the-art non-autoregressive TTS models, in terms of both speech quality and inference speed while having 58% fewer parameters.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">In this paper, a novel non-autoregressive text-to-speech model based on bidirectional-inference variational autoencoder called BVAE-TTS is proposed.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=o3iritJHLfO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="TBIzh9b5eaz" data-number="1257">
      <h4>
        <a href="https://openreview.net/forum?id=TBIzh9b5eaz">
            Risk-Averse Offline Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=TBIzh9b5eaz" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~N%C3%BAria_Armengol_Urp%C3%AD1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Núria_Armengol_Urpí1">Núria Armengol Urpí</a>, <a href="https://openreview.net/profile?id=~Sebastian_Curi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sebastian_Curi1">Sebastian Curi</a>, <a href="https://openreview.net/profile?id=~Andreas_Krause1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_Krause1">Andreas Krause</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#TBIzh9b5eaz-details-909" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TBIzh9b5eaz-details-909"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">offline, reinforcement learning, risk-averse, risk sensitive, robust, safety, safe</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. 
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=TBIzh9b5eaz&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="JkfYjnOEo6M" data-number="2982">
      <h4>
        <a href="https://openreview.net/forum?id=JkfYjnOEo6M">
            Group Equivariant Stand-Alone Self-Attention For Vision
        </a>
      
        
          <a href="https://openreview.net/pdf?id=JkfYjnOEo6M" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~David_W._Romero1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_W._Romero1">David W. Romero</a>, <a href="https://openreview.net/profile?id=~Jean-Baptiste_Cordonnier2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jean-Baptiste_Cordonnier2">Jean-Baptiste Cordonnier</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#JkfYjnOEo6M-details-63" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JkfYjnOEo6M-details-63"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">group equivariant transformers, group equivariant self-attention, group equivariance, self-attention, transformers</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We provide a general self-attention formulation to impose group equivariance to arbitrary symmetry groups. This is achieved by defining positional encodings that are invariant to the action of the group considered. Since the group acts on the positional encoding directly, group equivariant self-attention networks (GSA-Nets) are steerable by nature. Our experiments on vision benchmarks demonstrate consistent improvements of GSA-Nets over non-equivariant self-attention networks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We provide a general self-attention formulation to impose group equivariance to arbitrary symmetry groups.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=JkfYjnOEo6M&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="vYVI1CHPaQg" data-number="2162">
      <h4>
        <a href="https://openreview.net/forum?id=vYVI1CHPaQg">
            A Better Alternative to Error Feedback for Communication-Efficient Distributed Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=vYVI1CHPaQg" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Samuel_Horv%C3%A1th1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samuel_Horváth1">Samuel Horváth</a>, <a href="https://openreview.net/profile?id=~Peter_Richtarik1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peter_Richtarik1">Peter Richtarik</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 13 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#vYVI1CHPaQg-details-513" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vYVI1CHPaQg-details-513"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">distributed optimization, communication efficiency</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Modern large-scale machine learning applications require stochastic optimization algorithms to be implemented on distributed computing systems. A key bottleneck of such systems is the communication overhead for exchanging information across the workers, such as stochastic gradients. Among the many techniques proposed to remedy this issue, one of the most successful is the framework of compressed communication with error feedback (EF). EF remains the only known technique that can deal with the error induced by contractive compressors which are not unbiased, such as Top-<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="255" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container> or PowerSGD.  In this paper, we propose a new and theoretically and practically better alternative to EF for dealing with contractive compressors. In particular, we propose a construction which can transform any contractive compressor into an induced unbiased compressor. Following this transformation, existing methods able to work with unbiased compressors can be applied. We show that our approach leads to vast improvements over EF, including reduced memory requirements, better communication complexity guarantees and fewer assumptions. We further extend our results to federated learning with partial participation following an arbitrary distribution over the nodes and demonstrate the benefits thereof. We perform several numerical experiments which validate our theoretical findings.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Q1jmmQz72M2" data-number="1188">
      <h4>
        <a href="https://openreview.net/forum?id=Q1jmmQz72M2">
            Neural Delay Differential Equations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Q1jmmQz72M2" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Qunxi_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qunxi_Zhu1">Qunxi Zhu</a>, <a href="https://openreview.net/profile?id=~Yao_Guo3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yao_Guo3">Yao Guo</a>, <a href="https://openreview.net/profile?id=~Wei_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Lin1">Wei Lin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Q1jmmQz72M2-details-368" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Q1jmmQz72M2-details-368"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Delay differential equations, neural networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">    Neural Ordinary Differential Equations (NODEs), a framework of continuous-depth neural networks, have been widely applied, showing exceptional efficacy in coping with some representative datasets.  Recently, an augmented framework has been successfully developed for conquering some limitations emergent in application of the original framework.  Here we propose a new class of continuous-depth neural networks with delay, named as Neural Delay Differential Equations (NDDEs),  and, for computing the corresponding gradients, we use the adjoint sensitivity method to obtain the delayed dynamics of the adjoint. Since the differential equations with delays are usually seen as dynamical systems of infinite dimension possessing more fruitful dynamics, the NDDEs, compared to the NODEs, own a stronger capacity of nonlinear representations.  Indeed, we analytically validate that the NDDEs are of universal approximators, and further articulate an extension of the NDDEs, where the initial function of the NDDEs is supposed to satisfy ODEs.   More importantly, we use several illustrative examples to demonstrate the outstanding capacities of the NDDEs and the NDDEs with ODEs' initial value.  More precisely, (1) we successfully model the delayed dynamics where the trajectories in the lower-dimensional phase space could be mutually intersected, while the traditional NODEs without any argumentation are not directly applicable for such modeling, and (2) we achieve lower loss and higher accuracy not only for the  data produced synthetically by complex models but also for the real-world image datasets, i.e., CIFAR10, MNIST and SVHN.   Our results on the NDDEs reveal that appropriately articulating the elements of dynamical systems into the network design is truly beneficial to promoting the network performance.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a new class of continuous-depth neural networks with delay, named as Neural Delay Differential Equations and having better representation capability outperforming the Neural ODEs.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="wQRlSUZ5V7B" data-number="3058">
      <h4>
        <a href="https://openreview.net/forum?id=wQRlSUZ5V7B">
            Capturing Label Characteristics in VAEs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=wQRlSUZ5V7B" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tom_Joy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tom_Joy1">Tom Joy</a>, <a href="https://openreview.net/profile?email=sebastian.schmon%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sebastian.schmon@gmail.com">Sebastian Schmon</a>, <a href="https://openreview.net/profile?id=~Philip_Torr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philip_Torr1">Philip Torr</a>, <a href="https://openreview.net/profile?id=~Siddharth_N1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siddharth_N1">Siddharth N</a>, <a href="https://openreview.net/profile?id=~Tom_Rainforth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tom_Rainforth1">Tom Rainforth</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#wQRlSUZ5V7B-details-159" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wQRlSUZ5V7B-details-159"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">variational autoencoder, representation learning, deep generative models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present a principled approach to incorporating labels in variational autoencoders (VAEs) that captures the rich characteristic information associated with those labels. While prior work has typically conflated these by learning latent variables that directly correspond to label values, we argue this is contrary to the intended effect of supervision in VAEs—capturing rich label characteristics with the latents. For example, we may want to capture the characteristics of a face that make it look young, rather than just the age of the person. To this end, we develop a novel VAE model, the characteristic capturing VAE (CCVAE), which “reparameterizes” supervision through auxiliary variables and a concomitant variational objective. Through judicious structuring of mappings between latent and auxiliary variables, we show that the CCVAE can effectively learn meaningful representations of the characteristics of interest across a variety of supervision schemes. In particular, we show that the CCVAE allows for more effective and more general interventions to be performed, such as smooth traversals within the characteristics for a given label, diverse conditional generation, and transferring characteristics across datapoints.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a principled approach to incorporating labels in VAEs that captures the rich characteristic information associated with those labels.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=wQRlSUZ5V7B&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="dlEJsyHGeaL" data-number="408">
      <h4>
        <a href="https://openreview.net/forum?id=dlEJsyHGeaL">
            Graph Edit Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=dlEJsyHGeaL" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Benjamin_Paassen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Paassen1">Benjamin Paassen</a>, <a href="https://openreview.net/profile?id=~Daniele_Grattarola1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniele_Grattarola1">Daniele Grattarola</a>, <a href="https://openreview.net/profile?id=~Daniele_Zambon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniele_Zambon1">Daniele Zambon</a>, <a href="https://openreview.net/profile?email=cesare.alippi%40usi.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="cesare.alippi@usi.ch">Cesare Alippi</a>, <a href="https://openreview.net/profile?id=~Barbara_Eva_Hammer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Barbara_Eva_Hammer1">Barbara Eva Hammer</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 14 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#dlEJsyHGeaL-details-123" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dlEJsyHGeaL-details-123"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">graph neural networks, graph edit distance, time series prediction, structured prediction</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While graph neural networks have made impressive progress in classification and regression, few approaches to date perform time series prediction on graphs, and those that do are mostly limited to edge changes. We suggest that graph edits are a more natural interface for graph-to-graph learning. In particular,  graph edits are general enough to describe any graph-to-graph change, not only edge changes; they are sparse, making them easier to understand for humans and more efficient computationally; and they are local, avoiding the need for pooling layers in graph neural networks. In this paper, we propose a novel output layer - the graph edit network - which takes node embeddings as input and generates a sequence of graph edits that transform the input graph to the output graph. We prove that a mapping between the node sets of two graphs is sufficient to construct training data for a graph edit network and that an optimal mapping yields edit scripts that are almost as short as the graph edit distance between the graphs. We further provide a proof-of-concept empirical evaluation on several graph dynamical systems, which are difficult to learn for baselines from the literature.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that graph neural networks can predict graph edits and are connected to the graph edit distance via graph mappings</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=dlEJsyHGeaL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="hpH98mK5Puk" data-number="1927">
      <h4>
        <a href="https://openreview.net/forum?id=hpH98mK5Puk">
            InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective
        </a>
      
        
          <a href="https://openreview.net/pdf?id=hpH98mK5Puk" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Boxin_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boxin_Wang1">Boxin Wang</a>, <a href="https://openreview.net/profile?id=~Shuohang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuohang_Wang1">Shuohang Wang</a>, <a href="https://openreview.net/profile?id=~Yu_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Cheng1">Yu Cheng</a>, <a href="https://openreview.net/profile?id=~Zhe_Gan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhe_Gan1">Zhe Gan</a>, <a href="https://openreview.net/profile?email=ruoxijia%40vt.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ruoxijia@vt.edu">Ruoxi Jia</a>, <a href="https://openreview.net/profile?id=~Bo_Li19" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Li19">Bo Li</a>, <a href="https://openreview.net/profile?id=~Jingjing_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jingjing_Liu2">Jingjing Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#hpH98mK5Puk-details-952" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hpH98mK5Puk-details-952"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">adversarial robustness, information theory, BERT, adversarial training, NLI, QA</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Large-scale language models such as BERT have achieved state-of-the-art performance across a wide range of NLP tasks. Recent studies, however, show that such BERT-based models are vulnerable facing the threats of textual adversarial attacks. We aim to address this problem from an information-theoretic perspective, and propose InfoBERT, a novel learning framework for robust ﬁne-tuning of pre-trained language models. InfoBERT contains two mutual-information-based regularizers for model training: (i) an Information Bottleneck regularizer, which suppresses noisy mutual information between the input and the feature representation; and (ii) a Robust Feature regularizer, which increases the mutual information between local robust features and global features. We provide a principled way to theoretically analyze and improve the robustness of representation learning for language models in both standard and adversarial training. Extensive experiments demonstrate that InfoBERT achieves state-of-the-art robust accuracy over several adversarial datasets on Natural Language Inference (NLI) and Question Answering (QA) tasks.
      Our code is available at https://github.com/AI-secure/InfoBERT.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a novel learning framework, InfoBERT, for robust fine-tuning of pre-trained language models from an information-theoretic perspective, and achieve state-of-the-art robust accuracy over several adversarial datasets on NLI and QA tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9FWas6YbmB3" data-number="2159">
      <h4>
        <a href="https://openreview.net/forum?id=9FWas6YbmB3">
            DrNAS: Dirichlet Neural Architecture Search
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9FWas6YbmB3" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xiangning_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiangning_Chen1">Xiangning Chen</a>, <a href="https://openreview.net/profile?id=~Ruochen_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruochen_Wang2">Ruochen Wang</a>, <a href="https://openreview.net/profile?id=~Minhao_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minhao_Cheng1">Minhao Cheng</a>, <a href="https://openreview.net/profile?id=~Xiaocheng_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaocheng_Tang1">Xiaocheng Tang</a>, <a href="https://openreview.net/profile?id=~Cho-Jui_Hsieh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cho-Jui_Hsieh1">Cho-Jui Hsieh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9FWas6YbmB3-details-791" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9FWas6YbmB3-details-791"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper proposes a novel differentiable architecture search method by formulating it into a distribution learning problem. We treat the continuously relaxed architecture mixing weight as random variables, modeled by Dirichlet distribution. With recently developed pathwise derivatives, the Dirichlet parameters can be easily optimized with gradient-based optimizer in an end-to-end manner. This formulation improves the generalization ability and induces stochasticity that naturally encourages exploration in the search space. Furthermore, to alleviate the large memory consumption of differentiable NAS, we propose a simple yet effective progressive learning scheme that enables searching directly on large-scale tasks, eliminating the gap between search and evaluation phases. Extensive experiments demonstrate the effectiveness of our method. Specifically, we obtain a test error of 2.46\% for CIFAR-10, 23.7\% for ImageNet under the mobile setting. On NAS-Bench-201, we also achieve state-of-the-art results on all three datasets and provide insights for the effective design of neural architecture search algorithms.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=9FWas6YbmB3&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="1rxHOBjeDUW" data-number="1552">
      <h4>
        <a href="https://openreview.net/forum?id=1rxHOBjeDUW">
            Drop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration
        </a>
      
        
          <a href="https://openreview.net/pdf?id=1rxHOBjeDUW" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jaekyeom_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaekyeom_Kim1">Jaekyeom Kim</a>, <a href="https://openreview.net/profile?id=~Minjung_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minjung_Kim2">Minjung Kim</a>, <a href="https://openreview.net/profile?id=~Dongyeon_Woo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dongyeon_Woo1">Dongyeon Woo</a>, <a href="https://openreview.net/profile?id=~Gunhee_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gunhee_Kim1">Gunhee Kim</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#1rxHOBjeDUW-details-418" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1rxHOBjeDUW-details-418"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement learning, Information bottleneck</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a novel information bottleneck (IB) method named Drop-Bottleneck, which discretely drops features that are irrelevant to the target variable. Drop-Bottleneck not only enjoys a simple and tractable compression objective but also additionally provides a deterministic compressed representation of the input variable, which is useful for inference tasks that require consistent representation. Moreover, it can jointly learn a feature extractor and select features considering each feature dimension's relevance to the target task, which is unattainable by most neural network-based IB methods. We propose an exploration method based on Drop-Bottleneck for reinforcement learning tasks. In a multitude of noisy and reward sparse maze navigation tasks in VizDoom (Kempka et al., 2016) and DMLab (Beattie et al., 2016), our exploration method achieves state-of-the-art performance. As a new IB framework, we demonstrate that Drop-Bottleneck outperforms Variational Information Bottleneck (VIB) (Alemi et al., 2017) in multiple aspects including adversarial robustness and dimensionality reduction.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Our novel IB method, Drop-Bottleneck, discretely drops task-irrelevant input features to build the compressed representation and shows state-of-the-art performance on noisy, sparse-reward navigation tasks in reinforcement learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=1rxHOBjeDUW&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="7_G8JySGecm" data-number="3067">
      <h4>
        <a href="https://openreview.net/forum?id=7_G8JySGecm">
            Monte-Carlo Planning and Learning with Language Action Value Estimates
        </a>
      
        
          <a href="https://openreview.net/pdf?id=7_G8JySGecm" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Youngsoo_Jang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youngsoo_Jang2">Youngsoo Jang</a>, <a href="https://openreview.net/profile?id=~Seokin_Seo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seokin_Seo1">Seokin Seo</a>, <a href="https://openreview.net/profile?id=~Jongmin_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jongmin_Lee1">Jongmin Lee</a>, <a href="https://openreview.net/profile?id=~Kee-Eung_Kim4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kee-Eung_Kim4">Kee-Eung Kim</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#7_G8JySGecm-details-822" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7_G8JySGecm-details-822"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">natural language processing, Monte-Carlo tree search, reinforcement learning, interactive fiction</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Interactive Fiction (IF) games provide a useful testbed for language-based reinforcement learning agents, posing significant challenges of natural language understanding, commonsense reasoning, and non-myopic planning in the combinatorial search space. Agents based on standard planning algorithms struggle to play IF games due to the massive search space of language actions. Thus, language-grounded planning is a key ability of such agents, since inferring the consequence of language action based on semantic understanding can drastically improve search. In this paper, we introduce Monte-Carlo planning with Language Action Value Estimates (MC-LAVE) that combines a Monte-Carlo tree search with language-driven exploration. MC-LAVE invests more search effort into semantically promising language actions using locally optimistic language value estimates, yielding a significant reduction in the effective search space of language actions. We then present a reinforcement learning approach via MC-LAVE, which alternates between MC-LAVE planning and supervised learning of the self-generated language actions. In the experiments, we demonstrate that our method achieves new high scores in various IF games.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present Monte-Carlo planning with Language Action Value Estimates (MC-LAVE) that combines a Monte-Carlo tree search with language-driven exploration for Interactive Fiction games.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Eql5b1_hTE4" data-number="1387">
      <h4>
        <a href="https://openreview.net/forum?id=Eql5b1_hTE4">
            Robust early-learning: Hindering the memorization of noisy labels
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Eql5b1_hTE4" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xiaobo_Xia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaobo_Xia1">Xiaobo Xia</a>, <a href="https://openreview.net/profile?id=~Tongliang_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tongliang_Liu1">Tongliang Liu</a>, <a href="https://openreview.net/profile?id=~Bo_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Han1">Bo Han</a>, <a href="https://openreview.net/profile?id=~Chen_Gong5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chen_Gong5">Chen Gong</a>, <a href="https://openreview.net/profile?id=~Nannan_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nannan_Wang1">Nannan Wang</a>, <a href="https://openreview.net/profile?id=~Zongyuan_Ge1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zongyuan_Ge1">Zongyuan Ge</a>, <a href="https://openreview.net/profile?email=yichang%40jlu.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="yichang@jlu.edu.cn">Yi Chang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Eql5b1_hTE4-details-820" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Eql5b1_hTE4-details-820"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The \textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \textit{non-critical parameters}. Based on this, we propose \textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="45NZvF1UHam" data-number="1571">
      <h4>
        <a href="https://openreview.net/forum?id=45NZvF1UHam">
            Identifying Physical Law of Hamiltonian Systems via Meta-Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=45NZvF1UHam" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Seungjun_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seungjun_Lee1">Seungjun Lee</a>, <a href="https://openreview.net/profile?id=~Haesang_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haesang_Yang1">Haesang Yang</a>, <a href="https://openreview.net/profile?id=~Woojae_Seong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Woojae_Seong1">Woojae Seong</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#45NZvF1UHam-details-298" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="45NZvF1UHam-details-298"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Learning physical laws, meta-learning, Hamiltonian systems</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Hamiltonian mechanics is an effective tool to represent many physical processes with concise yet well-generalized mathematical expressions. A well-modeled Hamiltonian makes it easy for researchers to analyze and forecast many related phenomena that are governed by the same physical law. However, in general, identifying a functional or shared expression of the Hamiltonian is very difficult. It requires carefully designed experiments and the researcher's insight that comes from years of experience. We propose that meta-learning algorithms can be potentially powerful data-driven tools for identifying the physical law governing Hamiltonian systems without any mathematical assumptions on the representation, but with observations from a set of systems governed by the same physical law. We show that a well meta-trained learner can identify the shared representation of the Hamiltonian by evaluating our method on several types of physical systems with various experimental settings.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce meta-learning algorithms to identify the shared representation of Hamiltonian systems.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9G5MIc-goqB" data-number="1123">
      <h4>
        <a href="https://openreview.net/forum?id=9G5MIc-goqB">
            Reweighting Augmented Samples by Minimizing the Maximal Expected Loss
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9G5MIc-goqB" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mingyang_Yi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingyang_Yi1">Mingyang Yi</a>, <a href="https://openreview.net/profile?id=~Lu_Hou2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lu_Hou2">Lu Hou</a>, <a href="https://openreview.net/profile?id=~Lifeng_Shang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lifeng_Shang1">Lifeng Shang</a>, <a href="https://openreview.net/profile?id=~Xin_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xin_Jiang1">Xin Jiang</a>, <a href="https://openreview.net/profile?id=~Qun_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qun_Liu1">Qun Liu</a>, <a href="https://openreview.net/profile?id=~Zhi-Ming_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhi-Ming_Ma1">Zhi-Ming Ma</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9G5MIc-goqB-details-428" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9G5MIc-goqB-details-428"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">data augmentation, sample reweighting</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">a new reweighting strategy on augmented samples</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="n1HD8M6WGn" data-number="2488">
      <h4>
        <a href="https://openreview.net/forum?id=n1HD8M6WGn">
            Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=n1HD8M6WGn" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xuebo_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuebo_Liu1">Xuebo Liu</a>, <a href="https://openreview.net/profile?id=~Longyue_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Longyue_Wang3">Longyue Wang</a>, <a href="https://openreview.net/profile?id=~Derek_F._Wong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Derek_F._Wong1">Derek F. Wong</a>, <a href="https://openreview.net/profile?id=~Liang_Ding3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liang_Ding3">Liang Ding</a>, <a href="https://openreview.net/profile?email=lidiasc%40um.edu.mo" class="profile-link" data-toggle="tooltip" data-placement="top" title="lidiasc@um.edu.mo">Lidia S. Chao</a>, <a href="https://openreview.net/profile?id=~Zhaopeng_Tu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhaopeng_Tu1">Zhaopeng Tu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#n1HD8M6WGn-details-274" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="n1HD8M6WGn-details-274"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Encoder layer fusion, Transformer, Sequence-to-sequence learning, Machine translation, Summarization, Grammatical error correction</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Encoder layer fusion (EncoderFusion) is a technique to fuse all the encoder layers (instead of the uppermost layer) for sequence-to-sequence (Seq2Seq) models, which has proven effective on various NLP tasks. However, it is still not entirely clear why and when EncoderFusion should work. In this paper, our main contribution is to take a step further in understanding EncoderFusion. Many of previous studies believe that the success of EncoderFusion comes from exploiting surface and syntactic information embedded in lower encoder layers. Unlike them, we find that the encoder embedding layer is more important than other intermediate encoder layers. In addition, the uppermost decoder layer consistently pays more attention to the encoder embedding layer across NLP tasks. Based on this observation, we propose a simple fusion method, SurfaceFusion, by fusing only the encoder embedding layer for the softmax layer. Experimental results show that SurfaceFusion outperforms EncoderFusion on several NLP benchmarks, including machine translation, text summarization, and grammatical error correction. It obtains the state-of-the-art performance on WMT16 Romanian-English and WMT14 English-French translation tasks. Extensive analyses reveal that SurfaceFusion learns more expressive bilingual word embeddings by building a closer relationship between relevant source and target embeddings. Source code is freely available at https://github.com/SunbowLiu/SurfaceFusion.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="I4c4K9vBNny" data-number="1859">
      <h4>
        <a href="https://openreview.net/forum?id=I4c4K9vBNny">
            Spatial Dependency Networks: Neural Layers for Improved Generative Image Modeling
        </a>
      
        
          <a href="https://openreview.net/pdf?id=I4c4K9vBNny" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~%C4%90or%C4%91e_Miladinovi%C4%871" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Đorđe_Miladinović1">Đorđe Miladinović</a>, <a href="https://openreview.net/profile?id=~Aleksandar_Stani%C4%871" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aleksandar_Stanić1">Aleksandar Stanić</a>, <a href="https://openreview.net/profile?id=~Stefan_Bauer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefan_Bauer1">Stefan Bauer</a>, <a href="https://openreview.net/profile?id=~J%C3%BCrgen_Schmidhuber1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jürgen_Schmidhuber1">Jürgen Schmidhuber</a>, <a href="https://openreview.net/profile?id=~Joachim_M._Buhmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joachim_M._Buhmann1">Joachim M. Buhmann</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#I4c4K9vBNny-details-241" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="I4c4K9vBNny-details-241"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural networks, Deep generative models, Image Modeling, Variational Autoencoders</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">How to improve generative modeling by better exploiting spatial regularities and coherence in images? We introduce a novel neural network for building image generators (decoders) and apply it to variational autoencoders (VAEs). In our spatial dependency networks (SDNs), feature maps at each level of a deep neural net are computed in a spatially coherent way, using a sequential gating-based mechanism that distributes contextual information across 2-D space. We show that augmenting the decoder of a hierarchical VAE by spatial dependency layers considerably improves density estimation over baseline convolutional architectures and the state-of-the-art among the models within the same class. Furthermore, we demonstrate that SDN can be applied to large images by synthesizing samples of high quality and coherence. In a vanilla VAE setting, we find that a powerful SDN decoder also improves learning disentangled representations, indicating that neural architectures play an important role in this task. Our results suggest favoring spatial dependency over convolutional layers in various VAE settings. The accompanying source code is given at https://github.com/djordjemila/sdn.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A novel neural network layer for improved generative modeling of images, applied to variational autoencoders.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=I4c4K9vBNny&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Yz-XtK5RBxB" data-number="516">
      <h4>
        <a href="https://openreview.net/forum?id=Yz-XtK5RBxB">
            Deep Repulsive Clustering of Ordered Data Based on Order-Identity Decomposition
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Yz-XtK5RBxB" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Seon-Ho_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seon-Ho_Lee1">Seon-Ho Lee</a>, <a href="https://openreview.net/profile?id=~Chang-Su_Kim4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chang-Su_Kim4">Chang-Su Kim</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 13 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Yz-XtK5RBxB-details-769" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Yz-XtK5RBxB-details-769"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Clustering, order learning, age estimation, aesthetic assessment, historical color image classification</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose the deep repulsive clustering (DRC) algorithm of ordered data for effective order learning. First, we develop the order-identity decomposition (ORID) network to divide the information of an object instance into an order-related feature and an identity feature. Then, we group object instances into clusters according to their identity features using a repulsive term. Moreover, we estimate the rank of a test instance, by comparing it with references within the same cluster. Experimental results on facial age estimation, aesthetic score regression, and historical color image classification show that the proposed algorithm can cluster ordered data effectively and also yield excellent rank estimation performance.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A deep clustering algorithm for ordered data is proposed based on the order-identity decomposition.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="fAbkE6ant2" data-number="2786">
      <h4>
        <a href="https://openreview.net/forum?id=fAbkE6ant2">
            Revisiting Locally Supervised Learning: an Alternative to End-to-end Training
        </a>
      
        
          <a href="https://openreview.net/pdf?id=fAbkE6ant2" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yulin_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yulin_Wang1">Yulin Wang</a>, <a href="https://openreview.net/profile?id=~Zanlin_Ni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zanlin_Ni1">Zanlin Ni</a>, <a href="https://openreview.net/profile?id=~Shiji_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shiji_Song1">Shiji Song</a>, <a href="https://openreview.net/profile?id=~Le_Yang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Le_Yang2">Le Yang</a>, <a href="https://openreview.net/profile?id=~Gao_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gao_Huang1">Gao Huang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#fAbkE6ant2-details-256" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fAbkE6ant2-details-256"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Locally supervised training, Deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Due to the need to store the intermediate activations for back-propagation, end-to-end (E2E) training of deep networks usually suffers from high GPUs memory footprint. This paper aims to address this problem by revisiting the locally supervised learning, where a network is split into gradient-isolated modules and trained with local supervision. We experimentally show that simply training local modules with E2E loss tends to collapse task-relevant information at early layers, and hence hurts the performance of the full model. To avoid this issue, we propose an information propagation (InfoPro) loss, which encourages local modules to preserve as much useful information as possible, while progressively discard task-irrelevant information. As InfoPro loss is difficult to compute in its original form, we derive a feasible upper bound as a surrogate optimization objective, yielding a simple but effective algorithm. In fact, we show that the proposed method boils down to minimizing the combination of a reconstruction loss and a normal cross-entropy/contrastive term. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10, ImageNet and Cityscapes) validate that InfoPro is capable of achieving competitive performance with less than 40% memory footprint compared to E2E training, while allowing using training data with higher-resolution or larger batch sizes under the same GPU memory constraint. Our method also enables training local modules asynchronously for potential training acceleration.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We provide a deep understanding of locally supervised learning, and make it perform on par with end-to-end training, while with significantly reduced GPUs memory footprint. Code is available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="B5VvQrI49Pa" data-number="2805">
      <h4>
        <a href="https://openreview.net/forum?id=B5VvQrI49Pa">
            Nonseparable Symplectic Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=B5VvQrI49Pa" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Shiying_Xiong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shiying_Xiong1">Shiying Xiong</a>, <a href="https://openreview.net/profile?email=yunjin.tong.22%40dartmouth.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yunjin.tong.22@dartmouth.edu">Yunjin Tong</a>, <a href="https://openreview.net/profile?id=~Xingzhe_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xingzhe_He1">Xingzhe He</a>, <a href="https://openreview.net/profile?id=~Shuqi_Yang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuqi_Yang2">Shuqi Yang</a>, <a href="https://openreview.net/profile?email=yangcheng.iron%40bytedance.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yangcheng.iron@bytedance.com">Cheng Yang</a>, <a href="https://openreview.net/profile?id=~Bo_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bo_Zhu2">Bo Zhu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#B5VvQrI49Pa-details-440" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="B5VvQrI49Pa-details-440"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Data-driven modeling, nonseparable Hailtonian system, symplectic networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Predicting the behaviors of Hamiltonian systems has been drawing increasing attention in scientific machine learning. However, the vast majority of the literature was focused on predicting separable Hamiltonian systems with their kinematic and potential energy terms being explicitly decoupled, while building data-driven paradigms to predict nonseparable Hamiltonian systems that are ubiquitous in fluid dynamics and quantum mechanics were rarely explored. The main computational challenge lies in the effective embedding of symplectic priors to describe the inherently coupled evolution of position and momentum, which typically exhibits intricate dynamics. To solve the problem, we propose a novel neural network architecture, Nonseparable Symplectic Neural Networks (NSSNNs), to uncover and embed the symplectic structure of a nonseparable Hamiltonian system from limited observation data. The enabling mechanics of our approach is an augmented symplectic time integrator to decouple the position and momentum energy terms and facilitate their evolution. We demonstrated the efficacy and versatility of our method by predicting a wide range of Hamiltonian systems, both separable and nonseparable, including chaotic vortical flows. We showed the unique computational merits of our approach to yield long-term, accurate, and robust predictions for large-scale Hamiltonian systems by rigorously enforcing symplectomorphism.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value "> </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=B5VvQrI49Pa&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="0O_cQfw6uEh" data-number="3694">
      <h4>
        <a href="https://openreview.net/forum?id=0O_cQfw6uEh">
            Gradient Origin Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=0O_cQfw6uEh" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sam_Bond-Taylor1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sam_Bond-Taylor1">Sam Bond-Taylor</a>, <a href="https://openreview.net/profile?email=christopher.g.willcocks%40durham.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="christopher.g.willcocks@durham.ac.uk">Chris G. Willcocks</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#0O_cQfw6uEh-details-392" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0O_cQfw6uEh-details-392"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep Learning, Generative Models, Implicit Representation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper proposes a new type of generative model that is able to quickly learn a latent representation without an encoder. This is achieved using empirical Bayes to calculate the expectation of the posterior, which is implemented by initialising a latent vector with zeros, then using the gradient of the log-likelihood of the data with respect to this zero vector as new latent points. The approach has similar characteristics to autoencoders, but with a simpler architecture, and is demonstrated in a variational autoencoder equivalent that permits sampling. This also allows implicit representation networks to learn a space of implicit functions without requiring a hypernetwork, retaining their representation advantages across datasets. The experiments show that the proposed method converges faster, with significantly lower reconstruction error than autoencoders, while requiring half the parameters.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A new model that uses the negative gradient of the loss with respect to the origin as a latent vector is found to be superior to equivalent networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=0O_cQfw6uEh&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="gJYlaqL8i8" data-number="3454">
      <h4>
        <a href="https://openreview.net/forum?id=gJYlaqL8i8">
            Learning to Sample with Local and Global Contexts  in Experience Replay Buffer
        </a>
      
        
          <a href="https://openreview.net/pdf?id=gJYlaqL8i8" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Youngmin_Oh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youngmin_Oh2">Youngmin Oh</a>, <a href="https://openreview.net/profile?id=~Kimin_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kimin_Lee1">Kimin Lee</a>, <a href="https://openreview.net/profile?id=~Jinwoo_Shin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinwoo_Shin1">Jinwoo Shin</a>, <a href="https://openreview.net/profile?id=~Eunho_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eunho_Yang1">Eunho Yang</a>, <a href="https://openreview.net/profile?id=~Sung_Ju_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sung_Ju_Hwang1">Sung Ju Hwang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#gJYlaqL8i8-details-227" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gJYlaqL8i8-details-227"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, experience replay buffer, off-policy RL</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Experience replay, which enables the agents to remember and reuse experience from the past, has played a significant role in the success of off-policy reinforcement learning (RL). To utilize the experience replay efficiently, the existing sampling methods allow selecting out more meaningful experiences by imposing priorities on them based on certain metrics (e.g. TD-error). However, they may result in sampling highly biased, redundant transitions since they compute the sampling rate for each transition independently, without consideration of its importance in relation to other transitions. In this paper, we aim to address the issue by proposing a new learning-based sampling method that can compute the relative importance of transition. To this end, we design a novel permutation-equivariant neural architecture that takes contexts from not only features of each transition (local) but also those of others (global) as inputs. We validate our framework, which we refer to as Neural Experience Replay Sampler (NERS), on multiple benchmark tasks for both continuous and discrete control tasks and show that it can significantly improve the performance of various off-policy RL methods. Further analysis confirms that the improvements of the sample efficiency indeed are due to sampling diverse and meaningful transitions by NERS that considers both local and global contexts. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a learning-based neural replay which calculates the relative importance to sample experience for off-policy RL.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=gJYlaqL8i8&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="hx1IXFHAw7R" data-number="2317">
      <h4>
        <a href="https://openreview.net/forum?id=hx1IXFHAw7R">
            Provable Rich Observation Reinforcement Learning with Combinatorial Latent States
        </a>
      
        
          <a href="https://openreview.net/pdf?id=hx1IXFHAw7R" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Dipendra_Misra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dipendra_Misra1">Dipendra Misra</a>, <a href="https://openreview.net/profile?id=~Qinghua_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qinghua_Liu1">Qinghua Liu</a>, <a href="https://openreview.net/profile?id=~Chi_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chi_Jin1">Chi Jin</a>, <a href="https://openreview.net/profile?email=jcl%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jcl@microsoft.com">John Langford</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#hx1IXFHAw7R-details-236" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hx1IXFHAw7R-details-236"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement learning theory, Rich observation, Noise-contrastive learning, State abstraction, Factored MDP</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a novel setting for reinforcement learning that combines two common real-world difficulties: presence of observations (such as camera images) and factored states (such as location of objects). In our setting, the agent receives observations generated stochastically from a "latent" factored state. These observations are "rich enough" to enable decoding of the latent state and remove partial observability concerns. Since the latent state is combinatorial, the size of state space is exponential in the number of latent factors. We create a learning algorithm FactoRL (Fact-o-Rel) for this setting,  which uses noise-contrastive learning to identify latent structures in emission processes and discover a factorized state space. We derive polynomial sample complexity guarantees for FactoRL which polynomially depend upon the number factors, and very weakly depend on the size of the observation space.  We also provide a guarantee of polynomial time complexity when given access to an efficient planning algorithm.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a problem setup and a provable reinforcement learning algorithm for rich-observation problems with latent combinatorially large state space.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="r28GdiQF7vM" data-number="546">
      <h4>
        <a href="https://openreview.net/forum?id=r28GdiQF7vM">
            Sharper Generalization Bounds for Learning with Gradient-dominated Objective Functions
        </a>
      
        
          <a href="https://openreview.net/pdf?id=r28GdiQF7vM" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yunwen_Lei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yunwen_Lei1">Yunwen Lei</a>, <a href="https://openreview.net/profile?id=~Yiming_Ying1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yiming_Ying1">Yiming Ying</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#r28GdiQF7vM-details-789" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="r28GdiQF7vM-details-789"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">generalization bounds, non-convex learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Stochastic optimization has become the workhorse behind many successful machine learning applications, which motivates a lot of theoretical analysis to understand its empirical behavior. As a comparison, there is far less work to study the generalization behavior especially in a non-convex learning setting. In this paper, we study the generalization behavior of stochastic optimization by leveraging the algorithmic stability for learning with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="256" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>β</mi></math></mjx-assistive-mml></mjx-container>-gradient-dominated objective functions. We develop generalization bounds of the order <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="257" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mrow><mo>/</mo></mrow><mo stretchy="false">(</mo><mi>n</mi><mi>β</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> plus the convergence rate of the optimization algorithm, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="258" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container> is the sample size. Our stability analysis significantly improves the existing non-convex analysis by removing the bounded gradient assumption and implying better generalization bounds. We achieve this improvement by exploiting the smoothness of loss functions instead of the Lipschitz condition in Charles &amp; Papailiopoulos (2018). We apply our general results to various stochastic optimization algorithms, which show clearly how the variance-reduction techniques improve not only training but also generalization. Furthermore, our discussion explains how interpolation helps generalization for highly expressive models.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We develop sharper generalization bounds for learning with gradient-dominated objective functions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="rkQuFUmUOg3" data-number="401">
      <h4>
        <a href="https://openreview.net/forum?id=rkQuFUmUOg3">
            Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets
        </a>
      
        
          <a href="https://openreview.net/pdf?id=rkQuFUmUOg3" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hayeon_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hayeon_Lee1">Hayeon Lee</a>, <a href="https://openreview.net/profile?id=~Eunyoung_Hyung2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eunyoung_Hyung2">Eunyoung Hyung</a>, <a href="https://openreview.net/profile?id=~Sung_Ju_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sung_Ju_Hwang1">Sung Ju Hwang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>23 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#rkQuFUmUOg3-details-316" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rkQuFUmUOg3-details-316"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Machine Learning, Neural Architecture Search, Meta-learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Despite the success of recent Neural Architecture Search (NAS) methods on various tasks which have shown to output networks that largely outperform human-designed networks, conventional NAS methods have mostly tackled the optimization of searching for the network architecture for a single task (dataset), which does not generalize well across multiple tasks (datasets). Moreover, since such task-specific methods search for a neural architecture from scratch for every given task, they incur a large computational cost, which is problematic when the time and monetary budget are limited. In this paper, we propose an efficient NAS framework that is trained once on a database consisting of datasets and pretrained networks and can rapidly search for a neural architecture for a novel dataset. The proposed MetaD2A (Meta Dataset-to-Architecture) model can stochastically generate graphs (architectures) from a given set (dataset) via a cross-modal latent space learned with amortized meta-learning. Moreover, we also propose a meta-performance predictor to estimate and select the best architecture without direct training on target datasets. The experimental results demonstrate that our model meta-learned on subsets of ImageNet-1K and architectures from NAS-Bench 201 search space successfully generalizes to multiple unseen datasets including CIFAR-10 and CIFAR-100, with an average search time of 33 GPU seconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than NSGANetV2, a transferable NAS method, with comparable performance. We believe that the MetaD2A proposes a new research direction for rapid NAS as well as ways to utilize the knowledge from rich databases of datasets and architectures accumulated over the past years. Code is available at https://github.com/HayeonLee/MetaD2A.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an efficient NAS framework that is trained once on a database consisting of datasets and pretrained networks and can rapidly generate a neural architecture for a novel dataset.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=rkQuFUmUOg3&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="vhKe9UFbrJo" data-number="1601">
      <h4>
        <a href="https://openreview.net/forum?id=vhKe9UFbrJo">
            Relating by Contrasting: A Data-efficient Framework for Multimodal Generative Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=vhKe9UFbrJo" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yuge_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuge_Shi1">Yuge Shi</a>, <a href="https://openreview.net/profile?id=~Brooks_Paige1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brooks_Paige1">Brooks Paige</a>, <a href="https://openreview.net/profile?id=~Philip_Torr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philip_Torr1">Philip Torr</a>, <a href="https://openreview.net/profile?id=~Siddharth_N1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siddharth_N1">Siddharth N</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#vhKe9UFbrJo-details-467" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vhKe9UFbrJo-details-467"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep generative model, multi-modal learning, representation learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Multimodal learning for generative models often refers to the learning of abstract concepts from the commonality of information in multiple modalities, such as vision and language. While it has proven effective for learning generalisable representations, the training of such models often requires a large amount of related multimodal data that shares commonality, which can be expensive to come by. To mitigate this, we develop a novel contrastive framework for generative model learning, allowing us to train the model not just by the commonality between modalities, but by the distinction between "related" and "unrelated" multimodal data. We show in experiments that our method enables data-efficient multimodal learning on challenging datasets for various multimodal VAE models. We also show that under our proposed framework, the generative model can accurately identify related samples from unrelated ones, making it possible to make use of the plentiful unlabeled, unpaired multimodal data.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Ogga20D2HO-" data-number="2947">
      <h4>
        <a href="https://openreview.net/forum?id=Ogga20D2HO-">
            FedMix: Approximation of Mixup under Mean Augmented Federated Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Ogga20D2HO-" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tehrim_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tehrim_Yoon1">Tehrim Yoon</a>, <a href="https://openreview.net/profile?email=sym807%40kaist.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="sym807@kaist.ac.kr">Sumin Shin</a>, <a href="https://openreview.net/profile?id=~Sung_Ju_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sung_Ju_Hwang1">Sung Ju Hwang</a>, <a href="https://openreview.net/profile?id=~Eunho_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eunho_Yang1">Eunho Yang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>24 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Ogga20D2HO--details-749" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ogga20D2HO--details-749"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">federated learning, mixup</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Federated learning (FL) allows edge devices to collectively learn a model without directly sharing data within each device, thus preserving privacy and eliminating the need to store data globally. While there are promising results under the assumption of independent and identically distributed (iid) local data, current state-of-the-art algorithms suffer a performance degradation as the heterogeneity of local data across clients increases. To resolve this issue, we propose a simple framework, \emph{Mean Augmented Federated Learning (MAFL)}, where clients send and receive \emph{averaged} local data, subject to the privacy requirements of target applications. Under our framework, we propose a new augmentation algorithm, named \emph{FedMix}, which is inspired by a phenomenal yet simple data augmentation method, Mixup, but does not require local raw data to be directly shared among devices. Our method shows greatly improved performance in the standard benchmark datasets of FL, under highly non-iid federated settings, compared to conventional algorithms.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a new federated framework, Mean Augmented Federated Learning (MAFL), and propose an efficient algorithm, Federated Mixup (FedMix), which shows good performance on difficult non-iid situations.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ogga20D2HO-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="_IM-AfFhna9" data-number="3595">
      <h4>
        <a href="https://openreview.net/forum?id=_IM-AfFhna9">
            Generalized Variational Continual Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=_IM-AfFhna9" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Noel_Loo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noel_Loo1">Noel Loo</a>, <a href="https://openreview.net/profile?id=~Siddharth_Swaroop2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siddharth_Swaroop2">Siddharth Swaroop</a>, <a href="https://openreview.net/profile?id=~Richard_E_Turner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Richard_E_Turner1">Richard E Turner</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>22 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#_IM-AfFhna9-details-477" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_IM-AfFhna9-details-477"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We generalize VCL and Online-EWC and combine with task-specific FiLM layers</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=_IM-AfFhna9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ZTFeSBIX9C" data-number="2511">
      <h4>
        <a href="https://openreview.net/forum?id=ZTFeSBIX9C">
            Understanding and Improving Lexical Choice in Non-Autoregressive Translation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ZTFeSBIX9C" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Liang_Ding3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liang_Ding3">Liang Ding</a>, <a href="https://openreview.net/profile?id=~Longyue_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Longyue_Wang3">Longyue Wang</a>, <a href="https://openreview.net/profile?id=~Xuebo_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuebo_Liu1">Xuebo Liu</a>, <a href="https://openreview.net/profile?id=~Derek_F._Wong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Derek_F._Wong1">Derek F. Wong</a>, <a href="https://openreview.net/profile?id=~Dacheng_Tao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dacheng_Tao1">Dacheng Tao</a>, <a href="https://openreview.net/profile?id=~Zhaopeng_Tu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhaopeng_Tu1">Zhaopeng Tu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ZTFeSBIX9C-details-734" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZTFeSBIX9C-details-734"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Knowledge distillation (KD) is essential for training non-autoregressive translation (NAT) models by reducing the complexity of the raw data with an autoregressive teacher model. In this study, we empirically show that as a side effect of this training, the lexical choice errors on low-frequency words are propagated to the NAT model from the teacher model. To alleviate this problem, we propose to expose the raw data to NAT models to restore the useful information of low-frequency words, which are missed in the distilled data. To this end, we introduce an extra Kullback-Leibler divergence term derived by comparing the lexical choice of NAT model and that embedded in the raw data. Experimental results across language pairs and model architectures demonstrate the effectiveness and universality of the proposed approach.  Extensive analyses confirm our claim that our approach improves performance by reducing the lexical choice errors on low-frequency words.  Encouragingly, our approach pushes the SOTA NAT performance on the WMT14 English-German and WMT16 Romanian-English datasets up to 27.8 and 33.8 BLEU points, respectively. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We reveal the side effect of knowledge distillation from lexical choice perspective for Non-autoregressvie machine translation, and then propose a simple yet effective approach to improve it.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ufZN2-aehFa" data-number="637">
      <h4>
        <a href="https://openreview.net/forum?id=ufZN2-aehFa">
            Bayesian Context Aggregation for Neural Processes
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ufZN2-aehFa" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Michael_Volpp1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Volpp1">Michael Volpp</a>, <a href="https://openreview.net/profile?id=~Fabian_Fl%C3%BCrenbrock1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fabian_Flürenbrock1">Fabian Flürenbrock</a>, <a href="https://openreview.net/profile?id=~Lukas_Grossberger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lukas_Grossberger1">Lukas Grossberger</a>, <a href="https://openreview.net/profile?id=~Christian_Daniel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christian_Daniel1">Christian Daniel</a>, <a href="https://openreview.net/profile?id=~Gerhard_Neumann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gerhard_Neumann1">Gerhard Neumann</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ufZN2-aehFa-details-632" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ufZN2-aehFa-details-632"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Aggregation Methods, Neural Processes, Latent Variable Models, Meta Learning, Multi-task Learning, Deep Sets</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="P0p33rgyoE" data-number="3173">
      <h4>
        <a href="https://openreview.net/forum?id=P0p33rgyoE">
            Variational Intrinsic Control Revisited
        </a>
      
        
          <a href="https://openreview.net/pdf?id=P0p33rgyoE" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Taehwan_Kwon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Taehwan_Kwon1">Taehwan Kwon</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>24 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#P0p33rgyoE-details-294" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="P0p33rgyoE-details-294"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Unsupervised reinforcement learning, Information theory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we revisit variational intrinsic control (VIC), an unsupervised reinforcement learning method for finding the largest set of intrinsic options available to an agent. In the original work by Gregor et al. (2016), two VIC algorithms were proposed: one that represents the options explicitly, and the other that does it implicitly. We show that the intrinsic reward used in the latter is subject to bias in stochastic environments, causing convergence to suboptimal solutions. To correct this behavior, we propose two methods respectively based on the transitional probability model and Gaussian Mixture Model. We substantiate our claims through rigorous mathematical derivations and experimental analyses. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Revisitation of Variational Intrinsic Control (VIC) for the optimal behavior of implicit VIC under stochastic dynamics.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="3q5IqUrkcF" data-number="2231">
      <h4>
        <a href="https://openreview.net/forum?id=3q5IqUrkcF">
            Implicit Gradient Regularization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=3q5IqUrkcF" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~David_Barrett1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Barrett1">David Barrett</a>, <a href="https://openreview.net/profile?email=dherin%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dherin@google.com">Benoit Dherin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 11 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#3q5IqUrkcF-details-803" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3q5IqUrkcF-details-803"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">implicit regularization, deep learning, deep learning theory, theoretical issues in deep learning, theory, regularization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Gradient descent can be surprisingly good at optimizing deep neural networks without overfitting and without explicit regularization. We find that the discrete steps of gradient descent implicitly regularize models by penalizing gradient descent trajectories that have large loss gradients. We call this Implicit Gradient Regularization (IGR) and we use backward error analysis to calculate the size of this regularization. We confirm empirically that implicit gradient regularization biases gradient descent toward flat minima, where test errors are small and solutions are robust to noisy parameter perturbations. Furthermore, we demonstrate that the implicit gradient regularization term can be used as an explicit regularizer, allowing us to control this gradient regularization directly. More broadly, our work indicates that backward error analysis is a useful theoretical approach to the perennial question of how learning rate, model size, and parameter regularization interact to determine the properties of overparameterized models optimized with gradient descent.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We have found a hidden form of regularization in gradient descent - Implicit Gradient Regularization - that biases overparameterized models towards flat, low test error solutions and helps us to understand why deep learning works so well.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="_TM6rT7tXke" data-number="2378">
      <h4>
        <a href="https://openreview.net/forum?id=_TM6rT7tXke">
            Return-Based Contrastive Representation Learning for Reinforcement  Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=_TM6rT7tXke" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Guoqing_Liu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guoqing_Liu3">Guoqing Liu</a>, <a href="https://openreview.net/profile?id=~Chuheng_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuheng_Zhang1">Chuheng Zhang</a>, <a href="https://openreview.net/profile?id=~Li_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Li_Zhao1">Li Zhao</a>, <a href="https://openreview.net/profile?id=~Tao_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Qin1">Tao Qin</a>, <a href="https://openreview.net/profile?id=~Jinhua_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinhua_Zhu1">Jinhua Zhu</a>, <a href="https://openreview.net/profile?id=~Li_Jian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Li_Jian1">Li Jian</a>, <a href="https://openreview.net/profile?id=~Nenghai_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nenghai_Yu1">Nenghai Yu</a>, <a href="https://openreview.net/profile?id=~Tie-Yan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tie-Yan_Liu1">Tie-Yan Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 20 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#_TM6rT7tXke-details-595" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_TM6rT7tXke-details-595"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, auxiliary task, representation learning, contrastive learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recently, various auxiliary tasks have been proposed to accelerate representation learning and improve sample efficiency in deep reinforcement learning (RL). However, existing auxiliary tasks do not take the characteristics of RL problems into consideration and are unsupervised. By leveraging returns, the most important feedback signals in RL, we propose a novel auxiliary task that forces the learnt representations to discriminate state-action pairs with different returns. Our auxiliary loss is theoretically justified to learn representations that capture the structure of a new form of state-action abstraction, under which state-action pairs with similar return distributions are aggregated together. Empirically, our algorithm outperforms strong baselines on complex tasks in Atari games and DeepMind Control suite, and achieves even better performance when combined with existing auxiliary tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a novel contrastive learning based auxiliary task which forces the learnt representations to discriminate state-action pairs with different returns and achieve superior performance on complex tasks in terms of sample effiency.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=_TM6rT7tXke&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="4qR3coiNaIv" data-number="846">
      <h4>
        <a href="https://openreview.net/forum?id=4qR3coiNaIv">
            Scalable Bayesian Inverse Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=4qR3coiNaIv" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alex_James_Chan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_James_Chan1">Alex James Chan</a>, <a href="https://openreview.net/profile?id=~Mihaela_van_der_Schaar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihaela_van_der_Schaar2">Mihaela van der Schaar</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#4qR3coiNaIv-details-177" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4qR3coiNaIv-details-177"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Bayesian, Inverse reinforcement learning, Imitation Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Bayesian inference over the reward presents an ideal solution to the ill-posed nature of the inverse reinforcement learning problem. Unfortunately current methods generally do not scale well beyond the small tabular setting due to the need for an inner-loop MDP solver, and even non-Bayesian methods that do themselves scale often require extensive interaction with the environment to perform well, being inappropriate for high stakes or costly applications such as healthcare. In this paper we introduce our method, Approximate Variational Reward Imitation Learning (AVRIL), that addresses both of these issues by jointly learning an approximate posterior distribution over the reward that scales to arbitrarily complicated state spaces alongside an appropriate policy in a completely offline manner through a variational approach to said latent reward. Applying our method to real medical data alongside classic control simulations, we demonstrate Bayesian reward inference in environments beyond the scope of current methods, as well as task performance competitive with focused offline imitation learning algorithms.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A variational inference approach to Bayesian inverse reinforcement learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="QO9-y8also-" data-number="621">
      <h4>
        <a href="https://openreview.net/forum?id=QO9-y8also-">
            Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=QO9-y8also-" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Judy_Borowski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Judy_Borowski1">Judy Borowski</a>, <a href="https://openreview.net/profile?id=~Roland_Simon_Zimmermann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roland_Simon_Zimmermann1">Roland Simon Zimmermann</a>, <a href="https://openreview.net/profile?email=judith-schepers%40web.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="judith-schepers@web.de">Judith Schepers</a>, <a href="https://openreview.net/profile?id=~Robert_Geirhos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Robert_Geirhos1">Robert Geirhos</a>, <a href="https://openreview.net/profile?id=~Thomas_S._A._Wallis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_S._A._Wallis1">Thomas S. A. Wallis</a>, <a href="https://openreview.net/profile?id=~Matthias_Bethge1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthias_Bethge1">Matthias Bethge</a>, <a href="https://openreview.net/profile?id=~Wieland_Brendel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wieland_Brendel1">Wieland Brendel</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#QO9-y8also--details-27" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QO9-y8also--details-27"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">evaluation of interpretability, feature visualization, activation maximization, human psychophysics, understanding CNNs, explanation method</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Feature visualizations such as synthetic maximally activating images are a widely used explanation method to better understand the information processing of convolutional neural networks (CNNs). At the same time, there are concerns that these visualizations might not accurately represent CNNs' inner workings. Here, we measure how much extremely activating images help humans to predict CNN activations.
      Using a well-controlled psychophysical paradigm, we compare the informativeness of synthetic images by Olah et al. (2017) with a simple baseline visualization, namely exemplary natural images that also strongly activate a specific feature map. Given either synthetic or natural reference images, human participants choose which of two query images leads to strong positive activation. The experiment is designed to maximize participants' performance, and is the first to probe intermediate instead of final layer representations. We find that synthetic images indeed provide helpful information about feature map activations (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="259" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c38"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cB1"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c34"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>82</mn><mo>±</mo><mn>4</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> accuracy; chance would be <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="260" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c30"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>50</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container>). However, natural images --- originally intended to be a baseline --- outperform these synthetic images by a wide margin (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="261" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c39"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cB1"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>92</mn><mo>±</mo><mn>2</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container>). Additionally, participants are faster and more confident for natural images, whereas subjective impressions about the interpretability of the feature visualizations by Olah et al. (2017) are mixed. The higher informativeness of natural images holds across most layers, for both expert and lay participants as well as for hand- and randomly-picked feature visualizations. Even if only a single reference image is given, synthetic images provide less information than natural images (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="262" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cB1"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>65</mn><mo>±</mo><mn>5</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> vs. <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="263" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cB1"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c34"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>73</mn><mo>±</mo><mn>4</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container>). In summary, synthetic images from a popular feature visualization method are significantly less informative for assessing CNN activations than natural images. We argue that visualization methods should improve over this simple baseline.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Using human psychophysical experiments, we show that natural images can be significantly more informative for interpreting neural network activations than a state-of-the-art synthetic feature visualization.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=QO9-y8also-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="kE3vd639uRW" data-number="3017">
      <h4>
        <a href="https://openreview.net/forum?id=kE3vd639uRW">
            LiftPool: Bidirectional ConvNet Pooling
        </a>
      
        
          <a href="https://openreview.net/pdf?id=kE3vd639uRW" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jiaojiao_Zhao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiaojiao_Zhao2">Jiaojiao Zhao</a>, <a href="https://openreview.net/profile?id=~Cees_G._M._Snoek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cees_G._M._Snoek1">Cees G. M. Snoek</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#kE3vd639uRW-details-91" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kE3vd639uRW-details-91"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">bidirectional, pooling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Pooling is a critical operation in convolutional neural networks for increasing receptive fields and improving robustness to input variations. Most existing pooling operations downsample the feature maps,  which is a lossy process.   Moreover, they are not invertible: upsampling a downscaled feature map can not recover the lost information in the downsampling.  By adopting the philosophy of the classical Lifting Scheme from signal processing, we propose LiftPool for bidirectional pooling layers, including LiftDownPool and LiftUpPool.  LiftDownPool decomposes a feature map into various downsized sub-bands,  each of which contains information with different frequencies. As the pooling function in LiftDownPool is perfectly invertible, by performing LiftDownPool backward, a corresponding up-pooling layer LiftUpPool is able to generate a refined upsampled feature map using the detail subbands, which is useful for image-to-image translation challenges.  Experiments show the proposed methods achieve better results on image classification and semantic segmentation,  using various backbones. Moreover, LiftDownPool offers better robustness to input corruptions and perturbations.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="eLfqMl3z3lq" data-number="1676">
      <h4>
        <a href="https://openreview.net/forum?id=eLfqMl3z3lq">
            Adversarial score matching and improved sampling for image generation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=eLfqMl3z3lq" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alexia_Jolicoeur-Martineau1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexia_Jolicoeur-Martineau1">Alexia Jolicoeur-Martineau</a>, <a href="https://openreview.net/profile?email=remi.piche-taillefer%40umontreal.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="remi.piche-taillefer@umontreal.ca">Rémi Piché-Taillefer</a>, <a href="https://openreview.net/profile?id=~Ioannis_Mitliagkas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ioannis_Mitliagkas1">Ioannis Mitliagkas</a>, <a href="https://openreview.net/profile?id=~Remi_Tachet_des_Combes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Remi_Tachet_des_Combes1">Remi Tachet des Combes</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#eLfqMl3z3lq-details-583" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eLfqMl3z3lq-details-583"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">adversarial, score matching, Langevin dynamics, GAN, generative model</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Denoising Score Matching with Annealed Langevin Sampling (DSM-ALS) has recently found success in generative modeling. The approach works by first training a neural network to estimate the score of a distribution, and then using Langevin dynamics to sample from the data distribution assumed by the score network. Despite the convincing visual quality of samples, this method appears to perform worse than Generative Adversarial Networks (GANs) under the Fréchet Inception Distance, a standard metric for generative models. We show that this apparent gap vanishes when denoising the final Langevin samples using the score network.
      In addition, we propose two improvements to DSM-ALS:  1) Consistent Annealed Sampling as a more stable alternative to Annealed Langevin Sampling, and 2) a hybrid training formulation, composed of both Denoising Score Matching and adversarial objectives. By combining these two techniques and exploring different network architectures, we elevate score matching methods and obtain results competitive with state-of-the-art image generation on CIFAR-10.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Combining GANs with score matching and using Consistent Sampling (as an alternative to Langevin dynamics) for improved generative modeling</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=eLfqMl3z3lq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Qun8fv4qSby" data-number="3261">
      <h4>
        <a href="https://openreview.net/forum?id=Qun8fv4qSby">
            Transient Non-stationarity and Generalisation in Deep Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Qun8fv4qSby" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Maximilian_Igl1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maximilian_Igl1">Maximilian Igl</a>, <a href="https://openreview.net/profile?id=~Gregory_Farquhar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gregory_Farquhar1">Gregory Farquhar</a>, <a href="https://openreview.net/profile?id=~Jelena_Luketina1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jelena_Luketina1">Jelena Luketina</a>, <a href="https://openreview.net/profile?id=~Wendelin_Boehmer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wendelin_Boehmer1">Wendelin Boehmer</a>, <a href="https://openreview.net/profile?id=~Shimon_Whiteson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shimon_Whiteson1">Shimon Whiteson</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Qun8fv4qSby-details-525" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Qun8fv4qSby-details-525"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement Learning, Generalization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Non-stationarity can arise in Reinforcement Learning (RL) even in stationary environments. For example, most RL algorithms collect new data throughout training, using a non-stationary behaviour policy. Due to the transience of this non-stationarity, it is often not explicitly addressed in deep RL and a single neural network is continually updated. However, we find evidence that neural networks exhibit a memory effect, where these transient non-stationarities can permanently impact the latent representation and adversely affect generalisation performance. Consequently, to improve generalisation of deep RL agents, we propose Iterated Relearning (ITER). ITER augments standard RL training by repeated knowledge transfer of the current policy into a freshly initialised network, which thereby experiences less non-stationarity during training. Experimentally, we show that ITER improves performance on the challenging generalisation benchmarks ProcGen and Multiroom.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We find that transient non-stationarity can worsen generalization in reinforcement learning and propose a method to overcome this effeect.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Qun8fv4qSby&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="rq_Qr0c1Hyo" data-number="3269">
      <h4>
        <a href="https://openreview.net/forum?id=rq_Qr0c1Hyo">
            On the Origin of Implicit Regularization in Stochastic Gradient Descent
        </a>
      
        
          <a href="https://openreview.net/pdf?id=rq_Qr0c1Hyo" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Samuel_L_Smith1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samuel_L_Smith1">Samuel L Smith</a>, <a href="https://openreview.net/profile?id=~Benoit_Dherin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benoit_Dherin1">Benoit Dherin</a>, <a href="https://openreview.net/profile?id=~David_Barrett1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Barrett1">David Barrett</a>, <a href="https://openreview.net/profile?id=~Soham_De2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soham_De2">Soham De</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#rq_Qr0c1Hyo-details-262" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rq_Qr0c1Hyo-details-262"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">SGD, learning rate, batch size, optimization, generalization, implicit regularization, backward error analysis, SDE, stochastic differential equation, ODE, ordinary differential equation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">For small finite learning rates, the iterates of Random Shuffling SGD stay close to the path of gradient flow on a modified loss function containing an implicit regularizer.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-qh0M9XWxnv" data-number="3251">
      <h4>
        <a href="https://openreview.net/forum?id=-qh0M9XWxnv">
            Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-qh0M9XWxnv" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Muhammet_Balcilar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Muhammet_Balcilar1">Muhammet Balcilar</a>, <a href="https://openreview.net/profile?email=guillaume.renton%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="guillaume.renton@gmail.com">Guillaume Renton</a>, <a href="https://openreview.net/profile?email=pierre.heroux%40univ-rouen.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="pierre.heroux@univ-rouen.fr">Pierre Héroux</a>, <a href="https://openreview.net/profile?email=benoit.gauzere%40insa-rouen.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="benoit.gauzere@insa-rouen.fr">Benoit Gaüzère</a>, <a href="https://openreview.net/profile?id=~S%C3%A9bastien_Adam1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sébastien_Adam1">Sébastien Adam</a>, <a href="https://openreview.net/profile?id=~Paul_Honeine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Honeine1">Paul Honeine</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-qh0M9XWxnv-details-848" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-qh0M9XWxnv-details-848"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Graph Neural Networks, Spectral Graph Filter, Spectral Analysis</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In the recent literature of Graph Neural Networks (GNN), the expressive power of models has been studied through their capability to distinguish if two given graphs are isomorphic or not. Since the graph isomorphism problem is NP-intermediate, and Weisfeiler-Lehman (WL) test can give sufficient but not enough evidence in polynomial time, the theoretical power of GNNs is usually evaluated by the equivalence of WL-test order, followed by an empirical analysis of the models on some reference inductive and transductive datasets. However, such analysis does not account the signal processing pipeline, whose capability is generally evaluated in the spectral domain. In this paper, we argue that a spectral analysis of GNNs behavior can provide a complementary point of view to go one step further in the understanding of GNNs. By bridging the gap between the spectral and spatial design of graph convolutions, we theoretically demonstrate some equivalence of the graph convolution process regardless it is designed in the spatial or the spectral domain. Using this connection, we managed to re-formulate most of the state-of-the-art graph neural networks into one common framework. This general framework allows to lead a spectral analysis of the most popular GNNs, explaining their performance and showing their limits according to spectral point of view. Our theoretical spectral analysis is confirmed by experiments on various graph databases. Furthermore, we demonstrate the necessity of high and/or band-pass filters on a graph dataset, while the majority of GNN is limited to only low-pass and inevitably it fails.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper aims to analyse of the expressive power of Graph Neural Network in spectral domain.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Ig-VyQc-MLK" data-number="5">
      <h4>
        <a href="https://openreview.net/forum?id=Ig-VyQc-MLK">
            Pruning Neural Networks at Initialization: Why Are We Missing the Mark?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Ig-VyQc-MLK" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jonathan_Frankle1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Frankle1">Jonathan Frankle</a>, <a href="https://openreview.net/profile?id=~Gintare_Karolina_Dziugaite1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gintare_Karolina_Dziugaite1">Gintare Karolina Dziugaite</a>, <a href="https://openreview.net/profile?id=~Daniel_Roy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Roy1">Daniel Roy</a>, <a href="https://openreview.net/profile?id=~Michael_Carbin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Carbin1">Michael Carbin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 22 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Ig-VyQc-MLK-details-231" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ig-VyQc-MLK-details-231"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Pruning, Sparsity, Lottery Ticket, Science</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent work has explored the possibility of pruning neural networks at initialization. We assess proposals for doing so: SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), and magnitude pruning. Although these methods surpass the trivial baseline of random pruning, they remain below the accuracy of magnitude pruning after training, and we endeavor to understand why. We show that, unlike pruning after training, randomly shuffling the weights these methods prune within each layer or sampling new initial values preserves or improves accuracy. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune. This property suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Methods for pruning neural nets at initialization perform the same or better when shuffling or reinitializing the weights they prune in each layer, a way in which they differ from SOTA weight-pruning methods after training.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ig-VyQc-MLK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="2CjEVW-RGOJ" data-number="2989">
      <h4>
        <a href="https://openreview.net/forum?id=2CjEVW-RGOJ">
            SkipW: Resource Adaptable RNN with Strict Upper Computational Limit
        </a>
      
        
          <a href="https://openreview.net/pdf?id=2CjEVW-RGOJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=mayet.tsiry%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="mayet.tsiry@gmail.com">Tsiry Mayet</a>, <a href="https://openreview.net/profile?email=anne.lambert%40interdigital.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="anne.lambert@interdigital.com">Anne Lambert</a>, <a href="https://openreview.net/profile?email=pascal.leguyadec%40interdigital.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="pascal.leguyadec@interdigital.com">Pascal Leguyadec</a>, <a href="https://openreview.net/profile?email=francoise.lebolzer%40interdigital.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="francoise.lebolzer@interdigital.com">Francoise Le Bolzer</a>, <a href="https://openreview.net/profile?id=~Fran%C3%A7ois_Schnitzler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~François_Schnitzler1">François Schnitzler</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#2CjEVW-RGOJ-details-201" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2CjEVW-RGOJ-details-201"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Recurrent neural networks, Flexibility, Computational resources</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We introduce Skip-Window, a method to allow recurrent neural networks (RNNs) to trade off accuracy for computational cost during the analysis of a sequence. Similarly to existing approaches, Skip-Window extends existing RNN cells by adding a mechanism to encourage the model to process fewer inputs. Unlike existing approaches, Skip-Window is able to respect a strict computational budget, making this model more suitable for limited hardware. We evaluate this approach on two datasets: a human activity recognition task and adding task. Our results show that Skip-Window is able to exceed the accuracy of existing approaches for a lower computational cost while strictly limiting said cost.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Skip-Window is a method to allow recurrent neural networks (RNNs) to trade off accuracy for computational cost during the analysis of a sequence while keeping a strict upper computational limit.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="MDsQkFP1Aw" data-number="2533">
      <h4>
        <a href="https://openreview.net/forum?id=MDsQkFP1Aw">
            Into the Wild with AudioScope: Unsupervised Audio-Visual Separation of On-Screen Sounds
        </a>
      
        
          <a href="https://openreview.net/pdf?id=MDsQkFP1Aw" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Efthymios_Tzinis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Efthymios_Tzinis1">Efthymios Tzinis</a>, <a href="https://openreview.net/profile?id=~Scott_Wisdom1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Scott_Wisdom1">Scott Wisdom</a>, <a href="https://openreview.net/profile?email=arenjansen%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="arenjansen@google.com">Aren Jansen</a>, <a href="https://openreview.net/profile?email=shershey%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="shershey@google.com">Shawn Hershey</a>, <a href="https://openreview.net/profile?email=talremez%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="talremez@google.com">Tal Remez</a>, <a href="https://openreview.net/profile?id=~Dan_Ellis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Ellis1">Dan Ellis</a>, <a href="https://openreview.net/profile?id=~John_R._Hershey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_R._Hershey1">John R. Hershey</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>19 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#MDsQkFP1Aw-details-394" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MDsQkFP1Aw-details-394"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Audio-visual sound separation, in-the-wild data, unsupervised learning, self-supervised learning, universal sound separation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent progress in deep learning has enabled many advances in sound separation and visual scene understanding. However, extracting sound sources which are apparent in natural videos remains an open problem. In this work, we present AudioScope, a novel audio-visual sound separation framework that can be trained without supervision to isolate on-screen sound sources from real in-the-wild videos. Prior audio-visual separation work assumed artificial limitations on the domain of sound classes (e.g., to speech or music), constrained the number of sources, and required strong sound separation or visual segmentation labels. AudioScope overcomes these limitations, operating on an open domain of sounds, with variable numbers of sources, and without labels or prior visual segmentation.  The training procedure for AudioScope uses mixture invariant training (MixIT) to separate synthetic mixtures of mixtures (MoMs) into individual sources, where noisy labels for mixtures are provided by an unsupervised audio-visual coincidence model. Using the noisy labels, along with attention between video and audio features, AudioScope learns to identify audio-visual similarity and to suppress off-screen sounds. We demonstrate the effectiveness of our approach using a dataset of video clips extracted from open-domain YFCC100m video data. This dataset contains a wide diversity of sound classes recorded in unconstrained conditions, making the application of previous methods unsuitable. For evaluation and semi-supervised experiments, we collected human labels for presence of on-screen and off-screen sounds on a small subset of clips.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an open-domain unsupervised audio-visual on-screen separation system trained and tested on in-the-wild videos.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=MDsQkFP1Aw&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Qr0aRliE_Hb" data-number="3080">
      <h4>
        <a href="https://openreview.net/forum?id=Qr0aRliE_Hb">
            Simple Augmentation Goes a Long Way: ADRL for DNN Quantization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Qr0aRliE_Hb" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Lin_Ning1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lin_Ning1">Lin Ning</a>, <a href="https://openreview.net/profile?id=~Guoyang_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guoyang_Chen1">Guoyang Chen</a>, <a href="https://openreview.net/profile?email=weifeng.z%40alibaba-inc.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="weifeng.z@alibaba-inc.com">Weifeng Zhang</a>, <a href="https://openreview.net/profile?id=~Xipeng_Shen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xipeng_Shen1">Xipeng Shen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Qr0aRliE_Hb-details-70" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Qr0aRliE_Hb-details-70"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement Learning, Quantization, mixed precision, augmented deep reinforcement learning, DNN</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Mixed precision quantization improves DNN performance by assigning different layers with different bit-width values. Searching for the optimal bit-width for each layer, however, remains a challenge. Deep Reinforcement Learning (DRL) shows some recent promise. It however suffers instability due to function approximation errors, causing large variances in the early training stages, slow convergence, and suboptimal policies in the mixed-precision quantization problem. This paper proposes augmented DRL (ADRL) as a way to alleviate these issues. This new strategy augments the neural networks in DRL with a complementary scheme to boost the performance of learning. The paper examines the effectiveness of ADRL both analytically and empirically, showing that it can produce more accurate quantized models than the state of the art DRL-based quantization while improving the learning speed by 4.5-64 times. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Augments the neural networks in Deep Reinforcement Learning(DRL) with a complementary scheme to boost the performance of learning and solve the common low convergence problem in the early stage of DRL</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="bJxgv5C3sYc" data-number="788">
      <h4>
        <a href="https://openreview.net/forum?id=bJxgv5C3sYc">
            Few-Shot Bayesian Optimization with Deep Kernel Surrogates
        </a>
      
        
          <a href="https://openreview.net/pdf?id=bJxgv5C3sYc" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Martin_Wistuba1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Wistuba1">Martin Wistuba</a>, <a href="https://openreview.net/profile?id=~Josif_Grabocka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Josif_Grabocka1">Josif Grabocka</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#bJxgv5C3sYc-details-366" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bJxgv5C3sYc-details-366"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">bayesian optimization, metalearning, few-shot learning, automl</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Hyperparameter optimization (HPO) is a central pillar in the automation of machine learning solutions and is mainly performed via Bayesian optimization, where a parametric surrogate is learned to approximate the black box response function (e.g. validation error). Unfortunately, evaluating the response function is computationally intensive. As a remedy, earlier work emphasizes the need for transfer learning surrogates which learn to optimize hyperparameters for an algorithm from other tasks. In contrast to previous work, we propose to rethink HPO as a few-shot learning problem in which we train a shared deep surrogate model to quickly adapt (with few response evaluations) to the response function of a new task. We propose the use of a deep kernel network for a Gaussian process surrogate that is meta-learned in an end-to-end fashion in order to jointly approximate the response functions of a collection of training data sets. As a result, the novel few-shot optimization of our deep kernel surrogate leads to new state-of-the-art results at HPO compared to several recent methods on diverse metadata sets.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Model-agnostic meta-learning meets Bayesian optimization to speed-up hyperparameter optimization by learning on metadata from different data sets.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Drynvt7gg4L" data-number="1751">
      <h4>
        <a href="https://openreview.net/forum?id=Drynvt7gg4L">
            AdaSpeech: Adaptive Text to Speech for Custom Voice
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Drynvt7gg4L" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=t-miche%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="t-miche@microsoft.com">Mingjian Chen</a>, <a href="https://openreview.net/profile?id=~Xu_Tan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xu_Tan1">Xu Tan</a>, <a href="https://openreview.net/profile?email=bohan.li%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="bohan.li@microsoft.com">Bohan Li</a>, <a href="https://openreview.net/profile?email=yanqliu%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yanqliu@microsoft.com">Yanqing Liu</a>, <a href="https://openreview.net/profile?id=~Tao_Qin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Qin1">Tao Qin</a>, <a href="https://openreview.net/profile?id=~sheng_zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~sheng_zhao1">sheng zhao</a>, <a href="https://openreview.net/profile?id=~Tie-Yan_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tie-Yan_Liu1">Tie-Yan Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Drynvt7gg4L-details-267" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Drynvt7gg4L-details-267"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Text to speech, adaptation, fine-tuning, custom voice, acoustic condition modeling, conditional layer normalization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Custom voice, a specific text to speech (TTS) service in commercial speech platforms, aims to adapt a source TTS model to synthesize personal voice for a target speaker using few speech from her/him. Custom voice presents two unique challenges for TTS adaptation: 1) to support diverse customers, the adaptation model needs to handle diverse acoustic conditions which could be very different from source speech data, and 2) to support a large number of customers, the adaptation parameters need to be small enough for each target speaker to reduce memory usage while maintaining high voice quality. In this work, we propose AdaSpeech, an adaptive TTS system for high-quality and efficient customization of new voices. We design several techniques in AdaSpeech to address the two challenges in custom voice: 1) To handle different acoustic conditions, we model the acoustic information in both utterance and phoneme level. Specifically, we use one acoustic encoder to extract an utterance-level vector and another one to extract a sequence of phoneme-level vectors from the target speech during pre-training and fine-tuning; in inference, we extract the utterance-level vector from a reference speech and use an acoustic predictor to predict the phoneme-level vectors. 2) To better trade off the adaptation parameters and voice quality, we introduce conditional layer normalization in the mel-spectrogram decoder of AdaSpeech, and fine-tune this part in addition to speaker embedding for adaptation. We pre-train the source TTS model on LibriTTS datasets and fine-tune it on VCTK and LJSpeech datasets (with different acoustic conditions from LibriTTS) with few adaptation data, e.g., 20 sentences, about 1 minute speech. Experiment results show that AdaSpeech achieves much better adaptation quality than baseline methods, with only about 5K specific parameters for each speaker, which demonstrates its effectiveness for custom voice. The audio samples are available at https://speechresearch.github.io/adaspeech/.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose AdaSpeech, an adaptive TTS system for high-quality and efficient adaptation of new speaker in custom voice.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="TNkPBBYFkXg" data-number="2764">
      <h4>
        <a href="https://openreview.net/forum?id=TNkPBBYFkXg">
            HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients
        </a>
      
        
          <a href="https://openreview.net/pdf?id=TNkPBBYFkXg" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Enmao_Diao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Enmao_Diao1">Enmao Diao</a>, <a href="https://openreview.net/profile?id=~Jie_Ding2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jie_Ding2">Jie Ding</a>, <a href="https://openreview.net/profile?id=~Vahid_Tarokh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vahid_Tarokh1">Vahid Tarokh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#TNkPBBYFkXg-details-99" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TNkPBBYFkXg-details-99"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Federated Learning, Internet of Things, Heterogeneity</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Federated Learning (FL) is a method of training machine learning models on private data distributed over a large number of possibly heterogeneous clients such as mobile phones and IoT devices. In this work, we propose a new federated learning framework named HeteroFL to address heterogeneous clients equipped with very different computation and communication capabilities. Our solution can enable the training of heterogeneous local models with varying computation complexities and still produce a single global inference model. For the first time, our method challenges the underlying assumption of existing work that local models have to share the same architecture as the global model. We demonstrate several strategies to enhance FL training and conduct extensive empirical evaluations, including five computation complexity levels of three model architecture on three datasets. We show that adaptively distributing subnetworks according to clients' capabilities is both computation and communication efficient.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">In this work, we propose a new federated learning framework named HeteroFL to train heterogeneous local models with varying computation complexities.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=TNkPBBYFkXg&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="WAISmwsqDsb" data-number="509">
      <h4>
        <a href="https://openreview.net/forum?id=WAISmwsqDsb">
            DINO: A Conditional Energy-Based GAN for Domain Translation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=WAISmwsqDsb" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Konstantinos_Vougioukas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Konstantinos_Vougioukas1">Konstantinos Vougioukas</a>, <a href="https://openreview.net/profile?id=~Stavros_Petridis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stavros_Petridis1">Stavros Petridis</a>, <a href="https://openreview.net/profile?id=~Maja_Pantic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maja_Pantic1">Maja Pantic</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#WAISmwsqDsb-details-642" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WAISmwsqDsb-details-642"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Generative Modelling, Domain Translation, Conditional GANs, Energy-Based GANs</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Domain translation is the process of transforming data from one domain to another while preserving the common semantics. Some of the most popular domain translation systems are based on conditional generative adversarial networks, which use source domain data to drive the generator and as an input to the discriminator. However, this approach does not enforce the preservation of shared semantics since the conditional input can often be ignored by the discriminator. We propose an alternative method for conditioning and present a new framework, where two networks are simultaneously trained, in a supervised manner, to perform domain translation in opposite directions. Our method is not only better at capturing the shared information between two domains but is more generic and can be applied to a broader range of problems. The proposed framework performs well even in challenging cross-modal translations, such as video-driven speech reconstruction, for which other systems struggle to maintain correspondence.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A framework  for domain translation which uses a novel mechanism for conditioning energy-based GANs.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=WAISmwsqDsb&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="N33d7wjgzde" data-number="203">
      <h4>
        <a href="https://openreview.net/forum?id=N33d7wjgzde">
            Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=N33d7wjgzde" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tsung-Wei_Ke2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tsung-Wei_Ke2">Tsung-Wei Ke</a>, <a href="https://openreview.net/profile?id=~Jyh-Jing_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jyh-Jing_Hwang1">Jyh-Jing Hwang</a>, <a href="https://openreview.net/profile?id=~Stella_Yu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stella_Yu2">Stella Yu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#N33d7wjgzde-details-473" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="N33d7wjgzde-details-473"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">weakly supervised representation learning, representation learning for computer vision, metric learning, semantic segmentation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Weakly supervised segmentation is challenging as sparsely labeled pixels do not provide sufficient supervision:  A semantic segment may contain multiple distinctive regions whereas adjacent segments may appear similar.  Common approaches use the few labeled pixels in all training images to train a segmentation model, and then propagate labels within each image based on visual or feature similarity.  Instead, we treat segmentation as a semi-supervised pixel-wise metric learning problem, where pixels in different segments are mapped to distinctive features.   Naturally, our unlabeled pixels participate not only in data-driven grouping within each image, but also in discriminative feature learning within and across images.  Our results on Pascal VOC and DensePose datasets demonstrate our substantial accuracy gain on various forms of weak supervision including image-level tags, bounding boxes, labeled points, and scribbles.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a unified pixel-to-segment contrastive learning loss formulation for weakly supervised semantic segmentation with various types of annotations. </span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="8X2eaSZxTP" data-number="1636">
      <h4>
        <a href="https://openreview.net/forum?id=8X2eaSZxTP">
            PC2WF: 3D Wireframe Reconstruction from Raw Point Clouds
        </a>
      
        
          <a href="https://openreview.net/pdf?id=8X2eaSZxTP" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yujia_Liu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yujia_Liu3">Yujia Liu</a>, <a href="https://openreview.net/profile?id=~Stefano_D%26%23x27%3BAronco1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefano_D&#39;Aronco1">Stefano D'Aronco</a>, <a href="https://openreview.net/profile?id=~Konrad_Schindler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Konrad_Schindler1">Konrad Schindler</a>, <a href="https://openreview.net/profile?id=~Jan_Dirk_Wegner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jan_Dirk_Wegner1">Jan Dirk Wegner</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#8X2eaSZxTP-details-279" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8X2eaSZxTP-details-279"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep neural network, 3d point cloud, wireframe model</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We introduce PC2WF, the first end-to-end trainable deep network architecture to convert a 3D point cloud into a wireframe model. The network takes as input an unordered set of 3D points sampled from the surface of some object, and outputs a wireframe of that object, i.e., a sparse set of corner points linked by line segments. Recovering the wireframe is a challenging task, where the numbers of both vertices and edges are different for every instance, and a-priori unknown. Our architecture gradually builds up the model: It starts by encoding the points into feature vectors. Based on those features, it identifies a pool of candidate vertices, then prunes those candidates to a final set of corner vertices and refines their locations. Next, the corners are linked with an exhaustive set of candidate edges, which is again pruned to obtain the final wireframe. All steps are trainable, and errors can be backpropagated through the entire sequence. We validate the proposed model on a publicly available synthetic dataset, for which the ground truth wireframes are accessible, as well as on a new real-world dataset. Our model produces wireframe abstractions of good quality and outperforms several baselines.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">An end-to-end trainable deep neural network for converting a 3D point cloud into a wireframe model.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=8X2eaSZxTP&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="KtH8W3S_RE" data-number="1324">
      <h4>
        <a href="https://openreview.net/forum?id=KtH8W3S_RE">
            Multi-resolution modeling of a discrete stochastic process identifies causes of cancer
        </a>
      
        
          <a href="https://openreview.net/pdf?id=KtH8W3S_RE" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Adam_Uri_Yaari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adam_Uri_Yaari1">Adam Uri Yaari</a>, <a href="https://openreview.net/profile?id=~Maxwell_Sherman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maxwell_Sherman1">Maxwell Sherman</a>, <a href="https://openreview.net/profile?id=~Oliver_Clarke_Priebe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Oliver_Clarke_Priebe1">Oliver Clarke Priebe</a>, <a href="https://openreview.net/profile?id=~Po-Ru_Loh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Po-Ru_Loh1">Po-Ru Loh</a>, <a href="https://openreview.net/profile?id=~Boris_Katz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boris_Katz1">Boris Katz</a>, <a href="https://openreview.net/profile?id=~Andrei_Barbu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrei_Barbu3">Andrei Barbu</a>, <a href="https://openreview.net/profile?id=~Bonnie_Berger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bonnie_Berger1">Bonnie Berger</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#KtH8W3S_RE-details-350" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KtH8W3S_RE-details-350"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Computational Biology, non-stationary stochastic processes, cancer research, deep learning, probabelistic models, graphical models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Detection of cancer-causing mutations within the vast and mostly unexplored human genome is a major challenge. Doing so requires modeling the background mutation rate, a highly non-stationary stochastic process, across regions of interest varying in size from one to millions of positions. Here, we present the split-Poisson-Gamma (SPG) distribution, an extension of the classical Poisson-Gamma formulation, to model a discrete stochastic process at multiple resolutions. We demonstrate that the probability model has a closed-form posterior, enabling efficient and accurate linear-time prediction over any length scale after the parameters of the model have been inferred a single time. We apply our framework to model mutation rates in tumors and show that model parameters can be accurately inferred from high-dimensional epigenetic data using a convolutional neural network, Gaussian process, and maximum-likelihood estimation. Our method is both more accurate and more efficient than existing models over a large range of length scales. We demonstrate the usefulness of multi-resolution modeling by detecting genomic elements that drive tumor emergence and are of vastly differing sizes.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We integrate a deep learning framework with a probabilistic model to learn a discrete stochastic process at arbitrary length scales, the method accurately and efficiently model mutations load in a tumor and detect cancer driver mutations genome-wide</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=KtH8W3S_RE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="W3Wf_wKmqm9" data-number="2177">
      <h4>
        <a href="https://openreview.net/forum?id=W3Wf_wKmqm9">
            C-Learning: Horizon-Aware Cumulative Accessibility Estimation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=W3Wf_wKmqm9" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=panteha%40layer6.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="panteha@layer6.ai">Panteha Naderian</a>, <a href="https://openreview.net/profile?id=~Gabriel_Loaiza-Ganem1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gabriel_Loaiza-Ganem1">Gabriel Loaiza-Ganem</a>, <a href="https://openreview.net/profile?email=harry%40layer6.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="harry@layer6.ai">Harry J. Braviner</a>, <a href="https://openreview.net/profile?id=~Anthony_L._Caterini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anthony_L._Caterini1">Anthony L. Caterini</a>, <a href="https://openreview.net/profile?email=jesse%40layer6.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="jesse@layer6.ai">Jesse C. Cresswell</a>, <a href="https://openreview.net/profile?email=tong%40layer6.ai" class="profile-link" data-toggle="tooltip" data-placement="top" title="tong@layer6.ai">Tong Li</a>, <a href="https://openreview.net/profile?id=~Animesh_Garg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Animesh_Garg1">Animesh Garg</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>19 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#W3Wf_wKmqm9-details-945" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="W3Wf_wKmqm9-details-945"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, goal reaching, Q-learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Multi-goal reaching is an important problem in reinforcement learning needed to achieve algorithmic generalization. Despite recent advances in this field, current algorithms suffer from three major challenges: high sample complexity, learning only a single way of reaching the goals,  and difficulties in solving complex motion planning tasks. In order to address these limitations, we introduce the concept of cumulative accessibility functions, which measure the reachability of a goal from a given state within a specified horizon. We show that these functions obey a recurrence relation, which enables learning from offline interactions. We also prove that optimal cumulative accessibility functions are monotonic in the planning horizon. Additionally, our method can trade off speed and reliability in goal-reaching by suggesting multiple paths to a single goal depending on the provided horizon. We evaluate our approach on a set of multi-goal discrete and continuous control tasks. We show that our method outperforms state-of-the-art goal-reaching algorithms in success rate, sample complexity, and path optimality. Our code is available at https://github.com/layer6ai-labs/CAE, and additional visualizations can be found at https://sites.google.com/view/learning-cae/.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce C-learning, a Q-learning inspired method to learn horizon-dependent policies for goal reaching.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="vsU0efpivw" data-number="1113">
      <h4>
        <a href="https://openreview.net/forum?id=vsU0efpivw">
            Shapley Explanation Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=vsU0efpivw" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Rui_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rui_Wang1">Rui Wang</a>, <a href="https://openreview.net/profile?id=~Xiaoqian_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoqian_Wang1">Xiaoqian Wang</a>, <a href="https://openreview.net/profile?id=~David_I._Inouye1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_I._Inouye1">David I. Inouye</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#vsU0efpivw-details-23" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vsU0efpivw-details-23"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Shapley values, Feature Attribution, Interpretable Machine Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Shapley values have become one of the most popular feature attribution explanation methods. However, most prior work has focused on post-hoc Shapley explanations, which can be computationally demanding due to its exponential time complexity and preclude model regularization based on Shapley explanations during training. Thus, we propose to incorporate Shapley values themselves as latent representations in deep models thereby making Shapley explanations first-class citizens in the modeling paradigm. This intrinsic explanation approach enables layer-wise explanations, explanation regularization of the model during training, and fast explanation computation at test time. We define the Shapley transform that transforms the input into a Shapley representation given a specific function. We operationalize the Shapley transform as a neural network module and construct both shallow and deep networks, called ShapNets, by composing Shapley modules. We prove that our Shallow ShapNets compute the exact Shapley values and our Deep ShapNets maintain the missingness and accuracy properties of Shapley values. We demonstrate on synthetic and real-world datasets that our ShapNets enable layer-wise Shapley explanations, novel Shapley regularizations during training, and fast computation while maintaining reasonable performance. Code is available at https://github.com/inouye-lab/ShapleyExplanationNetworks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">To enable new capabilities, we propose to use Shapley values as inter-layer representations in deep neural networks rather than as post-hoc explanations.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="qbH974jKUVy" data-number="3567">
      <h4>
        <a href="https://openreview.net/forum?id=qbH974jKUVy">
            The role of Disentanglement in Generalisation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=qbH974jKUVy" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Milton_Llera_Montero1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Milton_Llera_Montero1">Milton Llera Montero</a>, <a href="https://openreview.net/profile?email=c.ludwig%40bristol.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="c.ludwig@bristol.ac.uk">Casimir JH Ludwig</a>, <a href="https://openreview.net/profile?id=~Rui_Ponte_Costa3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rui_Ponte_Costa3">Rui Ponte Costa</a>, <a href="https://openreview.net/profile?id=~Gaurav_Malhotra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gaurav_Malhotra1">Gaurav Malhotra</a>, <a href="https://openreview.net/profile?id=~Jeffrey_Bowers1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jeffrey_Bowers1">Jeffrey Bowers</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#qbH974jKUVy-details-489" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qbH974jKUVy-details-489"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">disentanglement, compositionality, compositional generalization, generalisation, generative models, variational autoencoders</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Combinatorial generalisation — the ability to understand and produce novel combinations of familiar elements — is a core capacity of human intelligence that current AI systems struggle with. Recently, it has been suggested that learning disentangled representations may help address this problem. It is claimed that such representations should be able to capture the compositional structure of the world which can then be combined to support combinatorial generalisation. In this study, we systematically tested how the degree of disentanglement affects various forms of generalisation, including two forms of combinatorial generalisation that varied in difficulty. We trained three classes of variational autoencoders (VAEs) on two datasets on an unsupervised task by excluding combinations of generative factors during training. At test time we ask the models to reconstruct the missing combinations in order to measure generalisation performance. Irrespective of the degree of disentanglement, we found that the models supported only weak combinatorial generalisation. We obtained the same outcome when we directly input perfectly disentangled representations as the latents, and when we tested a model on a more complex task that explicitly required independent generative factors to be controlled. While learning disentangled representations does improve interpretability and sample efficiency in some downstream tasks, our results suggest that they are not sufficient for supporting more difficult forms of generalisation.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Disentangled models do not achieve compositional generalization when tested systematically.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=qbH974jKUVy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="K9bw7vqp_s" data-number="255">
      <h4>
        <a href="https://openreview.net/forum?id=K9bw7vqp_s">
            Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch
        </a>
      
        
          <a href="https://openreview.net/pdf?id=K9bw7vqp_s" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Aojun_Zhou2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aojun_Zhou2">Aojun Zhou</a>, <a href="https://openreview.net/profile?id=~Yukun_Ma2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yukun_Ma2">Yukun Ma</a>, <a href="https://openreview.net/profile?email=junnan.zhu%40nlpr.ia.ac.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="junnan.zhu@nlpr.ia.ac.cn">Junnan Zhu</a>, <a href="https://openreview.net/profile?id=~Jianbo_Liu3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianbo_Liu3">Jianbo Liu</a>, <a href="https://openreview.net/profile?id=~Zhijie_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhijie_Zhang1">Zhijie Zhang</a>, <a href="https://openreview.net/profile?id=~Kun_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kun_Yuan1">Kun Yuan</a>, <a href="https://openreview.net/profile?id=~Wenxiu_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wenxiu_Sun1">Wenxiu Sun</a>, <a href="https://openreview.net/profile?id=~Hongsheng_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongsheng_Li3">Hongsheng Li</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#K9bw7vqp_s-details-732" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="K9bw7vqp_s-details-732"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">sparsity, efficient training and inference.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and
      decent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2× speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network’s topology change during the training process. Finally, We justify SR-STE’s advantages with SAD and demonstrate the effectiveness of SR-STE by performing
      comprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">a  simple  yet  universal  recipe  to  learn N:M sparse neural networks from scratch</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="XOjv2HxIF6i" data-number="930">
      <h4>
        <a href="https://openreview.net/forum?id=XOjv2HxIF6i">
            Unsupervised Meta-Learning through Latent-Space Interpolation in Generative Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=XOjv2HxIF6i" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Siavash_Khodadadeh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siavash_Khodadadeh1">Siavash Khodadadeh</a>, <a href="https://openreview.net/profile?email=sharare.zehtabian%40knights.ucf.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sharare.zehtabian@knights.ucf.edu">Sharare Zehtabian</a>, <a href="https://openreview.net/profile?id=~Saeed_Vahidian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Saeed_Vahidian1">Saeed Vahidian</a>, <a href="https://openreview.net/profile?email=wweijia%40eng.ucsd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="wweijia@eng.ucsd.edu">Weijia Wang</a>, <a href="https://openreview.net/profile?id=~Bill_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bill_Lin1">Bill Lin</a>, <a href="https://openreview.net/profile?id=~Ladislau_Boloni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ladislau_Boloni1">Ladislau Boloni</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#XOjv2HxIF6i-details-47" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XOjv2HxIF6i-details-47"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Meta-learning, Unsupervised learning, GANs</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Several recently proposed unsupervised meta-learning approaches rely on synthetic meta-tasks created using techniques such as random selection, clustering and/or augmentation. In this work, we describe a novel approach that generates meta-tasks using generative models. The proposed family of algorithms generate pairs of in-class and out-of-class samples from the latent space in a principled way, allowing us to create synthetic classes forming the training and validation data of a meta-task. We find that the proposed approach, LAtent Space Interpolation Unsupervised Meta-learning (LASIUM), outperforms or is competitive with current unsupervised learning baselines on few-shot classification tasks on the most widely used benchmark datasets. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We use interpolation in generative models latent space to generate tasks for unsupervised meta-learninig.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=XOjv2HxIF6i&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="7FNqrcPtieT" data-number="1052">
      <h4>
        <a href="https://openreview.net/forum?id=7FNqrcPtieT">
            On Data-Augmentation and Consistency-Based Semi-Supervised Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=7FNqrcPtieT" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Atin_Ghosh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Atin_Ghosh1">Atin Ghosh</a>, <a href="https://openreview.net/profile?id=~Alexandre_H._Thiery1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexandre_H._Thiery1">Alexandre H. Thiery</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#7FNqrcPtieT-details-461" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7FNqrcPtieT-details-461"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Semi-Supervised Learning, Regularization, Data augmentation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recently proposed consistency-based Semi-Supervised Learning (SSL) methods such as the Pi-model, temporal ensembling, the mean teacher, or the virtual adversarial training, achieve the state of the art results in several SSL tasks. These methods can typically reach performances that are comparable to their fully supervised counterparts while using only a fraction of labelled examples. Despite these methodological advances, the understanding of these methods is still relatively limited. To make progress, we analyse (variations of) the Pi-model in settings where analytically tractable results can be obtained. We establish links with Manifold Tangent Classifiers and demonstrate that the quality of the perturbations is key to obtaining reasonable SSL performances. Furthermore, we propose a simple extension of the Hidden Manifold Model that naturally incorporates data-augmentation schemes and offers a tractable framework for understanding SSL methods.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a simple and natural framework leveraging the Hidden Manifold Model to study modern SSL methods.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Ldau9eHU-qO" data-number="1279">
      <h4>
        <a href="https://openreview.net/forum?id=Ldau9eHU-qO">
            Learning from Demonstration with Weakly Supervised Disentanglement
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Ldau9eHU-qO" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yordan_Hristov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yordan_Hristov1">Yordan Hristov</a>, <a href="https://openreview.net/profile?id=~Subramanian_Ramamoorthy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Subramanian_Ramamoorthy1">Subramanian Ramamoorthy</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Ldau9eHU-qO-details-85" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ldau9eHU-qO-details-85"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">representation learning for robotics, physical symbol grounding, semi-supervised learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Robotic manipulation tasks, such as wiping with a soft sponge, require control from multiple rich sensory modalities. Human-robot interaction, aimed at teach- ing robots, is difficult in this setting as there is potential for mismatch between human and machine comprehension of the rich data streams. We treat the task of interpretable learning from demonstration as an optimisation problem over a probabilistic generative model. To account for the high-dimensionality of the data, a high-capacity neural network is chosen to represent the model. The latent variables in this model are explicitly aligned with high-level notions and concepts that are manifested in a set of demonstrations. We show that such alignment is best achieved through the use of labels from the end user, in an appropriately restricted vocabulary, in contrast to the conventional approach of the designer picking a prior over the latent variables. Our approach is evaluated in the context of two table-top robot manipulation tasks performed by a PR2 robot – that of dabbing liquids with a sponge (forcefully pressing a sponge and moving it along a surface) and pouring between different containers. The robot provides visual information, arm joint positions and arm joint efforts. We have made videos of the tasks and data available - see supplementary materials at: https://sites.google.com/view/weak-label-lfd.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a generative model-based approach to learning interpretable robot trajectory representations from demonstrations (image embeddings and end-effector trajectories) paired with coarse labels, which provide a form of weak supervision.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ldau9eHU-qO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="q_S44KLQ_Aa" data-number="1614">
      <h4>
        <a href="https://openreview.net/forum?id=q_S44KLQ_Aa">
            Neurally Augmented ALISTA
        </a>
      
        
          <a href="https://openreview.net/pdf?id=q_S44KLQ_Aa" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Freya_Behrens1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Freya_Behrens1">Freya Behrens</a>, <a href="https://openreview.net/profile?id=~Jonathan_Sauder2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Sauder2">Jonathan Sauder</a>, <a href="https://openreview.net/profile?id=~Peter_Jung2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peter_Jung2">Peter Jung</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#q_S44KLQ_Aa-details-941" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="q_S44KLQ_Aa-details-941"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">compressed sensing, sparse reconstruction, unrolled algorithms, learned ISTA</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value "> It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets.  In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce Neurally Augmented ALISTA, extending ALISTA to compute adaptive parameters to achieve improved recovery of individual sparse target vectors.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=q_S44KLQ_Aa&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="NcFEZOi-rLa" data-number="2256">
      <h4>
        <a href="https://openreview.net/forum?id=NcFEZOi-rLa">
            Shape or Texture: Understanding Discriminative Features in CNNs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=NcFEZOi-rLa" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Md_Amirul_Islam1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Md_Amirul_Islam1">Md Amirul Islam</a>, <a href="https://openreview.net/profile?id=~Matthew_Kowal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthew_Kowal1">Matthew Kowal</a>, <a href="https://openreview.net/profile?id=~Patrick_Esser1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Patrick_Esser1">Patrick Esser</a>, <a href="https://openreview.net/profile?id=~Sen_Jia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sen_Jia1">Sen Jia</a>, <a href="https://openreview.net/profile?id=~Bj%C3%B6rn_Ommer2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Björn_Ommer2">Björn Ommer</a>, <a href="https://openreview.net/profile?id=~Konstantinos_G._Derpanis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Konstantinos_G._Derpanis1">Konstantinos G. Derpanis</a>, <a href="https://openreview.net/profile?id=~Neil_Bruce1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neil_Bruce1">Neil Bruce</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>21 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#NcFEZOi-rLa-details-537" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NcFEZOi-rLa-details-537"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Shape, Texture, Shape Bias, Texture Bias, Shape Encoding, Mutual Information</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Contrasting the previous evidence that neurons in the later layers of a Convolutional Neural Network (CNN) respond to complex object shapes, recent studies have shown that CNNs actually exhibit a 'texture bias': given an image with both texture and shape cues (e.g., a stylized image), a CNN is biased towards predicting the category corresponding to the texture. However, these previous studies conduct experiments on the final classification output of the network, and fail to robustly evaluate the bias contained (i) in the latent representations, and (ii) on a per-pixel level. In this paper, we design a series of experiments that overcome these issues. We do this with the goal of better understanding what type of shape information contained in the network is discriminative, where shape information is encoded, as well as when the network learns about object shape during training. We show that a network learns the majority of overall shape information at the first few epochs of training and that this information is largely encoded in the last few layers of a CNN. Finally, we show that the encoding of shape does not imply the encoding of localized per-pixel semantic information. The experimental results and findings provide a more accurate understanding of the behaviour of current CNNs, thus helping to inform future design choices.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Exploring and quantifying shape information encoded in CNNs.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="te7PVH1sPxJ" data-number="727">
      <h4>
        <a href="https://openreview.net/forum?id=te7PVH1sPxJ">
            Convex Potential Flows: Universal Probability Distributions with Optimal Transport and Convex Optimization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=te7PVH1sPxJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Chin-Wei_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chin-Wei_Huang1">Chin-Wei Huang</a>, <a href="https://openreview.net/profile?id=~Ricky_T._Q._Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ricky_T._Q._Chen1">Ricky T. Q. Chen</a>, <a href="https://openreview.net/profile?id=~Christos_Tsirigotis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christos_Tsirigotis1">Christos Tsirigotis</a>, <a href="https://openreview.net/profile?id=~Aaron_Courville3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aaron_Courville3">Aaron Courville</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#te7PVH1sPxJ-details-23" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="te7PVH1sPxJ-details-23"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Normalizing flows, generative models, variational inference, invertible neural networks, universal approximation, optimal transport, convex optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Flow-based models are powerful tools for designing probabilistic models with tractable density. This paper introduces Convex Potential Flows (CP-Flow), a natural and efficient parameterization of invertible models inspired by the optimal transport (OT) theory. CP-Flows are the gradient map of a strongly convex neural potential function. The convexity implies invertibility and allows us to resort to convex optimization to solve the convex conjugate for efficient inversion. To enable maximum likelihood training, we derive a new gradient estimator of the log-determinant of the Jacobian, which involves solving an inverse-Hessian vector product using the conjugate gradient method. The gradient estimator has constant-memory cost, and can be made effectively unbiased by reducing the error tolerance level of the convex optimization routine. Theoretically, we prove that CP-Flows are universal density approximators and are optimal in the OT sense. Our empirical results show that CP-Flow performs competitively on standard benchmarks of density estimation and variational inference.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose to use an input-convex neural network to parameterize an invertible model with universal density approximation guarantees. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=te7PVH1sPxJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="AAes_3W-2z" data-number="692">
      <h4>
        <a href="https://openreview.net/forum?id=AAes_3W-2z">
            Wasserstein Embedding for Graph Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=AAes_3W-2z" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Soheil_Kolouri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soheil_Kolouri1">Soheil Kolouri</a>, <a href="https://openreview.net/profile?email=nnaderializadeh%40hrl.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="nnaderializadeh@hrl.com">Navid Naderializadeh</a>, <a href="https://openreview.net/profile?id=~Gustavo_K._Rohde1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gustavo_K._Rohde1">Gustavo K. Rohde</a>, <a href="https://openreview.net/profile?email=hhoffmann%40hrl.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="hhoffmann@hrl.com">Heiko Hoffmann</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 02 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#AAes_3W-2z-details-629" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AAes_3W-2z-details-629"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Wasserstein, graph embedding, graph-level prediction</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present Wasserstein Embedding for Graph Learning (WEGL), a novel and fast framework for embedding entire graphs in a vector space, in which various machine learning models are applicable for graph-level prediction tasks. We leverage new insights on defining similarity between graphs as a function of the similarity between their node embedding distributions. Specifically, we use the Wasserstein distance to measure the dissimilarity between node embeddings of different graphs. Unlike prior work, we avoid pairwise calculation of distances between graphs and reduce the computational complexity from quadratic to linear in the number of graphs. WEGL calculates Monge maps from a reference distribution to each node embedding and, based on these maps, creates a fixed-sized vector representation of the graph. We evaluate our new graph embedding approach on various benchmark graph-property prediction tasks, showing state-of-the-art classification performance while having superior computational efficiency. The code is available at https://github.com/navid-naderi/WEGL.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Wasserstein Embedding for Graph Learning (WEGL) is a novel and fast framework for embedding entire graphs into a vector space in which the Euclidean distance between representations approximates the 2-Wasserstein distance.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=AAes_3W-2z&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="60j5LygnmD" data-number="1967">
      <h4>
        <a href="https://openreview.net/forum?id=60j5LygnmD">
            Meta-learning with negative learning rates
        </a>
      
        
          <a href="https://openreview.net/pdf?id=60j5LygnmD" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alberto_Bernacchia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alberto_Bernacchia1">Alberto Bernacchia</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#60j5LygnmD-details-812" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="60j5LygnmD-details-812"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Meta-learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep learning models require a large amount of data to perform well. When data is scarce for a target task, we can transfer the knowledge gained by training on similar tasks to quickly learn the target. A successful approach is meta-learning, or "learning to learn" a distribution of tasks, where "learning" is represented by an outer loop, and "to learn" by an inner loop of gradient descent. However, a number of recent empirical studies argue that the inner loop is unnecessary and more simple models work equally well or even better. We study the performance of MAML as a function of the learning rate of the inner loop, where zero learning rate implies that there is no inner loop. Using random matrix theory and exact solutions of linear models, we calculate an algebraic expression for the test loss of MAML applied to mixed linear regression and nonlinear regression with overparameterized models. Surprisingly, while the optimal learning rate for adaptation is positive, we find that the optimal learning rate for training is always negative, a setting that has never been considered before. Therefore, not only does the performance increase by decreasing the learning rate to zero, as suggested by recent work, but it can be increased even further by decreasing the learning rate to negative
      values. These results help clarify under what circumstances meta-learning performs best.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show theoretically that the optimal inner learning rate of MAML during training is always negative in a family of models  </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=60j5LygnmD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="mCtadqIxOJ" data-number="2437">
      <h4>
        <a href="https://openreview.net/forum?id=mCtadqIxOJ">
            Representing Partial Programs with Blended Abstract Semantics
        </a>
      
        
          <a href="https://openreview.net/pdf?id=mCtadqIxOJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Maxwell_Nye1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maxwell_Nye1">Maxwell Nye</a>, <a href="https://openreview.net/profile?id=~Yewen_Pu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yewen_Pu1">Yewen Pu</a>, <a href="https://openreview.net/profile?email=mlbowers%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mlbowers@mit.edu">Matthew Bowers</a>, <a href="https://openreview.net/profile?id=~Jacob_Andreas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jacob_Andreas1">Jacob Andreas</a>, <a href="https://openreview.net/profile?id=~Joshua_B._Tenenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_B._Tenenbaum1">Joshua B. Tenenbaum</a>, <a href="https://openreview.net/profile?id=~Armando_Solar-Lezama1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Armando_Solar-Lezama1">Armando Solar-Lezama</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#mCtadqIxOJ-details-274" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mCtadqIxOJ-details-274"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">program synthesis, representation learning, abstract interpretation, modular neural networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Synthesizing programs from examples requires searching over a vast, combinatorial space of possible programs. In this search process, a key challenge is representing the behavior of a partially written program before it can be executed, to judge if it is on the right track and predict where to search next. We introduce a general technique for representing partially written programs in a program synthesis engine. We take inspiration from the technique of abstract interpretation, in which an approximate execution model is used to determine if an unfinished program will eventually satisfy a goal specification. Here we learn an approximate execution model implemented as a modular neural network. By constructing compositional program representations that implicitly encode the interpretation semantics of the underlying programming language, we can represent partial programs using a flexible combination of concrete execution state and learned neural representations, using the learned approximate semantics when concrete semantics are not known (in unfinished parts of the program). We show that these hybrid neuro-symbolic representations enable execution-guided synthesizers to use more powerful language constructs, such as loops and higher-order functions, and can be used to synthesize programs more accurately for a given search budget than pure neural approaches in several domains.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We use a combination of concrete execution and learned neural semantics to represent partial programs, resulting in more accurate program synthesis.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="w2mYg3d0eot" data-number="1337">
      <h4>
        <a href="https://openreview.net/forum?id=w2mYg3d0eot">
            Fast convergence of stochastic subgradient method under interpolation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=w2mYg3d0eot" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Huang_Fang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huang_Fang1">Huang Fang</a>, <a href="https://openreview.net/profile?email=zhenanf%40cs.ubc.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhenanf@cs.ubc.ca">Zhenan Fan</a>, <a href="https://openreview.net/profile?email=mpf%40cs.ubc.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="mpf@cs.ubc.ca">Michael Friedlander</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#w2mYg3d0eot-details-970" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="w2mYg3d0eot-details-970"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Optimization, stochastic subgradient method, interpolation, convergence analysis</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper studies the behaviour of the stochastic subgradient descent (SSGD) method applied to over-parameterized nonsmooth optimization problems that satisfy an interpolation condition. By leveraging the composite structure of the empirical risk minimization problems, we prove that SSGD converges, respectively, with rates <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="264" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mrow><mo>/</mo></mrow><mi>ϵ</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="265" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mrow><mo>/</mo></mrow><mi>ϵ</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> for convex and strongly-convex objectives when interpolation holds. These rates coincide with established rates for the stochastic gradient descent (SGD) method applied to smooth problems that also satisfy an interpolation condition. Our analysis provides a partial explanation for the empirical observation that sometimes SGD and SSGD behave similarly for training smooth and nonsmooth machine learning models. We also prove that the rate <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="266" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mrow><mo>/</mo></mrow><mi>ϵ</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> is optimal for the subgradient method in the convex and interpolation setting.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="l35SB-_raSQ" data-number="2185">
      <h4>
        <a href="https://openreview.net/forum?id=l35SB-_raSQ">
            A Hypergradient Approach to Robust Regression without Correspondence
        </a>
      
        
          <a href="https://openreview.net/pdf?id=l35SB-_raSQ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yujia_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yujia_Xie1">Yujia Xie</a>, <a href="https://openreview.net/profile?email=956986044myx%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="956986044myx@gmail.com">Yixiu Mao</a>, <a href="https://openreview.net/profile?id=~Simiao_Zuo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simiao_Zuo1">Simiao Zuo</a>, <a href="https://openreview.net/profile?id=~Hongteng_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongteng_Xu1">Hongteng Xu</a>, <a href="https://openreview.net/profile?id=~Xiaojing_Ye1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaojing_Ye1">Xiaojing Ye</a>, <a href="https://openreview.net/profile?id=~Tuo_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tuo_Zhao1">Tuo Zhao</a>, <a href="https://openreview.net/profile?id=~Hongyuan_Zha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongyuan_Zha1">Hongyuan Zha</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#l35SB-_raSQ-details-731" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="l35SB-_raSQ-details-731"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Regression without correspondence, differentiable programming, first-order optimization, Sinkhorn algorithm</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We consider a regression problem, where the correspondence between the input and output data is not available. Such shuffled data are commonly observed in many real world problems. Take flow cytometry as an example: the measuring instruments are unable to preserve the correspondence between the samples and the measurements. Due to the combinatorial nature of the problem, most of the existing methods are only applicable when the sample size is small, and are limited to linear regression models. To overcome such bottlenecks, we propose a new computational framework --- ROBOT --- for the shuffled regression problem, which is applicable to large data and complex models. Specifically, we propose to formulate regression without correspondence as a continuous optimization problem. Then by exploiting the interaction between the regression model and the data correspondence, we propose to develop a hypergradient approach based on differentiable programming techniques. Such a hypergradient approach essentially views the data correspondence as an operator of the regression model, and therefore it allows us to find a better descent direction for the model parameters by differentiating through the data correspondence. ROBOT is quite general, and can be further extended to an inexact correspondence setting, where the input and output data are not necessarily exactly aligned. Thorough numerical experiments show that ROBOT achieves better performance than existing methods in both linear and nonlinear regression tasks, including real-world applications such as flow cytometry and multi-object tracking.  </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a differentiable programming framework for the regression without correspondence problem.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=l35SB-_raSQ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="IrM64DGB21" data-number="1895">
      <h4>
        <a href="https://openreview.net/forum?id=IrM64DGB21">
            On the role of planning in model-based deep reinforcement learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=IrM64DGB21" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jessica_B_Hamrick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jessica_B_Hamrick1">Jessica B Hamrick</a>, <a href="https://openreview.net/profile?id=~Abram_L._Friesen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abram_L._Friesen1">Abram L. Friesen</a>, <a href="https://openreview.net/profile?id=~Feryal_Behbahani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Feryal_Behbahani1">Feryal Behbahani</a>, <a href="https://openreview.net/profile?id=~Arthur_Guez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arthur_Guez1">Arthur Guez</a>, <a href="https://openreview.net/profile?id=~Fabio_Viola2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fabio_Viola2">Fabio Viola</a>, <a href="https://openreview.net/profile?email=switherspoon%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="switherspoon@google.com">Sims Witherspoon</a>, <a href="https://openreview.net/profile?id=~Thomas_Anthony1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Anthony1">Thomas Anthony</a>, <a href="https://openreview.net/profile?id=~Lars_Holger_Buesing1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lars_Holger_Buesing1">Lars Holger Buesing</a>, <a href="https://openreview.net/profile?id=~Petar_Veli%C4%8Dkovi%C4%871" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Petar_Veličković1">Petar Veličković</a>, <a href="https://openreview.net/profile?id=~Theophane_Weber1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Theophane_Weber1">Theophane Weber</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#IrM64DGB21-details-58" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IrM64DGB21-details-58"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">model-based RL, planning, MuZero</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Model-based planning is often thought to be necessary for deep, careful reasoning and generalization in artificial agents. While recent successes of model-based reinforcement learning (MBRL) with deep function approximation have strengthened this hypothesis, the resulting diversity of model-based methods has also made it difficult to track which components drive success and why. In this paper, we seek to disentangle the contributions of recent methods by focusing on three questions: (1) How does planning benefit MBRL agents? (2) Within planning, what choices drive performance? (3) To what extent does planning improve generalization? To answer these questions, we study the performance of MuZero (Schrittwieser et al., 2019), a state-of-the-art MBRL algorithm with strong connections and overlapping components with many other MBRL algorithms. We perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. Our results suggest the following: (1) Planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks. (3) Planning alone is insufficient to drive strong generalization. These results indicate where and how to utilize planning in reinforcement learning settings, and highlight a number of open questions for future MBRL research.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">An empirical investigation into how planning drives performance in model-based RL algorithms.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="J8_GttYLFgr" data-number="942">
      <h4>
        <a href="https://openreview.net/forum?id=J8_GttYLFgr">
            Trajectory Prediction using Equivariant Continuous Convolution
        </a>
      
        
          <a href="https://openreview.net/pdf?id=J8_GttYLFgr" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Robin_Walters1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Robin_Walters1">Robin Walters</a>, <a href="https://openreview.net/profile?email=li.jinxi1%40northeastern.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="li.jinxi1@northeastern.edu">Jinxi Li</a>, <a href="https://openreview.net/profile?id=~Rose_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rose_Yu1">Rose Yu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#J8_GttYLFgr-details-492" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="J8_GttYLFgr-details-492"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">equivariant, symmetry, trajectory prediction, continuous convolution, argoverse</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Trajectory prediction is a critical part of many AI applications, for example, the safe operation of autonomous vehicles. However, current methods are prone to making inconsistent and physically unrealistic predictions. We leverage insights from  fluid dynamics to overcome this limitation by considering internal symmetry in real-world trajectories. We propose a novel model, Equivariant Continous COnvolution (ECCO) for improved trajectory prediction.  ECCO uses rotationally-equivariant continuous convolutions to embed the symmetries of the system. On both vehicle and pedestrian trajectory datasets, ECCO attains competitive accuracy  with significantly fewer parameters. It is also more sample efficient, generalizing automatically from few data points in any orientation.  Lastly, ECCO improves generalization with equivariance, resulting in more physically consistent predictions.   Our method provides a fresh perspective towards increasing trust and transparency in deep learning models. Our code and data can be found at https://github.com/Rose-STL-Lab/ECCO.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Our model, ECCO, uses rotationally-equivariant continuous convolution to improve generalization in trajectory prediction.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=J8_GttYLFgr&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="chPj_I5KMHG" data-number="3493">
      <h4>
        <a href="https://openreview.net/forum?id=chPj_I5KMHG">
            Grounding Language to Autonomously-Acquired Skills via Goal Generation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=chPj_I5KMHG" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ahmed_Akakzia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ahmed_Akakzia1">Ahmed Akakzia</a>, <a href="https://openreview.net/profile?id=~C%C3%A9dric_Colas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cédric_Colas1">Cédric Colas</a>, <a href="https://openreview.net/profile?id=~Pierre-Yves_Oudeyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pierre-Yves_Oudeyer1">Pierre-Yves Oudeyer</a>, <a href="https://openreview.net/profile?id=~Mohamed_CHETOUANI2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohamed_CHETOUANI2">Mohamed CHETOUANI</a>, <a href="https://openreview.net/profile?id=~Olivier_Sigaud1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Olivier_Sigaud1">Olivier Sigaud</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#chPj_I5KMHG-details-485" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="chPj_I5KMHG-details-485"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep reinforcement learning, intrinsic motivations, symbolic representations, autonomous learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We are interested in the autonomous acquisition of repertoires of skills. Language-conditioned reinforcement learning (LC-RL) approaches are great tools in this quest, as they allow to express abstract goals as sets of constraints on the states. However, most LC-RL agents are not autonomous and cannot learn without external instructions and feedback. Besides, their direct language condition cannot account for the goal-directed behavior of pre-verbal infants and strongly limits the expression of behavioral diversity for a given language input. To resolve these issues, we propose a new conceptual approach to language-conditioned RL: the Language-Goal-Behavior architecture (LGB). LGB decouples skill learning and language grounding via an intermediate semantic representation of the world. To showcase the properties of LGB, we present a specific implementation called DECSTR. DECSTR is an intrinsically motivated learning agent endowed with an innate semantic representation describing spatial relations between physical objects. In a first stage G -&gt; B, it freely explores its environment and targets self-generated semantic configurations. In a second stage (L -&gt; G), it trains a language-conditioned  goal generator to generate semantic goals that match the constraints expressed in language-based inputs. We showcase the additional properties of LGB w.r.t. both an end-to-end LC-RL approach and a similar approach leveraging non-semantic, continuous intermediate representations. Intermediate semantic representations help satisfy language commands in a diversity of ways, enable strategy switching after a failure and facilitate language grounding.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a new RL architecture called Language-Goal-Behavior that proposes to decouple skill learning and language grounding via the introduction of an intermediate semantic goal representation.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=chPj_I5KMHG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="a3wKPZpGtCF" data-number="2387">
      <h4>
        <a href="https://openreview.net/forum?id=a3wKPZpGtCF">
            Chaos of Learning Beyond Zero-sum and Coordination via Game Decompositions
        </a>
      
        
          <a href="https://openreview.net/pdf?id=a3wKPZpGtCF" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yun_Kuen_Cheung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yun_Kuen_Cheung1">Yun Kuen Cheung</a>, <a href="https://openreview.net/profile?email=yt851%40nyu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yt851@nyu.edu">Yixin Tao</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>21 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#a3wKPZpGtCF-details-62" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="a3wKPZpGtCF-details-62"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Learning in Games, Lyapunov Chaos, Game Decomposition, Multiplicative Weights Update, Follow-the-Regularized-Leader, Volume Analysis, Dynamical Systems</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">It is of primary interest for ML to understand how agents learn and interact dynamically in competitive environments and games (e.g. GANs). But this has been a difficult task, as irregular behaviors are commonly observed in such systems. This can be explained theoretically, for instance, by the works of Cheung and Piliouras (COLT 2019; NeurIPS 2020), which showed that in two-person zero-sum games, if agents employ one of the most well-known learning algorithms, Multiplicative Weights Update (MWU), then Lyapunov chaos occurs everywhere in the payoff space. In this paper, we study how persistent chaos can occur in the more general normal game settings, where the agents might have the motivation to coordinate (which is not true for zero-sum games) and the number of agents can be arbitrary.
      
      We characterize bimatrix games where MWU, its optimistic variant (OMWU) or Follow-the-Regularized-Leader (FTRL) algorithms are Lyapunov chaotic almost everywhere in the payoff space. Technically, our characterization is derived by extending the volume-expansion argument of Cheung and Piliouras via the canonical game decomposition into zero-sum and coordination components. Interestingly, the two components induce opposite volume-changing behaviors, so the overall behavior can be analyzed by comparing the strengths of the components against each other. The comparison is done via our new notion of "matrix domination" or via a linear program. For multi-player games, we present a local equivalence of volume change between general games and graphical games, which is used to perform volume and chaos analyses of MWU and OMWU in potential games.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We characterize games in which popular learning algorithms exhibit Lyapunov chaos.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="FX0vR39SJ5q" data-number="1757">
      <h4>
        <a href="https://openreview.net/forum?id=FX0vR39SJ5q">
            Isometric Transformation Invariant and Equivariant Graph Convolutional Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=FX0vR39SJ5q" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Masanobu_Horie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Masanobu_Horie1">Masanobu Horie</a>, <a href="https://openreview.net/profile?email=morita%40ricos.co.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="morita@ricos.co.jp">Naoki Morita</a>, <a href="https://openreview.net/profile?email=hishinuma%40ricos.co.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="hishinuma@ricos.co.jp">Toshiaki Hishinuma</a>, <a href="https://openreview.net/profile?email=ihara%40ricos.co.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="ihara@ricos.co.jp">Yu Ihara</a>, <a href="https://openreview.net/profile?email=mitsume%40kz.tsukuba.ac.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="mitsume@kz.tsukuba.ac.jp">Naoto Mitsume</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#FX0vR39SJ5q-details-724" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="FX0vR39SJ5q-details-724"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Machine Learning, Graph Neural Network, Invariance and Equivariance, Physical Simulation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Graphs are one of the most important data structures for representing pairwise relations between objects. Specifically, a graph embedded in a Euclidean space is essential to solving real problems, such as physical simulations. A crucial requirement for applying graphs in Euclidean spaces to physical simulations is learning and inferring the isometric transformation invariant and equivariant features in a computationally efficient manner. In this paper, we propose a set of transformation invariant and equivariant models based on graph convolutional networks, called IsoGCNs. We demonstrate that the proposed model has a competitive performance compared to state-of-the-art methods on tasks related to geometrical and physical simulation data. Moreover, the proposed model can scale up to graphs with 1M vertices and conduct an inference faster than a conventional finite element analysis, which the existing equivariant models cannot achieve.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We developed isometric transformation invariant and equivariant graph convolutional networks, which shows high prediction performance and computational efficiency.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=FX0vR39SJ5q&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="RSU17UoKfJF" data-number="1211">
      <h4>
        <a href="https://openreview.net/forum?id=RSU17UoKfJF">
            R-GAP: Recursive Gradient Attack on Privacy
        </a>
      
        
          <a href="https://openreview.net/pdf?id=RSU17UoKfJF" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Junyi_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junyi_Zhu1">Junyi Zhu</a>, <a href="https://openreview.net/profile?id=~Matthew_B._Blaschko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthew_B._Blaschko1">Matthew B. Blaschko</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#RSU17UoKfJF-details-936" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RSU17UoKfJF-details-936"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">privacy leakage from gradients, federated learning, collaborative learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP  works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network's security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="C70cp4Cn32" data-number="1804">
      <h4>
        <a href="https://openreview.net/forum?id=C70cp4Cn32">
            Multi-Level Local SGD: Distributed SGD for Heterogeneous Hierarchical Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=C70cp4Cn32" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Timothy_Castiglia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Timothy_Castiglia1">Timothy Castiglia</a>, <a href="https://openreview.net/profile?email=dasa2%40rpi.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="dasa2@rpi.edu">Anirban Das</a>, <a href="https://openreview.net/profile?id=~Stacy_Patterson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stacy_Patterson1">Stacy Patterson</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#C70cp4Cn32-details-149" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="C70cp4Cn32-details-149"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Machine Learning, Stochastic Gradient Descent, Federated Learning, Hierarchical Networks, Distributed, Heterogeneous, Convergence Analysis</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers. Our network model consists of a set of disjoint sub-networks, with a single hub and multiple workers; further, workers may have different operating rates. The hubs exchange information with one another via a connected, but not necessarily complete communication network. In our algorithm, sub-networks execute a distributed SGD algorithm, using a hub-and-spoke paradigm, and the hubs periodically average their models with neighboring hubs. We first provide a unified mathematical framework that describes the Multi-Level Local SGD algorithm. We then present a theoretical analysis of the algorithm; our analysis shows the dependence of the convergence error on the worker node heterogeneity, hub network topology, and the number of local, sub-network, and global iterations. We illustrate the effectiveness of our algorithm in a multi-level network with slow workers via simulation-based experiments.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose Multi-Level Local SGD, a distributed stochastic gradient method for learning a smooth, non-convex objective in a multi-level communication network with heterogeneous workers.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="qrwe7XHTmYb" data-number="1059">
      <h4>
        <a href="https://openreview.net/forum?id=qrwe7XHTmYb">
            GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding
        </a>
      
        
          <a href="https://openreview.net/pdf?id=qrwe7XHTmYb" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=lepikhin%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="lepikhin@google.com">Dmitry Lepikhin</a>, <a href="https://openreview.net/profile?email=hyouklee%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="hyouklee@google.com">HyoukJoong Lee</a>, <a href="https://openreview.net/profile?email=yuanzx%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yuanzx@google.com">Yuanzhong Xu</a>, <a href="https://openreview.net/profile?email=dehao%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dehao@google.com">Dehao Chen</a>, <a href="https://openreview.net/profile?id=~Orhan_Firat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Orhan_Firat1">Orhan Firat</a>, <a href="https://openreview.net/profile?id=~Yanping_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanping_Huang1">Yanping Huang</a>, <a href="https://openreview.net/profile?email=krikun%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="krikun@google.com">Maxim Krikun</a>, <a href="https://openreview.net/profile?id=~Noam_Shazeer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noam_Shazeer1">Noam Shazeer</a>, <a href="https://openreview.net/profile?id=~Zhifeng_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhifeng_Chen1">Zhifeng Chen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#qrwe7XHTmYb-details-320" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qrwe7XHTmYb-details-320"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost,ease of programming, and efficient implementation on parallel devices.  In this paper we demonstrate conditional computation as a remedy to the above mentioned impediments, and demonstrate its efficacy and utility.  We make extensive use of GShard, a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler to enable large scale models with up to trillions of parameters. GShard and conditional computation enable us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts. We demonstrate that such a giant model with 600 billion parameters can efficiently be trained on 2048 TPU v3 cores in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="TVjLza1t4hI" data-number="3359">
      <h4>
        <a href="https://openreview.net/forum?id=TVjLza1t4hI">
            Representation learning for improved interpretability and classification accuracy of clinical factors from EEG
        </a>
      
        
          <a href="https://openreview.net/pdf?id=TVjLza1t4hI" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=ghonk%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ghonk@google.com">Garrett Honke</a>, <a href="https://openreview.net/profile?id=~Irina_Higgins1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Irina_Higgins1">Irina Higgins</a>, <a href="https://openreview.net/profile?email=nthigpen%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="nthigpen@google.com">Nina Thigpen</a>, <a href="https://openreview.net/profile?id=~Vladimir_Miskovic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vladimir_Miskovic1">Vladimir Miskovic</a>, <a href="https://openreview.net/profile?email=katielink%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="katielink@google.com">Katie Link</a>, <a href="https://openreview.net/profile?email=sunnyd%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sunnyd@google.com">Sunny Duan</a>, <a href="https://openreview.net/profile?email=pramodg%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="pramodg@google.com">Pramod Gupta</a>, <a href="https://openreview.net/profile?email=julia.klawohn%40hu-berlin.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="julia.klawohn@hu-berlin.de">Julia Klawohn</a>, <a href="https://openreview.net/profile?id=~Greg_Hajcak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Greg_Hajcak1">Greg Hajcak</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#TVjLza1t4hI-details-928" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TVjLza1t4hI-details-928"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">EEG, ERP, electroencephalography, depression, representation learning, disentanglement, beta-VAE</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Despite extensive standardization, diagnostic interviews for mental health disorders encompass substantial subjective judgment. Previous studies have demonstrated that EEG-based neural measures can function as reliable objective correlates of depression, or even predictors of depression and its course. However, their clinical utility has not been fully realized because of 1) the lack of automated ways to deal with the inherent noise associated with EEG data at scale, and 2) the lack of knowledge of which aspects of the EEG signal may be markers of a clinical disorder. Here we adapt an unsupervised pipeline from the recent deep representation learning literature to address these problems by 1) learning a disentangled representation using <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="267" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>β</mi></math></mjx-assistive-mml></mjx-container>-VAE to denoise the signal, and 2) extracting interpretable features associated with a sparse set of clinical labels using a Symbol-Concept Association Network (SCAN). We demonstrate that our method is able to outperform the canonical hand-engineered baseline classification method on a number of factors, including participant age and depression diagnosis. Furthermore, our method recovers a representation that can be used to automatically extract denoised Event Related Potentials (ERPs) from novel, single EEG trajectories, and supports fast supervised re-mapping to various clinical labels, allowing clinicians to re-use a single EEG representation regardless of updates to the standardized diagnostic system. Finally, single factors of the learned disentangled representations often correspond to meaningful markers of clinical factors, as automatically detected by SCAN, allowing for human interpretability and post-hoc expert analysis of the recommendations made by the model.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We use disentangled representations of EEG signals to improve performance on clinical classification tasks, provide interpretable recommendations for post-hoc analysis and allow for extraction of ERPs from novel single EEG trajectories.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=TVjLza1t4hI&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="OmtmcPkkhT" data-number="2666">
      <h4>
        <a href="https://openreview.net/forum?id=OmtmcPkkhT">
            Multiplicative Filter Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=OmtmcPkkhT" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Rizal_Fathony1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rizal_Fathony1">Rizal Fathony</a>, <a href="https://openreview.net/profile?id=~Anit_Kumar_Sahu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anit_Kumar_Sahu1">Anit Kumar Sahu</a>, <a href="https://openreview.net/profile?id=~Devin_Willmott1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Devin_Willmott1">Devin Willmott</a>, <a href="https://openreview.net/profile?id=~J_Zico_Kolter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~J_Zico_Kolter1">J Zico Kolter</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#OmtmcPkkhT-details-498" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OmtmcPkkhT-details-498"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep Architectures, Implicit Neural Representations, Fourier Features</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Although deep networks are typically used to approximate functions over high dimensional inputs, recent work has increased interest in neural networks as function approximators for low-dimensional-but-complex functions, such as representing images as a function of pixel coordinates, solving differential equations, or representing signed distance fields or neural radiance fields.  Key to these recent successes has been the use of new elements such as sinusoidal nonlinearities, or Fourier features in positional encodings, which vastly outperform simple ReLU networks.  In this paper, we propose and empirically demonstrate that an arguably simpler class of function approximators can work just as well for such problems: multiplicative filter networks.  In these networks, we avoid traditional compositional depth altogether, and simply multiply together (linear functions of) sinusoidal or Gabor wavelet functions applied to the input.  This representation has the notable advantage that the entire function can simply be viewed as a linear function approximator over an exponential number of Fourier or Gabor basis functions, respectively.  Despite this simplicity, when compared to recent approaches that use Fourier features with ReLU networks or sinusoidal activation networks, we show that these multiplicative filter networks largely outperform or match the performance of these recent approaches on the domains highlighted in these past works.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=OmtmcPkkhT&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="7uVcpu-gMD" data-number="2069">
      <h4>
        <a href="https://openreview.net/forum?id=7uVcpu-gMD">
            Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=7uVcpu-gMD" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~R%C3%B3bert_Csord%C3%A1s1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Róbert_Csordás1">Róbert Csordás</a>, <a href="https://openreview.net/profile?id=~Sjoerd_van_Steenkiste1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sjoerd_van_Steenkiste1">Sjoerd van Steenkiste</a>, <a href="https://openreview.net/profile?id=~J%C3%BCrgen_Schmidhuber1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jürgen_Schmidhuber1">Jürgen Schmidhuber</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 07 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>23 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#7uVcpu-gMD-details-157" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7uVcpu-gMD-details-157"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">modularity, systematic generalization, compositionality</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Neural networks (NNs) whose subnetworks implement reusable functions are expected to offer numerous advantages, including compositionality through efficient recombination of functional building blocks, interpretability, preventing catastrophic interference, etc. Understanding if and how NNs are modular could provide insights into how to improve them. Current inspection methods, however, fail to link modules to their functionality. In this paper, we present a novel method based on learning binary weight masks to identify individual weights and subnets responsible for specific functions. Using this powerful tool, we contribute an extensive study of emerging modularity in NNs that covers several standard architectures and datasets. We demonstrate how common NNs fail to reuse submodules and offer new insights into the related issue of systematic generalization on language tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We develop a method for analyzing emerging functional modularity in neural networks based on differentiable weight masks and use it to point out important issues in current-day neural networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=7uVcpu-gMD&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ZDnzZrTqU9N" data-number="3696">
      <h4>
        <a href="https://openreview.net/forum?id=ZDnzZrTqU9N">
            Modeling the Second Player in Distributionally Robust Optimization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ZDnzZrTqU9N" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Paul_Michel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Michel1">Paul Michel</a>, <a href="https://openreview.net/profile?id=~Tatsunori_Hashimoto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tatsunori_Hashimoto1">Tatsunori Hashimoto</a>, <a href="https://openreview.net/profile?id=~Graham_Neubig1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Graham_Neubig1">Graham Neubig</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 31 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ZDnzZrTqU9N-details-14" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZDnzZrTqU9N-details-14"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">distributionally robust optimization, deep learning, robustness, adversarial learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Distributionally robust optimization (DRO) provides a framework for training machine learning models that are able to perform well on a collection of related data distributions (the "uncertainty set"). This is done by solving a min-max game: the model is trained to minimize its maximum expected loss among all distributions in the uncertainty set. While careful design of the uncertainty set is critical to the success of the DRO procedure, previous work has been limited to relatively simple alternatives that keep the min-max optimization problem exactly tractable, such as <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="268" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></mjx-assistive-mml></mjx-container>-divergence balls. In this paper, we argue instead for the use of neural generative models to characterize the worst-case distribution, allowing for more flexible and problem-specific selection of the uncertainty set. However, while simple conceptually, this approach poses a number of implementation and optimization challenges. To circumvent these issues, we propose a relaxation of the KL-constrained inner maximization objective that makes the DRO problem more amenable to gradient-based optimization of large scale generative models, and develop model selection heuristics to guide hyper-parameter search. On both toy settings and realistic NLP tasks, we find that the proposed approach yields models that are more robust than comparable baselines.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We use generative neural models to define the uncertainty set in distributionally robust optimization, and show that this helps train more robust classifiers</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="6isfR3JCbi" data-number="3161">
      <h4>
        <a href="https://openreview.net/forum?id=6isfR3JCbi">
            Private Post-GAN Boosting
        </a>
      
        
          <a href="https://openreview.net/pdf?id=6isfR3JCbi" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Marcel_Neunhoeffer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marcel_Neunhoeffer1">Marcel Neunhoeffer</a>, <a href="https://openreview.net/profile?id=~Steven_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_Wu1">Steven Wu</a>, <a href="https://openreview.net/profile?id=~Cynthia_Dwork2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cynthia_Dwork2">Cynthia Dwork</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#6isfR3JCbi-details-460" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6isfR3JCbi-details-460"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Differentially private GANs have proven to be a promising approach for generating realistic synthetic data without compromising the privacy of individuals. Due to the privacy-protective noise introduced in the  training, the convergence of GANs becomes even more elusive, which often leads to poor utility in the output generator at the end of training. We propose Private post-GAN boosting (Private PGB), a differentially private method that combines samples produced by the sequence of generators obtained during GAN training to create a high-quality synthetic dataset. To that end, our method leverages the Private Multiplicative Weights method (Hardt and Rothblum, 2010) to reweight generated samples. We evaluate Private PGB on two dimensional toy data, MNIST images, US Census data and a standard machine learning prediction task. Our experiments show that Private PGB improves upon a standard private GAN approach across a collection of quality measures. We also provide a non-private variant of PGB that improves the data quality of standard GAN training.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Ig53hpHxS4" data-number="1218">
      <h4>
        <a href="https://openreview.net/forum?id=Ig53hpHxS4">
            Flowtron: an Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Ig53hpHxS4" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Rafael_Valle1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rafael_Valle1">Rafael Valle</a>, <a href="https://openreview.net/profile?id=~Kevin_J._Shih1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kevin_J._Shih1">Kevin J. Shih</a>, <a href="https://openreview.net/profile?email=rprenger%40nvidia.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="rprenger@nvidia.com">Ryan Prenger</a>, <a href="https://openreview.net/profile?id=~Bryan_Catanzaro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bryan_Catanzaro1">Bryan Catanzaro</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Ig53hpHxS4-details-671" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ig53hpHxS4-details-671"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Text to speech synthesis, normalizing flows, deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech variation. Flowtron borrows insights from Autoregressive Flows and revamps Tacotron 2 in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be used to modulate many aspects of speech synthesis (timbre, expressivity, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. We provide results on speech variation, interpolation over time between samples and style transfer between seen and unseen speakers. Code and pre-trained models are publicly available at \href{https://github.com/NVIDIA/flowtron}{https://github.com/NVIDIA/flowtron}.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with style transfer and speech varation.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ig53hpHxS4&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="v9hAX77--cZ" data-number="2364">
      <h4>
        <a href="https://openreview.net/forum?id=v9hAX77--cZ">
            Learning Structural Edits via Incremental Tree Transformations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=v9hAX77--cZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ziyu_Yao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziyu_Yao1">Ziyu Yao</a>, <a href="https://openreview.net/profile?id=~Frank_F._Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frank_F._Xu1">Frank F. Xu</a>, <a href="https://openreview.net/profile?id=~Pengcheng_Yin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pengcheng_Yin1">Pengcheng Yin</a>, <a href="https://openreview.net/profile?id=~Huan_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huan_Sun1">Huan Sun</a>, <a href="https://openreview.net/profile?id=~Graham_Neubig1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Graham_Neubig1">Graham Neubig</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 05 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>31 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#v9hAX77--cZ-details-868" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="v9hAX77--cZ-details-868"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Tree-structured Data, Edit, Incremental Tree Transformations, Representation Learning, Imitation Learning, Source Code</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While most neural generative models generate outputs in a single pass, the human creative process is usually one of iterative building and refinement. Recent work has proposed models of editing processes, but these mostly focus on editing sequential data and/or only model a single editing pass. In this paper, we present a generic model for incremental editing of structured data (i.e. ''structural edits''). Particularly, we focus on tree-structured data, taking abstract syntax trees of computer programs as our canonical example. Our editor learns to iteratively generate tree edits (e.g. deleting or adding a subtree) and applies them to the partially edited data, thereby the entire editing process can be formulated as consecutive, incremental tree transformations. To show the unique benefits of modeling tree edits directly, we further propose a novel edit encoder for learning to represent edits, as well as an imitation learning method that allows the editor to be more robust. We evaluate our proposed editor on two source code edit datasets, where results show that, with the proposed edit encoder, our editor significantly improves accuracy over previous approaches that generate the edited program directly in one pass. Finally, we demonstrate that training our editor to imitate experts and correct its mistakes dynamically can further improve its performance.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A generic incremental editing model for tree-structured data</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=v9hAX77--cZ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="hSjxQ3B7GWq" data-number="2934">
      <h4>
        <a href="https://openreview.net/forum?id=hSjxQ3B7GWq">
            Sample-Efficient Automated Deep Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=hSjxQ3B7GWq" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~J%C3%B6rg_K.H._Franke1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jörg_K.H._Franke1">Jörg K.H. Franke</a>, <a href="https://openreview.net/profile?id=~Gregor_Koehler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gregor_Koehler1">Gregor Koehler</a>, <a href="https://openreview.net/profile?id=~Andr%C3%A9_Biedenkapp1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~André_Biedenkapp1">André Biedenkapp</a>, <a href="https://openreview.net/profile?id=~Frank_Hutter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frank_Hutter1">Frank Hutter</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#hSjxQ3B7GWq-details-33" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hSjxQ3B7GWq-details-33"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">AutoRL, Deep Reinforcement Learning, Hyperparameter Optimization, Neuroevolution</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Despite significant progress in challenging problems across various domains, applying state-of-the-art deep reinforcement learning (RL) algorithms remains challenging due to their sensitivity to the choice of hyperparameters. This sensitivity can partly be attributed to the non-stationarity of the RL problem, potentially requiring different hyperparameter settings at various stages of the learning process. Additionally, in the RL setting, hyperparameter optimization (HPO) requires a large number of environment interactions, hindering the transfer of the successes in RL to real-world applications. In this work, we tackle the issues of sample-efficient and dynamic HPO in RL. We propose a population-based automated RL (AutoRL) framework to meta-optimize arbitrary off-policy RL algorithms. In this framework, we optimize the hyperparameters and also the neural architecture while simultaneously training the agent. By sharing the collected experience across the population, we substantially increase the sample efficiency of the meta-optimization. We demonstrate the capabilities of our sample-efficient AutoRL approach in a case study with the popular TD3 algorithm in the MuJoCo benchmark suite, where we reduce the number of environment interactions needed for meta-optimization by up to an order of magnitude compared to population-based training.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">SEARL trains a population of off-policy RL agents while simultaneously optimizing the hyperparameters and the neural architecture sample-efficiently.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=hSjxQ3B7GWq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="lf7st0bJIA5" data-number="1156">
      <h4>
        <a href="https://openreview.net/forum?id=lf7st0bJIA5">
            Unsupervised Discovery of 3D Physical Objects
        </a>
      
        
          <a href="https://openreview.net/pdf?id=lf7st0bJIA5" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yilun_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yilun_Du1">Yilun Du</a>, <a href="https://openreview.net/profile?id=~Kevin_A._Smith1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kevin_A._Smith1">Kevin A. Smith</a>, <a href="https://openreview.net/profile?id=~Tomer_Ullman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tomer_Ullman1">Tomer Ullman</a>, <a href="https://openreview.net/profile?id=~Joshua_B._Tenenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_B._Tenenbaum1">Joshua B. Tenenbaum</a>, <a href="https://openreview.net/profile?id=~Jiajun_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiajun_Wu1">Jiajun Wu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#lf7st0bJIA5-details-183" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lf7st0bJIA5-details-183"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">unsupervised object discovery, surprisal, scene decomposition, physical scene understanding</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the problem of unsupervised physical object discovery. While existing frameworks aim to decompose scenes into 2D segments based off each object's appearance, we explore how physics, especially object interactions, facilitates disentangling of 3D geometry and position of objects from video, in an unsupervised manner. Drawing inspiration from developmental psychology, our Physical Object Discovery Network (POD-Net) uses both multi-scale pixel cues and physical motion cues to accurately segment observable and partially occluded objects of varying sizes, and infer properties of those objects. Our model reliably segments objects on both synthetic and real scenes.  The discovered object properties can also be used to reason about physical events.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an unsupervised framework for discovery 3D physical objects and show that these 3D objects to be used for tasks mimicking early infant cognition.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="bB2drc7DPuB" data-number="2174">
      <h4>
        <a href="https://openreview.net/forum?id=bB2drc7DPuB">
            Global optimality of softmax policy gradient with single hidden layer neural networks in the mean-field regime
        </a>
      
        
          <a href="https://openreview.net/pdf?id=bB2drc7DPuB" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Andrea_Agazzi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrea_Agazzi2">Andrea Agazzi</a>, <a href="https://openreview.net/profile?id=~Jianfeng_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianfeng_Lu1">Jianfeng Lu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#bB2drc7DPuB-details-467" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bB2drc7DPuB-details-467"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">policy gradient, entropy regularization, mean-field dynamics, neural networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the problem of policy optimization for infinite-horizon discounted Markov Decision Processes with softmax policy and nonlinear function approximation trained with policy gradient algorithms. We concentrate on the training dynamics in the mean-field regime, modeling e.g. the behavior of wide single hidden layer neural networks, when exploration is encouraged through entropy regularization. The dynamics of these models is established as a Wasserstein gradient flow of distributions in parameter space.  We further prove global optimality of the fixed points of this dynamics  under mild conditions on their initialization.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We prove that softmax policy gradient algorithms with single hidden layer neural networks in the mean-field regime can be expressed as a gradient flow in Wasserstein space and prove that all the fixed points of such dynamics are global optimizers</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Nc3TJqbcl3" data-number="1844">
      <h4>
        <a href="https://openreview.net/forum?id=Nc3TJqbcl3">
            Extracting Strong Policies for Robotics Tasks from Zero-Order Trajectory Optimizers
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Nc3TJqbcl3" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Cristina_Pinneri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cristina_Pinneri1">Cristina Pinneri</a>, <a href="https://openreview.net/profile?email=shambhuraj.sawant%40tuebingen.mpg.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="shambhuraj.sawant@tuebingen.mpg.de">Shambhuraj Sawant</a>, <a href="https://openreview.net/profile?email=sebastian.blaes%40tuebingen.mpg.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="sebastian.blaes@tuebingen.mpg.de">Sebastian Blaes</a>, <a href="https://openreview.net/profile?id=~Georg_Martius1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Georg_Martius1">Georg Martius</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Nc3TJqbcl3-details-304" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Nc3TJqbcl3-details-304"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, zero-order optimization, policy learning, model-based learning, robotics, model predictive control</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Solving high-dimensional, continuous robotic tasks is a challenging optimization problem. Model-based methods that rely on zero-order optimizers like the cross-entropy method (CEM) have so far shown strong performance and are considered state-of-the-art in the model-based reinforcement learning community. However, this success comes at the cost of high computational complexity, being therefore not suitable for real-time control. In this paper, we propose a technique to jointly optimize the trajectory and distill a policy, which is essential for fast execution in real robotic systems. Our method builds upon standard approaches, like guidance cost and dataset aggregation, and introduces a novel adaptive factor which prevents the optimizer from collapsing to the learner's behavior at the beginning of the training. The extracted policies reach unprecedented performance on challenging tasks as making a humanoid stand up and opening a door without reward shaping</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an adaptively guided imitation learning method that is able to extract strong policies for hard robotic tasks from zero-order trajectory optimizers.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ONBPHFZ7zG4" data-number="3475">
      <h4>
        <a href="https://openreview.net/forum?id=ONBPHFZ7zG4">
            Temporally-Extended ε-Greedy Exploration
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ONBPHFZ7zG4" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Will_Dabney1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Will_Dabney1">Will Dabney</a>, <a href="https://openreview.net/profile?id=~Georg_Ostrovski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Georg_Ostrovski1">Georg Ostrovski</a>, <a href="https://openreview.net/profile?id=~Andre_Barreto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andre_Barreto1">Andre Barreto</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ONBPHFZ7zG4-details-657" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ONBPHFZ7zG4-details-657"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, exploration</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent work on exploration in reinforcement learning (RL) has led to a series of increasingly complex solutions to the problem. This increase in complexity often comes at the expense of generality. Recent empirical studies suggest that, when applied to a broader set of domains, some sophisticated exploration methods are outperformed by simpler counterparts, such as ε-greedy. In this paper we propose an exploration algorithm that retains the simplicity of ε-greedy while reducing dithering. We build on a simple hypothesis: the main limitation of ε-greedy exploration is its lack of temporal persistence, which limits its ability to escape local optima. We propose a temporally extended form of ε-greedy that simply repeats the sampled action for a random duration. It turns out that, for many duration distributions, this suffices to improve exploration on a large set of domains. Interestingly, a class of distributions inspired by ecological models of animal foraging behaviour yields particularly strong performance.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We discuss a new framework for option-based exploration, present a thorough empirical study of a simple, generally applicable set of options within this framework, and observe improved performance over state-of-the-art agents and exploration methods.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="F-mvpFpn_0q" data-number="515">
      <h4>
        <a href="https://openreview.net/forum?id=F-mvpFpn_0q">
            Rapid Task-Solving in Novel Environments
        </a>
      
        
          <a href="https://openreview.net/pdf?id=F-mvpFpn_0q" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Samuel_Ritter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samuel_Ritter1">Samuel Ritter</a>, <a href="https://openreview.net/profile?id=~Ryan_Faulkner2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ryan_Faulkner2">Ryan Faulkner</a>, <a href="https://openreview.net/profile?id=~Laurent_Sartran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Laurent_Sartran1">Laurent Sartran</a>, <a href="https://openreview.net/profile?id=~Adam_Santoro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adam_Santoro1">Adam Santoro</a>, <a href="https://openreview.net/profile?id=~Matthew_Botvinick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthew_Botvinick1">Matthew Botvinick</a>, <a href="https://openreview.net/profile?id=~David_Raposo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Raposo1">David Raposo</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#F-mvpFpn_0q-details-562" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="F-mvpFpn_0q-details-562"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep reinforcement learning, meta learning, deep learning, exploration, planning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose the challenge of rapid task-solving in novel environments (RTS), wherein an agent must solve a series of tasks as rapidly as possible in an unfamiliar environment. An effective RTS agent must balance between exploring the unfamiliar environment and solving its current task, all while building a model of the new environment over which it can plan when faced with later tasks. While modern deep RL agents exhibit some of these abilities in isolation, none are suitable for the full RTS challenge. To enable progress toward RTS, we introduce two challenge domains: (1) a minimal RTS challenge called the Memory&amp;Planning Game and (2) One-Shot StreetLearn Navigation, which introduces scale and complexity from real-world data. We demonstrate that state-of-the-art deep RL agents fail at RTS in both domains, and that this failure is due to an inability to plan over gathered knowledge. We develop Episodic Planning Networks (EPNs) and show that deep-RL agents with EPNs excel at RTS, outperforming the nearest baseline by factors of 2-3 and learning to navigate held-out StreetLearn maps within a single episode. We show that EPNs learn to execute a value iteration-like planning algorithm and that they generalize to situations beyond their training experience.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Our agents meta-learn to explore, build models on-the-fly, and plan, enabling them to rapidly solve sequences of tasks in unfamiliar environments.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ZcKPWuhG6wy" data-number="1249">
      <h4>
        <a href="https://openreview.net/forum?id=ZcKPWuhG6wy">
            Tradeoffs in Data Augmentation: An Empirical Study
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ZcKPWuhG6wy" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Raphael_Gontijo-Lopes1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Raphael_Gontijo-Lopes1">Raphael Gontijo-Lopes</a>, <a href="https://openreview.net/profile?email=smullin-physics%40stanfordalumni.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="smullin-physics@stanfordalumni.org">Sylvia Smullin</a>, <a href="https://openreview.net/profile?id=~Ekin_Dogus_Cubuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ekin_Dogus_Cubuk1">Ekin Dogus Cubuk</a>, <a href="https://openreview.net/profile?id=~Ethan_Dyer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ethan_Dyer1">Ethan Dyer</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ZcKPWuhG6wy-details-526" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZcKPWuhG6wy-details-526"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Generalization, Interpretability, Understanding Data Augmentation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=ZcKPWuhG6wy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xoHdgbQJohv" data-number="3465">
      <h4>
        <a href="https://openreview.net/forum?id=xoHdgbQJohv">
            Multiscale Score Matching for Out-of-Distribution Detection
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xoHdgbQJohv" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ahsan_Mahmood1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ahsan_Mahmood1">Ahsan Mahmood</a>, <a href="https://openreview.net/profile?id=~Junier_Oliva1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Junier_Oliva1">Junier Oliva</a>, <a href="https://openreview.net/profile?id=~Martin_Andreas_Styner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Andreas_Styner1">Martin Andreas Styner</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xoHdgbQJohv-details-880" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xoHdgbQJohv-details-880"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">out-of-distribution detection, score matching, deep learning, outlier detection</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="269" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></mjx-assistive-mml></mjx-container> levels of noise. Once trained, we calculate the noisy score estimates for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="270" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></mjx-assistive-mml></mjx-container> in-distribution samples and take the L2-norms across the input dimensions (resulting in an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="271" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></mjx-assistive-mml></mjx-container>x<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="272" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></mjx-assistive-mml></mjx-container> matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="273" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>L</mi></math></mjx-assistive-mml></mjx-container>-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=xoHdgbQJohv&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="C3qvk5IQIJY" data-number="2213">
      <h4>
        <a href="https://openreview.net/forum?id=C3qvk5IQIJY">
            Understanding Over-parameterization in Generative Adversarial Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=C3qvk5IQIJY" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yogesh_Balaji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yogesh_Balaji1">Yogesh Balaji</a>, <a href="https://openreview.net/profile?email=sajedi%40usc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sajedi@usc.edu">Mohammadmahdi Sajedi</a>, <a href="https://openreview.net/profile?id=~Neha_Mukund_Kalibhat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Neha_Mukund_Kalibhat1">Neha Mukund Kalibhat</a>, <a href="https://openreview.net/profile?id=~Mucong_Ding1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mucong_Ding1">Mucong Ding</a>, <a href="https://openreview.net/profile?id=~Dominik_St%C3%B6ger1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dominik_Stöger1">Dominik Stöger</a>, <a href="https://openreview.net/profile?id=~Mahdi_Soltanolkotabi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mahdi_Soltanolkotabi1">Mahdi Soltanolkotabi</a>, <a href="https://openreview.net/profile?id=~Soheil_Feizi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Soheil_Feizi2">Soheil Feizi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#C3qvk5IQIJY-details-65" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="C3qvk5IQIJY-details-65"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">GAN, Over-parameterization, min-max optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A broad class of unsupervised deep learning methods such as Generative Adversarial Networks (GANs) involve training of overparameterized models where the number of parameters of the model exceeds a certain threshold. Indeed, most successful GANs used in practice are trained using overparameterized generator and discriminator networks, both in terms of depth and width. A large body of work in supervised learning have shown the importance of model overparameterization in the convergence of the gradient descent (GD) to globally optimal solutions. In contrast, the unsupervised setting and GANs in particular involve non-convex concave mini-max optimization problems that are often trained using Gradient Descent/Ascent (GDA).
      The role and benefits of model overparameterization in the convergence of GDA to a global saddle point in non-convex concave problems is far less understood. In this work, we present a comprehensive analysis of the importance of model overparameterization in GANs both theoretically and empirically. We theoretically show that in an overparameterized GAN model with a <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="274" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn></math></mjx-assistive-mml></mjx-container>-layer neural network generator and a linear discriminator, GDA converges to a global saddle point of the underlying non-convex concave min-max problem. To the best of our knowledge, this is the first result for global convergence of GDA in such settings. Our theory is based on a more general result that holds for a broader class of nonlinear generators and discriminators that obey certain assumptions (including deeper generators and random feature discriminators). Our theory utilizes and builds upon a novel connection with the convergence analysis of linear time-varying dynamical systems which may have broader implications for understanding the convergence behavior of GDA for non-convex concave problems involving overparameterized models. We also empirically study the role of model overparameterization in GANs using several large-scale experiments on CIFAR-10 and Celeb-A datasets. Our experiments show that overparameterization improves the quality of generated samples across various model architectures and datasets. Remarkably, we observe that overparameterization leads to faster and more stable convergence behavior of GDA across the board.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present an analysis of over-parameterization in GANs both theoretically and empirically.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="giit4HdDNa" data-number="1916">
      <h4>
        <a href="https://openreview.net/forum?id=giit4HdDNa">
            Go with the flow: Adaptive control for Neural ODEs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=giit4HdDNa" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mathieu_Chalvidal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mathieu_Chalvidal1">Mathieu Chalvidal</a>, <a href="https://openreview.net/profile?id=~Matthew_Ricci1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matthew_Ricci1">Matthew Ricci</a>, <a href="https://openreview.net/profile?email=rufin.vanrullen%40cnrs.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="rufin.vanrullen@cnrs.fr">Rufin VanRullen</a>, <a href="https://openreview.net/profile?id=~Thomas_Serre1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Serre1">Thomas Serre</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 29 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#giit4HdDNa-details-852" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="giit4HdDNa-details-852"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural ODEs, Optimal Control Theory, Hypernetworks, Normalizing flows</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Despite their elegant formulation and lightweight memory cost, neural ordinary differential equations (NODEs) suffer from known representational limitations. In particular, the single flow learned by NODEs cannot express all homeomorphisms from a given data space to itself, and their static weight parameterization restricts the type of functions they can learn compared to discrete architectures with layer-dependent weights. Here, we describe a new module called neurally-controlled ODE (N-CODE) designed to improve the expressivity of NODEs. The parameters of N-CODE modules are dynamic variables governed by a trainable map from initial or current activation state, resulting in forms of open-loop and closed-loop control, respectively. A single module is sufficient for learning a distribution on non-autonomous flows that adaptively drive neural representations. We provide theoretical and empirical evidence that N-CODE circumvents limitations of previous NODEs models and show how increased model expressivity manifests in several supervised and unsupervised learning problems. These favorable empirical results indicate the potential of using data- and activity-dependent plasticity in neural networks across numerous domains.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper presents a new method to enhance Neural ODEs representation power by dynamically controlling their weight parametrization.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="dx11_7vm5_r" data-number="2848">
      <h4>
        <a href="https://openreview.net/forum?id=dx11_7vm5_r">
            Linear Last-iterate Convergence in Constrained Saddle-point Optimization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=dx11_7vm5_r" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Chen-Yu_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chen-Yu_Wei1">Chen-Yu Wei</a>, <a href="https://openreview.net/profile?id=~Chung-Wei_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chung-Wei_Lee1">Chung-Wei Lee</a>, <a href="https://openreview.net/profile?id=~Mengxiao_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mengxiao_Zhang2">Mengxiao Zhang</a>, <a href="https://openreview.net/profile?id=~Haipeng_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haipeng_Luo1">Haipeng Luo</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#dx11_7vm5_r-details-407" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dx11_7vm5_r-details-407"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Saddle-point Optimization, Optimistic Mirror Decent, Optimistic Gradient Descent Ascent, Optimistic Multiplicative Weights Update, Last-iterate Convergence, Game Theory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for saddle-point optimization have received growing attention due to their favorable last-iterate convergence. However, their behaviors for simple bilinear games over the probability simplex are still not fully understood --- previous analysis lacks explicit convergence rates, only applies to an exponentially small learning rate, or requires additional assumptions such as the uniqueness of the optimal solution.
      
      In this work, we significantly expand the understanding of last-iterate convergence for OGDA and OMWU in the constrained setting. Specifically, for OMWU in bilinear games over the simplex, we show that when the equilibrium is unique, linear last-iterate convergence is achievable with a constant learning rate, which improves the result of (Daskalakis &amp; Panageas, 2019) under the same assumption. We then significantly extend the results to more general objectives and feasible sets for the projected OGDA algorithm, by introducing a sufficient condition under which OGDA exhibits concrete last-iterate convergence rates with a constant learning rate. We show that bilinear games over any polytope satisfy this condition and OGDA converges exponentially fast even without the unique equilibrium assumption. Our condition also holds for strongly-convex-strongly-concave functions, recovering the result of (Hsieh et al., 2019). Finally, we provide experimental results to further support our theory. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We prove Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) converge exponentially fast to the Nash equilibrium in the sense of last-iterate in various game settings including matrix games.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-gfhS00XfKj" data-number="841">
      <h4>
        <a href="https://openreview.net/forum?id=-gfhS00XfKj">
            Learning advanced mathematical computations from examples
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-gfhS00XfKj" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Francois_Charton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Francois_Charton1">Francois Charton</a>, <a href="https://openreview.net/profile?email=amaury.hayat%40enpc.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="amaury.hayat@enpc.fr">Amaury Hayat</a>, <a href="https://openreview.net/profile?id=~Guillaume_Lample1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guillaume_Lample1">Guillaume Lample</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>22 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-gfhS00XfKj-details-443" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-gfhS00XfKj-details-443"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">differential equations, computation, transformers, deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We train transformers to predict qualitative and numerical properties of differential equations</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=-gfhS00XfKj&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="NsMLjcFaO8O" data-number="1311">
      <h4>
        <a href="https://openreview.net/forum?id=NsMLjcFaO8O">
            WaveGrad: Estimating Gradients for Waveform Generation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=NsMLjcFaO8O" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Nanxin_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nanxin_Chen1">Nanxin Chen</a>, <a href="https://openreview.net/profile?id=~Yu_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Zhang2">Yu Zhang</a>, <a href="https://openreview.net/profile?id=~Heiga_Zen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Heiga_Zen1">Heiga Zen</a>, <a href="https://openreview.net/profile?id=~Ron_J_Weiss1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ron_J_Weiss1">Ron J Weiss</a>, <a href="https://openreview.net/profile?id=~Mohammad_Norouzi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammad_Norouzi1">Mohammad Norouzi</a>, <a href="https://openreview.net/profile?id=~William_Chan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~William_Chan1">William Chan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#NsMLjcFaO8O-details-991" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NsMLjcFaO8O-details-991"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">vocoder, diffusion, score matching, text-to-speech, gradient estimation, waveform generation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper introduces WaveGrad, a conditional model for waveform generation which estimates gradients of the data density. The model is built on prior work on score matching and diffusion probabilistic models. It starts from a Gaussian white noise signal and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram.
      WaveGrad offers a natural way to trade inference speed for sample quality by adjusting the number of refinement steps, and bridges the gap between non-autoregressive and autoregressive models in terms of audio quality.
      We find that it can generate high fidelity audio samples using as few as six iterations.
      Experiments reveal WaveGrad to generate high fidelity audio, outperforming adversarial non-autoregressive baselines and matching a strong likelihood-based autoregressive baseline using fewer sequential operations.  Audio samples are available at https://wavegrad.github.io/.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper introduces WaveGrad, a conditional model for waveform generation through estimating gradients of the data density.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="7EDgLu9reQD" data-number="3563">
      <h4>
        <a href="https://openreview.net/forum?id=7EDgLu9reQD">
            SALD: Sign Agnostic Learning with Derivatives
        </a>
      
        
          <a href="https://openreview.net/pdf?id=7EDgLu9reQD" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Matan_Atzmon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matan_Atzmon1">Matan Atzmon</a>, <a href="https://openreview.net/profile?id=~Yaron_Lipman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yaron_Lipman1">Yaron Lipman</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#7EDgLu9reQD-details-689" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7EDgLu9reQD-details-689"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">implicit neural representations, 3D shapes learning, sign agnostic learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Learning 3D geometry directly from raw data, such as point clouds, triangle soups, or unoriented meshes is still a challenging task that feeds many downstream computer vision and graphics applications. 
      
      In this paper, we introduce SALD: a method for learning implicit neural representations of shapes directly from raw data. We generalize sign agnostic learning (SAL) to include derivatives: given an unsigned distance function to the input raw data, we advocate a novel sign agnostic regression loss, incorporating both pointwise values and gradients of the unsigned distance function. Optimizing this loss leads to a signed implicit function solution, the zero level set of which is a high quality and valid manifold approximation to the input 3D data. The motivation behind SALD is that incorporating derivatives in a regression loss leads to a lower sample complexity, and consequently better fitting. In addition, we provide empirical evidence, as well as theoretical motivation in 2D that SAL enjoys a minimal surface property, favoring minimal area solutions. More importantly, we are able to show that this property still holds for SALD, i.e.,  with derivatives included.
      
      We demonstrate the efficacy of SALD for shape space learning on two challenging datasets: ShapeNet that contains inconsistent orientation and non-manifold meshes, and D-Faust that contains raw 3D scans (triangle soups). On both these datasets, we present state-of-the-art results.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Sign agnostic learning with derivatives for learning high fidelity 3D implicit neural representations shape space from raw data.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="0PtUPB9z6qK" data-number="1585">
      <h4>
        <a href="https://openreview.net/forum?id=0PtUPB9z6qK">
            Generalized Energy Based Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=0PtUPB9z6qK" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Michael_Arbel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Arbel1">Michael Arbel</a>, <a href="https://openreview.net/profile?email=liang.zhou.18%40ucl.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="liang.zhou.18@ucl.ac.uk">Liang Zhou</a>, <a href="https://openreview.net/profile?id=~Arthur_Gretton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arthur_Gretton1">Arthur Gretton</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>21 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#0PtUPB9z6qK-details-901" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0PtUPB9z6qK-details-901"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Sampling, MCMC, Generative Models, Adversarial training, Optimization, Density estimation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We introduce the Generalized Energy Based Model (GEBM) for generative modelling. These models combine two  trained components: a base distribution (generally an implicit model), which can learn the support of data with low intrinsic dimension in a high dimensional space; and an energy function, to refine the probability mass on the learned support. 
      Both the energy function and base jointly constitute the final model, unlike GANs, which retain only the base distribution (the "generator").  
      GEBMs are trained by alternating between learning the energy and the base. 
      We show that both training stages are well-defined: the energy is learned by maximising a generalized likelihood, and the resulting energy-based loss provides informative gradients for learning the base.
      Samples from the posterior on the latent space of the trained model can be obtained via MCMC, thus finding regions in this space that produce better quality samples.
      Empirically, the GEBM samples on image-generation tasks are of much better quality than those from the learned generator alone, indicating that all else being equal, the GEBM will outperform a GAN of the same complexity. When using normalizing flows as base measures, GEBMs succeed on density modelling tasks returning comparable performance to direct maximum likelihood of the same networks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce the Generalized Energy Based Model (GEBM) for generative modelling. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=0PtUPB9z6qK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="qVyeW-grC2k" data-number="1616">
      <h4>
        <a href="https://openreview.net/forum?id=qVyeW-grC2k">
            Long Range Arena : A Benchmark for Efficient Transformers 
        </a>
      
        
          <a href="https://openreview.net/pdf?id=qVyeW-grC2k" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yi_Tay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Tay1">Yi Tay</a>, <a href="https://openreview.net/profile?id=~Mostafa_Dehghani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mostafa_Dehghani1">Mostafa Dehghani</a>, <a href="https://openreview.net/profile?id=~Samira_Abnar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samira_Abnar1">Samira Abnar</a>, <a href="https://openreview.net/profile?id=~Yikang_Shen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yikang_Shen1">Yikang Shen</a>, <a href="https://openreview.net/profile?id=~Dara_Bahri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dara_Bahri1">Dara Bahri</a>, <a href="https://openreview.net/profile?id=~Philip_Pham1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philip_Pham1">Philip Pham</a>, <a href="https://openreview.net/profile?id=~Jinfeng_Rao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinfeng_Rao2">Jinfeng Rao</a>, <a href="https://openreview.net/profile?email=yangliuy%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yangliuy@google.com">Liu Yang</a>, <a href="https://openreview.net/profile?id=~Sebastian_Ruder2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sebastian_Ruder2">Sebastian Ruder</a>, <a href="https://openreview.net/profile?email=metzler%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="metzler@google.com">Donald Metzler</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#qVyeW-grC2k-details-524" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qVyeW-grC2k-details-524"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Transformers, Attention, Deep Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, Long Range Arena, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="275" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mi>K</mi></math></mjx-assistive-mml></mjx-container> to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="276" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>16</mn><mi>K</mi></math></mjx-assistive-mml></mjx-container> tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. Long Range Arena paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Better benchmarking for Xformers</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="MyHwDabUHZm" data-number="2341">
      <h4>
        <a href="https://openreview.net/forum?id=MyHwDabUHZm">
            Beyond Categorical Label Representations for Image Classification
        </a>
      
        
          <a href="https://openreview.net/pdf?id=MyHwDabUHZm" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Boyuan_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Boyuan_Chen1">Boyuan Chen</a>, <a href="https://openreview.net/profile?email=yl4019%40columbia.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yl4019@columbia.edu">Yu Li</a>, <a href="https://openreview.net/profile?email=sr3587%40columbia.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="sr3587@columbia.edu">Sunand Raghupathi</a>, <a href="https://openreview.net/profile?id=~Hod_Lipson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hod_Lipson1">Hod Lipson</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>23 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#MyHwDabUHZm-details-452" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MyHwDabUHZm-details-452"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Label Representation, Image Classification, Representation Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We find that the way we choose to represent data labels can have a profound effect on the quality of trained models. For example, training an image classifier to regress audio labels rather than traditional categorical probabilities produces a more reliable classification. This result is surprising, considering that audio labels are more complex than simpler numerical probabilities or text. We hypothesize that high dimensional, high entropy label representations are generally more useful because they provide a stronger error signal. We support this hypothesis with evidence from various label representations including constant matrices, spectrograms, shuffled spectrograms, Gaussian mixtures, and uniform random matrices of various dimensionalities. Our experiments reveal that high dimensional, high entropy labels achieve comparable accuracy to text (categorical) labels on standard image classification tasks, but features learned through our label representations exhibit more robustness under various adversarial attacks and better effectiveness with a limited amount of training data. These results suggest that label representation may play a more important role than previously thought.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We study the role of label representations for standard image classification task and found high-dimensional hign-entropy labes generally lead to more robust and data-efficient networks.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="eom0IUrF__F" data-number="1416">
      <h4>
        <a href="https://openreview.net/forum?id=eom0IUrF__F">
            CoCo: Controllable Counterfactuals for Evaluating Dialogue State Trackers
        </a>
      
        
          <a href="https://openreview.net/pdf?id=eom0IUrF__F" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~SHIYANG_LI2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~SHIYANG_LI2">SHIYANG LI</a>, <a href="https://openreview.net/profile?id=~Semih_Yavuz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Semih_Yavuz1">Semih Yavuz</a>, <a href="https://openreview.net/profile?id=~Kazuma_Hashimoto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kazuma_Hashimoto1">Kazuma Hashimoto</a>, <a href="https://openreview.net/profile?id=~Jia_Li8" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jia_Li8">Jia Li</a>, <a href="https://openreview.net/profile?email=tniu%40salesforce.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="tniu@salesforce.com">Tong Niu</a>, <a href="https://openreview.net/profile?id=~Nazneen_Rajani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nazneen_Rajani1">Nazneen Rajani</a>, <a href="https://openreview.net/profile?id=~Xifeng_Yan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xifeng_Yan1">Xifeng Yan</a>, <a href="https://openreview.net/profile?id=~Yingbo_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yingbo_Zhou1">Yingbo Zhou</a>, <a href="https://openreview.net/profile?id=~Caiming_Xiong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Caiming_Xiong1">Caiming Xiong</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#eom0IUrF__F-details-984" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eom0IUrF__F-details-984"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">task-oriented dialogue, dialogue state tracking, robustness, dst, evaluation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Dialogue state trackers have made significant progress on benchmark datasets, but their generalization capability to novel and realistic scenarios beyond the held- out conversations is less understood. We propose controllable counterfactuals (COCO) to bridge this gap and evaluate dialogue state tracking (DST) models on novel scenarios, i.e., would the system successfully tackle the request if the user responded differently but still consistently with the dialogue flow? COCO leverages turn-level belief states as counterfactual conditionals to produce novel conversation scenarios in two steps: (i) counterfactual goal generation at turn- level by dropping and adding slots followed by replacing slot values, (ii) counterfactual conversation generation that is conditioned on (i) and consistent with the dialogue flow. Evaluating state-of-the-art DST models on MultiWOZ dataset with COCO-generated counterfactuals results in a significant performance drop of up to 30.8% (from 49.4% to 18.6%) in absolute joint goal accuracy. In comparison, widely used techniques like paraphrasing only affect the accuracy by at most 2%. Human evaluations show that COCO-generated conversations perfectly reflect the underlying user goal with more than 95% accuracy and are as human-like as the original conversations, further strengthening its reliability and promise to be adopted as part of the robustness evaluation of DST models.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">CoCo is a principled method to more flexibly evaluate the robustness of the DST component of TOD systems.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="gwFTuzxJW0" data-number="976">
      <h4>
        <a href="https://openreview.net/forum?id=gwFTuzxJW0">
            Stochastic Security: Adversarial Defense Using Long-Run Dynamics of Energy-Based Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=gwFTuzxJW0" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mitch_Hill1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mitch_Hill1">Mitch Hill</a>, <a href="https://openreview.net/profile?id=~Jonathan_Craig_Mitchell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Craig_Mitchell1">Jonathan Craig Mitchell</a>, <a href="https://openreview.net/profile?id=~Song-Chun_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Song-Chun_Zhu1">Song-Chun Zhu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#gwFTuzxJW0-details-153" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="gwFTuzxJW0-details-153"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">adversarial defense, adversarial robustness, energy-based model, Markov chain Monte Carlo, Langevin sampling, adversarial attack</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The vulnerability of deep networks to adversarial attacks is a central problem for deep learning from the perspective of both cognition and security. The current most successful defense method is to train a classifier using adversarial images created during learning. Another defense approach involves transformation or purification of the original input to remove adversarial signals before the image is classified. We focus on defending naturally-trained classifiers using Markov Chain Monte Carlo (MCMC) sampling with an Energy-Based Model (EBM) for adversarial purification. In contrast to adversarial training, our approach is intended to secure highly vulnerable pre-existing classifiers. To our knowledge, no prior defensive transformation is capable of securing naturally-trained classifiers, and our method is the first to validate a post-training defense approach that is distinct from current successful defenses which modify classifier training.
      
      The memoryless behavior of long-run MCMC sampling will eventually remove adversarial signals, while metastable behavior preserves consistent appearance of MCMC samples after many steps to allow accurate long-run prediction. Balancing these factors can lead to effective purification and robust classification. We evaluate adversarial defense with an EBM using the strongest known attacks against purification. Our contributions are 1) an improved method for training EBM's with realistic long-run MCMC samples for effective purification, 2) an Expectation-Over-Transformation (EOT) defense that resolves ambiguities for evaluating stochastic defenses and from which the EOT attack naturally follows, and 3) state-of-the-art adversarial defense for naturally-trained classifiers and competitive defense compared to adversarial training on CIFAR-10, SVHN, and CIFAR-100. Our code and pre-trained models are available at https://github.com/point0bar1/ebm-defense.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Our defensive transformation using long-run MCMC sampling with a convergent EBM is the first method to successfully defend naturally-trained classifiers against adversarial attacks.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=gwFTuzxJW0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="LiX3ECzDPHZ" data-number="1373">
      <h4>
        <a href="https://openreview.net/forum?id=LiX3ECzDPHZ">
            X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback
        </a>
      
        
          <a href="https://openreview.net/pdf?id=LiX3ECzDPHZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=jenseng%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jenseng@berkeley.edu">Jensen Gao</a>, <a href="https://openreview.net/profile?id=~Siddharth_Reddy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siddharth_Reddy1">Siddharth Reddy</a>, <a href="https://openreview.net/profile?id=~Glen_Berseth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Glen_Berseth1">Glen Berseth</a>, <a href="https://openreview.net/profile?email=nhardy01%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="nhardy01@gmail.com">Nicholas Hardy</a>, <a href="https://openreview.net/profile?email=nikhilesh.natraj%40ucsf.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="nikhilesh.natraj@ucsf.edu">Nikhilesh Natraj</a>, <a href="https://openreview.net/profile?email=karunesh.ganguly%40ucsf.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="karunesh.ganguly@ucsf.edu">Karunesh Ganguly</a>, <a href="https://openreview.net/profile?id=~Anca_Dragan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anca_Dragan1">Anca Dragan</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#LiX3ECzDPHZ-details-859" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LiX3ECzDPHZ-details-859"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, human-computer interaction</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze or neural activity measured by a brain implant. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, a large-scale observational study on handwriting samples from 60 users, and a pilot study with one participant using an electrocorticography-based brain-computer interface. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We use online learning from user feedback to train an adaptive interface for typing words using inputs from a brain implant or webcam.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="J3OUycKwz-" data-number="3640">
      <h4>
        <a href="https://openreview.net/forum?id=J3OUycKwz-">
            Mapping the Timescale Organization of Neural Language Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=J3OUycKwz-" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hsiang-Yun_Sherry_Chien1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hsiang-Yun_Sherry_Chien1">Hsiang-Yun Sherry Chien</a>, <a href="https://openreview.net/profile?id=~Jinhan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinhan_Zhang1">Jinhan Zhang</a>, <a href="https://openreview.net/profile?id=~Christopher_Honey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Honey1">Christopher Honey</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#J3OUycKwz--details-746" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="J3OUycKwz--details-746"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">natural language processing, LSTM, timescale, hierarchy, temporal context</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In the human brain, sequences of language input are processed within a distributed and hierarchical architecture, in which higher stages of processing encode contextual information over longer timescales. In contrast, in recurrent neural networks which perform natural language processing, we know little about how the multiple timescales of contextual information are functionally organized. Therefore, we applied tools developed in neuroscience to map the “processing timescales” of individual units within a word-level LSTM language model. This timescale-mapping method assigned long timescales to units previously found to track long-range syntactic dependencies. Additionally, the mapping revealed a small subset of the network (less than 15% of units) with long timescales and whose function had not previously been explored. We next probed the functional organization of the network by examining the relationship between the processing timescale of units and their network connectivity. We identified two classes of long-timescale units: “controller” units composed a densely interconnected subnetwork and strongly projected to the rest of the network, while “integrator” units showed the longest timescales in the network, and expressed projection profiles closer to the mean projection profile. Ablating integrator and controller units affected model performance at different positions within a sentence, suggesting distinctive functions of these two sets of units. Finally, we tested the generalization of these results to a character-level LSTM model and models with different architectures. In summary, we demonstrated a model-free technique for mapping the timescale organization in recurrent neural networks, and we applied this method to reveal the timescale and functional organization of neural language models</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We demonstrated a model-free technique for mapping the timescale organization in neural network models, and we applied this method to reveal a hierarchical timescale organization within LSTM language models.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="vLaHRtHvfFp" data-number="147">
      <h4>
        <a href="https://openreview.net/forum?id=vLaHRtHvfFp">
            PDE-Driven Spatiotemporal Disentanglement
        </a>
      
        
          <a href="https://openreview.net/pdf?id=vLaHRtHvfFp" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=jeremie.dona%40lip6.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="jeremie.dona@lip6.fr">Jérémie Donà</a>, <a href="https://openreview.net/profile?id=~Jean-Yves_Franceschi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jean-Yves_Franceschi1">Jean-Yves Franceschi</a>, <a href="https://openreview.net/profile?id=~sylvain_lamprier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~sylvain_lamprier1">sylvain lamprier</a>, <a href="https://openreview.net/profile?id=~patrick_gallinari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~patrick_gallinari1">patrick gallinari</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#vLaHRtHvfFp-details-130" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vLaHRtHvfFp-details-130"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">disentanglement, spatiotemporal prediction, representation learning, dynamical systems, separation of variables</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A recent line of work in the machine learning community addresses the problem of predicting high-dimensional spatiotemporal phenomena by leveraging specific tools from the differential equations theory. Following this direction, we propose in this article a novel and general paradigm for this task based on a resolution method for partial differential equations: the separation of variables. This inspiration allows us to introduce a dynamical interpretation of spatiotemporal disentanglement. It induces a principled model based on learning disentangled spatial and temporal representations of a phenomenon to accurately predict future observations. We experimentally demonstrate the performance and broad applicability of our method against prior state-of-the-art models on physical and synthetic video datasets.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a novel interpretation of spatiotemporal disentanglement, inducing a simple and performant disentangled prediction model.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=vLaHRtHvfFp&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="V69LGwJ0lIN" data-number="870">
      <h4>
        <a href="https://openreview.net/forum?id=V69LGwJ0lIN">
            OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=V69LGwJ0lIN" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Anurag_Ajay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anurag_Ajay1">Anurag Ajay</a>, <a href="https://openreview.net/profile?id=~Aviral_Kumar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aviral_Kumar2">Aviral Kumar</a>, <a href="https://openreview.net/profile?id=~Pulkit_Agrawal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pulkit_Agrawal1">Pulkit Agrawal</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>, <a href="https://openreview.net/profile?id=~Ofir_Nachum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ofir_Nachum1">Ofir Nachum</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>23 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#V69LGwJ0lIN-details-754" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="V69LGwJ0lIN-details-754"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Offline Reinforcement Learning, Primitive Discovery, Unsupervised Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Reinforcement learning (RL) has achieved impressive performance in a variety of online settings in which an agent’s ability to query the environment for transitions and rewards is effectively unlimited. However, in many practical applications, the situation is reversed:  an agent may have access to large amounts of undirected offline experience data, while access to the online environment is severely limited. In this work, we focus on this offline setting. Our main insight is that, when presented with offline data composed of a variety of behaviors, an effective way to leverage this data is to extract a continuous space of recurring and temporally extended primitive behaviors before using these primitives for downstream task learning.  Primitives extracted in this way serve two purposes:  they delineate the behaviors that are supported by the data from those that are not, making them useful for avoiding distributional shift in offline RL; and they provide a degree of temporal abstraction, which reduces the effective horizon yielding better learning in theory, and improved offline RL in practice. In addition to benefiting offline policy optimization, we show that performing offline primitive learning in this way can also be leveraged for improving few-shot imitation learning as well as exploration and transfer in online RL on a variety of benchmark domains. Visualizations and code are available at https://sites.google.com/view/opal-iclr</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">An effective way to leverage multimodal offline behavioral data is to extract a continuous space of primitives, and use it for downstream task learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="yUxUNaj2Sl" data-number="2111">
      <h4>
        <a href="https://openreview.net/forum?id=yUxUNaj2Sl">
            Does enhanced shape bias improve neural network robustness to common corruptions?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=yUxUNaj2Sl" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Chaithanya_Kumar_Mummadi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chaithanya_Kumar_Mummadi1">Chaithanya Kumar Mummadi</a>, <a href="https://openreview.net/profile?id=~Ranjitha_Subramaniam1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ranjitha_Subramaniam1">Ranjitha Subramaniam</a>, <a href="https://openreview.net/profile?id=~Robin_Hutmacher1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Robin_Hutmacher1">Robin Hutmacher</a>, <a href="https://openreview.net/profile?id=~Julien_Vitay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Julien_Vitay1">Julien Vitay</a>, <a href="https://openreview.net/profile?id=~Volker_Fischer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Volker_Fischer1">Volker Fischer</a>, <a href="https://openreview.net/profile?id=~Jan_Hendrik_Metzen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jan_Hendrik_Metzen1">Jan Hendrik Metzen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#yUxUNaj2Sl-details-148" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="yUxUNaj2Sl-details-148"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">neural network robustness, shape bias, corruptions, distribution shift</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Convolutional neural networks (CNNs) learn to extract representations of complex features, such as object shapes and textures to solve image recognition tasks. Recent work indicates that CNNs trained on ImageNet are biased towards features that encode textures and that these alone are sufficient to generalize to unseen test data from the same distribution as the training data but often fail to generalize to out-of-distribution data. It has been shown that augmenting the training data with different image styles decreases this texture bias in favor of increased shape bias while at the same time improving robustness to common corruptions, such as noise and blur. Commonly, this is interpreted as shape bias increasing corruption robustness. However, this relationship is only hypothesized. We perform a systematic study of different ways of composing inputs based on natural images, explicit edge information, and stylization. While stylization is essential for achieving high corruption robustness, we do not find a clear correlation between shape bias and robustness. We conclude that the data augmentation caused by style-variation  accounts for the improved corruption robustness and increased shape bias is only a byproduct.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that robustness on common corruptions donot correlate with strong shape bias but with the effective data augmentation strategies like stylization</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=yUxUNaj2Sl&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="JbuYF437WB6" data-number="1297">
      <h4>
        <a href="https://openreview.net/forum?id=JbuYF437WB6">
            Directed Acyclic Graph Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=JbuYF437WB6" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Veronika_Thost1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Veronika_Thost1">Veronika Thost</a>, <a href="https://openreview.net/profile?id=~Jie_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jie_Chen1">Jie Chen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#JbuYF437WB6-details-748" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JbuYF437WB6-details-748"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Graph Neural Networks, Graph Representation Learning, Directed Acyclic Graphs, DAG, Inductive Bias</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Graph-structured data ubiquitously appears in science and engineering. Graph neural networks (GNNs) are designed to exploit the relational inductive bias exhibited in graphs; they have been shown to outperform other forms of neural networks in scenarios where structure information supplements node features. The most common GNN architecture aggregates information from neighborhoods based on message passing. Its generality has made it broadly applicable. In this paper, we focus on a special, yet widely used, type of graphs---DAGs---and inject a stronger inductive bias---partial ordering---into the neural network design. We propose the directed acyclic graph neural network, DAGNN, an architecture that processes information according to the flow defined by the partial order. DAGNN can be considered a framework that entails earlier works as special cases (e.g., models for trees and models updating node representations recurrently), but we identify several crucial components that prior architectures lack. We perform comprehensive experiments, including ablation studies, on representative DAG datasets (i.e., source code, neural architectures, and probabilistic graphical models) and demonstrate the superiority of DAGNN over simpler DAG architectures as well as general graph architectures.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose DAGNN, a graph neural network tailored to directed acyclic graphs that outperforms conventional GNNs by leveraging the partial order as strong inductive bias besides other suitable architectural features.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Rcmk0xxIQV" data-number="1649">
      <h4>
        <a href="https://openreview.net/forum?id=Rcmk0xxIQV">
            QPLEX: Duplex Dueling Multi-Agent Q-Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Rcmk0xxIQV" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jianhao_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianhao_Wang1">Jianhao Wang</a>, <a href="https://openreview.net/profile?id=~Zhizhou_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhizhou_Ren1">Zhizhou Ren</a>, <a href="https://openreview.net/profile?id=~Terry_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Terry_Liu2">Terry Liu</a>, <a href="https://openreview.net/profile?id=~Yang_Yu5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Yu5">Yang Yu</a>, <a href="https://openreview.net/profile?id=~Chongjie_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongjie_Zhang1">Chongjie Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 21 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Rcmk0xxIQV-details-925" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Rcmk0xxIQV-details-925"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multi-agent reinforcement learning, Value factorization, Dueling structure</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We explore value-based multi-agent reinforcement learning (MARL) in the popular paradigm of centralized training with decentralized execution (CTDE). CTDE has an important concept, Individual-Global-Max (IGM) principle, which requires the consistency between joint and local action selections to support efficient local decision-making. However, in order to achieve scalability, existing MARL methods either limit representation expressiveness of their value function classes or relax the IGM consistency, which may suffer from instability risk or may not perform well in complex domains. This paper presents a novel MARL approach, called duPLEX dueling multi-agent Q-learning (QPLEX), which takes a duplex dueling network architecture to factorize the joint value function. This duplex dueling structure encodes the IGM principle into the neural network architecture and thus enables efficient value function learning. Theoretical analysis shows that QPLEX achieves a complete IGM function class. Empirical experiments on StarCraft II micromanagement tasks demonstrate that QPLEX significantly outperforms state-of-the-art baselines in both online and offline data collection settings, and also reveal that QPLEX achieves high sample efficiency and can benefit from offline datasets without additional online exploration.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A novel multi-agent Q-learning algorithm with a complete IGM (Individual-Global-Max) function class.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Rcmk0xxIQV&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="v_1Soh8QUNc" data-number="3066">
      <h4>
        <a href="https://openreview.net/forum?id=v_1Soh8QUNc">
            Learning Energy-Based Models by Diffusion Recovery Likelihood
        </a>
      
        
          <a href="https://openreview.net/pdf?id=v_1Soh8QUNc" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ruiqi_Gao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruiqi_Gao2">Ruiqi Gao</a>, <a href="https://openreview.net/profile?id=~Yang_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Song1">Yang Song</a>, <a href="https://openreview.net/profile?id=~Ben_Poole1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ben_Poole1">Ben Poole</a>, <a href="https://openreview.net/profile?id=~Ying_Nian_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ying_Nian_Wu1">Ying Nian Wu</a>, <a href="https://openreview.net/profile?id=~Diederik_P_Kingma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Diederik_P_Kingma1">Diederik P Kingma</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#v_1Soh8QUNc-details-928" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="v_1Soh8QUNc-details-928"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">energy-based model, EBM, recovery likelihood, generative model, diffusion process, MCMC, Langevin dynamics, HMC</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While energy-based models (EBMs) exhibit a number of desirable properties, training and sampling on high-dimensional datasets remains challenging. Inspired by recent progress on diffusion probabilistic models, we present a diffusion recovery likelihood method to tractably learn and sample from a sequence of EBMs trained on increasingly noisy versions of a dataset. Each EBM is trained with recovery likelihood,  which maximizes the conditional probability of the data at a certain noise level given their noisy versions at a higher noise level. Optimizing recovery likelihood is more tractable than marginal likelihood, as sampling from the conditional distributions is much easier than sampling from the marginal distributions. After training, synthesized images can be generated by the sampling process that initializes from Gaussian white noise distribution and progressively samples the conditional distributions at decreasingly lower noise levels.  Our method generates high fidelity samples on various image datasets. On unconditional CIFAR-10 our method achieves FID 9.58 and inception score 8.30, superior to the majority of GANs. Moreover, we demonstrate that unlike previous work on EBMs, our long-run MCMC samples from the conditional distributions do not diverge and still represent realistic images, allowing us to accurately estimate the normalized density of data even for high-dimensional datasets. Our implementation is available at \url{https://github.com/ruiqigao/recovery_likelihood}.
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a diffusion recovery likelihood method to tractably learn and sample from a sequence of EBMs based on a diffusion process. High sample quality, stable long-run MCMC chains and good estimation of likelihood. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="7t1FcJUWhi3" data-number="2315">
      <h4>
        <a href="https://openreview.net/forum?id=7t1FcJUWhi3">
            Neural Networks for Learning Counterfactual G-Invariances from Single Environments
        </a>
      
        
          <a href="https://openreview.net/pdf?id=7t1FcJUWhi3" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~S_Chandra_Mouli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~S_Chandra_Mouli1">S Chandra Mouli</a>, <a href="https://openreview.net/profile?id=~Bruno_Ribeiro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bruno_Ribeiro1">Bruno Ribeiro</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>25 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#7t1FcJUWhi3-details-248" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7t1FcJUWhi3-details-248"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Extrapolation, G-invariance regularization, Counterfactual inference, Invariant subspaces</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Despite —or maybe because of— their astonishing capacity to fit data, neural networks are believed to have difficulties extrapolating beyond training data distribution. This work shows that, for extrapolations based on finite transformation groups, a model’s inability to extrapolate is unrelated to its capacity. Rather, the shortcoming is inherited from a learning hypothesis: Examples not explicitly observed with infinitely many training examples have underspecified outcomes in the learner’s model. In order to endow neural networks with the ability to extrapolate over group transformations, we introduce a learning framework counterfactually-guided by the learning hypothesis that any group invariance to (known) transformation groups is mandatory even without evidence, unless the learner deems it inconsistent with the training data. Unlike existing invariance-driven methods for (counterfactual) extrapolations, this framework allows extrapolations from a single environment. Finally, we introduce sequence and image extrapolation tasks that validate our framework and showcase the shortcomings of traditional approaches.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This work introduces a novel learning framework for single-environment extrapolations, where invariance to transformation groups is mandatory even without evidence, unless the learner deems it inconsistent with the training data.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="OMNB1G5xzd4" data-number="445">
      <h4>
        <a href="https://openreview.net/forum?id=OMNB1G5xzd4">
            Model-Based Offline Planning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=OMNB1G5xzd4" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=aarg%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="aarg@google.com">Arthur Argenson</a>, <a href="https://openreview.net/profile?id=~Gabriel_Dulac-Arnold1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gabriel_Dulac-Arnold1">Gabriel Dulac-Arnold</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#OMNB1G5xzd4-details-328" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OMNB1G5xzd4-details-328"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">off-line reinforcement learning, model-based reinforcement learning, model-based control, reinforcement learning, model predictive control, robotics</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Offline learning is a key part of making reinforcement learning (RL) useable in real systems. Offline RL looks at scenarios where there is data from a system's operation, but no direct access to the system when learning a policy. Recent work on training RL policies from offline data has shown results both with model-free policies learned directly from the data, or with planning on top of learnt models of the data. Model-free policies tend to be more performant, but are more opaque, harder to command externally, and less easy to integrate into larger systems. We propose an offline learner that generates a model that can be used to control the system directly through planning. This allows us to have easily controllable policies directly from data, without ever interacting with the system. We show the performance of our algorithm, Model-Based Offline Planning (MBOP) on a series of robotics-inspired tasks, and demonstrate its ability leverage planning to respect environmental constraints. We are able to find near-optimal polices for certain simulated systems from as little as 50 seconds of real-time system interaction, and create zero-shot goal-conditioned policies on a series of environments.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This approach adapts model-based reinforcement learning to offline regimes with little data, and shows state of the art control in offline scenarios.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=OMNB1G5xzd4&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xgGS6PmzNq6" data-number="1434">
      <h4>
        <a href="https://openreview.net/forum?id=xgGS6PmzNq6">
            On Dyadic Fairness: Exploring and Mitigating Bias in Graph Connections
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xgGS6PmzNq6" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Peizhao_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peizhao_Li1">Peizhao Li</a>, <a href="https://openreview.net/profile?id=~Yifei_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yifei_Wang3">Yifei Wang</a>, <a href="https://openreview.net/profile?id=~Han_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Han_Zhao1">Han Zhao</a>, <a href="https://openreview.net/profile?id=~Pengyu_Hong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pengyu_Hong1">Pengyu Hong</a>, <a href="https://openreview.net/profile?id=~Hongfu_Liu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongfu_Liu2">Hongfu Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xgGS6PmzNq6-details-49" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xgGS6PmzNq6-details-49"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">algorithmic fairness, graph-structured data</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Disparate impact has raised serious concerns in machine learning applications and its societal impacts. In response to the need of mitigating discrimination, fairness has been regarded as a crucial property in algorithmic design. In this work, we study the problem of disparate impact on graph-structured data. Specifically, we focus on dyadic fairness, which articulates a fairness concept that a predictive relationship between two instances should be independent of the sensitive attributes. Based on this, we theoretically relate the graph connections to dyadic fairness on link predictive scores in learning graph neural networks, and reveal that regulating weights on existing edges in a graph contributes to dyadic fairness conditionally. Subsequently, we propose our algorithm, \textbf{FairAdj}, to empirically learn a fair adjacency matrix with proper graph structural constraints for fair link prediction, and in the meanwhile preserve predictive accuracy as much as possible. Empirical validation demonstrates that our method delivers effective dyadic fairness in terms of various statistics, and at the same time enjoys a favorable fairness-utility tradeoff.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A new method on the fairness of predictive relationships in graph-structured data</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=xgGS6PmzNq6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="BtZhsSGNRNi" data-number="801">
      <h4>
        <a href="https://openreview.net/forum?id=BtZhsSGNRNi">
            Coping with Label Shift via Distributionally Robust Optimisation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=BtZhsSGNRNi" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jingzhao_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jingzhao_Zhang2">Jingzhao Zhang</a>, <a href="https://openreview.net/profile?id=~Aditya_Krishna_Menon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aditya_Krishna_Menon1">Aditya Krishna Menon</a>, <a href="https://openreview.net/profile?id=~Andreas_Veit1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_Veit1">Andreas Veit</a>, <a href="https://openreview.net/profile?id=~Srinadh_Bhojanapalli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Srinadh_Bhojanapalli1">Srinadh Bhojanapalli</a>, <a href="https://openreview.net/profile?id=~Sanjiv_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanjiv_Kumar1">Sanjiv Kumar</a>, <a href="https://openreview.net/profile?id=~Suvrit_Sra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Suvrit_Sra1">Suvrit Sra</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#BtZhsSGNRNi-details-474" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="BtZhsSGNRNi-details-474"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Label shift, distributional robust optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The label shift problem refers to the supervised learning setting where the train and test label distributions do not match. Existing work addressing label shift usually assumes access to an unlabelled test sample. This sample may be used to estimate the test label distribution, and to then train a suitably re-weighted classifier. While approaches using this idea have proven effective, their scope is limited as it is not always feasible to access the target domain; further, they require repeated retraining if the model is to be deployed in multiple test environments. Can one instead learn a single classifier that is robust to arbitrary label shifts from a broad family? In this paper, we answer this question by proposing a model that minimises an objective based on distributionally robust optimisation (DRO). We then design and analyse a gradient descent-proximal mirror ascent algorithm tailored for large-scale problems to optimise the proposed objective.  Finally, through experiments on CIFAR-100 and ImageNet, we show that our technique can significantly improve performance over a number of baselines in settings where label shift is present.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an objective to cope with label shift, and provide an adversarial algorithm to effectively optimize it.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="YCXrx6rRCXO" data-number="2225">
      <h4>
        <a href="https://openreview.net/forum?id=YCXrx6rRCXO">
            Faster Binary Embeddings for Preserving Euclidean Distances
        </a>
      
        
          <a href="https://openreview.net/pdf?id=YCXrx6rRCXO" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jinjie_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinjie_Zhang1">Jinjie Zhang</a>, <a href="https://openreview.net/profile?id=~Rayan_Saab1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rayan_Saab1">Rayan Saab</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>22 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#YCXrx6rRCXO-details-467" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YCXrx6rRCXO-details-467"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Binary Embeddings, Johnson-Lindenstrauss Transforms, Sigma Delta Quantization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a fast, distance-preserving, binary embedding algorithm to transform a high-dimensional dataset <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="277" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c54 TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2286"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.41em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">T</mi></mrow><mo>⊆</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>n</mi></msup></math></mjx-assistive-mml></mjx-container> into binary sequences in the cube <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="278" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c7B"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-cB1"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c7D"></mjx-c></mjx-mo><mjx-script style="vertical-align: 0.363em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo fence="false" stretchy="false">{</mo><mo>±</mo><mn>1</mn><msup><mo fence="false" stretchy="false">}</mo><mi>m</mi></msup></math></mjx-assistive-mml></mjx-container>. When <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="279" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c54 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">T</mi></mrow></math></mjx-assistive-mml></mjx-container> consists of well-spread (i.e., non-sparse) vectors, our embedding method applies a stable noise-shaping quantization scheme to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="280" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi><mi>x</mi></math></mjx-assistive-mml></mjx-container> where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="281" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.41em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></math></mjx-assistive-mml></mjx-container> is a sparse Gaussian random matrix. This contrasts with most binary embedding methods, which usually use <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="282" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c21A6"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c73"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c69"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c6E"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi><mo stretchy="false">↦</mo><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">g</mi><mi mathvariant="normal">n</mi></mrow><mo stretchy="false">(</mo><mi>A</mi><mi>x</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> for the embedding. Moreover, we show that Euclidean distances among the elements of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="283" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c54 TEX-C"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">T</mi></mrow></math></mjx-assistive-mml></mjx-container> are approximated by the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="284" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>1</mn></msub></math></mjx-assistive-mml></mjx-container> norm on the images of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="285" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c7B"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-cB1"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c7D"></mjx-c></mjx-mo><mjx-script style="vertical-align: 0.363em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo fence="false" stretchy="false">{</mo><mo>±</mo><mn>1</mn><msup><mo fence="false" stretchy="false">}</mo><mi>m</mi></msup></math></mjx-assistive-mml></mjx-container> under a fast linear transformation. This again contrasts with standard methods, where the Hamming distance is used instead.  Our method is both fast and memory efficient, with time complexity  <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="286" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> and space complexity <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="287" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> on well-spread data. When the data is not well-spread, we show that the approach still works provided that data is transformed via a Walsh-Hadamard matrix, but now the cost is <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="288" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-n" space="2"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mi>n</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> per data point.  Further, we prove that the method is accurate and its associated error is comparable to that of a continuous valued Johnson-Lindenstrauss embedding plus a quantization error that admits a polynomial decay as the embedding dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="289" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi></math></mjx-assistive-mml></mjx-container> increases.
      	Thus the length of the binary codes required to achieve a desired accuracy is quite small, and we show it can even be compressed further without compromising the accuracy. To illustrate our results, we test the proposed method on natural images and show that it achieves strong performance.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a fast binary embedding algorithm to preserve Euclidean distances among well-spread vectors and it achieves optimal bit complexity.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="HCSgyPUfeDj" data-number="1955">
      <h4>
        <a href="https://openreview.net/forum?id=HCSgyPUfeDj">
            Learning and Evaluating Representations for Deep One-Class Classification
        </a>
      
        
          <a href="https://openreview.net/pdf?id=HCSgyPUfeDj" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kihyuk_Sohn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kihyuk_Sohn1">Kihyuk Sohn</a>, <a href="https://openreview.net/profile?id=~Chun-Liang_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chun-Liang_Li1">Chun-Liang Li</a>, <a href="https://openreview.net/profile?id=~Jinsung_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinsung_Yoon1">Jinsung Yoon</a>, <a href="https://openreview.net/profile?email=minhojin%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="minhojin@google.com">Minho Jin</a>, <a href="https://openreview.net/profile?id=~Tomas_Pfister1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tomas_Pfister1">Tomas Pfister</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#HCSgyPUfeDj-details-422" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HCSgyPUfeDj-details-422"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep one-class classification, self-supervised learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present a two-stage framework for deep one-class classification. We first learn self-supervised  representations from one-class data, and then build one-class classifiers on learned representations. The framework not only allows to learn better representations, but also permits building one-class classifiers that are faithful to the target task. We argue that classifiers inspired by the statistical perspective in generative or discriminative models are more effective than existing approaches, such as a normality score from a surrogate classifier. We thoroughly evaluate different self-supervised representation learning algorithms under the proposed framework for one-class classification. Moreover, we present a novel distribution-augmented contrastive learning that extends training distributions via data augmentation to obstruct the uniformity of contrastive representations. In experiments, we demonstrate state-of-the-art performance on visual domain one-class classification benchmarks, including novelty and anomaly detection. Finally, we present visual explanations, confirming that the decision-making process of deep one-class classifiers is intuitive to humans. The code is available at https://github.com/google-research/deep_representation_one_class.
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a two-stage framework for deep one-class classification, composed of state-of-the-art self-supervised representation learning followed by generative or discriminative one-class classifiers.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="v8b3e5jN66j" data-number="1947">
      <h4>
        <a href="https://openreview.net/forum?id=v8b3e5jN66j">
            Conditional Negative Sampling for Contrastive Learning of Visual Representations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=v8b3e5jN66j" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mike_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mike_Wu1">Mike Wu</a>, <a href="https://openreview.net/profile?email=mmosse19%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mmosse19@stanford.edu">Milan Mosse</a>, <a href="https://openreview.net/profile?id=~Chengxu_Zhuang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chengxu_Zhuang1">Chengxu Zhuang</a>, <a href="https://openreview.net/profile?id=~Daniel_Yamins1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Yamins1">Daniel Yamins</a>, <a href="https://openreview.net/profile?id=~Noah_Goodman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noah_Goodman1">Noah Goodman</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#v8b3e5jN66j-details-904" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="v8b3e5jN66j-details-904"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">contrastive learning, hard negative mining, mutual information, lower bound, detection, segmentation, MoCo</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a "ring" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=v8b3e5jN66j&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="onxoVA9FxMw" data-number="557">
      <h4>
        <a href="https://openreview.net/forum?id=onxoVA9FxMw">
            On Position Embeddings in BERT
        </a>
      
        
          <a href="https://openreview.net/pdf?id=onxoVA9FxMw" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Benyou_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benyou_Wang2">Benyou Wang</a>, <a href="https://openreview.net/profile?id=~Lifeng_Shang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lifeng_Shang1">Lifeng Shang</a>, <a href="https://openreview.net/profile?id=~Christina_Lioma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christina_Lioma1">Christina Lioma</a>, <a href="https://openreview.net/profile?id=~Xin_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xin_Jiang1">Xin Jiang</a>, <a href="https://openreview.net/profile?id=~Hao_Yang7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Yang7">Hao Yang</a>, <a href="https://openreview.net/profile?id=~Qun_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qun_Liu1">Qun Liu</a>, <a href="https://openreview.net/profile?id=~Jakob_Grue_Simonsen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jakob_Grue_Simonsen1">Jakob Grue Simonsen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 02 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#onxoVA9FxMw-details-878" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="onxoVA9FxMw-details-878"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Position Embedding, BERT, pretrained language model.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Various Position Embeddings (PEs) have been proposed in Transformer based architectures~(e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vector space:  translation invariance, monotonicity, and  symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way.
      Moreover, we propose a new probing test (called `identical word probing') and  mathematical  indicators to quantitatively detect the general  attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span prediction (SQuAD) shows that: (1) both  classification and span prediction benefit from  translation invariance and local monotonicity, while symmetry slightly decreases performance;
      (2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction.  We contribute the first formal and quantitative analysis of desiderata for PEs, and  a principled discussion about their correlation to the performance of typical downstream tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper amis to understand and evaluate position embeddings, especially in pretrain language models</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="qkLMTphG5-h" data-number="831">
      <h4>
        <a href="https://openreview.net/forum?id=qkLMTphG5-h">
            Repurposing Pretrained Models for Robust Out-of-domain Few-Shot Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=qkLMTphG5-h" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Namyeong_Kwon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Namyeong_Kwon1">Namyeong Kwon</a>, <a href="https://openreview.net/profile?id=~Hwidong_Na1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hwidong_Na1">Hwidong Na</a>, <a href="https://openreview.net/profile?id=~Gabriel_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gabriel_Huang1">Gabriel Huang</a>, <a href="https://openreview.net/profile?id=~Simon_Lacoste-Julien1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Lacoste-Julien1">Simon Lacoste-Julien</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#qkLMTphG5-h-details-247" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qkLMTphG5-h-details-247"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Meta-learning, Few-shot learning, Out-of-domain, Uncertainty, Ensemble, Adversarial training, Stepsize optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Model-agnostic meta-learning (MAML) is a popular method for few-shot learning but assumes that we have access to the meta-training set. In practice, training on the meta-training set may not always be an option due to data privacy concerns, intellectual property issues, or merely lack of computing resources. In this paper, we consider the novel problem of repurposing pretrained MAML checkpoints to solve new few-shot classification tasks. Because of the potential distribution mismatch, the original MAML steps may no longer be optimal. Therefore we propose an alternative meta-testing procedure and combine MAML gradient steps with adversarial training and uncertainty-based stepsize adaptation. Our method outperforms "vanilla" MAML on same-domain and cross-domains benchmarks using both SGD and Adam optimizers and shows improved robustness to the choice of base stepsize.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an alternative meta-testing procedure and combine MAML gradient steps with adversarial training and uncertainty-based stepsize adaptation.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=qkLMTphG5-h&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="l-PrrQrK0QR" data-number="2753">
      <h4>
        <a href="https://openreview.net/forum?id=l-PrrQrK0QR">
            Dataset Meta-Learning from Kernel Ridge-Regression
        </a>
      
        
          <a href="https://openreview.net/pdf?id=l-PrrQrK0QR" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Timothy_Nguyen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Timothy_Nguyen1">Timothy Nguyen</a>, <a href="https://openreview.net/profile?id=~Zhourong_Chen3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhourong_Chen3">Zhourong Chen</a>, <a href="https://openreview.net/profile?id=~Jaehoon_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaehoon_Lee2">Jaehoon Lee</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#l-PrrQrK0QR-details-524" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="l-PrrQrK0QR-details-524"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">dataset distillation, dataset compression, meta-learning, kernel-ridge regression, neural kernels, infinite-width networks, dataset corruption</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. 
      We introduce the novel concept of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="290" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi></math></mjx-assistive-mml></mjx-container>-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar performance. We introduce a meta-learning algorithm Kernel Inducing Points (KIP) for obtaining such remarkable datasets, drawing inspiration from recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime. Consequently, we obtain state of the art results for neural network dataset distillation with potential applications to privacy-preservation.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a meta-learning approach to distilling datasets, achieving state of the art performance for kernel-ridge regression and neural networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="bM3L3I_853" data-number="20">
      <h4>
        <a href="https://openreview.net/forum?id=bM3L3I_853">
            AdaFuse: Adaptive Temporal Fusion Network for Efficient Action Recognition
        </a>
      
        
          <a href="https://openreview.net/pdf?id=bM3L3I_853" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yue_Meng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yue_Meng1">Yue Meng</a>, <a href="https://openreview.net/profile?id=~Rameswar_Panda1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rameswar_Panda1">Rameswar Panda</a>, <a href="https://openreview.net/profile?id=~Chung-Ching_Lin2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chung-Ching_Lin2">Chung-Ching Lin</a>, <a href="https://openreview.net/profile?id=~Prasanna_Sattigeri1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Prasanna_Sattigeri1">Prasanna Sattigeri</a>, <a href="https://openreview.net/profile?id=~Leonid_Karlinsky3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Leonid_Karlinsky3">Leonid Karlinsky</a>, <a href="https://openreview.net/profile?id=~Kate_Saenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kate_Saenko1">Kate Saenko</a>, <a href="https://openreview.net/profile?id=~Aude_Oliva1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aude_Oliva1">Aude Oliva</a>, <a href="https://openreview.net/profile?id=~Rogerio_Feris1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rogerio_Feris1">Rogerio Feris</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 04 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#bM3L3I_853-details-873" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bM3L3I_853-details-873"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Temporal modelling is the key for efficient video action recognition. While understanding temporal information can improve recognition accuracy for dynamic actions, removing temporal redundancy and reusing past features can significantly save computation leading to efficient action recognition. In this paper, we introduce an adaptive temporal fusion network, called AdaFuse, that dynamically fuses channels from current and past feature maps for strong temporal modelling. Specifically, the necessary information from the historical convolution feature maps is fused with current pruned feature maps with the goal of improving both recognition accuracy and efficiency. In addition, we use a skipping operation to further reduce the computation cost of action recognition. Extensive experiments on SomethingV1 &amp; V2, Jester and Mini-Kinetics show that our approach can achieve about 40% computation savings with comparable accuracy to state-of-the-art methods. The project page can be found at https://mengyuest.github.io/AdaFuse/</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=bM3L3I_853&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="uz5uw6gM0m" data-number="368">
      <h4>
        <a href="https://openreview.net/forum?id=uz5uw6gM0m">
            One Network Fits All? Modular versus Monolithic Task Formulations in Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=uz5uw6gM0m" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=thetish%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="thetish@google.com">Atish Agarwala</a>, <a href="https://openreview.net/profile?email=abhidas%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="abhidas@google.com">Abhimanyu Das</a>, <a href="https://openreview.net/profile?id=~Brendan_Juba1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brendan_Juba1">Brendan Juba</a>, <a href="https://openreview.net/profile?id=~Rina_Panigrahy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rina_Panigrahy1">Rina Panigrahy</a>, <a href="https://openreview.net/profile?id=~Vatsal_Sharan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vatsal_Sharan1">Vatsal Sharan</a>, <a href="https://openreview.net/profile?email=wanxin%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="wanxin@google.com">Xin Wang</a>, <a href="https://openreview.net/profile?id=~Qiuyi_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qiuyi_Zhang1">Qiuyi Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 29 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#uz5uw6gM0m-details-158" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uz5uw6gM0m-details-158"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning theory, multi-task learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Can deep learning solve multiple, very different tasks simultaneously? We investigate how the representations of the underlying tasks affect the ability of a single neural network to learn them jointly. We present theoretical and empirical findings that a single neural network is capable of simultaneously learning multiple tasks from a combined data set, for a variety of methods for representing tasks---for example, when the distinct tasks are encoded by well-separated clusters or decision trees over some task-code attributes. Indeed, more strongly, we present a novel analysis that shows that families of simple programming-like constructs for the codes encoding the tasks are learnable by two-layer neural networks with standard training. We study more generally how the complexity of learning such combined tasks grows with the complexity of the task codes; we find that learning many tasks can be provably hard, even though the individual tasks are easy to learn. We provide empirical support for the usefulness of the learning bounds by training networks on clusters, decision trees, and SQL-style aggregation.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Theoretical bounds and experimental results showing that neural networks trained with SGD can provably solve multiple, very different tasks simultaneously.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=uz5uw6gM0m&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="fGF8qAqpXXG" data-number="2926">
      <h4>
        <a href="https://openreview.net/forum?id=fGF8qAqpXXG">
            Vector-output ReLU Neural Network Problems are Copositive Programs: Convex Analysis of Two Layer Networks and Polynomial-time Algorithms
        </a>
      
        
          <a href="https://openreview.net/pdf?id=fGF8qAqpXXG" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Arda_Sahiner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arda_Sahiner1">Arda Sahiner</a>, <a href="https://openreview.net/profile?id=~Tolga_Ergen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tolga_Ergen1">Tolga Ergen</a>, <a href="https://openreview.net/profile?id=~John_M._Pauly1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~John_M._Pauly1">John M. Pauly</a>, <a href="https://openreview.net/profile?id=~Mert_Pilanci3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mert_Pilanci3">Mert Pilanci</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#fGF8qAqpXXG-details-538" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fGF8qAqpXXG-details-538"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">neural networks, theory, convex optimization, copositive programming, convex duality, nonnegative PCA, semi-nonnegative matrix factorization, computational complexity, global optima, semi-infinite duality, convolutional neural networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We describe the convex semi-infinite dual of the two-layer vector-output ReLU neural network training problem. This semi-infinite dual admits a finite dimensional representation, but its support is over a convex set which is difficult to characterize. In particular, we demonstrate that the non-convex neural network training problem is equivalent to a finite-dimensional convex copositive program. Our work is the first to identify this strong connection between the global optima of neural networks and those of copositive programs. We thus demonstrate how neural networks implicitly attempt to solve copositive programs via semi-nonnegative matrix factorization, and draw key insights from this formulation. We describe the first algorithms for provably finding the global minimum of the vector output neural network training problem, which are polynomial in the number of samples for a fixed data rank, yet exponential in the dimension. However, in the case of convolutional architectures, the computational complexity is exponential in only the filter size and polynomial in all other parameters. We describe the circumstances in which we can find the global optimum of this neural network training problem exactly with soft-thresholded SVD, and provide a copositive relaxation which is guaranteed to be exact for certain classes of problems, and which corresponds with the solution of Stochastic Gradient Descent in practice.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We demonstrate that two-layer vector-output ReLU networks can be expressed as copositive programs, and introduce algorithms for provably finding their global optima, which are polynomial in the number of samples for a fixed data rank.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=fGF8qAqpXXG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-ODN6SbiUU" data-number="2541">
      <h4>
        <a href="https://openreview.net/forum?id=-ODN6SbiUU">
            In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-ODN6SbiUU" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mamshad_Nayeem_Rizve1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mamshad_Nayeem_Rizve1">Mamshad Nayeem Rizve</a>, <a href="https://openreview.net/profile?id=~Kevin_Duarte1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kevin_Duarte1">Kevin Duarte</a>, <a href="https://openreview.net/profile?id=~Yogesh_S_Rawat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yogesh_S_Rawat1">Yogesh S Rawat</a>, <a href="https://openreview.net/profile?id=~Mubarak_Shah3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mubarak_Shah3">Mubarak Shah</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-ODN6SbiUU-details-806" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-ODN6SbiUU-details-806"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Semi-Supervised Learning, Pseudo-Labeling, Uncertainty, Calibration, Deep Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The recent research in semi-supervised learning (SSL) is mostly dominated by consistency regularization based methods which achieve strong performance. However, they heavily rely on domain-specific data augmentations, which are not easy to generate for all data modalities. Pseudo-labeling (PL) is a general SSL approach that does not have this constraint but performs relatively poorly in its original formulation. We argue that PL underperforms due to the erroneous high confidence predictions from poorly calibrated models; these predictions generate many incorrect pseudo-labels, leading to noisy training. We propose an uncertainty-aware pseudo-label selection (UPS) framework which improves pseudo labeling accuracy by drastically reducing the amount of noise encountered in the training process. Furthermore, UPS generalizes the pseudo-labeling process, allowing for the creation of negative pseudo-labels; these negative pseudo-labels can be used for multi-label classification as well as negative learning to improve the single-label classification. We achieve strong performance when compared to recent SSL methods on the CIFAR-10 and CIFAR-100 datasets. Also, we demonstrate the versatility of our method on the video dataset UCF-101 and the multi-label dataset Pascal VOC.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present an uncertainty-aware pseudo-label selection framework for semi-supervised learning which greatly reduces the noise introduced by the pseudo-labeling process.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="D3PcGLdMx0" data-number="698">
      <h4>
        <a href="https://openreview.net/forum?id=D3PcGLdMx0">
            MELR: Meta-Learning via Modeling Episode-Level Relationships for Few-Shot Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=D3PcGLdMx0" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Nanyi_Fei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nanyi_Fei1">Nanyi Fei</a>, <a href="https://openreview.net/profile?id=~Zhiwu_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiwu_Lu1">Zhiwu Lu</a>, <a href="https://openreview.net/profile?id=~Tao_Xiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Xiang1">Tao Xiang</a>, <a href="https://openreview.net/profile?id=~Songfang_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Songfang_Huang1">Songfang Huang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>19 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#D3PcGLdMx0-details-728" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="D3PcGLdMx0-details-728"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">few-shot learning, episodic training, cross-episode attention</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Most recent few-shot learning (FSL) approaches are based on episodic training whereby each episode samples few training instances (shots) per class to imitate the test condition. However, this strict adhering to test condition has a negative side effect, that is, the trained model is susceptible to the poor sampling of few shots. In this work, for the first time, this problem is addressed by exploiting inter-episode relationships. Specifically, a novel meta-learning via modeling episode-level relationships (MELR) framework is proposed. By sampling two episodes containing the same set of classes for meta-training, MELR is designed to ensure that the meta-learned model is robust against the presence of poorly-sampled shots in the meta-test stage. This is achieved through two key components: (1) a Cross-Episode Attention Module (CEAM) to improve the ability of alleviating the effects of poorly-sampled shots, and (2) a Cross-Episode Consistency Regularization (CECR) to enforce that the two classifiers learned from the two episodes are consistent even when there are unrepresentative instances. Extensive experiments for non-transductive standard FSL on two benchmarks show that our MELR achieves 1.0%-5.0% improvements over the baseline (i.e., ProtoNet) used for FSL in our model and outperforms the latest competitors under the same settings.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This is the first work on explicitly modeling episode-level relationships for few-shot learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="iQQK02mxVIT" data-number="1205">
      <h4>
        <a href="https://openreview.net/forum?id=iQQK02mxVIT">
            Why resampling outperforms reweighting for correcting sampling bias with stochastic gradients
        </a>
      
        
          <a href="https://openreview.net/pdf?id=iQQK02mxVIT" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=jingan%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jingan@stanford.edu">Jing An</a>, <a href="https://openreview.net/profile?id=~Lexing_Ying1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lexing_Ying1">Lexing Ying</a>, <a href="https://openreview.net/profile?email=yuhuazhu%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yuhuazhu@stanford.edu">Yuhua Zhu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 11 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>27 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#iQQK02mxVIT-details-295" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iQQK02mxVIT-details-295"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">biased sampling, reweighting, resampling, stability, stochastic asymptotics</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A data set sampled from a certain population is biased if the subgroups of the population are sampled at proportions that are significantly different from their underlying proportions. Training machine learning models on biased data sets requires correction techniques to compensate for the bias. We consider two commonly-used techniques, resampling and reweighting, that rebalance the proportions of the subgroups to maintain the desired objective function. Though statistically equivalent, it has been observed that resampling outperforms reweighting when combined with stochastic gradient algorithms. By analyzing illustrative examples, we explain the reason behind this phenomenon using tools from dynamical stability and stochastic asymptotics. We also present experiments from regression, classification, and off-policy prediction to demonstrate that this is a general phenomenon. We argue that it is imperative to consider the objective function design and the optimization algorithm together while addressing the sampling bias.
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We explain why resampling outperforms reweighting for correcting sampling bias when stochastic gradient algorithms are used.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Ptaz_zIFbX" data-number="1555">
      <h4>
        <a href="https://openreview.net/forum?id=Ptaz_zIFbX">
            Prediction and generalisation over directed actions by grid cells
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Ptaz_zIFbX" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Changmin_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changmin_Yu1">Changmin Yu</a>, <a href="https://openreview.net/profile?email=behrens%40fmrib.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="behrens@fmrib.ox.ac.uk">Timothy Behrens</a>, <a href="https://openreview.net/profile?email=n.burgess%40ucl.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="top" title="n.burgess@ucl.ac.uk">Neil Burgess</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>6 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Ptaz_zIFbX-details-253" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ptaz_zIFbX-details-253"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Computational neuroscience, grid cells, normative models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Knowing how the effects of directed actions generalise to new situations (e.g. moving North, South, East and West, or turning left, right, etc.) is key to rapid generalisation across new situations. Markovian tasks can be characterised by a state space and a transition matrix and recent work has proposed that neural grid codes provide an efficient representation of the state space, as eigenvectors of a transition matrix reflecting diffusion across states, that allows efficient prediction of future state distributions. Here we extend the eigenbasis prediction model, utilising tools from Fourier analysis, to prediction over arbitrary translation-invariant directed transition structures (i.e. displacement and diffusion), showing that a single set of eigenvectors can support predictions over arbitrary directed actions via action-specific eigenvalues. We show how to define a "sense of direction" to combine actions to reach a target state (ignoring task-specific deviations from translation-invariance), and demonstrate that adding the Fourier representations to a deep Q network aids policy learning in continuous control tasks. We show the equivalence between the generalised prediction framework and traditional models of grid cell firing driven by self-motion to perform path integration, either using oscillatory interference (via Fourier components as velocity-controlled oscillators) or continuous attractor networks (via analysis of the update dynamics). We thus provide a unifying framework for the role of the grid system in predictive planning, sense of direction and path integration: supporting generalisable inference over directed actions across different tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Extending existing normative prediction models of grid cells to directed transitions, and provide a unifying framework for mechanistic and normative models of grid cells.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Ptaz_zIFbX&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="MaZFq7bJif7" data-number="719">
      <h4>
        <a href="https://openreview.net/forum?id=MaZFq7bJif7">
            Hopper: Multi-hop Transformer for Spatiotemporal Reasoning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=MaZFq7bJif7" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Honglu_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Honglu_Zhou1">Honglu Zhou</a>, <a href="https://openreview.net/profile?id=~Asim_Kadav1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Asim_Kadav1">Asim Kadav</a>, <a href="https://openreview.net/profile?id=~Farley_Lai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Farley_Lai1">Farley Lai</a>, <a href="https://openreview.net/profile?id=~Alexandru_Niculescu-Mizil1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexandru_Niculescu-Mizil1">Alexandru Niculescu-Mizil</a>, <a href="https://openreview.net/profile?id=~Martin_Renqiang_Min1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Renqiang_Min1">Martin Renqiang Min</a>, <a href="https://openreview.net/profile?id=~Mubbasir_Kapadia2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mubbasir_Kapadia2">Mubbasir Kapadia</a>, <a href="https://openreview.net/profile?id=~Hans_Peter_Graf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hans_Peter_Graf1">Hans Peter Graf</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 22 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#MaZFq7bJif7-details-206" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MaZFq7bJif7-details-206"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multi-hop Reasoning, Object Permanence, Spatiotemporal Understanding, Video Recognition, Transformer</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper considers the problem of spatiotemporal object-centric reasoning in videos. Central to our approach is the notion of object permanence, i.e., the ability to reason about the location of objects as they move through the video while being occluded, contained or carried by other objects. Existing deep learning based approaches often suffer from spatiotemporal biases when applied to video reasoning problems. We propose Hopper, which uses a Multi-hop Transformer for reasoning object permanence in videos. Given a video and a localization query, Hopper reasons over image and object tracks to automatically hop over critical frames in an iterative fashion to predict the final position of the object of interest. We demonstrate the effectiveness of using a contrastive loss to reduce spatiotemporal biases. We evaluate over CATER dataset and find that Hopper achieves 73.2% Top-1 accuracy using just 1 FPS by hopping through just a few critical frames. We also demonstrate Hopper can perform long-term reasoning by building a CATER-h dataset that requires multi-step reasoning to localize objects of interest correctly.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose Hopper, Multi-Hop Transformer, and CATER-h dataset to approach object-centric spatiotemporal reasoning in videos.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=MaZFq7bJif7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="D_KeYoqCYC" data-number="3647">
      <h4>
        <a href="https://openreview.net/forum?id=D_KeYoqCYC">
            Sparse encoding for more-interpretable feature-selecting representations in probabilistic matrix factorization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=D_KeYoqCYC" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Joshua_C_Chang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_C_Chang1">Joshua C Chang</a>, <a href="https://openreview.net/profile?email=patrick%40mederrata.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="patrick@mederrata.com">Patrick Fletcher</a>, <a href="https://openreview.net/profile?email=jungmin%40mederrata.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jungmin@mederrata.com">Jungmin Han</a>, <a href="https://openreview.net/profile?email=ted%40mederrata.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ted@mederrata.com">Ted L Chang</a>, <a href="https://openreview.net/profile?email=shashaank%40mederrata.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="shashaank@mederrata.com">Shashaank Vattikuti</a>, <a href="https://openreview.net/profile?email=bart.desmet%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="bart.desmet@gmail.com">Bart Desmet</a>, <a href="https://openreview.net/profile?email=ayah.zirikly%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ayah.zirikly@gmail.com">Ayah Zirikly</a>, <a href="https://openreview.net/profile?email=carsonc%40niddk.nih.gov" class="profile-link" data-toggle="tooltip" data-placement="top" title="carsonc@niddk.nih.gov">Carson C Chow</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 06 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#D_KeYoqCYC-details-498" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="D_KeYoqCYC-details-498"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">poisson matrix factorization, generalized additive model, probabilistic matrix factorization, bayesian, sparse coding, interpretability, factor analysis</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Dimensionality reduction methods for count data are critical to a wide range of applications in medical informatics and other fields where model interpretability is paramount. For such data, hierarchical Poisson matrix factorization (HPF) and other sparse probabilistic non-negative matrix factorization (NMF) methods are considered to be interpretable generative models. They consist of sparse transformations for decoding their learned representations into predictions. However, sparsity in representation decoding does not necessarily imply sparsity in the encoding of representations from the original data features.  HPF is often incorrectly interpreted in the literature as if it possesses encoder sparsity. The distinction between decoder sparsity and encoder sparsity is subtle but important. Due to the lack of encoder sparsity, HPF does not possess the column-clustering property of classical NMF -- the factor loading matrix does not sufficiently define how each factor is formed from the original features. We address this deficiency by self-consistently enforcing encoder sparsity, using a generalized additive model  (GAM), thereby allowing one to relate each representation coordinate to a subset of the original data features. In doing so, the method also gains the ability to perform feature selection. We demonstrate our method on simulated data and give an example of how encoder sparsity is of practical use in a concrete application of representing inpatient comorbidities in Medicare patients.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a simple modification to existing sparse matrix factorization methods to rectify widespread erroneous interpretation of the factors.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=D_KeYoqCYC&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xGZG2kS5bFk" data-number="1186">
      <h4>
        <a href="https://openreview.net/forum?id=xGZG2kS5bFk">
             Dance Revolution: Long-Term Dance Generation with Music via Curriculum Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xGZG2kS5bFk" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ruozi_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruozi_Huang1">Ruozi Huang</a>, <a href="https://openreview.net/profile?id=~Huang_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huang_Hu1">Huang Hu</a>, <a href="https://openreview.net/profile?id=~Wei_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Wu1">Wei Wu</a>, <a href="https://openreview.net/profile?email=kesawada%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="kesawada@microsoft.com">Kei Sawada</a>, <a href="https://openreview.net/profile?email=mi_zhang%40fudan.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="mi_zhang@fudan.edu.cn">Mi Zhang</a>, <a href="https://openreview.net/profile?email=djiang%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="djiang@microsoft.com">Daxin Jiang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xGZG2kS5bFk-details-394" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xGZG2kS5bFk-details-394"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multimodal Learning, Computer Vision, Sequence Modeling, Generative Models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Dancing to music is one of human's innate abilities since ancient times. In machine learning research, however, synthesizing dance movements from music is a challenging problem. Recently, researchers synthesize human motion sequences through autoregressive models like recurrent neural network (RNN). Such an approach often generates short sequences due to an accumulation of prediction errors that are fed back into the neural network. This problem becomes even more severe in the long motion sequence generation. Besides, the consistency between dance and music in terms of style, rhythm and beat is yet to be taken into account during modeling. In this paper, we formalize the music-driven dance generation as a sequence-to-sequence learning problem and devise a novel seq2seq architecture to efficiently process long sequences of music features and capture the fine-grained correspondence between music and dance. Furthermore, we propose a novel curriculum learning strategy to alleviate error accumulation of autoregressive models in long motion sequence generation, which gently changes the training process from a fully guided teacher-forcing scheme using the previous ground-truth movements, towards a less guided autoregressive scheme mostly using the generated movements instead. Extensive experiments show that our approach significantly outperforms the existing state-of-the-arts on automatic metrics and human evaluation. We also make a demo video to demonstrate the superior performance of our proposed approach at https://www.youtube.com/watch?v=lmE20MEheZ8.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=xGZG2kS5bFk&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Qk-Wq5AIjpq" data-number="1383">
      <h4>
        <a href="https://openreview.net/forum?id=Qk-Wq5AIjpq">
            PAC Confidence Predictions for Deep Neural Network Classifiers
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Qk-Wq5AIjpq" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sangdon_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sangdon_Park1">Sangdon Park</a>, <a href="https://openreview.net/profile?email=lishuo1%40seas.upenn.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="lishuo1@seas.upenn.edu">Shuo Li</a>, <a href="https://openreview.net/profile?id=~Insup_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Insup_Lee1">Insup Lee</a>, <a href="https://openreview.net/profile?id=~Osbert_Bastani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Osbert_Bastani1">Osbert Bastani</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Qk-Wq5AIjpq-details-141" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Qk-Wq5AIjpq-details-141"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">classification, calibration, probably approximated correct guarantee, fast DNN inference, safe planning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A key challenge for deploying deep neural networks (DNNs) in safety critical settings is the need to provide rigorous ways to quantify their uncertainty. In this paper, we propose a novel algorithm for constructing predicted classification confidences for DNNs that comes with provable correctness guarantees. Our approach uses Clopper-Pearson confidence intervals for the Binomial distribution in conjunction with the histogram binning approach to calibrated prediction. In addition, we demonstrate how our predicted confidences can be used to enable downstream guarantees in two settings: (i) fast DNN inference, where we demonstrate how to compose a fast but inaccurate DNN with an accurate but slow DNN in a rigorous way to improve performance without sacrificing accuracy, and (ii) safe planning, where we guarantee safety when using a DNN to predict whether a given action is safe based on visual observations. In our experiments, we demonstrate that our approach can be used to provide guarantees for state-of-the-art DNNs.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a novel algorithm for constructing predicted classification confidences for DNNs that comes with provable correctness guarantees, and demonstrate how our predicted confidences can be used to enable downstream guarantees in two settings.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="mQPBmvyAuk" data-number="1836">
      <h4>
        <a href="https://openreview.net/forum?id=mQPBmvyAuk">
            BREEDS: Benchmarks for Subpopulation Shift
        </a>
      
        
          <a href="https://openreview.net/pdf?id=mQPBmvyAuk" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Shibani_Santurkar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shibani_Santurkar1">Shibani Santurkar</a>, <a href="https://openreview.net/profile?id=~Dimitris_Tsipras1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dimitris_Tsipras1">Dimitris Tsipras</a>, <a href="https://openreview.net/profile?id=~Aleksander_Madry1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aleksander_Madry1">Aleksander Madry</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#mQPBmvyAuk-details-405" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mQPBmvyAuk-details-405"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">benchmarks, distribution shift, hierarchy, robustness</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We develop a methodology for assessing the robustness of models to subpopulation shift---specifically, their ability to generalize to novel data subpopulations that were not observed during training. Our approach leverages the class structure underlying existing datasets to control the data subpopulations that comprise the training and test distributions. This enables us to synthesize realistic distribution shifts whose sources can be precisely controlled and characterized, within existing
      large-scale datasets. Applying this methodology to the ImageNet dataset, we create a suite of subpopulation shift benchmarks of varying granularity. We then validate that the corresponding shifts are tractable by obtaining human baselines. Finally, we utilize these benchmarks to measure the sensitivity of standard model architectures as well as the effectiveness of existing train-time robustness interventions. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We develop a methodology for constructing large-scale subpopulation shift benchmarks and use them to assess model robustness as well as the effectiveness existing robustness interventions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=mQPBmvyAuk&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="7dpmlkBuJFC" data-number="1989">
      <h4>
        <a href="https://openreview.net/forum?id=7dpmlkBuJFC">
            Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification
        </a>
      
        
          <a href="https://openreview.net/pdf?id=7dpmlkBuJFC" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yingxue_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yingxue_Zhou1">Yingxue Zhou</a>, <a href="https://openreview.net/profile?id=~Steven_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_Wu1">Steven Wu</a>, <a href="https://openreview.net/profile?id=~Arindam_Banerjee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arindam_Banerjee1">Arindam Banerjee</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#7dpmlkBuJFC-details-421" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7dpmlkBuJFC-details-421"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Differentially private SGD (DP-SGD) is one of the most popular methods for solving differentially private empirical risk minimization (ERM). Due to its noisy perturbation on each gradient update, the error rate of DP-SGD scales with the ambient dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="291" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></mjx-assistive-mml></mjx-container>, the number of parameters in the model. Such dependence can be problematic for over-parameterized models where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="292" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c226B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>≫</mo><mi>n</mi></math></mjx-assistive-mml></mjx-container>, the number of training samples. Existing lower bounds on private ERM show that such dependence on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="293" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></mjx-assistive-mml></mjx-container> is inevitable in the worst case. In this paper, we circumvent the dependence on the ambient dimension by leveraging a low-dimensional structure of gradient space in deep networks---that is, the stochastic gradients for deep nets usually stay in a low dimensional subspace in the training process. We propose Projected DP-SGD that performs noise reduction by projecting the noisy gradients to a low-dimensional subspace, which is given by the top gradient eigenspace on a small public dataset. We provide a general sample complexity analysis on the public dataset for the gradient subspace identification problem and demonstrate that under certain low-dimensional assumptions the public sample complexity only grows logarithmically in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="294" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></mjx-assistive-mml></mjx-container>. Finally, we provide a theoretical analysis and empirical evaluations to show that our method can substantially improve the accuracy of DP-SGD in the high privacy regime (corresponding to low privacy loss <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="295" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi></math></mjx-assistive-mml></mjx-container>).
      
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="rRFIni1CYmy" data-number="3087">
      <h4>
        <a href="https://openreview.net/forum?id=rRFIni1CYmy">
            End-to-End Egospheric Spatial Memory
        </a>
      
        
          <a href="https://openreview.net/pdf?id=rRFIni1CYmy" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Daniel_James_Lenton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_James_Lenton1">Daniel James Lenton</a>, <a href="https://openreview.net/profile?id=~Stephen_James1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephen_James1">Stephen James</a>, <a href="https://openreview.net/profile?id=~Ronald_Clark2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ronald_Clark2">Ronald Clark</a>, <a href="https://openreview.net/profile?id=~Andrew_Davison1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_Davison1">Andrew Davison</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#rRFIni1CYmy-details-238" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rRFIni1CYmy-details-238"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">egocentric, differentiable memory, spatial awareness, mapping, image-to-action learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Spatial memory, or the ability to remember and recall specific locations and objects, is central to autonomous agents' ability to carry out tasks in real environments. However, most existing artificial memory modules are not very adept at storing spatial information. We propose a parameter-free module, Egospheric Spatial Memory (ESM), which encodes the memory in an ego-sphere around the agent, enabling expressive 3D representations. ESM can be trained end-to-end via either imitation or reinforcement learning, and improves both training efficiency and final performance against other memory baselines on both drone and manipulator visuomotor control tasks. The explicit egocentric geometry also enables us to seamlessly combine the learned controller with other non-learned modalities, such as local obstacle avoidance. We further show applications to semantic segmentation on the ScanNet dataset, where ESM naturally combines image-level and map-level inference modalities. Through our broad set of experiments, we show that ESM provides a general computation graph for embodied spatial reasoning, and the module forms a bridge between real-time mapping systems and differentiable memory architectures. Implementation at: https://github.com/ivy-dl/memory.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">End-to-End Egospheric Spatial Memory (ESM) forms a bridge between real-time mapping and differentiable memory</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="djwS0m4Ft_A" data-number="1422">
      <h4>
        <a href="https://openreview.net/forum?id=djwS0m4Ft_A">
            Evaluating the Disentanglement of Deep Generative Models through Manifold Topology
        </a>
      
        
          <a href="https://openreview.net/pdf?id=djwS0m4Ft_A" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sharon_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sharon_Zhou1">Sharon Zhou</a>, <a href="https://openreview.net/profile?id=~Eric_Zelikman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eric_Zelikman1">Eric Zelikman</a>, <a href="https://openreview.net/profile?email=fredlu%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="fredlu@stanford.edu">Fred Lu</a>, <a href="https://openreview.net/profile?id=~Andrew_Y._Ng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_Y._Ng1">Andrew Y. Ng</a>, <a href="https://openreview.net/profile?id=~Gunnar_E._Carlsson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gunnar_E._Carlsson1">Gunnar E. Carlsson</a>, <a href="https://openreview.net/profile?id=~Stefano_Ermon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefano_Ermon1">Stefano Ermon</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#djwS0m4Ft_A-details-957" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="djwS0m4Ft_A-details-957"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">generative models, evaluation, disentanglement</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Learning disentangled representations is regarded as a fundamental task for improving the generalization, robustness, and interpretability of generative models. However, measuring disentanglement has been challenging and inconsistent, often dependent on an ad-hoc external model or specific to a certain dataset. To address this, we present a method for quantifying disentanglement that only uses the generative model, by measuring the topological similarity of conditional submanifolds in the learned representation. This method showcases both unsupervised and supervised variants. To illustrate the effectiveness and applicability of our method, we empirically evaluate several state-of-the-art models across multiple datasets. We find that our method ranks models similarly to existing methods. We make our code publicly available at https://github.com/stanfordmlgroup/disentanglement.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Evaluate disentanglement of generative models by measuring manifold topology using persistent homology</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=djwS0m4Ft_A&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="oyZxhRI2RiE" data-number="3773">
      <h4>
        <a href="https://openreview.net/forum?id=oyZxhRI2RiE">
            SCoRe: Pre-Training for Context Representation in Conversational Semantic Parsing
        </a>
      
        
          <a href="https://openreview.net/pdf?id=oyZxhRI2RiE" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tao_Yu5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Yu5">Tao Yu</a>, <a href="https://openreview.net/profile?id=~Rui_Zhang7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rui_Zhang7">Rui Zhang</a>, <a href="https://openreview.net/profile?id=~Alex_Polozov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_Polozov1">Alex Polozov</a>, <a href="https://openreview.net/profile?id=~Christopher_Meek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Meek1">Christopher Meek</a>, <a href="https://openreview.net/profile?id=~Ahmed_Hassan_Awadallah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ahmed_Hassan_Awadallah1">Ahmed Hassan Awadallah</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#oyZxhRI2RiE-details-589" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oyZxhRI2RiE-details-589"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Conversational Semantic Parsing (CSP) is the task of converting a sequence of natural language queries to formal language (e.g., SQL, SPARQL) that can be executed against a structured ontology (e.g.  databases, knowledge bases).  To accomplish  this  task,  a  CSP  system  needs  to  model  the  relation  between  the unstructured language utterance and the structured ontology while representing the multi-turn dynamics of the dialog. Pre-trained language models (LMs) are the state-of-the-art for various natural language processing tasks. However, existing pre-trained LMs that use language modeling training objectives over free-form text have limited ability to represent natural language references to contextual structural data. In this work, we present SCORE, a new pre-training approach for CSP tasks designed to induce representations that capture the alignment between the dialogue flow and the structural context. We demonstrate the broad applicability of SCORE to CSP tasks by combining SCORE with strong base systems on four different tasks (SPARC, COSQL, MWOZ, and SQA). We show that SCORE can improve the performance over all these base systems by a significant margin and achieves state-of-the-art results on three of them.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="iWLByfvUhN" data-number="2214">
      <h4>
        <a href="https://openreview.net/forum?id=iWLByfvUhN">
            Decoupling Global and Local Representations via Invertible Generative Flows
        </a>
      
        
          <a href="https://openreview.net/pdf?id=iWLByfvUhN" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xuezhe_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuezhe_Ma1">Xuezhe Ma</a>, <a href="https://openreview.net/profile?id=~Xiang_Kong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiang_Kong1">Xiang Kong</a>, <a href="https://openreview.net/profile?id=~Shanghang_Zhang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shanghang_Zhang4">Shanghang Zhang</a>, <a href="https://openreview.net/profile?id=~Eduard_H_Hovy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eduard_H_Hovy1">Eduard H Hovy</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#iWLByfvUhN-details-997" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iWLByfvUhN-details-997"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Generative Models, Generative Flow, Normalizing Flow, Image Generation, Representation Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this work, we propose a new generative model that is capable of automatically decoupling global and local representations of images in an entirely unsupervised setting, by embedding a generative flow in the VAE framework to model the decoder.
      Specifically, the proposed model utilizes the variational auto-encoding framework to learn a (low-dimensional) vector of latent variables to capture the global information of an image, which is fed as a conditional input to a flow-based invertible decoder with architecture borrowed from style transfer literature.
      Experimental results on standard image benchmarks demonstrate the effectiveness of our model in terms of density estimation, image generation and unsupervised representation learning.
      Importantly, this work demonstrates that with only architectural inductive biases, a generative model with a likelihood-based objective is capable of learning decoupled representations, requiring no explicit supervision.
      The code for our model is available at \url{https://github.com/XuezheMax/wolf}.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Generative Flow for Decoupled Representation Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="3k20LAiHYL2" data-number="2398">
      <h4>
        <a href="https://openreview.net/forum?id=3k20LAiHYL2">
            Pre-training Text-to-Text Transformers for Concept-centric Common Sense
        </a>
      
        
          <a href="https://openreview.net/pdf?id=3k20LAiHYL2" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Wangchunshu_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wangchunshu_Zhou1">Wangchunshu Zhou</a>, <a href="https://openreview.net/profile?id=~Dong-Ho_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dong-Ho_Lee1">Dong-Ho Lee</a>, <a href="https://openreview.net/profile?id=~Ravi_Kiran_Selvam1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ravi_Kiran_Selvam1">Ravi Kiran Selvam</a>, <a href="https://openreview.net/profile?id=~Seyeon_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seyeon_Lee1">Seyeon Lee</a>, <a href="https://openreview.net/profile?id=~Xiang_Ren1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiang_Ren1">Xiang Ren</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>21 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#3k20LAiHYL2-details-348" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3k20LAiHYL2-details-348"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Language Model Pre-training, Commonsense Reasoning, Self-supervised Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Pretrained language models (PTLM) have achieved impressive results in a range of natural language understanding (NLU) and generation (NLG) tasks that require a syntactic and semantic understanding of the text. However, current pre-training objectives such as masked token prediction (for BERT-style PTLMs) and masked span infilling (for T5-style PTLMs) do not explicitly model the relational and compositional commonsense knowledge about everyday concepts, which is crucial to many downstream tasks requiring commonsense reasoning. To augment PTLMs with common sense, we propose generative and contrastive objectives as intermediate self-supervised pre-training tasks between general pre-training and downstream task-specific fine-tuning. We also propose a joint training framework to unify generative and contrastive objectives so that these objectives can be more effective.
      Our proposed objectives can pack more commonsense knowledge into the parameters of a pre-trained text-to-text transformer without relying on external knowledge bases, yielding better performance on both NLU and NLG tasks. We apply our method on a pre-trained T5 model in an intermediate task transfer learning fashion to train a concept-aware language model (CALM) and experiment with five commonsense benchmarks (four NLU tasks and one NLG task). Experimental results show that CALM outperforms baseline methods by a consistent margin.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose self-supervised objectives and a joint training framework to augment pre-trained language models with common sense without relying on external knowledge bases.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=3k20LAiHYL2&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tH6_VWZjoq" data-number="2635">
      <h4>
        <a href="https://openreview.net/forum?id=tH6_VWZjoq">
            Local Search Algorithms for Rank-Constrained Convex Optimization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tH6_VWZjoq" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kyriakos_Axiotis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyriakos_Axiotis1">Kyriakos Axiotis</a>, <a href="https://openreview.net/profile?email=sviri%40verizonmedia.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sviri@verizonmedia.com">Maxim Sviridenko</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tH6_VWZjoq-details-898" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tH6_VWZjoq-details-898"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">low rank, rank-constrained convex optimization, matrix completion</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose greedy and local search algorithms for rank-constrained convex optimization, namely solving <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="296" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-munder><mjx-row><mjx-base style="padding-left: 1.005em;"><mjx-mo class="mjx-n"><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6E"></mjx-c></mjx-mo></mjx-base></mjx-row><mjx-row><mjx-under style="padding-top: 0.167em;"><mjx-mrow size="s"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-n"><mjx-c class="mjx-c72"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c61"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c6E"></mjx-c></mjx-mi><mjx-mi class="mjx-n"><mjx-c class="mjx-c6B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c2264"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.289em;"><mjx-mo class="mjx-n" style="font-size: 83.3%;"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-mrow></mjx-under></mjx-row></mjx-munder><mjx-mstyle space="2"><mjx-mspace style="width: 0.167em;"></mjx-mspace></mjx-mstyle><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><munder><mo data-mjx-texclass="OP">min</mo><mrow><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">k</mi></mrow><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo>≤</mo><msup><mi>r</mi><mo>∗</mo></msup></mrow></munder><mstyle scriptlevel="0"><mspace width="thinmathspace"></mspace></mstyle><mi>R</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> given a convex function <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="297" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.41em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi><mo>:</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup><mo stretchy="false">→</mo><mrow><mi mathvariant="double-struck">R</mi></mrow></math></mjx-assistive-mml></mjx-container> and a parameter <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="298" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mo class="mjx-n" size="s"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>r</mi><mo>∗</mo></msup></math></mjx-assistive-mml></mjx-container>. These algorithms consist of repeating two steps: (a) adding a new rank-1 matrix to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="299" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></mjx-assistive-mml></mjx-container> and (b) enforcing the rank constraint on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="300" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></mjx-assistive-mml></mjx-container>. We refine and improve the theoretical analysis of Shalev-Shwartz et al. (2011), and show that if the rank-restricted condition number of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="301" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi></math></mjx-assistive-mml></mjx-container> is <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="302" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D705 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>κ</mi></math></mjx-assistive-mml></mjx-container>, a solution <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="303" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>A</mi></math></mjx-assistive-mml></mjx-container> with rank <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="304" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mo class="mjx-n" size="s"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6E"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7B"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D705 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-n" space="2"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mfrac space="2"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mrow size="s"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mn class="mjx-b"><mjx-c class="mjx-c1D7CE TEX-B"></mjx-c></mjx-mn></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mo class="mjx-n" style="font-size: 83.3%;"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-mrow></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msup space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D705 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c7D"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>r</mi><mo>∗</mo></msup><mo>⋅</mo><mo data-mjx-texclass="OP" movablelimits="true">min</mo><mo fence="false" stretchy="false">{</mo><mi>κ</mi><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mrow><mi>R</mi><mo stretchy="false">(</mo><mrow><mn mathvariant="bold">0</mn></mrow><mo stretchy="false">)</mo><mo>−</mo><mi>R</mi><mo stretchy="false">(</mo><msup><mi>A</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow><mi>ϵ</mi></mfrac><mo>,</mo><msup><mi>κ</mi><mn>2</mn></msup><mo fence="false" stretchy="false">}</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="305" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2264"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mo class="mjx-n" size="s"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mo>≤</mo><mi>R</mi><mo stretchy="false">(</mo><msup><mi>A</mi><mo>∗</mo></msup><mo stretchy="false">)</mo><mo>+</mo><mi>ϵ</mi></math></mjx-assistive-mml></mjx-container> can be recovered, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="306" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mo class="mjx-n" size="s"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>A</mi><mo>∗</mo></msup></math></mjx-assistive-mml></mjx-container> is the optimal solution. This significantly generalizes associated results on sparse convex optimization, as well as rank-constrained convex optimization for smooth functions. We then introduce new practical variants of these algorithms that have superior runtime and recover better solutions in practice. We demonstrate the versatility of these methods on a wide range of applications involving matrix completion and robust principal component analysis.
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Efficient greedy and local search algorithms for optimizing a convex objective under a rank constraint.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=tH6_VWZjoq&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="8E1-f3VhX1o" data-number="2396">
      <h4>
        <a href="https://openreview.net/forum?id=8E1-f3VhX1o">
            Combining Label Propagation and Simple Models out-performs Graph Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=8E1-f3VhX1o" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=qh53%40cornell.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="qh53@cornell.edu">Qian Huang</a>, <a href="https://openreview.net/profile?id=~Horace_He1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Horace_He1">Horace He</a>, <a href="https://openreview.net/profile?email=as2626%40cornell.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="as2626@cornell.edu">Abhay Singh</a>, <a href="https://openreview.net/profile?id=~Ser-Nam_Lim3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ser-Nam_Lim3">Ser-Nam Lim</a>, <a href="https://openreview.net/profile?id=~Austin_Benson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Austin_Benson1">Austin Benson</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#8E1-f3VhX1o-details-263" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8E1-f3VhX1o-details-263"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">graphs, graph neural networks, label propagation, simple, residual</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Graph Neural Networks (GNNs) are a predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an “error correlation” that spreads residual errors in training data to correct errors in test data and (ii) a “prediction correlation” that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C&amp;S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques that have long been used in graph-based semi-supervised learning. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best-known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as is common in traditional methods) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains in some cases.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=8E1-f3VhX1o&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="cu7IUiOhujH" data-number="2415">
      <h4>
        <a href="https://openreview.net/forum?id=cu7IUiOhujH">
            Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=cu7IUiOhujH" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Beliz_Gunel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Beliz_Gunel1">Beliz Gunel</a>, <a href="https://openreview.net/profile?id=~Jingfei_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jingfei_Du1">Jingfei Du</a>, <a href="https://openreview.net/profile?id=~Alexis_Conneau1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexis_Conneau1">Alexis Conneau</a>, <a href="https://openreview.net/profile?id=~Veselin_Stoyanov2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Veselin_Stoyanov2">Veselin Stoyanov</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#cu7IUiOhujH-details-548" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="cu7IUiOhujH-details-548"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">pre-trained language model fine-tuning, supervised contrastive learning, natural language understanding, few-shot learning, robustness, generalization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. However, the cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, our proposed SCL loss obtains significant improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in few-shot learning settings, without requiring specialized architecture, data augmentations, memory banks, or additional unsupervised data. Our proposed fine-tuning objective leads to models that are more robust to different levels of noise in the fine-tuning training data, and can generalize better to related tasks with limited labeled data.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Cz3dbFm5u-" data-number="686">
      <h4>
        <a href="https://openreview.net/forum?id=Cz3dbFm5u-">
            SAFENet: A Secure, Accurate and Fast Neural Network Inference
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Cz3dbFm5u-" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Qian_Lou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qian_Lou1">Qian Lou</a>, <a href="https://openreview.net/profile?id=~Yilin_Shen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yilin_Shen1">Yilin Shen</a>, <a href="https://openreview.net/profile?id=~Hongxia_Jin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongxia_Jin1">Hongxia Jin</a>, <a href="https://openreview.net/profile?id=~Lei_Jiang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lei_Jiang1">Lei Jiang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 20 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Cz3dbFm5u--details-791" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Cz3dbFm5u--details-791"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Cryptographic inference, Channel-Wise Approximated Activation, Hyper-Parameter Optimization, Garbled Circuits</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The advances in neural networks have driven many companies to provide prediction services to users in a wide range of applications. However, current prediction systems raise privacy concerns regarding the user's private data. A cryptographic neural network inference service is an efficient way to allow two parties to execute neural network inference without revealing either party’s data or model. Nevertheless, existing cryptographic neural network inference services suffer from huge running latency; in particular, the latency of communication-expensive cryptographic activation function is 3 orders of magnitude higher than plaintext-domain activation function. And activations are the necessary components of the modern neural networks. Therefore, slow cryptographic activation has become the primary obstacle of efficient cryptographic inference. 
      
      In this paper, we propose a new technique, called SAFENet, to enable a Secure, Accurate and Fast nEural Network inference service. To speedup secure inference and guarantee inference accuracy, SAFENet includes channel-wise activation approximation with multiple-degree options. This is implemented by keeping the most useful activation channels and replacing the remaining, less useful, channels with various-degree polynomials. SAFENet also supports mixed-precision activation approximation by automatically assigning different replacement ratios to various layer; further increasing the approximation ratio and reducing inference latency. Our experimental results show SAFENet obtains the state-of-the-art inference latency and performance, reducing latency by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="307" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>38</mn><mi mathvariant="normal">%</mi><mo>∼</mo><mn>61</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> or improving accuracy by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="308" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c223C"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c34"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1.8</mn><mi mathvariant="normal">%</mi><mo>∼</mo><mn>4</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> over prior techniques on various encrypted datasets.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose SAFENet that supports automatic channel-wise activation approximation to enable a Secure, Accurate and Fast nEural Network inference service.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="sRA5rLNpmQc" data-number="2380">
      <h4>
        <a href="https://openreview.net/forum?id=sRA5rLNpmQc">
            Provably robust classification of adversarial examples with detection
        </a>
      
        
          <a href="https://openreview.net/pdf?id=sRA5rLNpmQc" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Fatemeh_Sheikholeslami1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fatemeh_Sheikholeslami1">Fatemeh Sheikholeslami</a>, <a href="https://openreview.net/profile?id=~Ali_Lotfi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ali_Lotfi1">Ali Lotfi</a>, <a href="https://openreview.net/profile?id=~J_Zico_Kolter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~J_Zico_Kolter1">J Zico Kolter</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#sRA5rLNpmQc-details-162" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sRA5rLNpmQc-details-162"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Adversarial robustness, robust deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Adversarial attacks against deep networks can be defended against either by building robust classifiers or, by creating classifiers that can \emph{detect} the presence of adversarial perturbations.  Although it may intuitively seem easier to simply detect attacks rather than build a robust classifier, this has not bourne out in practice even empirically, as most detection methods have subsequently been broken by adaptive attacks, thus necessitating \emph{verifiable} performance for detection mechanisms.  In this paper, we propose a new method for jointly training a provably robust classifier and detector.  Specifically, we show that by introducing an additional "abstain/detection" into a classifier, we can modify existing certified defense mechanisms to allow the classifier to either robustly classify \emph{or} detect adversarial attacks.  We extend the common interval bound propagation (IBP) method for certified robustness under <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="309" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-n" size="s"><mjx-c class="mjx-c221E"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi mathvariant="normal">∞</mi></msub></math></mjx-assistive-mml></mjx-container> perturbations to account for our new robust objective, and show that the method outperforms traditional IBP used in isolation, especially for large perturbation sizes.  Specifically, tests on MNIST and CIFAR-10 datasets exhibit promising results, for example with provable robust error less than <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="310" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>63.63</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="311" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c39"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>67.92</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container>, for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="312" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>55.6</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="313" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c36"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c37"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>66.37</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> natural error, for <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="314" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi><mo>=</mo><mn>8</mn><mrow><mo>/</mo></mrow><mn>255</mn></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="315" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>16</mn><mrow><mo>/</mo></mrow><mn>255</mn></math></mjx-assistive-mml></mjx-container> on the CIFAR-10 dataset, respectively.
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a joint classifier/detector training scheme with provable performance guarantees against adversarial perturbations.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=sRA5rLNpmQc&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="c9-WeM-ceB" data-number="899">
      <h4>
        <a href="https://openreview.net/forum?id=c9-WeM-ceB">
            Saliency is a Possible Red Herring When Diagnosing Poor Generalization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=c9-WeM-ceB" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Joseph_D_Viviano1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joseph_D_Viviano1">Joseph D Viviano</a>, <a href="https://openreview.net/profile?id=~Becks_Simpson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Becks_Simpson1">Becks Simpson</a>, <a href="https://openreview.net/profile?id=~Francis_Dutil1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Francis_Dutil1">Francis Dutil</a>, <a href="https://openreview.net/profile?id=~Yoshua_Bengio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoshua_Bengio1">Yoshua Bengio</a>, <a href="https://openreview.net/profile?id=~Joseph_Paul_Cohen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joseph_Paul_Cohen1">Joseph Paul Cohen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 11 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>22 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#c9-WeM-ceB-details-301" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="c9-WeM-ceB-details-301"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Feature Attribution, Generalization, Saliency</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Poor generalization is one symptom of models that learn to predict target variables using spuriously-correlated image features present only in the training distribution instead of the true image features that denote a class. It is often thought that this can be diagnosed visually using attribution (aka saliency) maps. We study if this assumption is correct. In some prediction tasks, such as for medical images, one may have some images with masks drawn by a human expert, indicating a region of the image containing relevant information to make the prediction. We study multiple methods that take advantage of such auxiliary labels, by training networks to ignore distracting features which may be found outside of the region of interest. This mask information is only used during training and has an impact on generalization accuracy depending on the severity of the shift between the training and test distributions. Surprisingly, while these methods improve generalization performance in the presence of a covariate shift, there is no strong correspondence between the correction of attribution towards the features a human expert have labelled as important and generalization performance. These results suggest that the root cause of poor generalization may not always be spatially defined, and raise questions about the utility of masks as 'attribution priors' as well as saliency maps for explainable predictions.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We controlled feature construction on images using masks to help models generalize to test distributions with covariate shift and noticed that it didn't affect the saliency maps in the way one would expect even though it improved generalization.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="c8P9NQVtmnO" data-number="312">
      <h4>
        <a href="https://openreview.net/forum?id=c8P9NQVtmnO">
            Fourier Neural Operator for Parametric Partial Differential Equations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=c8P9NQVtmnO" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zongyi_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zongyi_Li1">Zongyi Li</a>, <a href="https://openreview.net/profile?id=~Nikola_Borislavov_Kovachki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nikola_Borislavov_Kovachki1">Nikola Borislavov Kovachki</a>, <a href="https://openreview.net/profile?id=~Kamyar_Azizzadenesheli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kamyar_Azizzadenesheli1">Kamyar Azizzadenesheli</a>, <a href="https://openreview.net/profile?email=bgl%40caltech.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="bgl@caltech.edu">Burigede liu</a>, <a href="https://openreview.net/profile?id=~Kaushik_Bhattacharya1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaushik_Bhattacharya1">Kaushik Bhattacharya</a>, <a href="https://openreview.net/profile?id=~Andrew_Stuart2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrew_Stuart2">Andrew Stuart</a>, <a href="https://openreview.net/profile?id=~Anima_Anandkumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anima_Anandkumar1">Anima Anandkumar</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#c8P9NQVtmnO-details-787" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="c8P9NQVtmnO-details-787"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Partial differential equation, Fourier transform, Neural operators</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces.  Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A novel neural operator based on Fourier transformation for learning partial differential equations.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=c8P9NQVtmnO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="g11CZSghXyY" data-number="1420">
      <h4>
        <a href="https://openreview.net/forum?id=g11CZSghXyY">
            Combining Ensembles and Data Augmentation Can Harm Your Calibration
        </a>
      
        
          <a href="https://openreview.net/pdf?id=g11CZSghXyY" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yeming_Wen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yeming_Wen1">Yeming Wen</a>, <a href="https://openreview.net/profile?id=~Ghassen_Jerfel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ghassen_Jerfel1">Ghassen Jerfel</a>, <a href="https://openreview.net/profile?id=~Rafael_Muller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rafael_Muller1">Rafael Muller</a>, <a href="https://openreview.net/profile?id=~Michael_W_Dusenberry1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_W_Dusenberry1">Michael W Dusenberry</a>, <a href="https://openreview.net/profile?id=~Jasper_Snoek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jasper_Snoek1">Jasper Snoek</a>, <a href="https://openreview.net/profile?id=~Balaji_Lakshminarayanan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Balaji_Lakshminarayanan1">Balaji Lakshminarayanan</a>, <a href="https://openreview.net/profile?id=~Dustin_Tran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dustin_Tran1">Dustin Tran</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#g11CZSghXyY-details-326" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="g11CZSghXyY-details-326"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Ensembles, Uncertainty estimates, Calibration</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Ensemble methods which average over multiple neural network predictions are a simple approach to improve a model’s calibration and robustness. Similarly, data augmentation techniques, which encode prior information in the form of invariant feature transformations, are effective for improving calibration and robustness. In this paper, we show a surprising pathology: combining ensembles and data augmentation can harm model calibration. This leads to a trade-off in practice, whereby improved accuracy by combining the two techniques comes at the expense of calibration. On the other hand, selecting only one of the techniques ensures good uncertainty estimates at the expense of accuracy. We investigate this pathology and identify a compounding under-confidence among methods which marginalize over sets of weights and data augmentation techniques which soften labels. Finally, we propose a simple correction, achieving the best of both worlds with significant accuracy and calibration gains over using only ensembles or data augmentation individually. Applying the correction produces new state-of-the art in uncertainty calibration and robustness across CIFAR-10, CIFAR-100, and ImageNet.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We found that combining ensembles and data augmentation worsens calibration than applying them individually, and we proposed a simple fix to it.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="fw-BHZ1KjxJ" data-number="2128">
      <h4>
        <a href="https://openreview.net/forum?id=fw-BHZ1KjxJ">
            SOLAR: Sparse Orthogonal Learned and Random Embeddings
        </a>
      
        
          <a href="https://openreview.net/pdf?id=fw-BHZ1KjxJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tharun_Medini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tharun_Medini1">Tharun Medini</a>, <a href="https://openreview.net/profile?id=~Beidi_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Beidi_Chen1">Beidi Chen</a>, <a href="https://openreview.net/profile?id=~Anshumali_Shrivastava1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anshumali_Shrivastava1">Anshumali Shrivastava</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#fw-BHZ1KjxJ-details-345" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="fw-BHZ1KjxJ-details-345"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Sparse Embedding, Inverted Index, Learning to Hash, Embedding Models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Dense embedding models are commonly deployed in commercial search engines, wherein all the document vectors are pre-computed, and near-neighbor search (NNS) is performed with the query vector to find relevant documents. However, the bottleneck of indexing a large number of dense vectors and performing an NNS hurts the query time and accuracy of these models. In this paper, we argue that high-dimensional and ultra-sparse embedding is a significantly superior alternative to dense low-dimensional embedding for both query efficiency and accuracy. Extreme sparsity eliminates the need for NNS by replacing them with simple lookups, while its high dimensionality ensures that the embeddings are informative even when sparse. However, learning extremely high dimensional embeddings leads to blow up in the model size. To make the training feasible, we propose a partitioning algorithm that learns such high dimensional embeddings across multiple GPUs without any communication. This is facilitated by our novel asymmetric mixture of Sparse, Orthogonal, Learned and Random (SOLAR) Embeddings. The label vectors are random, sparse, and near-orthogonal by design, while the query vectors are learned and sparse. We theoretically prove that our way of one-sided learning is equivalent to learning both query and label embeddings. With these unique properties, we can successfully train 500K dimensional SOLAR embeddings for the tasks of searching through 1.6M books and multi-label classification on the three largest public datasets. We achieve superior precision and recall compared to the respective state-of-the-art baselines for each task with up to 10 times faster speed.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a distributed training scheme to learn high dimensional sparse embeddings that are much better than dense embeddings on both precision and speed.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=fw-BHZ1KjxJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="u2YNJPcQlwq" data-number="2685">
      <h4>
        <a href="https://openreview.net/forum?id=u2YNJPcQlwq">
            Efficient Empowerment Estimation for Unsupervised Stabilization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=u2YNJPcQlwq" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ruihan_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruihan_Zhao1">Ruihan Zhao</a>, <a href="https://openreview.net/profile?id=~Kevin_Lu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kevin_Lu2">Kevin Lu</a>, <a href="https://openreview.net/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="https://openreview.net/profile?id=~Stas_Tiomkin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stas_Tiomkin1">Stas Tiomkin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#u2YNJPcQlwq-details-828" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="u2YNJPcQlwq-details-828"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">unsupervised stabilization, representation of dynamical systems, neural networks, empowerment, intrinsic motivation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Intrinsically motivated artificial agents learn advantageous behavior without externally-provided rewards. Previously, it was shown that maximizing mutual information between agent actuators and future states, known as the empowerment principle, enables unsupervised stabilization of dynamical systems at upright positions, which is a prototypical intrinsically motivated behavior for upright standing and walking. This follows from the coincidence between the objective of stabilization and the objective of empowerment. Unfortunately, sample-based estimation of this kind of mutual information is challenging. Recently, various variational lower bounds (VLBs) on empowerment have been proposed as solutions; however, they are often biased, unstable in training, and have high sample complexity. In this work, we propose an alternative solution based on a trainable representation of a dynamical system as a Gaussian channel, which allows us to efficiently calculate an unbiased estimator of empowerment by convex optimization. We demonstrate our solution for sample-based unsupervised stabilization on different dynamical control systems and show the advantages of our method by comparing it to the existing VLB approaches. Specifically, we show that our method has a lower sample complexity, is more stable in training, possesses the essential properties of the empowerment function, and allows estimation of empowerment from images. Consequently, our method opens a path to wider and easier adoption of empowerment for various applications.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an efficient estimation of empowerment which is demonstrated on unsupervised stabilization of dynamical systems, and compared to the existing relevant methods.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="z5Z023VBmDZ" data-number="1719">
      <h4>
        <a href="https://openreview.net/forum?id=z5Z023VBmDZ">
            More or Less: When and How to Build Convolutional Neural Network Ensembles
        </a>
      
        
          <a href="https://openreview.net/pdf?id=z5Z023VBmDZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Abdul_Wasay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abdul_Wasay1">Abdul Wasay</a>, <a href="https://openreview.net/profile?id=~Stratos_Idreos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stratos_Idreos1">Stratos Idreos</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#z5Z023VBmDZ-details-307" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="z5Z023VBmDZ-details-307"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">ensemble learning, empirical study, machine learning systems, computer vision</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Convolutional neural networks are utilized to solve increasingly more complex problems and with more data. As a  result, researchers and practitioners seek to scale the representational power of such models by adding more parameters. However, increasing parameters requires additional critical resources in terms of memory and compute,  leading to increased training and inference cost. Thus a consistent challenge is to obtain as high as possible accuracy within a parameter budget. As neural network designers navigate this complex landscape, they are guided by conventional wisdom that is informed from past empirical studies. We identify a critical part of this design space that is not well-understood: How to decide between the alternatives of expanding a single convolutional network model or increasing the number of networks in the form of an ensemble. We study this question in detail across various network architectures and data sets. We build an extensive experimental framework that captures numerous angles of the possible design space in terms of how a new set of parameters can be used in a model.  We consider a holistic set of metrics such as training time, inference time, and memory usage. The framework provides a robust assessment by making sure it controls for the number of parameters. Contrary to conventional wisdom, we show that when we perform a holistic and robust assessment, we uncover a wide design space, where ensembles provide better accuracy, train faster, and deploy at speed comparable to single convolutional networks with the same total number of parameters. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that when we perform a holistic assessment, we uncover a wide design space, where ensembles not only provide better accuracy but also train and deploy with fewer resources than comparable single convolutional network models.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="DILxQP08O3B" data-number="624">
      <h4>
        <a href="https://openreview.net/forum?id=DILxQP08O3B">
            VTNet: Visual Transformer Network for Object Goal Navigation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=DILxQP08O3B" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Heming_Du2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Heming_Du2">Heming Du</a>, <a href="https://openreview.net/profile?id=~Xin_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xin_Yu1">Xin Yu</a>, <a href="https://openreview.net/profile?id=~Liang_Zheng4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liang_Zheng4">Liang Zheng</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#DILxQP08O3B-details-347" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="DILxQP08O3B-details-347"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Object goal navigation aims to steer an agent towards a target object based on observations of the agent. It is of pivotal importance to design effective visual representations of the observed scene in determining navigation actions.  In this paper, we introduce a Visual Transformer Network (VTNet) for learning informative visual representation in navigation.  VTNet is a highly effective structure that embodies two key properties for visual representations: First, the relationships among all the object instances in a scene are exploited; Second, the spatial locations of objects and image regions are emphasized so that directional navigation signals can be learned. Furthermore, we also develop a pre-training scheme to associate the visual representations with navigation signals, and thus facilitate navigation policy learning. In a nutshell, VTNet embeds object and region features with their location cues as spatial-aware descriptors and then incorporates all the encoded descriptors through attention operations to achieve informative representation for navigation. Given such visual representations, agents are able to explore the correlations between visual observations and navigation actions. For example, an agent would prioritize ``turning right'' over ``turning left'' when the visual representation emphasizes on the right side of activation map. Experiments in the artificial environment AI2-Thor demonstrate that VTNet significantly outperforms state-of-the-art methods in unseen testing environments.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="7pgFL2Dkyyy" data-number="2020">
      <h4>
        <a href="https://openreview.net/forum?id=7pgFL2Dkyyy">
            Class Normalization for (Continual)? Generalized Zero-Shot Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=7pgFL2Dkyyy" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ivan_Skorokhodov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ivan_Skorokhodov1">Ivan Skorokhodov</a>, <a href="https://openreview.net/profile?id=~Mohamed_Elhoseiny1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohamed_Elhoseiny1">Mohamed Elhoseiny</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>26 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#7pgFL2Dkyyy-details-780" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="7pgFL2Dkyyy-details-780"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">zero-shot learning, normalization, continual learning, initialization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in the zero-shot learning (ZSL) world, these ideas have received only marginal attention. This work studies normalization in ZSL scenario from both theoretical and practical perspectives. First, we give a theoretical explanation to two popular tricks used in zero-shot learning: normalize+scale and attributes normalization and show that they help training by preserving variance during a forward pass. Next, we demonstrate that they are insufficient to normalize a deep ZSL model and propose Class Normalization (CN): a normalization scheme, which alleviates this issue both provably and in practice. Third, we show that ZSL models typically have more irregular loss surface compared to traditional classifiers and that the proposed method partially remedies this problem. Then, we test our approach on 4 standard ZSL datasets and outperform sophisticated modern SotA with a simple MLP optimized without any bells and whistles and having ~50 times faster training speed. Finally, we generalize ZSL to a broader problem — continual ZSL, and introduce some principled metrics and rigorous baselines for this new setup. The source code is available at https://github.com/universome/class-norm.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We develop theoretical understanding of signal normalization inside zero-shot learning models, propose a novel normalization scheme and use it to achieve SotA ZSL performance with a simple MLP</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="po-DLlBuAuz" data-number="216">
      <h4>
        <a href="https://openreview.net/forum?id=po-DLlBuAuz">
            Batch Reinforcement Learning Through Continuation Method
        </a>
      
        
          <a href="https://openreview.net/pdf?id=po-DLlBuAuz" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yijie_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yijie_Guo1">Yijie Guo</a>, <a href="https://openreview.net/profile?id=~Shengyu_Feng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shengyu_Feng1">Shengyu Feng</a>, <a href="https://openreview.net/profile?id=~Nicolas_Le_Roux2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicolas_Le_Roux2">Nicolas Le Roux</a>, <a href="https://openreview.net/profile?id=~Ed_Chi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ed_Chi1">Ed Chi</a>, <a href="https://openreview.net/profile?id=~Honglak_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Honglak_Lee2">Honglak Lee</a>, <a href="https://openreview.net/profile?id=~Minmin_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minmin_Chen1">Minmin Chen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#po-DLlBuAuz-details-790" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="po-DLlBuAuz-details-790"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">batch reinforcement learning, continuation method, relaxed regularization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Many real-world applications of reinforcement learning (RL) require the agent to learn from a fixed set of trajectories, without collecting new interactions.  Policy optimization under this setting is extremely challenging as: 1) the geometry of the objective function is hard to optimize efficiently; 2) the shift of data distributions causes high noise in the value estimation. In this work, we propose a simple yet effective policy iteration approach to batch RL using global optimization techniques known as continuation.  By constraining the difference between the learned policy and the behavior policy that generates the fixed trajectories, and continuously relaxing the constraint, our method 1) helps the agent escape local optima; 2) reduces the error in policy evaluation in the optimization procedure.   We present results on a variety of control tasks, game environments, and a recommendation task to empirically demonstrate the efficacy of our proposed method.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=po-DLlBuAuz&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="AHOs7Sm5H7R" data-number="3164">
      <h4>
        <a href="https://openreview.net/forum?id=AHOs7Sm5H7R">
            Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=AHOs7Sm5H7R" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhiyuan_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiyuan_Li2">Zhiyuan Li</a>, <a href="https://openreview.net/profile?id=~Yuping_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuping_Luo1">Yuping Luo</a>, <a href="https://openreview.net/profile?id=~Kaifeng_Lyu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaifeng_Lyu2">Kaifeng Lyu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#AHOs7Sm5H7R-details-951" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AHOs7Sm5H7R-details-951"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">matrix factorization, gradient descent, implicit regularization, implicit bias</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Matrix factorization is a simple and natural test-bed to investigate the implicit regularization of gradient descent. Gunasekar et al. (2017) conjectured that gradient flow with infinitesimal initialization converges to the solution that minimizes the nuclear norm, but a series of recent papers argued that the language of norm minimization is not sufficient to give a full characterization for the implicit regularization. In this work, we provide theoretical and empirical evidence that for depth-2 matrix factorization, gradient flow with infinitesimal initialization is mathematically equivalent to a simple heuristic rank minimization algorithm, Greedy Low-Rank Learning, under some reasonable assumptions. This generalizes the rank minimization view from previous works to a much broader setting and enables us to construct counter-examples to refute the conjecture from Gunasekar et al. (2017). We also extend the results to the case where depth &gt;= 3, and we show that the benefit of being deeper is that the above convergence has a much weaker dependence over initialization magnitude so that this rank minimization is more likely to take effect for initialization with practical scale.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We prove that for depth-2 matrix factorization, gradient flow with infinitesimal initialization is mathematically equivalent to a simple heuristic rank minimization algorithm, Greedy Low-Rank Learning, under some reasonable assumptions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="vYeQQ29Tbvx" data-number="310">
      <h4>
        <a href="https://openreview.net/forum?id=vYeQQ29Tbvx">
            Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=vYeQQ29Tbvx" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jonathan_Frankle1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Frankle1">Jonathan Frankle</a>, <a href="https://openreview.net/profile?id=~David_J._Schwab1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_J._Schwab1">David J. Schwab</a>, <a href="https://openreview.net/profile?id=~Ari_S._Morcos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ari_S._Morcos1">Ari S. Morcos</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 09 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#vYeQQ29Tbvx-details-756" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vYeQQ29Tbvx-details-756"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">affine parameters, random features, batchnorm</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A wide variety of deep learning techniques from style transfer to multitask learning rely on training affine transformations of features. Most prominent among these is the popular feature normalization technique BatchNorm, which normalizes activations and then subsequently applies a learned affine transform. In this paper, we aim to understand the role and expressive power of affine parameters used to transform features in this way. To isolate the contribution of these parameters from that of the learned features they transform, we investigate the performance achieved when training only these parameters in BatchNorm and freezing all weights at their random initializations. Doing so leads to surprisingly high performance considering the significant limitations that this style of training imposes. For example, sufficiently deep ResNets reach 82% (CIFAR-10) and 32% (ImageNet, top-5) accuracy in this configuration, far higher than when training an equivalent number of randomly chosen parameters elsewhere in the network. BatchNorm achieves this performance in part by naturally learning to disable around a third of the random features. Not only do these results highlight the expressive power of affine parameters in deep learning, but - in a broader sense - they characterize the expressive power of neural networks constructed simply by shifting and rescaling random features.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We study the role and expressive power of learned affine parameters that transform features by freezing all weights at their random initializations and training only BatchNorm.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="IgIk8RRT-Z" data-number="2447">
      <h4>
        <a href="https://openreview.net/forum?id=IgIk8RRT-Z">
            CompOFA – Compound Once-For-All Networks for Faster Multi-Platform Deployment
        </a>
      
        
          <a href="https://openreview.net/pdf?id=IgIk8RRT-Z" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Manas_Sahni1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manas_Sahni1">Manas Sahni</a>, <a href="https://openreview.net/profile?email=shreyavarshini%40gatech.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="shreyavarshini@gatech.edu">Shreya Varshini</a>, <a href="https://openreview.net/profile?id=~Alind_Khare1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alind_Khare1">Alind Khare</a>, <a href="https://openreview.net/profile?email=atumanov%40gatech.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="atumanov@gatech.edu">Alexey Tumanov</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#IgIk8RRT-Z-details-464" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IgIk8RRT-Z-details-464"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Efficient Deep Learning, Latency-aware Neural Architecture Search, AutoML</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The emergence of CNNs in mainstream deployment has necessitated methods to design and train efficient architectures tailored to maximize the accuracy under diverse hardware and latency constraints. To scale these resource-intensive tasks with an increasing number of deployment targets, Once-For-All (OFA) proposed an approach to jointly train several models at once with a constant training cost. However, this cost remains as high as 40-50 GPU days and also suffers from a combinatorial explosion of sub-optimal model configurations. We seek to reduce this search space -- and hence the training budget -- by constraining search to models close to the accuracy-latency Pareto frontier. We incorporate insights of compound relationships between model dimensions to build CompOFA, a design space smaller by several orders of magnitude.  Through experiments on ImageNet, we demonstrate that even with simple heuristics we can achieve a 2x reduction in training time and 216x speedup in model search/extraction time compared to the state of the art, without loss of Pareto optimality! We also show that this smaller design space is dense enough to support equally accurate models for a similar diversity of hardware and latency targets, while also reducing the complexity of the training and subsequent extraction algorithms. Our source code is available at https://github.com/gatech-sysml/CompOFA</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">CNN design-space and system insights for faster latency-guided training and searching of models for diverse deployment targets.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ahAUv8TI2Mz" data-number="2713">
      <h4>
        <a href="https://openreview.net/forum?id=ahAUv8TI2Mz">
            Adaptive and Generative Zero-Shot Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ahAUv8TI2Mz" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yu-Ying_Chou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu-Ying_Chou1">Yu-Ying Chou</a>, <a href="https://openreview.net/profile?id=~Hsuan-Tien_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hsuan-Tien_Lin1">Hsuan-Tien Lin</a>, <a href="https://openreview.net/profile?id=~Tyng-Luh_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tyng-Luh_Liu1">Tyng-Luh Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ahAUv8TI2Mz-details-185" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ahAUv8TI2Mz-details-185"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Generalized zero-shot learning, mixup</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We address the problem of generalized zero-shot learning (GZSL) where the task is to predict the class label of a target image whether its label belongs to the seen or unseen category. Similar to ZSL, the learning setting assumes that all class-level semantic features are given, while only the images of seen classes are available for training. By exploring the correlation between image features and the corresponding semantic features, the main idea of the proposed approach is to enrich the semantic-to-visual (S2V) embeddings via a seamless fusion of adaptive and generative learning. To this end, we extend the semantic features of each class by supplementing image-adaptive attention so that the learned S2V embedding can account for not only inter-class but also intra-class variations. In addition, to break the limit of training with images only from seen classes, we design a generative scheme to simultaneously generate virtual class labels and their visual features by sampling and interpolating over seen counterparts. In inference, a testing image will give rise to two different S2V embeddings, seen and virtual. The former is used to decide whether the underlying label is of the unseen category or otherwise a specific seen class; the latter is to predict an unseen class label. To demonstrate the effectiveness of our method, we report state-of-the-art results on four standard GZSL datasets, including an ablation study of the proposed modules. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ac288vnG_7U" data-number="2879">
      <h4>
        <a href="https://openreview.net/forum?id=ac288vnG_7U">
            Learning to Make Decisions via Submodular Regularization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ac288vnG_7U" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=alievaayya%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="alievaayya@gmail.com">Ayya Alieva</a>, <a href="https://openreview.net/profile?email=aaceves%40caltech.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="aaceves@caltech.edu">Aiden Aceves</a>, <a href="https://openreview.net/profile?id=~Jialin_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jialin_Song1">Jialin Song</a>, <a href="https://openreview.net/profile?id=~Stephen_Mayo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephen_Mayo1">Stephen Mayo</a>, <a href="https://openreview.net/profile?id=~Yisong_Yue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yisong_Yue1">Yisong Yue</a>, <a href="https://openreview.net/profile?id=~Yuxin_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuxin_Chen1">Yuxin Chen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ac288vnG_7U-details-557" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ac288vnG_7U-details-557"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Many sequential decision making tasks can be viewed as combinatorial optimization problems over a large number of actions. When the cost of evaluating an action is high, even a greedy algorithm, which iteratively picks the best action given the history, is prohibitive to run. In this paper, we aim to learn a greedy heuristic for sequentially selecting actions as a surrogate for invoking the expensive oracle when evaluating an action. In particular, we focus on a class of combinatorial problems that can be solved via submodular maximization (either directly on the objective function or via submodular surrogates). We introduce a data-driven optimization framework based on the submodular-norm loss, a novel loss function that encourages the resulting objective to exhibit diminishing returns. Our framework outputs a surrogate objective that is efficient to train, approximately submodular, and can be made permutation-invariant. The latter two properties allow us to prove strong approximation guarantees for the learned greedy heuristic. Furthermore, we show that our model can be easily integrated with modern deep imitation learning pipelines for sequential prediction tasks. We demonstrate the performance of our algorithm on a variety of batched and sequential optimization tasks, including set cover, active learning, and Bayesian optimization for protein engineering.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A data-driven sequential decision making framework based on a novel submodular-regularized loss.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=ac288vnG_7U&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="M88oFvqp_9" data-number="12">
      <h4>
        <a href="https://openreview.net/forum?id=M88oFvqp_9">
            Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains
        </a>
      
        
          <a href="https://openreview.net/pdf?id=M88oFvqp_9" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Utkarsh_Ojha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Utkarsh_Ojha1">Utkarsh Ojha</a>, <a href="https://openreview.net/profile?id=~Krishna_Kumar_Singh4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Krishna_Kumar_Singh4">Krishna Kumar Singh</a>, <a href="https://openreview.net/profile?id=~Yong_Jae_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yong_Jae_Lee2">Yong Jae Lee</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#M88oFvqp_9-details-680" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="M88oFvqp_9-details-680"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">multi-domain disentanglement, generative adversarial networks, appearance transfer</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars).  The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively.  This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains.  Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a framework for multi-domain disentanglement, facilitating transfer of appearance from one domain to another.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="O9bnihsFfXU" data-number="913">
      <h4>
        <a href="https://openreview.net/forum?id=O9bnihsFfXU">
            Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=O9bnihsFfXU" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Aviral_Kumar2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aviral_Kumar2">Aviral Kumar</a>, <a href="https://openreview.net/profile?id=~Rishabh_Agarwal2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rishabh_Agarwal2">Rishabh Agarwal</a>, <a href="https://openreview.net/profile?id=~Dibya_Ghosh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dibya_Ghosh1">Dibya Ghosh</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>21 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#O9bnihsFfXU-details-966" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="O9bnihsFfXU-details-966"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep Q-learning, data-efficient RL, rank-collapse, offline RL</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We identify an implicit under-parameterization phenomenon in value-based deep RL methods that use bootstrapping: when value functions, approximated using deep neural networks, are trained with gradient descent using iterated regression onto target values generated by previous instances of the value network, more gradient updates decrease the expressivity of the current value network. We char- acterize this loss of expressivity via a drop in the rank of the learned value net- work features, and show that this typically corresponds to a performance drop. We demonstrate this phenomenon on Atari and Gym benchmarks, in both offline and online RL settings. We formally analyze this phenomenon and show that it results from a pathological interaction between bootstrapping and gradient-based optimization. We further show that mitigating implicit under-parameterization by controlling rank collapse can improve performance.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Identifies and studies feature matrix rank collapse (i.e. implicit regularization) in deep Q-learning methods.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-Lr-u0b42he" data-number="3026">
      <h4>
        <a href="https://openreview.net/forum?id=-Lr-u0b42he">
            Disentangling 3D Prototypical Networks for Few-Shot Concept Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-Lr-u0b42he" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mihir_Prabhudesai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mihir_Prabhudesai1">Mihir Prabhudesai</a>, <a href="https://openreview.net/profile?id=~Shamit_Lal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shamit_Lal1">Shamit Lal</a>, <a href="https://openreview.net/profile?id=~Darshan_Patil1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Darshan_Patil1">Darshan Patil</a>, <a href="https://openreview.net/profile?id=~Hsiao-Yu_Tung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hsiao-Yu_Tung1">Hsiao-Yu Tung</a>, <a href="https://openreview.net/profile?id=~Adam_W_Harley1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adam_W_Harley1">Adam W Harley</a>, <a href="https://openreview.net/profile?id=~Katerina_Fragkiadaki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Katerina_Fragkiadaki1">Katerina Fragkiadaki</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-Lr-u0b42he-details-722" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-Lr-u0b42he-details-722"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Disentanglement, Few Shot Learning, 3D Vision, VQA</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present neural architectures that disentangle RGB-D images into objects’ shapes and styles and a map of the background scene, and explore their applications for few-shot 3D object detection and few-shot concept classification. Our networks incorporate architectural biases that reflect the image formation process, 3D  geometry of the world scene, and shape-style interplay. They are trained end-to-end self-supervised by predicting views in static scenes, alongside a small number of 3D object boxes. Objects and scenes are represented in terms of 3D feature grids in the bottleneck of the network. We show the proposed 3D neural representations are compositional: they can generate novel 3D scene feature maps by mixing object shapes and styles, resizing and adding the resulting object 3D feature maps over background scene feature maps. We show object detectors trained on hallucinated 3D neural scenes generalize better to novel environments. We show classifiers for object categories, color, materials, and spatial relationships trained over the  disentangled 3D feature sub-spaces generalize better with dramatically fewer exemplars over the current state-of-the-art, and enable a visual question answering system that uses them as its modules to generalize one-shot to novel objects in the scene.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present neural architectures that disentangle RGB-D images into objects’ shapes and styles and a map of the background scene, and explore their applications for few-shot 3D object detection and few-shot concept classification.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=-Lr-u0b42he&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="TSRTzJnuEBS" data-number="1024">
      <h4>
        <a href="https://openreview.net/forum?id=TSRTzJnuEBS">
            Anytime Sampling for Autoregressive Models via Ordered Autoencoding
        </a>
      
        
          <a href="https://openreview.net/pdf?id=TSRTzJnuEBS" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yilun_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yilun_Xu1">Yilun Xu</a>, <a href="https://openreview.net/profile?id=~Yang_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Song1">Yang Song</a>, <a href="https://openreview.net/profile?id=~Sahaj_Garg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sahaj_Garg1">Sahaj Garg</a>, <a href="https://openreview.net/profile?email=gonglinyuan%40hotmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="gonglinyuan@hotmail.com">Linyuan Gong</a>, <a href="https://openreview.net/profile?id=~Rui_Shu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rui_Shu1">Rui Shu</a>, <a href="https://openreview.net/profile?id=~Aditya_Grover1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aditya_Grover1">Aditya Grover</a>, <a href="https://openreview.net/profile?id=~Stefano_Ermon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefano_Ermon1">Stefano Ermon</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 11 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#TSRTzJnuEBS-details-706" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TSRTzJnuEBS-details-706"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Autoregressive models are widely used for tasks such as image and audio generation. The sampling process of these models, however, does not allow interruptions and cannot adapt to real-time computational resources. This challenge impedes the deployment of powerful autoregressive models, which involve a slow sampling process that is sequential in nature and typically scales linearly with respect to the data dimension.  To address this difficulty, we propose a new family of autoregressive models that enables anytime sampling. Inspired by Principal Component Analysis, we learn a structured representation space where dimensions are ordered based on their importance with respect to reconstruction. Using an autoregressive model in this latent space, we trade off sample quality for computational efficiency by truncating the generation process before decoding into the original data space. Experimentally, we demonstrate in several image and audio generation tasks that sample quality degrades gracefully as we reduce the computational budget for sampling. The approach suffers almost no loss in sample quality (measured by FID) using only 60\% to 80\% of all latent dimensions for image data. Code is available at https://github.com/Newbeeer/Anytime-Auto-Regressive-Model.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a new family of autoregressive model that enables anytime sampling</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=TSRTzJnuEBS&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="pHXfe1cOmA" data-number="3167">
      <h4>
        <a href="https://openreview.net/forum?id=pHXfe1cOmA">
            HyperDynamics: Meta-Learning Object and Agent Dynamics with Hypernetworks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=pHXfe1cOmA" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhou_Xian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhou_Xian1">Zhou Xian</a>, <a href="https://openreview.net/profile?id=~Shamit_Lal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shamit_Lal1">Shamit Lal</a>, <a href="https://openreview.net/profile?id=~Hsiao-Yu_Tung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hsiao-Yu_Tung1">Hsiao-Yu Tung</a>, <a href="https://openreview.net/profile?id=~Emmanouil_Antonios_Platanios1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Emmanouil_Antonios_Platanios1">Emmanouil Antonios Platanios</a>, <a href="https://openreview.net/profile?id=~Katerina_Fragkiadaki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Katerina_Fragkiadaki1">Katerina Fragkiadaki</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>19 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#pHXfe1cOmA-details-696" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pHXfe1cOmA-details-696"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose HyperDynamics, a dynamics meta-learning framework that conditions on an agent’s interactions with the environment and optionally its visual observations, and generates the parameters of neural dynamics models based on inferred properties of the dynamical system. Physical and visual properties of the environment that are not part of the low-dimensional state yet affect its temporal dynamics are inferred from the interaction history and visual observations, and are implicitly captured in the generated parameters. We test HyperDynamics on a set of object pushing and locomotion tasks. It outperforms existing dynamics models in the literature that adapt to environment variations by learning dynamics over high dimensional visual observations, capturing the interactions of the agent in recurrent state representations, or using gradient-based meta-optimization. We also show our method matches the performance of an ensemble of separately trained experts, while also being able to generalize well to unseen environment variations at test time. We attribute its good performance to the multiplicative interactions between the inferred system properties—captured in the generated parameters—and the low-dimensional state representation of the dynamical system.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=pHXfe1cOmA&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="TgSVWXw22FQ" data-number="1968">
      <h4>
        <a href="https://openreview.net/forum?id=TgSVWXw22FQ">
            Improving Zero-Shot Voice Style Transfer via Disentangled Representation Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=TgSVWXw22FQ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Siyang_Yuan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siyang_Yuan1">Siyang Yuan</a>, <a href="https://openreview.net/profile?id=~Pengyu_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pengyu_Cheng1">Pengyu Cheng</a>, <a href="https://openreview.net/profile?id=~Ruiyi_Zhang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruiyi_Zhang3">Ruiyi Zhang</a>, <a href="https://openreview.net/profile?id=~Weituo_Hao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weituo_Hao1">Weituo Hao</a>, <a href="https://openreview.net/profile?id=~Zhe_Gan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhe_Gan1">Zhe Gan</a>, <a href="https://openreview.net/profile?id=~Lawrence_Carin2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lawrence_Carin2">Lawrence Carin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#TgSVWXw22FQ-details-639" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TgSVWXw22FQ-details-639"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Style Transfer, Mutual Information, Zero-shot Learning, Disentanglement</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Voice style transfer, also called voice conversion, seeks to modify one speaker's voice to generate speech as if it came from another (target) speaker. Previous works have made progress on voice conversion with parallel training data and pre-known speakers. However, zero-shot voice style transfer, which learns from non-parallel data and generates voices for previously unseen speakers, remains a challenging problem. In this paper we propose a novel zero-shot voice transfer method via disentangled representation learning. The proposed method first encodes speaker-related style and voice content of each input voice into separate low-dimensional embedding spaces, and then transfers to a new voice by combining the source content embedding and target style embedding through a decoder. With information-theoretic guidance, the style and content embedding spaces are representative and (ideally) independent of each other. On real-world datasets, our method outperforms other baselines and obtains state-of-the-art results in terms of transfer accuracy and voice naturalness.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">An information-theoretic disentangled representation learning framework for zero-shot voice style transfer.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=TgSVWXw22FQ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="kyaIeYj4zZ" data-number="3175">
      <h4>
        <a href="https://openreview.net/forum?id=kyaIeYj4zZ">
            GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing
        </a>
      
        
          <a href="https://openreview.net/pdf?id=kyaIeYj4zZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tao_Yu5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Yu5">Tao Yu</a>, <a href="https://openreview.net/profile?id=~Chien-Sheng_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chien-Sheng_Wu1">Chien-Sheng Wu</a>, <a href="https://openreview.net/profile?id=~Xi_Victoria_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xi_Victoria_Lin1">Xi Victoria Lin</a>, <a href="https://openreview.net/profile?id=~bailin_wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~bailin_wang1">bailin wang</a>, <a href="https://openreview.net/profile?email=yichern.tan%40yale.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yichern.tan@yale.edu">Yi Chern Tan</a>, <a href="https://openreview.net/profile?email=x.yang%40salesforce.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="x.yang@salesforce.com">Xinyi Yang</a>, <a href="https://openreview.net/profile?id=~Dragomir_Radev2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dragomir_Radev2">Dragomir Radev</a>, <a href="https://openreview.net/profile?id=~richard_socher1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~richard_socher1">richard socher</a>, <a href="https://openreview.net/profile?id=~Caiming_Xiong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Caiming_Xiong1">Caiming Xiong</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#kyaIeYj4zZ-details-969" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kyaIeYj4zZ-details-969"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">text-to-sql, semantic parsing, pre-training, nlp</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present GraPPa, an effective pre-training approach for table semantic parsing that learns a compositional inductive bias in the joint representations of textual and tabular data. We construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG). We pre-train our model on the synthetic data to inject important structural properties commonly found in semantic parsing into the pre-training language model. To maintain the model's ability to represent real-world data, we also include masked language modeling (MLM) on several existing table-related datasets to regularize our pre-training process.  Our proposed pre-training strategy is much data-efficient. When incorporated with strong base semantic parsers, GraPPa achieves new state-of-the-art results on four popular fully supervised and weakly supervised table semantic parsing tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Language model pre-training for table semantic parsing.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="VcB4QkSfyO" data-number="2234">
      <h4>
        <a href="https://openreview.net/forum?id=VcB4QkSfyO">
            Estimating Lipschitz constants of monotone deep equilibrium models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=VcB4QkSfyO" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Chirag_Pabbaraju1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chirag_Pabbaraju1">Chirag Pabbaraju</a>, <a href="https://openreview.net/profile?id=~Ezra_Winston1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ezra_Winston1">Ezra Winston</a>, <a href="https://openreview.net/profile?id=~J_Zico_Kolter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~J_Zico_Kolter1">J Zico Kolter</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>19 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#VcB4QkSfyO-details-933" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="VcB4QkSfyO-details-933"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep equilibrium models, Lipschitz constants</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Several methods have been proposed in recent years to provide bounds on the Lipschitz constants of deep networks, which can be used to provide robustness guarantees, generalization bounds, and characterize the smoothness of decision boundaries. However, existing bounds get substantially weaker with increasing depth of the network, which makes it unclear how to apply such bounds to recently proposed models such as the deep equilibrium (DEQ) model, which can be viewed as representing an infinitely-deep network. In this paper, we show that monotone DEQs, a recently-proposed subclass of DEQs, have Lipschitz constants that can be bounded as a simple function of the strong monotonicity parameter of the network. We derive simple-yet-tight bounds on both the input-output mapping and the weight-output mapping defined by these networks, and demonstrate that they are small relative to those for comparable standard DNNs. We show that one can use these bounds to design monotone DEQ models, even with e.g. multi-scale convolutional structure, that still have constraints on the Lipschitz constant. We also highlight how to use these bounds to develop PAC-Bayes generalization bounds that do not depend on any depth of the network, and which avoid the exponential depth-dependence of comparable DNN bounds.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Monotone deep equilibrium models have Lipschitz constants which are simple to bound and small relative to those of standard DNNs, which suffer with depth.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="kEnBH98BGs5" data-number="1579">
      <h4>
        <a href="https://openreview.net/forum?id=kEnBH98BGs5">
            Estimating informativeness of samples with Smooth Unique Information
        </a>
      
        
          <a href="https://openreview.net/pdf?id=kEnBH98BGs5" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hrayr_Harutyunyan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hrayr_Harutyunyan1">Hrayr Harutyunyan</a>, <a href="https://openreview.net/profile?id=~Alessandro_Achille1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alessandro_Achille1">Alessandro Achille</a>, <a href="https://openreview.net/profile?id=~Giovanni_Paolini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Giovanni_Paolini1">Giovanni Paolini</a>, <a href="https://openreview.net/profile?id=~Orchid_Majumder1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Orchid_Majumder1">Orchid Majumder</a>, <a href="https://openreview.net/profile?id=~Avinash_Ravichandran1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Avinash_Ravichandran1">Avinash Ravichandran</a>, <a href="https://openreview.net/profile?id=~Rahul_Bhotika1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rahul_Bhotika1">Rahul Bhotika</a>, <a href="https://openreview.net/profile?id=~Stefano_Soatto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefano_Soatto1">Stefano Soatto</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#kEnBH98BGs5-details-321" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kEnBH98BGs5-details-321"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">sample information, information theory, stability theory, ntk, dataset summarization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We define a notion of information that an individual sample provides to the training of a neural network, and we specialize it to measure both how much a sample informs the final weights and how much it informs the function computed by the weights. Though related, we show that these quantities have a  qualitatively different behavior. We give efficient approximations of these quantities using a linearized network and demonstrate empirically that the approximation is accurate for real-world architectures, such as pre-trained ResNets. We apply these measures to several problems, such as dataset summarization, analysis of under-sampled classes, comparison of informativeness of different data sources, and detection of adversarial and corrupted examples. Our work generalizes existing frameworks, but enjoys better computational properties for heavily over-parametrized models, which makes it possible to apply it to real-world networks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We define, both in weight-space and function-space, a notion of unique information that an individual sample provides to the training of a deep network and show how to compute it efficiently for large networks using a linearization of the model.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="mCLVeEpplNE" data-number="1088">
      <h4>
        <a href="https://openreview.net/forum?id=mCLVeEpplNE">
            NBDT: Neural-Backed Decision Tree
        </a>
      
        
          <a href="https://openreview.net/pdf?id=mCLVeEpplNE" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Alvin_Wan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alvin_Wan1">Alvin Wan</a>, <a href="https://openreview.net/profile?id=~Lisa_Dunlap1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lisa_Dunlap1">Lisa Dunlap</a>, <a href="https://openreview.net/profile?id=~Daniel_Ho2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Ho2">Daniel Ho</a>, <a href="https://openreview.net/profile?id=~Jihan_Yin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jihan_Yin1">Jihan Yin</a>, <a href="https://openreview.net/profile?id=~Scott_Lee2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Scott_Lee2">Scott Lee</a>, <a href="https://openreview.net/profile?id=~Suzanne_Petryk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Suzanne_Petryk1">Suzanne Petryk</a>, <a href="https://openreview.net/profile?id=~Sarah_Adel_Bargal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sarah_Adel_Bargal1">Sarah Adel Bargal</a>, <a href="https://openreview.net/profile?id=~Joseph_E._Gonzalez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joseph_E._Gonzalez1">Joseph E. Gonzalez</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 28 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>23 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#mCLVeEpplNE-details-933" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mCLVeEpplNE-details-933"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">explainability, computer vision, interpretability</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Machine learning applications such as finance and medicine demand accurate and justifiable predictions, barring most deep learning methods from use. In response, previous work combines decision trees with deep learning, yielding models that (1) sacrifice interpretability for accuracy or (2) sacrifice accuracy for interpretability. We forgo this dilemma by jointly improving accuracy and interpretability using Neural-Backed Decision Trees (NBDTs). NBDTs replace a neural network's final linear layer with a differentiable sequence of decisions and a surrogate loss. This forces the model to learn high-level concepts and lessens reliance on highly-uncertain decisions, yielding (1) accuracy: NBDTs match or outperform modern neural networks on CIFAR, ImageNet and better generalize to unseen classes by up to 16%. Furthermore, our surrogate loss improves the original model's accuracy by up to 2%. NBDTs also afford (2) interpretability: improving human trustby clearly identifying model mistakes and assisting in dataset debugging. Code and pretrained NBDTs are at https://github.com/alvinwan/neural-backed-decision-trees.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Neural-Backed Decision Trees improve interpretability and accuracy: (1) out-generalize, improve, and match or outperform baseline neural networks; (2) show visual evidence of generalization, reveal ambiguous ImageNet labels, and improve human trust.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=mCLVeEpplNE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="JHcqXGaqiGn" data-number="495">
      <h4>
        <a href="https://openreview.net/forum?id=JHcqXGaqiGn">
            Accurate Learning of Graph Representations with Graph Multiset Pooling
        </a>
      
        
          <a href="https://openreview.net/pdf?id=JHcqXGaqiGn" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jinheon_Baek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinheon_Baek1">Jinheon Baek</a>, <a href="https://openreview.net/profile?id=~Minki_Kang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Minki_Kang1">Minki Kang</a>, <a href="https://openreview.net/profile?id=~Sung_Ju_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sung_Ju_Hwang1">Sung Ju Hwang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>34 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#JHcqXGaqiGn-details-426" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JHcqXGaqiGn-details-426"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Graph representation learning, Graph pooling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Graph neural networks have been widely used on modeling graph data, achieving impressive results on node classification and link prediction tasks. Yet, obtaining an accurate representation for a graph further requires a pooling function that maps a set of node representations into a compact form. A simple sum or average over all node representations considers all node features equally without consideration of their task relevance, and any structural dependencies among them. Recently proposed hierarchical graph pooling methods, on the other hand, may yield the same representation for two different graphs that are distinguished by the Weisfeiler-Lehman test, as they suboptimally preserve information from the node features. To tackle these limitations of existing graph pooling methods, we first formulate the graph pooling problem as a multiset encoding problem with auxiliary information about the graph structure, and propose a Graph Multiset Transformer (GMT) which is a multi-head attention based global pooling layer that captures the interaction between nodes according to their structural dependencies. We show that GMT satisfies both injectiveness and permutation invariance, such that it is at most as powerful as the Weisfeiler-Lehman graph isomorphism test. Moreover, our methods can be easily extended to the previous node clustering approaches for hierarchical graph pooling. Our experimental results show that GMT significantly outperforms state-of-the-art graph pooling methods on graph classification benchmarks with high memory and time efficiency, and obtains even larger performance gain on graph reconstruction and generation tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A novel graph pooling method for graph representation learning, that considers multiset with attention-based operations.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="PbEHqvFtcS" data-number="2007">
      <h4>
        <a href="https://openreview.net/forum?id=PbEHqvFtcS">
            Byzantine-Resilient Non-Convex Stochastic Gradient Descent
        </a>
      
        
          <a href="https://openreview.net/pdf?id=PbEHqvFtcS" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zeyuan_Allen-Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zeyuan_Allen-Zhu1">Zeyuan Allen-Zhu</a>, <a href="https://openreview.net/profile?email=faezeeb75%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="faezeeb75@gmail.com">Faeze Ebrahimianghazani</a>, <a href="https://openreview.net/profile?id=~Jerry_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jerry_Li1">Jerry Li</a>, <a href="https://openreview.net/profile?id=~Dan_Alistarh7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Alistarh7">Dan Alistarh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Apr 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#PbEHqvFtcS-details-759" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PbEHqvFtcS-details-759"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">distributed machine learning, distributed deep learning, robust deep learning, non-convex optimization, Byzantine resilience</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study adversary-resilient stochastic distributed optimization, in which <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="316" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi></math></mjx-assistive-mml></mjx-container> machines can independently compute stochastic gradients, and cooperate to jointly optimize over their local objective functions. However, an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="317" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D6FC TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></mjx-assistive-mml></mjx-container>-fraction of the machines are Byzantine, in that they may behave in arbitrary, adversarial ways. We consider a variant of this procedure in the challenging non-convex case. Our main result is a new algorithm SafeguardSGD, which can provably escape saddle points and find approximate local minima of the non-convex objective. The algorithm is based on a new concentration filtering technique, and its sample and time complexity bounds match the best known theoretical bounds in the stochastic, distributed setting when no Byzantine machines are present. 
      
      Our algorithm is very practical: it improves upon the performance of all prior methods when training deep neural networks, it is relatively lightweight, and it is the first method to withstand two recently-proposed Byzantine attacks. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">New algorithm for non-convex distributed optimization against Byzantine attacks, with strong theoretical guarantees, and improves on the performance of prior methods for training deep neural networks against Byzantine attacks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9z_dNsC4B5t" data-number="1508">
      <h4>
        <a href="https://openreview.net/forum?id=9z_dNsC4B5t">
            MetaNorm: Learning to Normalize Few-Shot Batches Across Domains
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9z_dNsC4B5t" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yingjun_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yingjun_Du1">Yingjun Du</a>, <a href="https://openreview.net/profile?id=~Xiantong_Zhen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiantong_Zhen1">Xiantong Zhen</a>, <a href="https://openreview.net/profile?id=~Ling_Shao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ling_Shao1">Ling Shao</a>, <a href="https://openreview.net/profile?id=~Cees_G._M._Snoek1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cees_G._M._Snoek1">Cees G. M. Snoek</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9z_dNsC4B5t-details-686" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9z_dNsC4B5t-details-686"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Meta-learning, batch normalization, few-shot domain generalization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Batch normalization plays a crucial role when training deep neural networks. However, batch statistics become unstable with small batch sizes and are unreliable in the presence of distribution shifts. We propose MetaNorm, a simple yet effective meta-learning normalization. It tackles the aforementioned issues in a unified way by leveraging the meta-learning setting and learns to infer adaptive statistics for batch normalization. MetaNorm is generic, flexible and model-agnostic, making it a simple plug-and-play module that is seamlessly embedded into existing meta-learning approaches. It can be efficiently implemented by lightweight hypernetworks with low computational cost. We verify its effectiveness by extensive evaluation on representative tasks suffering from the small batch and domain shift problems: few-shot learning and domain generalization. We further introduce an even more challenging setting: few-shot domain generalization. Results demonstrate that MetaNorm consistently achieves better, or at least competitive, accuracy compared to existing batch normalization methods.  </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose MetaNorm, a simple yet effective meta-learning normalization approach that learns adaptive statistics for few-shot classification and domain generalization</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="cP5IcoAkfKa" data-number="721">
      <h4>
        <a href="https://openreview.net/forum?id=cP5IcoAkfKa">
            Large Batch Simulation for Deep Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=cP5IcoAkfKa" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Brennan_Shacklett1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brennan_Shacklett1">Brennan Shacklett</a>, <a href="https://openreview.net/profile?id=~Erik_Wijmans1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Erik_Wijmans1">Erik Wijmans</a>, <a href="https://openreview.net/profile?email=petrenko%40usc.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="petrenko@usc.edu">Aleksei Petrenko</a>, <a href="https://openreview.net/profile?id=~Manolis_Savva1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manolis_Savva1">Manolis Savva</a>, <a href="https://openreview.net/profile?id=~Dhruv_Batra1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dhruv_Batra1">Dhruv Batra</a>, <a href="https://openreview.net/profile?id=~Vladlen_Koltun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vladlen_Koltun1">Vladlen Koltun</a>, <a href="https://openreview.net/profile?id=~Kayvon_Fatahalian2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kayvon_Fatahalian2">Kayvon Fatahalian</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#cP5IcoAkfKa-details-820" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="cP5IcoAkfKa-details-820"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, simulation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We accelerate deep reinforcement learning-based training in visually complex 3D environments by two orders of magnitude over prior work, realizing end-to-end training speeds of over 19,000 frames of experience per second on a single GPU and up to 72,000 frames per second on a single eight-GPU machine. The key idea of our approach is to design a 3D renderer and embodied navigation simulator around the principle of “batch simulation”: accepting and executing large batches of requests simultaneously.  Beyond exposing large amounts of work at once, batch simulation allows implementations to amortize in-memory storage of scene assets, rendering work, data loading, and synchronization costs across many simulation requests, dramatically improving the number of simulated agents per GPU and overall simulation throughput.  To balance DNN inference and training costs with faster simulation, we also build a computationally efficient policy DNN that maintains high task performance, and modify training algorithms to maintain sample efficiency when training with large mini-batches. By combining batch simulation and DNN performance optimizations, we demonstrate that PointGoal navigation agents can be trained in complex 3D environments on a single GPU in 1.5 days to 97% of the accuracy of agents trained on a prior state-of-the-art system using a 64-GPU cluster over three days.  We provide open-source reference implementations of our batch 3D renderer and simulator to facilitate incorporation of these ideas into RL systems.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=cP5IcoAkfKa&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ehJqJQk9cw" data-number="1878">
      <h4>
        <a href="https://openreview.net/forum?id=ehJqJQk9cw">
            Personalized Federated Learning with First Order Model Optimization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ehJqJQk9cw" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Michael_Zhang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Zhang4">Michael Zhang</a>, <a href="https://openreview.net/profile?id=~Karan_Sapra2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Karan_Sapra2">Karan Sapra</a>, <a href="https://openreview.net/profile?id=~Sanja_Fidler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanja_Fidler1">Sanja Fidler</a>, <a href="https://openreview.net/profile?id=~Serena_Yeung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Serena_Yeung1">Serena Yeung</a>, <a href="https://openreview.net/profile?id=~Jose_M._Alvarez2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jose_M._Alvarez2">Jose M. Alvarez</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ehJqJQk9cw-details-414" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ehJqJQk9cw-details-414"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Federated learning, personalized learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While federated learning traditionally aims to train a single global model across decentralized local datasets, one model may not always be ideal for all participating clients. Here we propose an alternative, where each client only federates with other relevant clients to obtain a stronger model per client-specific objectives. To achieve this personalization, rather than computing a single model average with constant weights for the entire federation as in traditional FL, we efficiently calculate optimal weighted model combinations for each client, based on figuring out how much a client can benefit from another's model. We do not assume knowledge of any underlying data distributions or client similarities, and allow each client to optimize for arbitrary target distributions of interest, enabling greater flexibility for personalization. We evaluate and characterize our method on a variety of federated settings, datasets, and degrees of local data heterogeneity. Our method outperforms existing alternatives, while also enabling new features for personalized FL such as transfer outside of local data distributions.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a new federated learning framework that efficiently computes a personalized weighted combination of available models for each client, outperforming existing work for personalized federated learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="l0V53bErniB" data-number="1031">
      <h4>
        <a href="https://openreview.net/forum?id=l0V53bErniB">
            Combining Physics and Machine Learning for Network Flow Estimation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=l0V53bErniB" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Arlei_Lopes_da_Silva1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arlei_Lopes_da_Silva1">Arlei Lopes da Silva</a>, <a href="https://openreview.net/profile?email=furkan%40cs.ucsb.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="furkan@cs.ucsb.edu">Furkan Kocayusufoglu</a>, <a href="https://openreview.net/profile?email=saber%40ucsb.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="saber@ucsb.edu">Saber Jafarpour</a>, <a href="https://openreview.net/profile?email=bullo%40engineering.ucsb.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="bullo@engineering.ucsb.edu">Francesco Bullo</a>, <a href="https://openreview.net/profile?id=~Ananthram_Swami1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ananthram_Swami1">Ananthram Swami</a>, <a href="https://openreview.net/profile?id=~Ambuj_Singh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ambuj_Singh1">Ambuj Singh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#l0V53bErniB-details-874" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="l0V53bErniB-details-874"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">graphs, networks, bilevel optimization, metalearning, flow graphs</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The flow estimation problem consists of predicting missing edge flows in a network (e.g., traffic, power, and water) based on partial observations. These missing flows depend both on the underlying \textit{physics} (edge features and a flow conservation law) as well as the observed edge flows. This paper introduces an optimization framework for computing missing edge flows and solves the problem using bilevel optimization and deep learning. More specifically, we learn regularizers that depend on edge features (e.g., number of lanes in a road, the resistance of a power line) using neural networks. Empirical results show that our method accurately predicts missing flows, outperforming the best baseline, and is able to capture relevant physical properties in traffic and power networks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Paper solves missing flow estimation problem using bilevel optimization and deep learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=l0V53bErniB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="m4UCf24r0Y" data-number="2515">
      <h4>
        <a href="https://openreview.net/forum?id=m4UCf24r0Y">
            Knowledge Distillation as Semiparametric Inference
        </a>
      
        
          <a href="https://openreview.net/pdf?id=m4UCf24r0Y" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tri_Dao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tri_Dao1">Tri Dao</a>, <a href="https://openreview.net/profile?email=govinda.kamath%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="govinda.kamath@microsoft.com">Govinda M Kamath</a>, <a href="https://openreview.net/profile?id=~Vasilis_Syrgkanis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vasilis_Syrgkanis1">Vasilis Syrgkanis</a>, <a href="https://openreview.net/profile?id=~Lester_Mackey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lester_Mackey1">Lester Mackey</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Apr 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#m4UCf24r0Y-details-7" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="m4UCf24r0Y-details-7"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">knowledge distillation, semiparametric inference, generalization bounds, model compression, cross-fitting, orthogonal machine learning, loss correction</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A popular approach to model compression is to train an inexpensive student model to mimic the class probabilities of a highly accurate but cumbersome teacher model. Surprisingly, this two-step knowledge distillation process often leads to higher accuracy than training the student directly on labeled data. To explain and enhance this phenomenon, we cast knowledge distillation as a semiparametric inference problem with the optimal student model as the target, the unknown Bayes class probabilities as nuisance, and the teacher probabilities as a plug-in nuisance estimate.  By adapting modern semiparametric tools, we derive new guarantees for the prediction error of standard distillation and develop two enhancements—cross-fitting and loss correction—to mitigate the impact of teacher overfitting and underfitting on student performance. We validate our findings empirically on both tabular and image data and observe consistent improvements from our knowledge distillation enhancements.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">By framing distillation as semiparametric inference, we derive new guarantees for standard distillation and develop two enhancements—cross-fitting and loss correction—to mitigate the impact of teacher overfitting and underfitting.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=m4UCf24r0Y&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="NX1He-aFO_F" data-number="3292">
      <h4>
        <a href="https://openreview.net/forum?id=NX1He-aFO_F">
            Learning Value Functions in Deep Policy Gradients using Residual Variance
        </a>
      
        
          <a href="https://openreview.net/pdf?id=NX1He-aFO_F" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yannis_Flet-Berliac1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yannis_Flet-Berliac1">Yannis Flet-Berliac</a>, <a href="https://openreview.net/profile?id=~reda_ouhamma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~reda_ouhamma1">reda ouhamma</a>, <a href="https://openreview.net/profile?id=~odalric-ambrym_maillard1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~odalric-ambrym_maillard1">odalric-ambrym maillard</a>, <a href="https://openreview.net/profile?id=~Philippe_Preux1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philippe_Preux1">Philippe Preux</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>21 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#NX1He-aFO_F-details-671" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NX1He-aFO_F-details-671"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a method to improve the learning of the critic in the actor-critic framework.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="AY8zfZm0tDd" data-number="1662">
      <h4>
        <a href="https://openreview.net/forum?id=AY8zfZm0tDd">
            Randomized Ensembled Double Q-Learning: Learning Fast Without a Model
        </a>
      
        
          <a href="https://openreview.net/pdf?id=AY8zfZm0tDd" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xinyue_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinyue_Chen1">Xinyue Chen</a>, <a href="https://openreview.net/profile?id=~Che_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Che_Wang1">Che Wang</a>, <a href="https://openreview.net/profile?id=~Zijian_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zijian_Zhou1">Zijian Zhou</a>, <a href="https://openreview.net/profile?id=~Keith_W._Ross1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Keith_W._Ross1">Keith W. Ross</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#AY8zfZm0tDd-details-439" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="AY8zfZm0tDd-details-439"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Artificial Integlligence, Machine Learning, Deep Reinforcement Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="318" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c226B"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>≫</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container>; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="319" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c226B"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>≫</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container>. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose and analyze a novel model-free algorithm that achieves strong performance with a high update-to-data ratio.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=AY8zfZm0tDd&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="_kxlwvhOodK" data-number="2588">
      <h4>
        <a href="https://openreview.net/forum?id=_kxlwvhOodK">
             Decentralized Attribution of Generative Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=_kxlwvhOodK" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Changhoon_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changhoon_Kim1">Changhoon Kim</a>, <a href="https://openreview.net/profile?id=~Yi_Ren3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Ren3">Yi Ren</a>, <a href="https://openreview.net/profile?id=~Yezhou_Yang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yezhou_Yang1">Yezhou Yang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#_kxlwvhOodK-details-273" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_kxlwvhOodK-details-273"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">GANs, Generative Model, Deepfake, Model Attribution</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Growing applications of generative models have led to new threats such as malicious personation and digital copyright infringement. 
      One solution to these threats is model attribution, i.e., the identification of user-end models where the contents under question are generated.
      Existing studies showed empirical feasibility of attribution through a centralized classifier trained on all existing user-end models. 
      However, this approach is not scalable in a reality where the number of models ever grows. Neither does it provide an attributability guarantee.
      To this end, this paper studies decentralized attribution, which relies on binary classifiers associated with each user-end model. 
      Each binary classifier is parameterized by a user-specific key and distinguishes its associated model distribution from the authentic data distribution. 
      We develop sufficient conditions of the keys that guarantee an attributability lower bound.
      Our method is validated on MNIST, CelebA, and FFHQ datasets. We also examine the trade-off between generation quality and robustness of attribution against adversarial post-processes.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper investigates the feasibility of decentralized attribution of generative models.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=_kxlwvhOodK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="vujTf_I8Kmc" data-number="2058">
      <h4>
        <a href="https://openreview.net/forum?id=vujTf_I8Kmc">
            Attentional Constellation Nets for Few-Shot Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=vujTf_I8Kmc" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Weijian_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weijian_Xu1">Weijian Xu</a>, <a href="https://openreview.net/profile?id=~yifan_xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~yifan_xu1">yifan xu</a>, <a href="https://openreview.net/profile?id=~Huaijin_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huaijin_Wang1">Huaijin Wang</a>, <a href="https://openreview.net/profile?id=~Zhuowen_Tu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhuowen_Tu1">Zhuowen Tu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 04 Apr 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#vujTf_I8Kmc-details-422" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vujTf_I8Kmc-details-422"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">few-shot learning, constellation models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The success of deep convolutional neural networks builds on top of the learning of effective convolution operations, capturing a hierarchy of structured features via filtering, activation, and pooling. However, the explicit structured features, e.g. object parts, are not expressive in the existing CNN frameworks. In this paper, we tackle the few-shot learning problem and make an effort to enhance structured features by expanding CNNs with a constellation model, which performs cell feature clustering and encoding with a dense part representation; the relationships among the cell features are further modeled by an attention mechanism. With the additional constellation branch to increase the awareness of object parts, our method is able to attain the advantages of the CNNs while making the overall internal representations more robust in the few-shot learning setting. Our approach attains a significant improvement over the existing methods in few-shot learning on the CIFAR-FS, FC100, and mini-ImageNet benchmarks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We tackle the few-shot learning problem by introducing an explicit cell feature clustering procedure with relation learning via self-attention.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="dyjPVUc2KB" data-number="1570">
      <h4>
        <a href="https://openreview.net/forum?id=dyjPVUc2KB">
            Adapting to Reward Progressivity via Spectral Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=dyjPVUc2KB" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Michael_Dann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Dann1">Michael Dann</a>, <a href="https://openreview.net/profile?email=john.thangarajah%40rmit.edu.au" class="profile-link" data-toggle="tooltip" data-placement="top" title="john.thangarajah@rmit.edu.au">John Thangarajah</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#dyjPVUc2KB-details-158" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dyjPVUc2KB-details-158"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement Learning, Deep Reinforcement Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper we consider reinforcement learning tasks with progressive rewards; that is, tasks where the rewards tend to increase in magnitude over time. We hypothesise that this property may be problematic for value-based deep reinforcement learning agents, particularly if the agent must first succeed in relatively unrewarding regions of the task in order to reach more rewarding regions. To address this issue, we propose Spectral DQN, which decomposes the reward into frequencies such that the high frequencies only activate when large rewards are found. This allows the training loss to be balanced so that it gives more even weighting across small and large reward regions. In two domains with extreme reward progressivity, where standard value-based methods struggle significantly, Spectral DQN is able to make much farther progress. Moreover, when evaluated on a set of six standard Atari games that do not overtly favour the approach, Spectral DQN remains more than competitive: While it underperforms one of the benchmarks in a single game, it comfortably surpasses the benchmarks in three games. These results demonstrate that the approach is not overfit to its target problem, and suggest that Spectral DQN may have advantages beyond addressing reward progressivity.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">In this paper, we identify a problem with value-based deep reinforcement learning that has not previously been investigated -- namely, reward progressivity -- and propose an approach that addresses it via magnitudinal decomposition of the reward.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="IqtonxWI0V3" data-number="3365">
      <h4>
        <a href="https://openreview.net/forum?id=IqtonxWI0V3">
            TropEx: An Algorithm for Extracting Linear Terms in Deep Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=IqtonxWI0V3" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Martin_Trimmel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Trimmel1">Martin Trimmel</a>, <a href="https://openreview.net/profile?id=~Henning_Petzka1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Henning_Petzka1">Henning Petzka</a>, <a href="https://openreview.net/profile?id=~Cristian_Sminchisescu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cristian_Sminchisescu1">Cristian Sminchisescu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#IqtonxWI0V3-details-771" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="IqtonxWI0V3-details-771"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">linear regions, linear terms, deep learning theory, deep neural networks, rectified linear unit, relu network, piecewise linear function, tropical function</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep neural networks with rectified linear (ReLU) activations are piecewise linear functions, where hyperplanes partition the input space into an astronomically high number of linear regions. Previous work focused on counting linear regions to measure the network's expressive power and on analyzing geometric properties of the hyperplane configurations. In contrast, we aim to understand the impact of the linear terms on network performance, by examining the information encoded in their coefficients. To this end, we derive TropEx, a nontrivial tropical algebra-inspired algorithm to systematically extract linear terms based on data. Applied to convolutional and fully-connected networks, our algorithm uncovers significant differences in how the different networks utilize linear regions for generalization. This underlines the importance of systematic linear term exploration, to better understand generalization in neural networks trained with complex data sets.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an algorithm for extracting linear terms of piecewise linear deep neural network functions and apply it to study differences between convolutional and fully-connected networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=IqtonxWI0V3&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="HIGSa_3kOx3" data-number="2690">
      <h4>
        <a href="https://openreview.net/forum?id=HIGSa_3kOx3">
            Reset-Free Lifelong Learning with Skill-Space Planning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=HIGSa_3kOx3" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kevin_Lu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kevin_Lu2">Kevin Lu</a>, <a href="https://openreview.net/profile?id=~Aditya_Grover1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aditya_Grover1">Aditya Grover</a>, <a href="https://openreview.net/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="https://openreview.net/profile?id=~Igor_Mordatch4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Igor_Mordatch4">Igor Mordatch</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#HIGSa_3kOx3-details-533" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HIGSa_3kOx3-details-533"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reset-free, lifelong, reinforcement learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The objective of \textit{lifelong} reinforcement learning (RL) is to optimize agents which can continuously adapt and interact in changing environments. However, current RL approaches fail drastically when environments are non-stationary and interactions are non-episodic. We propose \textit{Lifelong Skill Planning} (LiSP), an algorithmic framework for lifelong RL based on planning in an abstract space of higher-order skills. We learn the skills in an unsupervised manner using intrinsic rewards and plan over the learned skills using a learned dynamics model. Moreover, our framework permits skill discovery even from offline data, thereby reducing the need for excessive real-world interactions. We demonstrate empirically that LiSP successfully enables long-horizon planning and learns agents that can avoid catastrophic failures even in challenging non-stationary and non-episodic environments derived from gridworld and MuJoCo benchmarks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=HIGSa_3kOx3&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="euDnVs0Ynts" data-number="3006">
      <h4>
        <a href="https://openreview.net/forum?id=euDnVs0Ynts">
            Robust Learning of Fixed-Structure Bayesian Networks in Nearly-Linear Time
        </a>
      
        
          <a href="https://openreview.net/pdf?id=euDnVs0Ynts" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yu_Cheng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Cheng2">Yu Cheng</a>, <a href="https://openreview.net/profile?id=~Honghao_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Honghao_Lin1">Honghao Lin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#euDnVs0Ynts-details-532" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="euDnVs0Ynts-details-532"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Bayesian networks, robust statistics, learning theory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the problem of learning Bayesian networks where an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="320" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi></math></mjx-assistive-mml></mjx-container>-fraction of the samples are adversarially corrupted.  We focus on the fully-observable case where the underlying graph structure is known.  In this work, we present the first nearly-linear time algorithm for this problem with a dimension-independent error guarantee.  Previous robust algorithms with comparable error guarantees are slower by at least a factor of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="321" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>d</mi><mrow><mo>/</mo></mrow><mi>ϵ</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="322" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></mjx-assistive-mml></mjx-container> is the number of variables in the Bayesian network and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="323" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D716 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi></math></mjx-assistive-mml></mjx-container> is the fraction of corrupted samples.
      
      Our algorithm and analysis are considerably simpler than those in previous work.  We achieve this by establishing a direct connection between robust learning of Bayesian networks and robust mean estimation.  As a subroutine in our algorithm, we develop a robust mean estimation algorithm whose runtime is nearly-linear in the number of nonzeros in the input samples, which may be of independent interest.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We give the first nearly-linear time algorithm for the robust learning of fixed-structure Bayesian networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=euDnVs0Ynts&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="dOcQK-f4byz" data-number="1994">
      <h4>
        <a href="https://openreview.net/forum?id=dOcQK-f4byz">
            Teaching Temporal Logics to Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=dOcQK-f4byz" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Christopher_Hahn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Hahn1">Christopher Hahn</a>, <a href="https://openreview.net/profile?id=~Frederik_Schmitt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frederik_Schmitt1">Frederik Schmitt</a>, <a href="https://openreview.net/profile?id=~Jens_U._Kreber1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jens_U._Kreber1">Jens U. Kreber</a>, <a href="https://openreview.net/profile?id=~Markus_Norman_Rabe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Markus_Norman_Rabe1">Markus Norman Rabe</a>, <a href="https://openreview.net/profile?email=finkbeiner%40cispa.saarland" class="profile-link" data-toggle="tooltip" data-placement="top" title="finkbeiner@cispa.saarland">Bernd Finkbeiner</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#dOcQK-f4byz-details-748" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dOcQK-f4byz-details-748"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Logic, Verification, Transformer</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study two fundamental questions in neuro-symbolic computing: can deep learning tackle challenging problems in logics end-to-end, and can neural networks learn the semantics of logics. In this work we focus on linear-time temporal logic (LTL), as it is widely used in verification. We train a Transformer on the problem to directly predict a solution, i.e. a trace, to a given LTL formula. The training data is generated with classical solvers, which, however, only provide one of many possible solutions to each formula. We demonstrate that it is sufficient to train on those particular solutions to formulas, and that Transformers can predict solutions even to formulas from benchmarks from the literature on which the classical solver timed out. Transformers also generalize to the semantics of the logics: while they often deviate from the solutions found by the classical solvers, they still predict correct solutions to most formulas.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We study two fundamental questions in neuro-symbolic computing: can deep learning tackle challenging problems in logics end-to-end, and can neural networks learn the semantics of logics.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=dOcQK-f4byz&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="5l9zj5G7vDY" data-number="3098">
      <h4>
        <a href="https://openreview.net/forum?id=5l9zj5G7vDY">
            Spatially Structured Recurrent Modules
        </a>
      
        
          <a href="https://openreview.net/pdf?id=5l9zj5G7vDY" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Nasim_Rahaman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nasim_Rahaman1">Nasim Rahaman</a>, <a href="https://openreview.net/profile?id=~Anirudh_Goyal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anirudh_Goyal1">Anirudh Goyal</a>, <a href="https://openreview.net/profile?id=~Muhammad_Waleed_Gondal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Muhammad_Waleed_Gondal1">Muhammad Waleed Gondal</a>, <a href="https://openreview.net/profile?id=~Manuel_Wuthrich1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Manuel_Wuthrich1">Manuel Wuthrich</a>, <a href="https://openreview.net/profile?id=~Stefan_Bauer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefan_Bauer1">Stefan Bauer</a>, <a href="https://openreview.net/profile?id=~Yash_Sharma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yash_Sharma1">Yash Sharma</a>, <a href="https://openreview.net/profile?id=~Yoshua_Bengio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoshua_Bengio1">Yoshua Bengio</a>, <a href="https://openreview.net/profile?id=~Bernhard_Sch%C3%B6lkopf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernhard_Schölkopf1">Bernhard Schölkopf</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#5l9zj5G7vDY-details-249" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5l9zj5G7vDY-details-249"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">spatio-temporal modelling, modular architectures, recurrent neural networks, partially observed environments</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Capturing the structure of a data-generating process by means of appropriate inductive biases can help in learning models that generalise well and are robust to changes in the input distribution. While methods that harness spatial and temporal structures find broad application, recent work has demonstrated the potential of models that leverage sparse and modular structure using an ensemble of sparingly interacting modules. In this work, we take a step towards dynamic models that are capable of simultaneously exploiting both modular and spatiotemporal structures. To this end, we model the dynamical system as a collection of autonomous but sparsely interacting sub-systems that interact according to a learned topology which is informed by the spatial structure of the underlying system. This gives rise to a class of models that are well suited for capturing the dynamics of systems that only offer local views into their state, along with corresponding spatial locations of those views. On the tasks of video prediction from cropped frames and multi-agent world modelling from partial observations in the challenging Starcraft2 domain, we find our models to be more robust to the number of available views and better capable of generalisation to novel tasks without additional training than strong baselines that perform equally well or better on the training distribution. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We model a dynamical system as a collection of recurrent modules that interact according to a spatially informed but learned topology. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=5l9zj5G7vDY&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="lgNx56yZh8a" data-number="2193">
      <h4>
        <a href="https://openreview.net/forum?id=lgNx56yZh8a">
            Bayesian Few-Shot Classification with One-vs-Each Pólya-Gamma Augmented Gaussian Processes
        </a>
      
        
          <a href="https://openreview.net/pdf?id=lgNx56yZh8a" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jake_Snell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jake_Snell1">Jake Snell</a>, <a href="https://openreview.net/profile?id=~Richard_Zemel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Richard_Zemel1">Richard Zemel</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#lgNx56yZh8a-details-593" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lgNx56yZh8a-details-593"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">few-shot learning, gaussian processes, bayesian deep learning, uncertainty estimation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Few-shot classification (FSC), the task of adapting a classifier to unseen classes given a small labeled dataset, is an important step on the path toward human-like machine learning. Bayesian methods are well-suited to tackling the fundamental issue of overfitting in the few-shot scenario because they allow practitioners to specify prior beliefs and update those beliefs in light of observed data. Contemporary approaches to Bayesian few-shot classification maintain a posterior distribution over model parameters, which is slow and requires storage that scales with model size. Instead, we propose a Gaussian process classifier based on a novel combination of Pólya-Gamma augmentation and the one-vs-each softmax approximation that allows us to efficiently marginalize over functions rather than model parameters. We demonstrate improved accuracy and uncertainty quantification on both standard few-shot classification benchmarks and few-shot domain transfer tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a Gaussian process approach to few-shot classification based on the one-vs-each softmax approximation and Pólya-Gamma augmentation, and demonstrate competitive few-shot accuracy and strong uncertainty quantification.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tV6oBfuyLTQ" data-number="3426">
      <h4>
        <a href="https://openreview.net/forum?id=tV6oBfuyLTQ">
            Parameter-Based Value Functions
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tV6oBfuyLTQ" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Francesco_Faccio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Francesco_Faccio1">Francesco Faccio</a>, <a href="https://openreview.net/profile?id=~Louis_Kirsch1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Louis_Kirsch1">Louis Kirsch</a>, <a href="https://openreview.net/profile?id=~J%C3%BCrgen_Schmidhuber1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jürgen_Schmidhuber1">Jürgen Schmidhuber</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>23 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tV6oBfuyLTQ-details-736" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tV6oBfuyLTQ-details-736"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement Learning, Off-Policy Reinforcement Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Traditional off-policy actor-critic Reinforcement Learning (RL) algorithms learn value functions of a single target policy. However, when value functions are updated to track the learned policy, they forget potentially useful information about old policies. We introduce a class of value functions called Parameter-Based Value Functions (PBVFs) whose inputs include the policy parameters. They can generalize across different policies. PBVFs can evaluate the performance of any policy given a state, a state-action pair, or a distribution over the RL agent's initial states. First we show how PBVFs yield novel off-policy policy gradient theorems. Then we derive off-policy actor-critic algorithms based on PBVFs trained by Monte Carlo or Temporal Difference methods. We show how learned PBVFs can zero-shot learn new policies that outperform any policy seen during training. Finally our algorithms are evaluated on a selection of discrete and continuous control tasks using shallow policies and deep neural networks. Their performance is comparable to state-of-the-art methods.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose value functions whose inputs include the policy parameters and which can generalize across different policies</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Ec85b0tUwbA" data-number="3138">
      <h4>
        <a href="https://openreview.net/forum?id=Ec85b0tUwbA">
            Hyperbolic Neural Networks++
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Ec85b0tUwbA" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ryohei_Shimizu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ryohei_Shimizu1">Ryohei Shimizu</a>, <a href="https://openreview.net/profile?id=~YUSUKE_Mukuta1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~YUSUKE_Mukuta1">YUSUKE Mukuta</a>, <a href="https://openreview.net/profile?id=~Tatsuya_Harada1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tatsuya_Harada1">Tatsuya Harada</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Ec85b0tUwbA-details-31" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ec85b0tUwbA-details-31"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Hyperbolic Geometry, Poincaré Ball Model, Parameter-Reduced MLR, Geodesic-Aware FC Layer, Convolutional Layer, Attention Mechanism</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Hyperbolic spaces, which have the capacity to embed tree structures without distortion owing to their exponential volume growth, have recently been applied to machine learning to better capture the hierarchical nature of data. In this study, we generalize the fundamental components of neural networks in a single hyperbolic geometry model, namely, the Poincaré ball model. This novel methodology constructs a multinomial logistic regression, fully-connected layers, convolutional layers, and attention mechanisms under a unified mathematical interpretation, without increasing the parameters. Experiments show the superior parameter efficiency of our methods compared to conventional hyperbolic components, and stability and outperformance over their Euclidean counterparts.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present novel methods for constructing hyperbolic neural network architectures in the Poincaré ball model, including a parameter-reduced MLR, geodesic-aware FC layers, convolutional layers, and attention mechanisms. </span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="JFKR3WqwyXR" data-number="3695">
      <h4>
        <a href="https://openreview.net/forum?id=JFKR3WqwyXR">
            Neural Jump Ordinary Differential Equations: Consistent Continuous-Time Prediction and Filtering
        </a>
      
        
          <a href="https://openreview.net/pdf?id=JFKR3WqwyXR" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Calypso_Herrera1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Calypso_Herrera1">Calypso Herrera</a>, <a href="https://openreview.net/profile?id=~Florian_Krach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Krach1">Florian Krach</a>, <a href="https://openreview.net/profile?email=jteichma%40math.ethz.ch" class="profile-link" data-toggle="tooltip" data-placement="top" title="jteichma@math.ethz.ch">Josef Teichmann</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#JFKR3WqwyXR-details-948" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JFKR3WqwyXR-details-948"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural ODE, conditional expectation, irregular-observed data modelling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Combinations of neural ODEs with recurrent neural networks (RNN), like GRU-ODE-Bayes or ODE-RNN are well suited to model irregularly observed time series. While those models outperform existing discrete-time approaches, no theoretical guarantees for their predictive capabilities are available. Assuming that the irregularly-sampled time series data originates from a continuous stochastic process, the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="324" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>L</mi><mn>2</mn></msup></math></mjx-assistive-mml></mjx-container>-optimal online prediction is the conditional expectation given the currently available information. We introduce the Neural Jump ODE (NJ-ODE) that provides a data-driven approach to learn, continuously in time, the conditional expectation of a stochastic process. Our approach models the conditional expectation between two observations with a neural ODE and jumps whenever a new observation is made. We define a novel training framework, which allows us to prove theoretical guarantees for the first time. In particular, we show that the output of our model converges to the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="325" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>L</mi><mn>2</mn></msup></math></mjx-assistive-mml></mjx-container>-optimal prediction. This can be interpreted as solution to a special filtering problem. We provide experiments showing that the theoretical results also hold empirically. Moreover, we experimentally show that our model outperforms the baselines in more complex learning tasks and give comparisons on real-world datasets.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Online prediction and filtering of irregularly-observed time series data using Neural Jump ODE with theoretical convergence guarantees. </span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="dx4b7lm8jMM" data-number="3363">
      <h4>
        <a href="https://openreview.net/forum?id=dx4b7lm8jMM">
            Seq2Tens: An Efficient Representation of Sequences by Low-Rank Tensor Projections
        </a>
      
        
          <a href="https://openreview.net/pdf?id=dx4b7lm8jMM" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Csaba_Toth2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Csaba_Toth2">Csaba Toth</a>, <a href="https://openreview.net/profile?id=~Patric_Bonnier1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Patric_Bonnier1">Patric Bonnier</a>, <a href="https://openreview.net/profile?id=~Harald_Oberhauser1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Harald_Oberhauser1">Harald Oberhauser</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#dx4b7lm8jMM-details-992" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dx4b7lm8jMM-details-992"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">time series, sequential data, representation learning, low-rank tensors, classification, generative modelling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Sequential data such as time series, video, or text can be challenging to analyse as the ordered structure gives rise to complex dependencies. At the heart of this is non-commutativity, in the sense that reordering the elements of a sequence can completely change its meaning. We use a classical mathematical object -- the free algebra -- to capture this non-commutativity. To address the innate computational complexity of this algebra, we use compositions of low-rank tensor projections. This yields modular and scalable building blocks that give state-of-the-art performance on standard benchmarks such as multivariate time series classification, mortality prediction and generative models for video.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">An Efficient Representation of Sequences by Low-Rank Tensor Projections</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=dx4b7lm8jMM&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="8cpHIfgY4Dj" data-number="405">
      <h4>
        <a href="https://openreview.net/forum?id=8cpHIfgY4Dj">
            FOCAL: Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=8cpHIfgY4Dj" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Lanqing_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lanqing_Li1">Lanqing Li</a>, <a href="https://openreview.net/profile?email=yangrui19%40mails.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="yangrui19@mails.tsinghua.edu.cn">Rui Yang</a>, <a href="https://openreview.net/profile?id=~Dijun_Luo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dijun_Luo1">Dijun Luo</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#8cpHIfgY4Dj-details-449" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8cpHIfgY4Dj-details-449"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">offline/batch reinforcement learning, meta-reinforcement learning, multi-task reinforcement learning, distance metric learning, contrastive learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the offline meta-reinforcement learning (OMRL) problem, a paradigm which enables reinforcement learning (RL) algorithms to quickly adapt to unseen tasks without any interactions with the environments, making RL truly practical in many real-world applications. This problem is still not fully understood, for which two major challenges need to be addressed. First, offline RL usually suffers from bootstrapping errors of out-of-distribution state-actions which leads to divergence of value functions. Second, meta-RL requires efficient and robust task inference learned jointly with control policy. In this work, we enforce behavior regularization on learned policy as a general approach to offline RL, combined with a deterministic context encoder for efficient task inference. We propose a novel negative-power distance metric on bounded context embedding space, whose gradients propagation is detached from the Bellman backup. We provide analysis and insight showing that some simple design choices can yield substantial improvements over recent approaches involving meta-RL and distance metric learning. To the best of our knowledge, our method is the first model-free and end-to-end OMRL algorithm, which is computationally efficient and demonstrated to outperform prior algorithms on several meta-RL benchmarks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A novel model-free, end-to-end fully-offline meta-RL algorithm designed to maximize practicality, performance and sample/computational efficiency.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="8Sqhl-nF50" data-number="1725">
      <h4>
        <a href="https://openreview.net/forum?id=8Sqhl-nF50">
            On the Curse of Memory in Recurrent Neural Networks: Approximation and Optimization Analysis
        </a>
      
        
          <a href="https://openreview.net/pdf?id=8Sqhl-nF50" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhong_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhong_Li2">Zhong Li</a>, <a href="https://openreview.net/profile?id=~Jiequn_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jiequn_Han1">Jiequn Han</a>, <a href="https://openreview.net/profile?id=~Weinan_E1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weinan_E1">Weinan E</a>, <a href="https://openreview.net/profile?id=~Qianxiao_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qianxiao_Li1">Qianxiao Li</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>19 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#8Sqhl-nF50-details-823" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8Sqhl-nF50-details-823"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">recurrent neural network, dynamical system, universal approximation, optimization, curse of memory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the approximation properties and optimization dynamics of recurrent neural networks (RNNs) when applied to learn input-output relationships in temporal data. We consider the simple but representative setting of using continuous-time linear RNNs to learn from data generated by linear relationships. Mathematically, the latter can be understood as a sequence of linear functionals.  We prove a universal approximation theorem of such linear functionals and characterize the approximation rate.  Moreover, we perform a fine-grained dynamical analysis of training linear RNNs by gradient methods.  A unifying theme uncovered is the non-trivial effect of memory, a notion that can be made precise in our framework, on both approximation and optimization: when there is long-term memory in the target, it takes a large number of neurons to approximate it. Moreover, the training process will suffer from slow downs.  In particular, both of these effects become exponentially more pronounced with increasing memory - a phenomenon we call the “curse of memory”.  These analyses represent a basic step towards a concrete mathematical understanding of new phenomenons that may arise in learning temporal relationships using recurrent architectures.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We study the approximation properties and optimization dynamics of RNNs in the linear setting, where we uncover precisely the adverse effect of memory on learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="PH5PH9ZO_4" data-number="1336">
      <h4>
        <a href="https://openreview.net/forum?id=PH5PH9ZO_4">
            Generating Adversarial Computer Programs using Optimized Obfuscations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=PH5PH9ZO_4" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Shashank_Srikant1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shashank_Srikant1">Shashank Srikant</a>, <a href="https://openreview.net/profile?id=~Sijia_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sijia_Liu1">Sijia Liu</a>, <a href="https://openreview.net/profile?email=tamaram%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="tamaram@mit.edu">Tamara Mitrovska</a>, <a href="https://openreview.net/profile?id=~Shiyu_Chang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shiyu_Chang2">Shiyu Chang</a>, <a href="https://openreview.net/profile?id=~Quanfu_Fan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Quanfu_Fan1">Quanfu Fan</a>, <a href="https://openreview.net/profile?id=~Gaoyuan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gaoyuan_Zhang1">Gaoyuan Zhang</a>, <a href="https://openreview.net/profile?id=~Una-May_O%26%23x27%3BReilly1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Una-May_O&#39;Reilly1">Una-May O'Reilly</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#PH5PH9ZO_4-details-108" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PH5PH9ZO_4-details-108"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Machine Learning (ML) for Programming Languages (PL)/Software Engineering (SE), Adversarial computer programs, Program obfuscation, Combinatorial optimization, Differentiable program generator, Models for code</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Machine learning (ML) models that learn and predict properties of computer programs are increasingly being adopted and deployed. 
      These models have demonstrated success in applications such as auto-completing code, summarizing large programs, and detecting bugs and malware in programs. 
      In this work, we investigate principled ways to adversarially perturb a computer program to fool such learned models, and thus determine their adversarial robustness. We use program obfuscations, which have conventionally been used to avoid attempts at reverse engineering programs, as adversarial perturbations. These perturbations modify programs in ways that do not alter their functionality but can be crafted to deceive an ML model when making a decision. We provide a general formulation for an adversarial program that allows applying multiple obfuscation transformations to a program in any language. We develop first-order optimization algorithms to  efficiently determine two key aspects -- which parts of the program to transform, and what transformations to use. We show that it is important to optimize both these aspects to generate the best adversarially perturbed program. Due to the discrete nature of this problem, we also propose using randomized smoothing to improve the attack loss landscape to ease optimization. 
      We evaluate our work on Python and Java programs on the problem of program summarization. 
      We show that our best attack proposal achieves a <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="326" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>52</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> improvement over a state-of-the-art attack generation approach for programs trained on a \textsc{seq2seq} model.
      We further show that our formulation is better at training models that are robust to adversarial attacks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A differentiable generator of adversarial computer programs which can deceive ML models trained on computer programs.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="umIdUL8rMH" data-number="1549">
      <h4>
        <a href="https://openreview.net/forum?id=umIdUL8rMH">
            BOIL: Towards Representation Change for Few-shot Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=umIdUL8rMH" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jaehoon_Oh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaehoon_Oh1">Jaehoon Oh</a>, <a href="https://openreview.net/profile?id=~Hyungjun_Yoo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hyungjun_Yoo1">Hyungjun Yoo</a>, <a href="https://openreview.net/profile?id=~ChangHwan_Kim2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~ChangHwan_Kim2">ChangHwan Kim</a>, <a href="https://openreview.net/profile?id=~Se-Young_Yun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Se-Young_Yun1">Se-Young Yun</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#umIdUL8rMH-details-203" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="umIdUL8rMH-details-203"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Model Agnostic Meta-Learning (MAML) is one of the most representative of gradient-based meta-learning algorithms. MAML learns new tasks with a few data samples using inner updates from a meta-initialization point and learns the meta-initialization parameters with outer updates. It has recently been hypothesized that representation reuse, which makes little change in efficient representations, is the dominant factor in the performance of the meta-initialized model through MAML in contrast to representation change, which causes a significant change in representations. In this study, we investigate the necessity of representation change for the ultimate goal of few-shot learning, which is solving domain-agnostic tasks. To this aim, we propose a novel meta-learning algorithm, called BOIL (Body Only update in Inner Loop), which updates only the body (extractor) of the model and freezes the head (classifier) during inner loop updates. BOIL leverages representation change rather than representation reuse. A frozen head cannot achieve better results than even a random guessing classifier at the initial point of new tasks, and feature vectors (representations) have to move quickly to their corresponding frozen head vectors. We visualize this property using cosine similarity, CKA, and empirical results without the head. Although the inner loop updates purely hinge on representation change, BOIL empirically shows significant performance improvement over MAML, particularly on cross-domain tasks. The results imply that representation change in gradient-based meta-learning approaches is a critical component.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a novel meta-learning algorithm, BOIL, based on representation change.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=umIdUL8rMH&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Jacdvfjicf7" data-number="627">
      <h4>
        <a href="https://openreview.net/forum?id=Jacdvfjicf7">
            Interpreting and Boosting Dropout from a Game-Theoretic View
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Jacdvfjicf7" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hao_Zhang22" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Zhang22">Hao Zhang</a>, <a href="https://openreview.net/profile?id=~Sen_Li2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sen_Li2">Sen Li</a>, <a href="https://openreview.net/profile?id=~YinChao_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~YinChao_Ma1">YinChao Ma</a>, <a href="https://openreview.net/profile?id=~Mingjie_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingjie_Li3">Mingjie Li</a>, <a href="https://openreview.net/profile?id=~Yichen_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yichen_Xie1">Yichen Xie</a>, <a href="https://openreview.net/profile?id=~Quanshi_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Quanshi_Zhang1">Quanshi Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Jacdvfjicf7-details-25" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Jacdvfjicf7-details-25"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Dropout, Interpretability, Interactions</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper aims to understand and improve the utility of the dropout operation from the perspective of game-theoretical interactions. We prove that dropout can suppress the strength of interactions between input variables of deep neural networks (DNNs). The theoretical proof is also verified by various experiments. Furthermore, we find that such interactions were strongly related to the over-fitting problem in deep learning. So, the utility of dropout can be regarded as decreasing interactions to alleviating the significance of over-fitting. Based on this understanding, we propose the interaction loss to further improve the utility of dropout. Experimental results on various DNNs and datasets have shown that the interaction loss can effectively improve the utility of dropout and boost the performance of DNNs.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We prove and improve the utility of the dropout operation from a game-theoretic view.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Jacdvfjicf7&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9p2ekP904Rs" data-number="3652">
      <h4>
        <a href="https://openreview.net/forum?id=9p2ekP904Rs">
            Representation Learning via Invariant Causal Mechanisms
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9p2ekP904Rs" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jovana_Mitrovic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jovana_Mitrovic1">Jovana Mitrovic</a>, <a href="https://openreview.net/profile?id=~Brian_McWilliams2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brian_McWilliams2">Brian McWilliams</a>, <a href="https://openreview.net/profile?id=~Jacob_C_Walker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jacob_C_Walker1">Jacob C Walker</a>, <a href="https://openreview.net/profile?id=~Lars_Holger_Buesing1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lars_Holger_Buesing1">Lars Holger Buesing</a>, <a href="https://openreview.net/profile?id=~Charles_Blundell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Charles_Blundell1">Charles Blundell</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 22 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9p2ekP904Rs-details-489" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9p2ekP904Rs-details-489"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Representation Learning, Self-supervised Learning, Contrastive Methods, Causality</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signal by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework.  We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method,  and provide an alternative theoretical explanation for the  success  of  these  methods. Empirically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on 51 out of 57 games.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a new self-supervised objective with an explicit invariance regularizer and provide an alternative explanation for the success of contrastive learning using causality; we outperform competing methods on ImageNet and Atari. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=9p2ekP904Rs&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="4IwieFS44l" data-number="1961">
      <h4>
        <a href="https://openreview.net/forum?id=4IwieFS44l">
            Fooling a Complete Neural Network Verifier
        </a>
      
        
          <a href="https://openreview.net/pdf?id=4IwieFS44l" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=zomborid%40inf.u-szeged.hu" class="profile-link" data-toggle="tooltip" data-placement="top" title="zomborid@inf.u-szeged.hu">Dániel Zombori</a>, <a href="https://openreview.net/profile?email=banhelyi%40inf.u-szeged.hu" class="profile-link" data-toggle="tooltip" data-placement="top" title="banhelyi@inf.u-szeged.hu">Balázs Bánhelyi</a>, <a href="https://openreview.net/profile?email=csendes%40inf.u-szeged.hu" class="profile-link" data-toggle="tooltip" data-placement="top" title="csendes@inf.u-szeged.hu">Tibor Csendes</a>, <a href="https://openreview.net/profile?email=imegyeri%40inf.u-szeged.hu" class="profile-link" data-toggle="tooltip" data-placement="top" title="imegyeri@inf.u-szeged.hu">István Megyeri</a>, <a href="https://openreview.net/profile?id=~M%C3%A1rk_Jelasity1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Márk_Jelasity1">Márk Jelasity</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 02 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#4IwieFS44l-details-774" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="4IwieFS44l-details-774"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">adversarial examples, complete verifiers, numerical errors</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The efficient and accurate characterization of the robustness of neural networks to input perturbation is an important open problem. Many approaches exist including heuristic and exact (or complete) methods. Complete methods are expensive but their mathematical formulation guarantees that they provide exact robustness metrics. However, this guarantee is valid only if we assume that the verified network applies arbitrary-precision arithmetic and the verifier is reliable. In practice, however, both the networks and the verifiers apply limited-precision floating point arithmetic. In this paper, we show that numerical roundoff errors can be exploited to craft adversarial networks, in which the actual robustness and the robustness computed by a state-of-the-art complete verifier radically differ. We also show that such adversarial networks can be used to insert a backdoor into any network in such a way that the backdoor is completely missed by the verifier. The attack is easy to detect in its naive form but, as we show, the adversarial network can be transformed to make its detection less trivial. We offer a simple defense against our particular attack based on adding a very small perturbation to the network weights. However, our conjecture is that other numerical attacks are possible, and exact verification has to take into account all the details of the computation executed by the verified networks, which makes the problem significantly harder.
      
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an attack (along with a defense)  to fool complete verification based on exploiting numerical errors.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="F2v4aqEL6ze" data-number="463">
      <h4>
        <a href="https://openreview.net/forum?id=F2v4aqEL6ze">
            CPR: Classifier-Projection Regularization for Continual Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=F2v4aqEL6ze" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sungmin_Cha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sungmin_Cha1">Sungmin Cha</a>, <a href="https://openreview.net/profile?id=~Hsiang_Hsu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hsiang_Hsu1">Hsiang Hsu</a>, <a href="https://openreview.net/profile?email=gxq9106%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="gxq9106@gmail.com">Taebaek Hwang</a>, <a href="https://openreview.net/profile?id=~Flavio_Calmon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Flavio_Calmon1">Flavio Calmon</a>, <a href="https://openreview.net/profile?id=~Taesup_Moon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Taesup_Moon1">Taesup Moon</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 21 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#F2v4aqEL6ze-details-241" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="F2v4aqEL6ze-details-241"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">continual learning, regularization, wide local minima</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a general, yet simple patch that can be applied to existing regularization-based continual learning methods called classifier-projection regularization (CPR). Inspired by both recent results on neural networks with wide local minima and information theory, CPR adds an additional regularization term that maximizes the entropy of a classifier's output probability. We demonstrate that this additional term can be interpreted as a projection of the conditional probability given by a classifier's output to the uniform distribution. By applying the Pythagorean theorem for KL divergence, we then prove that this projection may (in theory) improve the performance of continual learning methods. In our extensive experimental results, we apply CPR to several state-of-the-art regularization-based continual learning methods and benchmark performance on popular image recognition datasets. Our results demonstrate that CPR indeed promotes a wide local minima and significantly improves both accuracy and plasticity while simultaneously mitigating the catastrophic forgetting of baseline continual learning methods. The codes and scripts for this work are available at https://github.com/csm9493/CPR_CL.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We devise wide local minima promoting regularization term for continual learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=F2v4aqEL6ze&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="U4XLJhqwNF1" data-number="1020">
      <h4>
        <a href="https://openreview.net/forum?id=U4XLJhqwNF1">
            CO2: Consistent Contrast for Unsupervised Visual Representation Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=U4XLJhqwNF1" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Chen_Wei2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chen_Wei2">Chen Wei</a>, <a href="https://openreview.net/profile?id=~Huiyu_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huiyu_Wang1">Huiyu Wang</a>, <a href="https://openreview.net/profile?id=~Wei_Shen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Shen2">Wei Shen</a>, <a href="https://openreview.net/profile?id=~Alan_Yuille1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alan_Yuille1">Alan Yuille</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#U4XLJhqwNF1-details-571" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="U4XLJhqwNF1-details-571"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">unsupervised representation learning, contrastive learning, consistency regularization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Contrastive learning has recently been a core for unsupervised visual representation learning. Without human annotation, the common practice is to perform an instance discrimination task: Given a query image crop, label crops from the same image as positives, and crops from other randomly sampled images as negatives. An important limitation of this label assignment is that it can not reflect the heterogeneous similarity of the query crop to crops from other images, but regarding them as equally negative. To address this issue, inspired by consistency regularization in semi-supervised learning, we propose Consistent Contrast (CO2), which introduces a consistency term into unsupervised contrastive learning framework. The consistency term takes the similarity of the query crop to crops from other images as unlabeled, and the corresponding similarity of a positive crop as a pseudo label. It then encourages consistency between these two similarities. Empirically, CO2 improves Momentum Contrast (MoCo) by 2.9% top-1 accuracy on ImageNet linear protocol, 3.8% and 1.1% top-5 accuracy on 1% and 10% labeled semi-supervised settings. It also transfers to image classification, object detection, and semantic segmentation on PASCAL VOC. This shows that CO2 learns better visual representations for downstream tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Introduce a consistency regularization term into unsupervised contrastive learning framework.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="SHvF5xaueVn" data-number="464">
      <h4>
        <a href="https://openreview.net/forum?id=SHvF5xaueVn">
            GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images
        </a>
      
        
          <a href="https://openreview.net/pdf?id=SHvF5xaueVn" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sungmin_Cha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sungmin_Cha1">Sungmin Cha</a>, <a href="https://openreview.net/profile?email=pte1236%40skku.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="pte1236@skku.edu">Taeeon Park</a>, <a href="https://openreview.net/profile?email=bjkim2006%40naver.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="bjkim2006@naver.com">Byeongjoon Kim</a>, <a href="https://openreview.net/profile?email=jongdukbaek%40yonsei.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="jongdukbaek@yonsei.ac.kr">Jongduk Baek</a>, <a href="https://openreview.net/profile?id=~Taesup_Moon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Taesup_Moon1">Taesup Moon</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 21 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#SHvF5xaueVn-details-730" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SHvF5xaueVn-details-730"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">blind denoising, unsupervised learning, iterative training, generative learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We tackle a challenging blind image denoising problem, in which only single distinct noisy images are available for training a denoiser, and no information about noise is known, except for it being zero-mean, additive, and independent of the clean image. In such a setting, which often occurs in practice, it is not possible to train a denoiser with the standard discriminative training or with the recently developed Noise2Noise (N2N) training; the former requires the underlying clean image for the given noisy image, and the latter requires two independently realized noisy image pair for a clean image. To that end, we propose GAN2GAN (Generated-Artificial-Noise to Generated-Artificial-Noise) method that first learns a generative model that can 1) simulate the noise in the given noisy images and 2) generate a rough, noisy estimates of the clean images, then 3) iteratively trains a denoiser with subsequently synthesized noisy image pairs (as in N2N), obtained from the generative model. In results, we show the denoiser trained with our GAN2GAN achieves an impressive denoising performance on both synthetic and real-world datasets for the blind denoising setting; it almost approaches the performance of the standard discriminatively-trained or N2N-trained models that have more information than ours, and it significantly outperforms the recent baseline for the same setting, \textit{e.g.}, Noise2Void, and a more conventional yet strong one, BM3D. The official code of our method is available at https://github.com/csm9493/GAN2GAN.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We devise GAN2GAN method that trains a blind denoiser solely based on the single noisy images. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=SHvF5xaueVn&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="wxRwhSdORKG" data-number="275">
      <h4>
        <a href="https://openreview.net/forum?id=wxRwhSdORKG">
            Learning Subgoal Representations with Slow Dynamics
        </a>
      
        
          <a href="https://openreview.net/pdf?id=wxRwhSdORKG" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Siyuan_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siyuan_Li1">Siyuan Li</a>, <a href="https://openreview.net/profile?email=zll19%40mails.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="zll19@mails.tsinghua.edu.cn">Lulu Zheng</a>, <a href="https://openreview.net/profile?email=wjh19%40mails.tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="wjh19@mails.tsinghua.edu.cn">Jianhao Wang</a>, <a href="https://openreview.net/profile?id=~Chongjie_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongjie_Zhang1">Chongjie Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#wxRwhSdORKG-details-283" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wxRwhSdORKG-details-283"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Hierarchical Reinforcement Learning, Representation Learning, Exploration</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In goal-conditioned Hierarchical Reinforcement Learning (HRL), a high-level policy periodically sets subgoals for a low-level policy, and the low-level policy is trained to reach those subgoals. A proper subgoal representation function, which abstracts a state space to a latent subgoal space, is crucial for effective goal-conditioned HRL, since different low-level behaviors are induced by reaching subgoals in the compressed representation space. Observing that the high-level agent operates at an abstract temporal scale, we propose a slowness objective to effectively learn the subgoal representation (i.e., the high-level action space). We provide a theoretical grounding for the slowness objective. That is, selecting slow features as the subgoal space can achieve efficient hierarchical exploration. As a result of better exploration ability, our approach significantly outperforms state-of-the-art HRL and exploration methods on a number of benchmark continuous-control tasks. Thanks to the generality of the proposed subgoal representation learning method, empirical results also demonstrate that the learned representation and corresponding low-level policies can be transferred between distinct tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a slowness objective to learn subgoal representations in hierarchical reinforcement learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=wxRwhSdORKG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ESG-DMKQKsD" data-number="906">
      <h4>
        <a href="https://openreview.net/forum?id=ESG-DMKQKsD">
            Bowtie Networks: Generative Modeling for Joint Few-Shot Recognition and Novel-View Synthesis
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ESG-DMKQKsD" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhipeng_Bao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhipeng_Bao1">Zhipeng Bao</a>, <a href="https://openreview.net/profile?id=~Yu-Xiong_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu-Xiong_Wang1">Yu-Xiong Wang</a>, <a href="https://openreview.net/profile?id=~Martial_Hebert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martial_Hebert1">Martial Hebert</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 22 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ESG-DMKQKsD-details-495" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ESG-DMKQKsD-details-495"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">computer vision, object recognition, few-shot learning, generative models, adversarial training</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a novel task of joint few-shot recognition and novel-view synthesis: given only one or few images of a novel object from arbitrary views with only category annotation, we aim to simultaneously learn an object classifier and generate images of that type of object from new viewpoints. While existing work copes with two or more tasks mainly by multi-task learning of shareable feature representations, we take a different perspective. We focus on the interaction and cooperation between a generative model and a discriminative model, in a way that facilitates knowledge to flow across tasks in complementary directions. To this end, we propose bowtie networks that jointly learn 3D geometric and semantic representations with a feedback loop. Experimental evaluation on challenging fine-grained recognition datasets demonstrates that our synthesized images are realistic from multiple viewpoints and significantly improve recognition performance as ways of data augmentation, especially in the low-data regime. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a novel feedback-based bowtie network to learn a shared generative model for joint few-shot recognition and novel-view synthesis, consistently and significantly improving performance for both tasks, especially in the low-data regime.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ZW0yXJyNmoG" data-number="1684">
      <h4>
        <a href="https://openreview.net/forum?id=ZW0yXJyNmoG">
            Taming GANs with Lookahead-Minmax
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ZW0yXJyNmoG" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tatjana_Chavdarova2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tatjana_Chavdarova2">Tatjana Chavdarova</a>, <a href="https://openreview.net/profile?id=~Matteo_Pagliardini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matteo_Pagliardini1">Matteo Pagliardini</a>, <a href="https://openreview.net/profile?id=~Sebastian_U_Stich1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sebastian_U_Stich1">Sebastian U Stich</a>, <a href="https://openreview.net/profile?id=~Fran%C3%A7ois_Fleuret2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~François_Fleuret2">François Fleuret</a>, <a href="https://openreview.net/profile?id=~Martin_Jaggi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Jaggi1">Martin Jaggi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 14 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ZW0yXJyNmoG-details-723" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZW0yXJyNmoG-details-723"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Minmax, Generative Adversarial Networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Generative Adversarial Networks are notoriously challenging to train. The underlying minmax optimization is highly susceptible to the variance of the stochastic gradient and the rotational component of the associated game vector field. To tackle these challenges, we propose the Lookahead algorithm for minmax optimization, originally developed for single objective minimization only. The backtracking step of our Lookahead–minmax naturally handles the rotational game dynamics, a property which was identified to be key for enabling gradient ascent descent methods to converge on challenging examples often analyzed in the literature. Moreover, it implicitly handles high variance without using large mini-batches, known to be essential for reaching state of the art performance. Experimental results on MNIST, SVHN, CIFAR-10, and ImageNet demonstrate a clear advantage of combining Lookahead–minmax with Adam or extragradient, in terms of performance and improved stability, for negligible memory and computational cost. Using 30-fold fewer parameters and 16-fold smaller minibatches we outperform the reported performance of the class-dependent BigGAN on CIFAR-10 by obtaining FID of 12.19 without using the class labels, bringing state-of-the-art GAN training within reach of common computational resources.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A novel optimizer for GANs and games in general. </span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=ZW0yXJyNmoG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="USCNapootw" data-number="3751">
      <h4>
        <a href="https://openreview.net/forum?id=USCNapootw">
            Certify or Predict: Boosting Certified Robustness with Compositional Architectures
        </a>
      
        
          <a href="https://openreview.net/pdf?id=USCNapootw" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mark_Niklas_Mueller2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mark_Niklas_Mueller2">Mark Niklas Mueller</a>, <a href="https://openreview.net/profile?id=~Mislav_Balunovic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mislav_Balunovic1">Mislav Balunovic</a>, <a href="https://openreview.net/profile?id=~Martin_Vechev1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Vechev1">Martin Vechev</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>23 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#USCNapootw-details-466" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="USCNapootw-details-466"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Provable Robustness, Network Architecture, Robustness, Adversarial Accuracy, Certified Robustness</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A core challenge with existing certified defense mechanisms is that while they improve certified robustness, they also tend to drastically decrease natural accuracy, making it difficult to use these methods in practice. In this work, we propose a new architecture which addresses this challenge and enables one to boost the certified robustness of any state-of-the-art deep network, while controlling the overall accuracy loss, without requiring retraining. The key idea is to combine this model with a (smaller) certified network where at inference time, an adaptive selection mechanism decides on the network to process the input sample. The approach is compositional: one can combine any pair of state-of-the-art (e.g., EfficientNet or ResNet) and certified networks, without restriction. The resulting architecture enables much higher natural accuracy than previously possible with certified defenses alone, while substantially boosting the certified robustness of deep networks. We demonstrate the effectiveness of this adaptive approach on a variety of datasets and architectures.  For instance, on CIFAR-10 with an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="327" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-n" size="s"><mjx-c class="mjx-c221E"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi mathvariant="normal">∞</mi></msub></math></mjx-assistive-mml></mjx-container> perturbation of 2/255, we are the first to obtain a high natural accuracy (90.1%) with non-trivial certified robustness (27.5%). Notably, prior state-of-the-art methods incur a substantial drop in accuracy for a similar certified robustness.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a compositional network architecture boosting the certified robustness of an accurate state-of-the-art network, by combining it with a shallow, provable network using a certified, adaptive selection mechanism.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="t86MwoUCCNe" data-number="3418">
      <h4>
        <a href="https://openreview.net/forum?id=t86MwoUCCNe">
            New Bounds For Distributed Mean Estimation and Variance Reduction
        </a>
      
        
          <a href="https://openreview.net/pdf?id=t86MwoUCCNe" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=peter.davies%40ist.ac.at" class="profile-link" data-toggle="tooltip" data-placement="top" title="peter.davies@ist.ac.at">Peter Davies</a>, <a href="https://openreview.net/profile?email=krishnavijay1999%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="krishnavijay1999@gmail.com">Vijaykrishna Gurunanthan</a>, <a href="https://openreview.net/profile?email=n76.moshrefi%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="n76.moshrefi@gmail.com">Niusha Moshrefi</a>, <a href="https://openreview.net/profile?email=saleh.ashkboos%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="saleh.ashkboos@gmail.com">Saleh Ashkboos</a>, <a href="https://openreview.net/profile?id=~Dan_Alistarh7" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Alistarh7">Dan Alistarh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#t86MwoUCCNe-details-796" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="t86MwoUCCNe-details-796"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">distributed machine learning, mean estimation, variance reduction, lattices</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value "> We consider the problem of distributed mean estimation (DME), in which <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="328" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container> machines are each given a local <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="329" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></mjx-assistive-mml></mjx-container>-dimensional vector <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="330" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b" noic="true"><mjx-c class="mjx-c1D431 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.41em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow><mi mathvariant="bold">x</mi></mrow><mi>v</mi></msub><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>d</mi></msup></math></mjx-assistive-mml></mjx-container>, and must cooperate to estimate the mean of their inputs <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="331" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D707 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mfrac space="4"><mjx-frac><mjx-num><mjx-nstrut></mjx-nstrut><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-num><mjx-dbox><mjx-dtable><mjx-line></mjx-line><mjx-row><mjx-den><mjx-dstrut></mjx-dstrut><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-munderover space="2" limits="false"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2211 TEX-S1"></mjx-c></mjx-mo><mjx-script style="vertical-align: -0.285em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-spacer style="margin-top: 0.284em;"></mjx-spacer><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-munderover><mjx-msub space="2"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b" noic="true"><mjx-c class="mjx-c1D431 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>μ</mi></mrow><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo data-mjx-texclass="OP">∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mrow><mi mathvariant="bold">x</mi></mrow><mi>v</mi></msub></math></mjx-assistive-mml></mjx-container>, while minimizing total communication cost. DME is a fundamental construct in distributed machine learning, and there has been considerable work on variants of this problem, especially in the context of distributed variance reduction for stochastic gradients in parallel SGD. Previous work typically assumes an upper bound on the norm of the input vectors, and achieves an error bound in terms of this norm. However, in many real applications, the input vectors are concentrated around the correct output <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="332" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D707 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>μ</mi></mrow></math></mjx-assistive-mml></mjx-container>, but <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="333" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D707 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>μ</mi></mrow></math></mjx-assistive-mml></mjx-container> itself has large norm. In such cases, previous output error bounds perform poorly. 
                  In this paper, we show that output error bounds need not depend on input norm. We provide a method of quantization which allows distributed mean estimation to be performed with solution quality dependent only on the distance between inputs, not on input norm, and show an analogous result for distributed variance reduction. The technique is based on a new connection with lattice theory. We also provide lower bounds showing that the communication to error trade-off of our algorithms is asymptotically optimal. As the lattices achieving optimal bounds under <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="334" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container>-norm can be computationally impractical, we also present an extension which leverages  easy-to-use cubic lattices, and is loose only up to a logarithmic factor in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="335" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>d</mi></math></mjx-assistive-mml></mjx-container>. We show experimentally that our method yields practical improvements for common applications, relative to prior approaches.  </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We provide optimal algorithms and lower bounds for distributed mean estimation and variance reduction via a new connection to lattice theory, and show that this technique can be used to improve upon current approaches in practice.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=t86MwoUCCNe&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="j9Rv7qdXjd" data-number="1775">
      <h4>
        <a href="https://openreview.net/forum?id=j9Rv7qdXjd">
            Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels
        </a>
      
        
          <a href="https://openreview.net/pdf?id=j9Rv7qdXjd" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Binxin_Ru1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Binxin_Ru1">Binxin Ru</a>, <a href="https://openreview.net/profile?id=~Xingchen_Wan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xingchen_Wan1">Xingchen Wan</a>, <a href="https://openreview.net/profile?id=~Xiaowen_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaowen_Dong1">Xiaowen Dong</a>, <a href="https://openreview.net/profile?id=~Michael_Osborne1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Osborne1">Michael Osborne</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>22 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#j9Rv7qdXjd-details-489" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="j9Rv7qdXjd-details-489"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Current neural architecture search (NAS) strategies focus only on finding a single, good, architecture. They offer little insight into why a specific network is performing well, or how we should modify the architecture if we want further improvements. We propose a Bayesian optimisation (BO) approach for NAS that combines the Weisfeiler-Lehman graph kernel with a Gaussian process surrogate. Our method not only optimises the architecture in a highly data-efficient manner, but also affords interpretability by discovering useful network features and their corresponding impact on the network performance. Moreover, our method is capable of capturing the topological structures of the architectures and is scalable to large graphs, thus making the high-dimensional and graph-like search spaces amenable to BO. We demonstrate empirically that our surrogate model is capable of identifying useful motifs which can guide the generation of new architectures. We finally show that our method outperforms existing NAS approaches to achieve the state of the art on both closed- and open-domain search spaces.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a NAS method that is sample-efficient, highly performant and interpretable.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=j9Rv7qdXjd&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="-_Zp7r2-cGK" data-number="595">
      <h4>
        <a href="https://openreview.net/forum?id=-_Zp7r2-cGK">
            A Discriminative Gaussian Mixture Model with Sparsity
        </a>
      
        
          <a href="https://openreview.net/pdf?id=-_Zp7r2-cGK" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hideaki_Hayashi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hideaki_Hayashi1">Hideaki Hayashi</a>, <a href="https://openreview.net/profile?id=~Seiichi_Uchida1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seiichi_Uchida1">Seiichi Uchida</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>19 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#-_Zp7r2-cGK-details-380" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="-_Zp7r2-cGK-details-380"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">classification, sparse Bayesian learning, Gaussian mixture model</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In probabilistic classification, a discriminative model based on the softmax function has a potential limitation in that it assumes unimodality for each class in the feature space. The mixture model can address this issue, although it leads to an increase in the number of parameters. We propose a sparse classifier based on a discriminative GMM, referred to as a sparse discriminative Gaussian mixture (SDGM). In the SDGM, a GMM-based discriminative model is trained via sparse Bayesian learning. Using this sparse learning framework, we can simultaneously remove redundant Gaussian components and reduce the number of parameters used in the remaining components during learning; this learning method reduces the model complexity, thereby improving the generalization capability. Furthermore, the SDGM can be embedded into neural networks (NNs), such as convolutional NNs, and can be trained in an end-to-end manner. Experimental results demonstrated that the proposed method outperformed the existing softmax-based discriminative models.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A sparse classifier based on a discriminative Gaussian mixture model, which can also be embedded into a neural network.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=-_Zp7r2-cGK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="qpsl2dR9twy" data-number="1946">
      <h4>
        <a href="https://openreview.net/forum?id=qpsl2dR9twy">
            Communication in Multi-Agent Reinforcement Learning: Intention Sharing
        </a>
      
        
          <a href="https://openreview.net/pdf?id=qpsl2dR9twy" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Woojun_Kim1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Woojun_Kim1">Woojun Kim</a>, <a href="https://openreview.net/profile?id=~Jongeui_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jongeui_Park1">Jongeui Park</a>, <a href="https://openreview.net/profile?id=~Youngchul_Sung1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Youngchul_Sung1">Youngchul Sung</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#qpsl2dR9twy-details-845" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qpsl2dR9twy-details-845"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multi-agent reinforcement learning, communication, intention, attention</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Communication is one of the core components for learning coordinated behavior in multi-agent systems.
      In this paper, we propose a new communication scheme named  Intention Sharing (IS) for multi-agent reinforcement learning in order to enhance the coordination among agents. In the proposed IS scheme, each agent generates an imagined trajectory by modeling the environment dynamics and other agents' actions. The imagined trajectory is the simulated future trajectory of each agent based on the learned model of the environment dynamics and other agents and represents each agent's future action plan. Each agent compresses this imagined trajectory capturing its future action plan to generate its intention message for communication by applying an attention mechanism to learn the relative importance of the components in the imagined trajectory based on the received message from other agents. Numeral results show that the proposed IS scheme outperforms other communication schemes in multi-agent reinforcement learning.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper propose a new communication scheme named intention sharing to enhance the coordination among agents.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="1FvkSpWosOl" data-number="3278">
      <h4>
        <a href="https://openreview.net/forum?id=1FvkSpWosOl">
            Is Attention Better Than Matrix Decomposition?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=1FvkSpWosOl" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhengyang_Geng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhengyang_Geng1">Zhengyang Geng</a>, <a href="https://openreview.net/profile?id=~Meng-Hao_Guo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Meng-Hao_Guo1">Meng-Hao Guo</a>, <a href="https://openreview.net/profile?id=~Hongxu_Chen2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hongxu_Chen2">Hongxu Chen</a>, <a href="https://openreview.net/profile?id=~Xia_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xia_Li3">Xia Li</a>, <a href="https://openreview.net/profile?id=~Ke_Wei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ke_Wei1">Ke Wei</a>, <a href="https://openreview.net/profile?id=~Zhouchen_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhouchen_Lin1">Zhouchen Lin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#1FvkSpWosOl-details-934" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1FvkSpWosOl-details-934"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">attention models, matrix decomposition, computer vision</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">As an essential ingredient of modern deep learning, attention mechanism, especially self-attention, plays a vital role in the global correlation discovery. However, is hand-crafted attention irreplaceable when modeling the global context? Our intriguing finding is that self-attention is not better than the matrix decomposition~(MD) model developed 20 years ago regarding the performance and computational cost for encoding the long-distance dependencies. We model the global context issue as a low-rank completion problem and show that its optimization algorithms can help design global information blocks. This paper then proposes a series of Hamburgers, in which we employ the optimization algorithms for solving MDs to factorize the input representations into sub-matrices and reconstruct a low-rank embedding. Hamburgers with different MDs can perform favorably against the popular global context module self-attention when carefully coping with gradients back-propagated through MDs. Comprehensive experiments are conducted in the vision tasks where it is crucial to learn the global context, including semantic segmentation and image generation, demonstrating significant improvements over self-attention and its variants. Code is available at https://github.com/Gsunshine/Enjoy-Hamburger.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="nVZtXBI6LNn" data-number="3717">
      <h4>
        <a href="https://openreview.net/forum?id=nVZtXBI6LNn">
            Fast and Complete: Enabling Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers
        </a>
      
        
          <a href="https://openreview.net/pdf?id=nVZtXBI6LNn" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kaidi_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kaidi_Xu1">Kaidi Xu</a>, <a href="https://openreview.net/profile?id=~Huan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huan_Zhang1">Huan Zhang</a>, <a href="https://openreview.net/profile?id=~Shiqi_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shiqi_Wang2">Shiqi Wang</a>, <a href="https://openreview.net/profile?id=~Yihan_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yihan_Wang2">Yihan Wang</a>, <a href="https://openreview.net/profile?id=~Suman_Jana1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Suman_Jana1">Suman Jana</a>, <a href="https://openreview.net/profile?id=~Xue_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xue_Lin1">Xue Lin</a>, <a href="https://openreview.net/profile?id=~Cho-Jui_Hsieh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cho-Jui_Hsieh1">Cho-Jui Hsieh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Feb 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#nVZtXBI6LNn-details-654" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="nVZtXBI6LNn-details-654"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">neural network verification, branch and bound</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Formal verification of neural networks (NNs) is a challenging and important problem. Existing efficient complete solvers typically require the branch-and-bound (BaB) process, which splits the problem domain into sub-domains and solves each sub-domain using faster but weaker incomplete verifiers, such as Linear Programming (LP) on linearly relaxed sub-domains.  In this paper, we propose to use the backward mode linear relaxation based perturbation analysis (LiRPA) to replace LP during the BaB process, which can be efficiently implemented on the typical machine learning accelerators such as GPUs and TPUs.  However, unlike LP, LiRPA when applied naively can produce much weaker bounds and even cannot check certain conflicts of sub-domains during splitting, making the entire procedure incomplete after BaB. To address these challenges, we apply a fast gradient based bound tightening procedure combined with batch splits and the design of minimal usage of LP bound procedure, enabling us to effectively use LiRPA on the accelerator hardware for the challenging complete NN verification problem and significantly outperform LP-based approaches. On a single GPU, we demonstrate an order of magnitude speedup compared to existing LP-based approaches.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We use fast bound propagation methods on GPUs for complete neural network verification and achieve large speedup compared to SOTA.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="GH7QRzUDdXG" data-number="1446">
      <h4>
        <a href="https://openreview.net/forum?id=GH7QRzUDdXG">
            A Geometric Analysis of Deep Generative Image Models and Its Applications
        </a>
      
        
          <a href="https://openreview.net/pdf?id=GH7QRzUDdXG" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Binxu_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Binxu_Wang1">Binxu Wang</a>, <a href="https://openreview.net/profile?id=~Carlos_R_Ponce1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Carlos_R_Ponce1">Carlos R Ponce</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#GH7QRzUDdXG-details-521" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GH7QRzUDdXG-details-521"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep generative model, Interpretability, GAN, Differential Geometry, Optimization, Model Inversion, Feature Visualization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Generative adversarial networks (GANs) have emerged as a powerful unsupervised method to model the statistical patterns of real-world data sets, such as natural images. These networks are trained to map random inputs in their latent space to new samples representative of the learned data. However, the structure of the latent space is hard to intuit due to its high dimensionality and the non-linearity of the generator, which limits the usefulness of the models. Understanding the latent space requires a way to identify input codes for existing real-world images (inversion), and a way to identify directions with known image transformations (interpretability). Here, we use a geometric framework to address both issues simultaneously. We develop an architecture-agnostic method to compute the Riemannian metric of the image manifold created by GANs. The eigen-decomposition of the metric isolates axes that account for different levels of image variability. An empirical analysis of several pretrained GANs shows that image variation around each position is concentrated along surprisingly few major axes (the space is highly anisotropic) and the directions that create this large variation are similar at different positions in the space (the space is homogeneous). We show that many of the top eigenvectors correspond to interpretable transforms in the image space, with a substantial part of eigenspace corresponding to minor transforms which could be compressed out. This geometric understanding unifies key previous results related to GAN interpretability. We show that the use of this metric allows for more efficient optimization in the latent space (e.g. GAN inversion) and facilitates unsupervised discovery of interpretable axes. Our results illustrate that defining the geometry of the GAN image manifold can serve as a general framework for understanding GANs. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We developed tools to compute the metric tensor of image manifold learnt by GANs, empirically analyzed their geometry, and found this knowledge useful to GAN inversion and finding interpretable axes.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=GH7QRzUDdXG&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9SS69KwomAM" data-number="2829">
      <h4>
        <a href="https://openreview.net/forum?id=9SS69KwomAM">
            Solving Compositional Reinforcement Learning Problems via Task Reduction
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9SS69KwomAM" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yunfei_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yunfei_Li1">Yunfei Li</a>, <a href="https://openreview.net/profile?email=wuyilin98%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="wuyilin98@gmail.com">Yilin Wu</a>, <a href="https://openreview.net/profile?id=~Huazhe_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Huazhe_Xu1">Huazhe Xu</a>, <a href="https://openreview.net/profile?id=~Xiaolong_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaolong_Wang3">Xiaolong Wang</a>, <a href="https://openreview.net/profile?id=~Yi_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Wu1">Yi Wu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9SS69KwomAM-details-499" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9SS69KwomAM-details-499"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">compositional task, sparse reward, reinforcement learning, task reduction, imitation learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a deep RL algorithm for learning compositional strategies to solve sparse-reward continuous-control problems.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=9SS69KwomAM&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="JoCR4h9O3Ew" data-number="745">
      <h4>
        <a href="https://openreview.net/forum?id=JoCR4h9O3Ew">
            ARMOURED: Adversarially Robust MOdels using Unlabeled data by REgularizing Diversity
        </a>
      
        
          <a href="https://openreview.net/pdf?id=JoCR4h9O3Ew" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kangkang_Lu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kangkang_Lu1">Kangkang Lu</a>, <a href="https://openreview.net/profile?id=~Cuong_Manh_Nguyen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cuong_Manh_Nguyen1">Cuong Manh Nguyen</a>, <a href="https://openreview.net/profile?id=~Xun_Xu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xun_Xu1">Xun Xu</a>, <a href="https://openreview.net/profile?id=~Kiran_Chari1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kiran_Chari1">Kiran Chari</a>, <a href="https://openreview.net/profile?id=~Yu_Jing_Goh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Jing_Goh1">Yu Jing Goh</a>, <a href="https://openreview.net/profile?id=~Chuan-Sheng_Foo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuan-Sheng_Foo1">Chuan-Sheng Foo</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#JoCR4h9O3Ew-details-837" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JoCR4h9O3Ew-details-837"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Adversarial Robustness, Semi-supervised Learning, Multi-view Learning, Diversity Regularization, Entropy Maximization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Adversarial attacks pose a major challenge for modern deep neural networks. Recent advancements show that adversarially robust generalization requires a large amount of labeled data for training. If annotation becomes a burden, can unlabeled data help bridge the gap? In this paper, we propose ARMOURED, an adversarially robust training method based on semi-supervised learning that consists of two components. The first component applies multi-view learning to simultaneously optimize multiple independent networks and utilizes unlabeled data to enforce labeling consistency. The second component reduces adversarial transferability among the networks via diversity regularizers inspired by determinantal point processes and entropy maximization. Experimental results show that under small perturbation budgets, ARMOURED is robust against strong adaptive adversaries. Notably, ARMOURED does not rely on generating adversarial samples during training. When used in combination with adversarial training, ARMOURED yields competitive performance with the state-of-the-art adversarially-robust benchmarks on SVHN and outperforms them on CIFAR-10, while offering higher clean accuracy.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">ARMOURED is a novel technique for adversarially robust learning that elegantly unifies semi-supervised learning and diversity regularization through a multi-view learning framework. </span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="j1RMMKeP2gR" data-number="1615">
      <h4>
        <a href="https://openreview.net/forum?id=j1RMMKeP2gR">
            Acting in Delayed Environments with Non-Stationary Markov Policies
        </a>
      
        
          <a href="https://openreview.net/pdf?id=j1RMMKeP2gR" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Esther_Derman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Esther_Derman1">Esther Derman</a>, <a href="https://openreview.net/profile?id=~Gal_Dalal2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gal_Dalal2">Gal Dalal</a>, <a href="https://openreview.net/profile?id=~Shie_Mannor2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shie_Mannor2">Shie Mannor</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#j1RMMKeP2gR-details-662" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="j1RMMKeP2gR-details-662"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, delay</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The standard Markov Decision Process (MDP) formulation hinges on the assumption that an action is executed immediately after it was chosen. However, assuming it is often unrealistic and can lead to catastrophic failures in applications such as robotic manipulation, cloud computing, and finance. We introduce a framework for learning and planning in MDPs where the decision-maker commits actions that are executed with a delay of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="336" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi></math></mjx-assistive-mml></mjx-container> steps. The brute-force state augmentation baseline where the state is concatenated to the last <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="337" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi></math></mjx-assistive-mml></mjx-container> committed actions suffers from an exponential complexity in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="338" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi></math></mjx-assistive-mml></mjx-container>, as we show for policy iteration. We then prove that with execution delay, deterministic Markov policies in the original state-space are sufficient for attaining maximal reward, but need to be non-stationary. As for stationary Markov policies, we show they are sub-optimal in general. Consequently, we devise a non-stationary Q-learning style model-based algorithm that solves delayed execution tasks without resorting to state-augmentation. Experiments on tabular, physical, and Atari domains reveal that it converges quickly to high performance even for substantial delays, while standard approaches that either ignore the delay or rely on state-augmentation struggle or fail due to divergence. The code is available at \url{https://github.com/galdl/rl_delay_basic.git}.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">In this paper, we derive theoretical results on execution-delay MDPs, and devise a DQN-based algorithm to empirically tackle this setup.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=j1RMMKeP2gR&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="oFp8Mx_V5FL" data-number="3658">
      <h4>
        <a href="https://openreview.net/forum?id=oFp8Mx_V5FL">
            Overfitting for Fun and Profit: Instance-Adaptive Data Compression
        </a>
      
        
          <a href="https://openreview.net/pdf?id=oFp8Mx_V5FL" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ties_van_Rozendaal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ties_van_Rozendaal1">Ties van Rozendaal</a>, <a href="https://openreview.net/profile?email=ihuijben%40qti.qualcomm.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ihuijben@qti.qualcomm.com">Iris AM Huijben</a>, <a href="https://openreview.net/profile?id=~Taco_Cohen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Taco_Cohen1">Taco Cohen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#oFp8Mx_V5FL-details-857" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="oFp8Mx_V5FL-details-857"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Neural data compression, Learned compression, Generative modeling, Overfitting, Finetuning, Instance learning, Instance adaptation, Variational autoencoders, Rate-distortion optimization, Model compression, Weight quantization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Neural data compression has been shown to outperform classical methods in terms of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="339" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi><mi>D</mi></math></mjx-assistive-mml></mjx-container> performance, with results still improving rapidly.
      At a high level, neural compression is based on an autoencoder that tries to reconstruct the input instance from a (quantized) latent representation, coupled with a prior that is used to losslessly compress these latents.
      Due to limitations on model capacity and imperfect optimization and generalization, such models will suboptimally compress test data in general.
      However, one of the great strengths of learned compression is that if the test-time data distribution is known and relatively low-entropy (e.g. a camera watching a static scene, a dash cam in an autonomous car, etc.), the model can easily be finetuned or adapted to this distribution, leading to improved <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="340" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi><mi>D</mi></math></mjx-assistive-mml></mjx-container> performance.
      In this paper we take this concept to the extreme, adapting the full model to a single video, and sending model updates (quantized and compressed using a parameter-space prior) along with the latent representation. Unlike previous work, we finetune not only the encoder/latents but the entire model, and - during finetuning - take into account both the effect of model quantization and the additional costs incurred by sending the model updates. We evaluate an image compression model on I-frames (sampled at 2 fps) from videos of the Xiph dataset, and demonstrate that full-model adaptation improves <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="341" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D445 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi><mi>D</mi></math></mjx-assistive-mml></mjx-container> performance by ~1 dB, with respect to encoder-only finetuning.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that we can finetune an entire data compression model on a single instance, and improve the rate-distortion performance,&nbsp;taking into account the additional costs for sending the&nbsp;model updates.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="vQzcqQWIS0q" data-number="1708">
      <h4>
        <a href="https://openreview.net/forum?id=vQzcqQWIS0q">
            Learnable Embedding sizes for Recommender Systems
        </a>
      
        
          <a href="https://openreview.net/pdf?id=vQzcqQWIS0q" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Siyi_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siyi_Liu1">Siyi Liu</a>, <a href="https://openreview.net/profile?id=~Chen_Gao3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chen_Gao3">Chen Gao</a>, <a href="https://openreview.net/profile?id=~Yihong_Chen3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yihong_Chen3">Yihong Chen</a>, <a href="https://openreview.net/profile?email=jindp%40tsinghua.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="jindp@tsinghua.edu.cn">Depeng Jin</a>, <a href="https://openreview.net/profile?id=~Yong_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yong_Li3">Yong Li</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 11 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#vQzcqQWIS0q-details-156" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="vQzcqQWIS0q-details-156"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Recommender Systems, Deep Learning, Embedding Size</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The embedding-based representation learning is commonly used in deep learning recommendation models to map the raw sparse features to dense vectors. The traditional embedding manner that assigns a uniform size to all features has two issues. First, the numerous features inevitably lead to a gigantic embedding table that causes a high memory usage cost. Second, it is likely to cause the over-fitting problem for those features that do not require too large representation capacity. Existing works that try to address the problem always cause a significant drop in recommendation performance or suffers from the limitation of unaffordable training time cost. In this paper, we proposed a novel approach, named PEP (short for Plug-in Embedding Pruning), to reduce the size of the embedding table while avoiding the drop of recommendation accuracy. PEP prunes embedding parameter where the pruning threshold(s) can be adaptively learned from data. Therefore we can automatically obtain a mixed-dimension embedding-scheme by pruning redundant parameters for each feature. PEP is a general framework that can plug in various base recommendation models. Extensive experiments demonstrate it can efficiently cut down embedding parameters and boost the base model's performance. Specifically, it achieves strong recommendation performance while reducing 97-99% parameters. As for the computation cost, PEP only brings an additional 20-30% time cost compare with base models. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Learning flexible feature-aware embedding sizes effectively and efficiently for recommendation models.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="RmcPm9m3tnk" data-number="2593">
      <h4>
        <a href="https://openreview.net/forum?id=RmcPm9m3tnk">
            Generative Scene Graph Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=RmcPm9m3tnk" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Fei_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fei_Deng1">Fei Deng</a>, <a href="https://openreview.net/profile?id=~Zhuo_Zhi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhuo_Zhi1">Zhuo Zhi</a>, <a href="https://openreview.net/profile?email=donghun%40etri.re.kr" class="profile-link" data-toggle="tooltip" data-placement="top" title="donghun@etri.re.kr">Donghun Lee</a>, <a href="https://openreview.net/profile?id=~Sungjin_Ahn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sungjin_Ahn1">Sungjin Ahn</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#RmcPm9m3tnk-details-594" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RmcPm9m3tnk-details-594"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">object-centric representations, generative modeling, scene generation, variational autoencoders</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose the first object-centric generative model capable of unsupervised scene graph discovery from multi-object scenes without access to predefined parts.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="d-XzF81Wg1" data-number="1876">
      <h4>
        <a href="https://openreview.net/forum?id=d-XzF81Wg1">
            Deconstructing the Regularization of BatchNorm
        </a>
      
        
          <a href="https://openreview.net/pdf?id=d-XzF81Wg1" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yann_Dauphin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yann_Dauphin1">Yann Dauphin</a>, <a href="https://openreview.net/profile?id=~Ekin_Dogus_Cubuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ekin_Dogus_Cubuk1">Ekin Dogus Cubuk</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#d-XzF81Wg1-details-716" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="d-XzF81Wg1-details-716"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning, batch normalization, regularization, understanding neural networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Batch normalization (BatchNorm) has become a standard technique in deep learning. Its popularity is in no small part due to its often positive effect on generalization. Despite this success, the regularization effect of the technique is still poorly understood. This study aims to decompose BatchNorm into separate mechanisms that are much simpler. We identify three effects of BatchNorm and assess their impact directly with ablations and interventions. Our experiments show that preventing explosive growth at the final layer at initialization and during training can recover a large part of BatchNorm's generalization boost. This regularization mechanism can lift accuracy by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="342" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c39"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2.9</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> for Resnet-50 on Imagenet without BatchNorm. We show it is linked to other methods like Dropout and recent initializations like Fixup. Surprisingly, this simple mechanism matches the improvement of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="343" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c39"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>0.9</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> of the more complex Dropout regularization for the state-of-the-art Efficientnet-B8 model on Imagenet. This demonstrates the underrated effectiveness of simple regularizations and sheds light on directions to further improve generalization for deep nets.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We deconstruct the regularization effect of batch normalization and show that preventing explosive growth at the final layer at initialization and during training can recover a large part of BatchNorm's generalization boost.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=d-XzF81Wg1&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="TYXs_y84xRj" data-number="2432">
      <h4>
        <a href="https://openreview.net/forum?id=TYXs_y84xRj">
            PolarNet: Learning to Optimize Polar Keypoints for Keypoint Based Object Detection
        </a>
      
        
          <a href="https://openreview.net/pdf?id=TYXs_y84xRj" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Wu_Xiongwei1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wu_Xiongwei1">Wu Xiongwei</a>, <a href="https://openreview.net/profile?id=~Doyen_Sahoo1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Doyen_Sahoo1">Doyen Sahoo</a>, <a href="https://openreview.net/profile?id=~Steven_HOI1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_HOI1">Steven HOI</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#TYXs_y84xRj-details-200" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="TYXs_y84xRj-details-200"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Object Detection, Deep Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A variety of anchor-free object detectors have been actively proposed as possible alternatives to the mainstream anchor-based detectors that often rely on complicated design of anchor boxes. Despite achieving promising performance on par with anchor-based detectors, the existing anchor-free detectors such as FCOS or CenterNet predict objects based on standard Cartesian coordinates, which often yield poor quality keypoints. Further, the feature representation is also scale-sensitive. In this paper, we propose a new anchor-free keypoint based detector ``PolarNet", where keypoints are represented as a set of Polar coordinates instead of Cartesian coordinates. The ``PolarNet" detector learns offsets pointing to the corners of objects in order to learn high quality keypoints. Additionally, PolarNet uses features of corner points to localize objects, making the localization scale-insensitive. Finally in our experiments, we show that PolarNet, an anchor-free detector, outperforms the existing anchor-free detectors, and it is able to achieve highly competitive result on COCO test-dev benchmark (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="344" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c34"></mjx-c><mjx-c class="mjx-c37"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c38"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>47.8</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> and <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="345" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c30"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>50.3</mn><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> AP under the single-model single-scale and multi-scale testing) which is on par with the state-of-the-art two-stage anchor-based object detectors. The code and the models are available at https://github.com/XiongweiWu/PolarNetV1</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="CYO5T-YjWZV" data-number="2311">
      <h4>
        <a href="https://openreview.net/forum?id=CYO5T-YjWZV">
            Simple Spectral Graph Convolution
        </a>
      
        
          <a href="https://openreview.net/pdf?id=CYO5T-YjWZV" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hao_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Zhu2">Hao Zhu</a>, <a href="https://openreview.net/profile?id=~Piotr_Koniusz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Piotr_Koniusz1">Piotr Koniusz</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#CYO5T-YjWZV-details-465" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="CYO5T-YjWZV-details-465"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Graph Convolutional Network, Oversmoothing</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Graph Convolutional Networks (GCNs) are leading methods for learning graph representations. However, without specially designed architectures, the performance of GCNs degrades quickly with increased depth. As the aggregated neighborhood size and neural network depth are two completely orthogonal aspects of graph representation, several methods focus on summarizing the neighborhood by aggregating K-hop neighborhoods of nodes while using shallow neural networks. However, these methods still encounter oversmoothing, and suffer from high computation and storage costs. In this paper, we use a modified Markov Diffusion Kernel to derive a variant of GCN called Simple Spectral Graph Convolution (SSGC). Our spectral analysis shows that our simple spectral graph convolution used in SSGC is a trade-off of low- and high-pass filter bands which capture the global and local contexts of each node. We provide two theoretical claims which demonstrate that we can aggregate over a sequence of increasingly larger neighborhoods compared to competitors while limiting severe oversmoothing.  Our experimental evaluations show that SSGC with a linear learner is competitive in text and node classification tasks. Moreover, SSGC is comparable to other state-of-the-art methods for node clustering and community prediction tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A simple and efficient method for graph convolution based on the Markov Diffusion Kernel, which works well on different tasks under unsupervised, semi-supervised and supervised settings.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="pGIHq1m7PU" data-number="644">
      <h4>
        <a href="https://openreview.net/forum?id=pGIHq1m7PU">
            Explainable Subgraph Reasoning for Forecasting on Temporal Knowledge Graphs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=pGIHq1m7PU" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhen_Han3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhen_Han3">Zhen Han</a>, <a href="https://openreview.net/profile?email=peng.chen%40tum.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="peng.chen@tum.de">Peng Chen</a>, <a href="https://openreview.net/profile?id=~Yunpu_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yunpu_Ma1">Yunpu Ma</a>, <a href="https://openreview.net/profile?id=~Volker_Tresp1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Volker_Tresp1">Volker Tresp</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 31 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>19 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#pGIHq1m7PU-details-901" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pGIHq1m7PU-details-901"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Temporal knowledge graph, future link prediction, graph neural network, subgraph reasoning.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Modeling time-evolving knowledge graphs (KGs) has recently gained increasing interest. Here, graph representation learning has become the dominant paradigm for link prediction on temporal KGs. However, the embedding-based approaches largely operate in a black-box fashion, lacking the ability to interpret their predictions. This paper provides a link forecasting framework that reasons over query-relevant subgraphs of temporal KGs and jointly models the structural dependencies and the temporal dynamics. Especially, we propose a temporal relational attention mechanism and a novel reverse representation update scheme to guide the extraction of an enclosing subgraph around the query. The subgraph is expanded by an iterative sampling of temporal neighbors and by attention propagation. Our approach provides human-understandable evidence explaining the forecast. We evaluate our model on four benchmark temporal knowledge graphs for the link forecasting task. While being more explainable, our model obtains a relative improvement of up to 20 <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="346" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> on Hits@1 compared to the previous best temporal KG forecasting method. We also conduct a survey with 53 respondents, and the results show that the evidence extracted by the model for link forecasting is aligned with human understanding. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an explainable attention-based reasoning model for predicting future links on temporal knowledge graphs.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="hsFN92eQEla" data-number="1801">
      <h4>
        <a href="https://openreview.net/forum?id=hsFN92eQEla">
            EVALUATION OF NEURAL ARCHITECTURES TRAINED WITH SQUARE LOSS VS CROSS-ENTROPY IN CLASSIFICATION TASKS
        </a>
      
        
          <a href="https://openreview.net/pdf?id=hsFN92eQEla" class="pdf-link" title="Download PDF" target="_blank"><img src="./poster_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Like_Hui1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Like_Hui1">Like Hui</a>, <a href="https://openreview.net/profile?email=mbelkin%40ucsd.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="mbelkin@ucsd.edu">Mikhail Belkin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 06 Mar 2021)</span>
          <span class="item">ICLR 2021 Poster</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#hsFN92eQEla-details-342" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hsFN92eQEla-details-342"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">large scale learning, square loss vs cross-entropy, classification, experimental evaluation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Modern neural architectures for classification tasks are trained using the cross-entropy loss, which is widely believed to be empirically superior to the square loss. In this work we provide evidence indicating that this belief may not be well-founded. 
      We explore several major neural architectures and a range of standard benchmark datasets for NLP, automatic speech recognition (ASR) and computer vision tasks to show that these architectures, with the same hyper-parameter settings as reported in the literature, perform comparably or better when trained with the square loss, even after equalizing computational resources.
      Indeed, we observe that the square loss produces better results in the dominant majority of NLP and ASR experiments. Cross-entropy appears to have a slight edge on computer vision tasks.
      
      We argue that there is little compelling empirical or theoretical evidence indicating a clear-cut advantage to the cross-entropy loss. Indeed, in our experiments, performance on nearly all non-vision tasks  can be improved, sometimes significantly, by switching to the square loss. Furthermore, training with square loss appears to be less sensitive to the randomness in initialization. We posit that
      training using the square loss for classification needs to be a part of best practices of modern deep learning on equal footing with cross-entropy. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">An experimental evaluation of neural architectures in classification tasks shows that training with square loss produces better results than the cross-entropy in the majority of NLP and ASR experiments.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=hsFN92eQEla&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
</ul>
</div>
    <div role="tabpanel" class="tab-pane fade  " id="withdrawn-rejected-submissions">
      
    </div>
</div>
</div></div></div></main></div></div></div><footer class="sitemap"><div class="container"><div class="row hidden-xs"><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/about">About OpenReview</a></li><li><a href="https://openreview.net/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="https://openreview.net/venues">All Venues</a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/contact">Contact</a></li><li><a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/faq">Frequently Asked Questions</a></li><li><a href="https://openreview.net/legal/terms">Terms of Service</a></li><li><a href="https://openreview.net/legal/privacy">Privacy Policy</a></li></ul></div></div><div class="row visible-xs-block"><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://openreview.net/about">About OpenReview</a></li><li><a href="https://openreview.net/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="https://openreview.net/venues">All Venues</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://openreview.net/faq">Frequently Asked Questions</a></li><li><a href="https://openreview.net/contact">Contact</a></li><li><a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li><li><a href="https://openreview.net/legal/terms">Terms of Service</a></li><li><a href="https://openreview.net/legal/privacy">Privacy Policy</a></li></ul></div></div></div></footer><footer class="sponsor"><div class="container"><div class="row"><div class="col-sm-10 col-sm-offset-1"><p class="text-center">OpenReview is created by the<!-- --> <a href="http://www.iesl.cs.umass.edu/" target="_blank" rel="noopener noreferrer">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.</p></div></div></div></footer><div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog"><div class="modal-dialog "><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button><h3 class="modal-title">Send Feedback</h3></div><div class="modal-body"><p>Enter your feedback below and we'll get back to you as soon as possible.</p><form><div class="form-group"><input type="email" name="from" class="form-control" placeholder="Email" required=""></div><div class="form-group"><input type="text" name="subject" class="form-control" placeholder="Subject"></div><div class="form-group"><textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea></div></form></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button><button type="button" class="btn btn-primary">Send</button></div></div></div></div><div id="bibtex-modal" class="modal fade" tabindex="-1" role="dialog"><div class="modal-dialog "><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button><h3 class="modal-title">BibTeX Record</h3></div><div class="modal-body"><pre class="bibtex-content"></pre><em class="instructions">Click anywhere on the box above to highlight complete record</em></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Done</button></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"groupId":"ICLR.cc/2021/Conference","webfieldCode":"// Webfield Code for ICLR.cc/2021/Conference\nwindow.user = {\"id\":\"guest_1617527484753\",\"isGuest\":true};\n$(function() {\n  var args = {\"id\":\"ICLR.cc/2021/Conference\"};\n  var group = {\"id\":\"ICLR.cc/2021/Conference\",\"cdate\":1587911460292,\"ddate\":null,\"tcdate\":1587911460292,\"tmdate\":1611607761057,\"tddate\":null,\"signatures\":[\"ICLR.cc/2021/Conference\"],\"signatories\":[\"ICLR.cc/2021/Conference\"],\"readers\":[\"everyone\"],\"nonreaders\":[],\"writers\":[\"ICLR.cc/2021/Conference\"],\"members\":[\"ICLR.cc/2021/Conference/Program_Chairs\",\"OpenReview.net/Support\"],\"details\":{\"writable\":false}};\n  var document = null;\n  var window = null;\n  var model = {\n    tokenPayload: function() {\n      return { user: user }\n    }\n  };\n\n  $('#group-container').empty();\n  \n\n  // ------------------------------------\n// Venue homepage template\n//\n// This webfield displays the conference header (#header), the submit button (#invitation),\n// and a tabbed interface for viewing various types of notes.\n// ------------------------------------\n\n// Constants\nvar PARENT_GROUP_ID = 'ICLR.cc/2021';\nvar CONFERENCE_ID = 'ICLR.cc/2021/Conference';\nvar BLIND_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Blind_Submission';\nvar WITHDRAWN_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Withdrawn_Submission';\nvar DESK_REJECTED_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Desk_Rejected_Submission';\nvar DECISION_INVITATION_REGEX = 'ICLR.cc/2021/Conference/Paper.*/-/Decision';\nvar DECISION_HEADING_MAP = {\n  \"Accept (Oral)\": \"Oral Presentations\", \n  \"Accept (Spotlight)\": \"Spotlight Presentations\", \n  \"Accept (Poster)\": \"Poster Presentations\", \n  \"Reject\": \"Withdrawn/Rejected Submissions\"\n  \n};\nvar PAGE_SIZE = 25;\n\nvar HEADER = {\"title\": \"International Conference on Learning Representations\", \"subtitle\": \"ICLR 2021\", \"location\": \"Vienna, Austria\", \"date\": \"May 04 2021\", \"website\": \"https://iclr.cc/\", \"instructions\": \"\u003cp class='dark'\u003ePlease see the venue website for more information.\u003cbr\u003e \u003cp class='dark'\u003e\u003cstrong\u003eAbstract Submission End:\u003c/strong\u003e Sep 28 2020 03:00PM UTC-0\u003c/p\u003e\u003cp class='dark'\u003e\u003cstrong\u003ePaper Submission End:\u003c/strong\u003e Oct 2 2020 03:00PM UTC-0\u003c/p\u003e\", \"deadline\": \"\", \"contact\": \"iclr2021programchairs@googlegroups.com\", \"reviewers_name\": \"Reviewers\", \"area_chairs_name\": \"Area_Chairs\", \"reviewers_id\": \"ICLR.cc/2021/Conference/Reviewers\", \"authors_id\": \"ICLR.cc/2021/Conference/Authors\", \"program_chairs_id\": \"ICLR.cc/2021/Conference/Program_Chairs\", \"area_chairs_id\": \"ICLR.cc/2021/Conference/Area_Chairs\", \"submission_id\": \"ICLR.cc/2021/Conference/-/Submission\", \"blind_submission_id\": \"ICLR.cc/2021/Conference/-/Blind_Submission\", \"withdrawn_submission_id\": \"ICLR.cc/2021/Conference/-/Withdrawn_Submission\", \"desk_rejected_submission_id\": \"ICLR.cc/2021/Conference/-/Desk_Rejected_Submission\", \"public\": true};\n\nvar paperDisplayOptions = {\n  pdfLink: true,\n  replyCount: true,\n  showContents: true\n};\n\nvar sections = [];\n\n// Main is the entry point to the webfield code and runs everything\nfunction main() {\n  if (args \u0026\u0026 args.referrer) {\n    OpenBanner.referrerLink(args.referrer);\n  } else if (PARENT_GROUP_ID.length){\n    OpenBanner.venueHomepageLink(PARENT_GROUP_ID);\n  }\n  Webfield.ui.setup('#group-container', CONFERENCE_ID);  // required\n\n  renderConferenceHeader();\n\n  renderConferenceTabs();\n\n  load().then(renderContent).then(Webfield.ui.done);\n}\n\n// Load makes all the API calls needed to get the data to render the page\nfunction load() {\n  var notesP = Webfield.getAll('/notes', {\n    invitation: BLIND_SUBMISSION_ID,\n    details: 'replyCount,invitation,original'\n  });\n\n  var withdrawnNotesP = WITHDRAWN_SUBMISSION_ID ? Webfield.getAll('/notes', {\n    invitation: WITHDRAWN_SUBMISSION_ID,\n    details: 'replyCount,invitation,original'\n  }) : $.Deferred().resolve([]);\n\n  var deskRejectedNotesP = DESK_REJECTED_SUBMISSION_ID ? Webfield.getAll('/notes', {\n    invitation: DESK_REJECTED_SUBMISSION_ID,\n    details: 'replyCount,invitation,original'\n  }) : $.Deferred().resolve([]);\n\n  var decisionNotesP = Webfield.getAll('/notes', {\n    invitation: DECISION_INVITATION_REGEX,\n  });\n\n  var userGroupsP;\n  if (!user || _.startsWith(user.id, 'guest_')) {\n    userGroupsP = $.Deferred().resolve([]);\n  } else {\n    userGroupsP = Webfield.getAll('/groups', {\n      regex: CONFERENCE_ID + '/.*',\n      member: user.id,\n      web: true\n    }).then(function(groups) {\n      if (!groups || !groups.length) {\n        return [];\n      }\n\n      return groups.map(function(g) { return g.id; });\n    });\n  }\n\n  return $.when(notesP, decisionNotesP, withdrawnNotesP, deskRejectedNotesP, userGroupsP);\n}\n\nfunction renderConferenceHeader() {\n  Webfield.ui.venueHeader(HEADER);\n  Webfield.ui.spinner('#notes', { inline: true });\n}\n\nfunction getElementId(decision) {\n  if (!decision) return decision;\n  return decision.replace(/\\W/g, '-')\n    .replace('(', '')\n    .replace(')', '')\n    .toLowerCase();\n}\n\nfunction renderConferenceTabs() {\n  sections.push({\n    heading: 'Your Consoles',\n    id: 'your-consoles',\n  });\n  var tabNames = new Set();\n  for (var decision in DECISION_HEADING_MAP) {\n    tabNames.add(DECISION_HEADING_MAP[decision]);\n  }\n  var tabArray = Array.from(tabNames);\n  tabArray.forEach(function(tabName) {\n    sections.push({\n      heading: tabName,\n      id: getElementId(tabName)\n    });\n  })\n\n  Webfield.ui.tabPanel(sections, {\n    container: '#notes',\n    hidden: true\n  });\n}\n\nfunction createConsoleLinks(allGroups) {\n  var uniqueGroups = _.sortedUniq(allGroups.sort());\n\n  return uniqueGroups.map(function(group) {\n    var groupName = group.split('/').pop();\n    if (groupName.slice(-1) === 's') {\n      groupName = groupName.slice(0, -1);\n    }\n\n    return [\n      '\u003cli class=\"note invitation-link\"\u003e',\n        '\u003ca href=\"/group?id=' + group + '\"\u003e' + groupName.replace(/_/g, ' ') + ' Console\u003c/a\u003e',\n      '\u003c/li\u003e'\n    ].join('');\n  });\n}\n\nfunction groupNotesByDecision(notes, decisionNotes, withdrawnNotes, deskRejectedNotes) {\n  // Categorize notes into buckets defined by DECISION_HEADING_MAP\n  var notesDict = _.keyBy(notes, 'id');\n\n  var papersByDecision = {};\n  for (var decision in DECISION_HEADING_MAP) {\n    papersByDecision[getElementId(DECISION_HEADING_MAP[decision])] = [];\n  }\n\n  decisionNotes.forEach(function(d) {\n    var tabName = DECISION_HEADING_MAP[d.content.decision];\n    if (tabName) {\n      var decisionKey = getElementId(tabName);\n      if (notesDict[d.forum] \u0026\u0026 papersByDecision[decisionKey]) {\n        papersByDecision[decisionKey].push(notesDict[d.forum]);\n      }\n    }\n\n  });\n\n  if (papersByDecision['withdrawn-rejected-submissions']) {\n    papersByDecision['withdrawn-rejected-submissions'] = papersByDecision['withdrawn-rejected-submissions'].concat(withdrawnNotes.concat(deskRejectedNotes));\n  }\n  return papersByDecision;\n}\n\nfunction renderContent(notes, decisionNotes, withdrawnNotes, deskRejectedNotes, userGroups) {\n  var papersByDecision = groupNotesByDecision(notes, decisionNotes, withdrawnNotes, deskRejectedNotes);\n\n  // Your Consoles Tab\n  if (userGroups \u0026\u0026 userGroups.length) {\n    var consoleLinks = createConsoleLinks(userGroups);\n    $('#your-consoles').html('\u003cul class=\"list-unstyled submissions-list\"\u003e' +\n      consoleLinks.join('\\n') + '\u003c/ul\u003e');\n\n    $('.tabs-container a[href=\"#your-consoles\"]').parent().show();\n  } else {\n    $('.tabs-container a[href=\"#your-consoles\"]').parent().hide();\n  }\n\n  // Register event handlers\n  $('#group-container').on('shown.bs.tab', 'ul.nav-tabs li a', function(e) {\n    var containerSelector = $(e.target).attr('href');\n    var containerId = containerSelector.substring(1);\n    if (!papersByDecision.hasOwnProperty(containerId) || !$(containerSelector).length) {\n      return;\n    }\n\n    setTimeout(function() {\n      Webfield.ui.searchResults(\n        papersByDecision[containerId],\n        Object.assign({}, paperDisplayOptions, { showTags: false, container: containerSelector })\n      );\n    }, 150);\n  });\n\n  $('#group-container').on('hidden.bs.tab', 'ul.nav-tabs li a', function(e) {\n    var containerSelector = $(e.target).attr('href');\n    var containerId = containerSelector.substring(1);\n    if (!papersByDecision.hasOwnProperty(containerId) || !$(containerSelector).length) {\n      return;\n    }\n\n    Webfield.ui.spinner(containerSelector, { inline: true });\n  });\n\n  $('#notes \u003e .spinner-container').remove();\n  $('.tabs-container').show();\n}\n\n// Go!\nmain();\n\n});\n//# sourceURL=webfieldCode.js","query":{"id":"ICLR.cc/2021/Conference"}}},"page":"/group","query":{"id":"ICLR.cc/2021/Conference"},"buildId":"v1.0.9-1-gf539684","isFallback":false,"gip":true,"head":[["meta",{"charSet":"utf-8"}],["meta",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["meta",{"property":"og:image","content":"https://openreview.net/images/openreview_logo_512.png"}],["meta",{"property":"og:type","content":"website"}],["meta",{"property":"og:site_name","content":"OpenReview"}],["meta",{"name":"twitter:card","content":"summary"}],["meta",{"name":"twitter:site","content":"@openreviewnet"}],["title",{"children":"ICLR 2021 Conference | OpenReview"}],["meta",{"name":"description","content":"Welcome to the OpenReview homepage for ICLR 2021 Conference"}],["meta",{"property":"og:title","content":"ICLR 2021 Conference"}],["meta",{"property":"og:description","content":"Welcome to the OpenReview homepage for ICLR 2021 Conference"}]]}</script><script nomodule="" src="./poster_files/polyfills-f9ac226678fc858257de.js.下载"></script><script src="./poster_files/main-6b56d8b97b908e519b08.js.下载" async=""></script><script src="./poster_files/webpack-d7b2fb72fb7257504a38.js.下载" async=""></script><script src="./poster_files/framework.b11cd6ab3c62dae3dfb8.js.下载" async=""></script><script src="./poster_files/29107295.c7a36c5cb4964dc936e4.js.下载" async=""></script><script src="./poster_files/b637e9a5.d9354c88856ddbaf0be2.js.下载" async=""></script><script src="./poster_files/42e8bd60.2605cce5b4c2063fa3d3.js.下载" async=""></script><script src="./poster_files/55e0983c962a44019f922501f82fb92be7dc6038.b1fe41cb81db81e91237.js.下载" async=""></script><script src="./poster_files/181a3372a136420bacd8143b1a96dfd629357ddc.7ec9ffd88886a5000e00.js.下载" async=""></script><script src="./poster_files/a070395ba8e9275b849fcd4310593532831ac4b7.8744cf13ed507c350a25.js.下载" async=""></script><script src="./poster_files/1220f4ffb4828c6d46a13072dc033abc74519993.3d3dd7c19e661b4a6bfa.js.下载" async=""></script><script src="./poster_files/styles.c669c7da917090bc8543.js.下载" async=""></script><script src="./poster_files/_app-0232e1e8985093b3faa8.js.下载" async=""></script><script src="./poster_files/group-11f7765f7db1e915182b.js.下载" async=""></script><script src="./poster_files/_buildManifest.js.下载" async=""></script><script src="./poster_files/_ssgManifest.js.下载" async=""></script><script> </script><script>// Webfield Code for ICLR.cc/2021/Conference
window.user = {"id":"guest_1617527484753","isGuest":true};
$(function() {
  var args = {"id":"ICLR.cc/2021/Conference"};
  var group = {"id":"ICLR.cc/2021/Conference","cdate":1587911460292,"ddate":null,"tcdate":1587911460292,"tmdate":1611607761057,"tddate":null,"signatures":["ICLR.cc/2021/Conference"],"signatories":["ICLR.cc/2021/Conference"],"readers":["everyone"],"nonreaders":[],"writers":["ICLR.cc/2021/Conference"],"members":["ICLR.cc/2021/Conference/Program_Chairs","OpenReview.net/Support"],"details":{"writable":false}};
  var document = null;
  var window = null;
  var model = {
    tokenPayload: function() {
      return { user: user }
    }
  };

  $('#group-container').empty();
  

  // ------------------------------------
// Venue homepage template
//
// This webfield displays the conference header (#header), the submit button (#invitation),
// and a tabbed interface for viewing various types of notes.
// ------------------------------------

// Constants
var PARENT_GROUP_ID = 'ICLR.cc/2021';
var CONFERENCE_ID = 'ICLR.cc/2021/Conference';
var BLIND_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Blind_Submission';
var WITHDRAWN_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Withdrawn_Submission';
var DESK_REJECTED_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Desk_Rejected_Submission';
var DECISION_INVITATION_REGEX = 'ICLR.cc/2021/Conference/Paper.*/-/Decision';
var DECISION_HEADING_MAP = {
  "Accept (Oral)": "Oral Presentations", 
  "Accept (Spotlight)": "Spotlight Presentations", 
  "Accept (Poster)": "Poster Presentations", 
  "Reject": "Withdrawn/Rejected Submissions"
  
};
var PAGE_SIZE = 25;

var HEADER = {"title": "International Conference on Learning Representations", "subtitle": "ICLR 2021", "location": "Vienna, Austria", "date": "May 04 2021", "website": "https://iclr.cc/", "instructions": "<p class='dark'>Please see the venue website for more information.<br> <p class='dark'><strong>Abstract Submission End:</strong> Sep 28 2020 03:00PM UTC-0</p><p class='dark'><strong>Paper Submission End:</strong> Oct 2 2020 03:00PM UTC-0</p>", "deadline": "", "contact": "iclr2021programchairs@googlegroups.com", "reviewers_name": "Reviewers", "area_chairs_name": "Area_Chairs", "reviewers_id": "ICLR.cc/2021/Conference/Reviewers", "authors_id": "ICLR.cc/2021/Conference/Authors", "program_chairs_id": "ICLR.cc/2021/Conference/Program_Chairs", "area_chairs_id": "ICLR.cc/2021/Conference/Area_Chairs", "submission_id": "ICLR.cc/2021/Conference/-/Submission", "blind_submission_id": "ICLR.cc/2021/Conference/-/Blind_Submission", "withdrawn_submission_id": "ICLR.cc/2021/Conference/-/Withdrawn_Submission", "desk_rejected_submission_id": "ICLR.cc/2021/Conference/-/Desk_Rejected_Submission", "public": true};

var paperDisplayOptions = {
  pdfLink: true,
  replyCount: true,
  showContents: true
};

var sections = [];

// Main is the entry point to the webfield code and runs everything
function main() {
  if (args && args.referrer) {
    OpenBanner.referrerLink(args.referrer);
  } else if (PARENT_GROUP_ID.length){
    OpenBanner.venueHomepageLink(PARENT_GROUP_ID);
  }
  Webfield.ui.setup('#group-container', CONFERENCE_ID);  // required

  renderConferenceHeader();

  renderConferenceTabs();

  load().then(renderContent).then(Webfield.ui.done);
}

// Load makes all the API calls needed to get the data to render the page
function load() {
  var notesP = Webfield.getAll('/notes', {
    invitation: BLIND_SUBMISSION_ID,
    details: 'replyCount,invitation,original'
  });

  var withdrawnNotesP = WITHDRAWN_SUBMISSION_ID ? Webfield.getAll('/notes', {
    invitation: WITHDRAWN_SUBMISSION_ID,
    details: 'replyCount,invitation,original'
  }) : $.Deferred().resolve([]);

  var deskRejectedNotesP = DESK_REJECTED_SUBMISSION_ID ? Webfield.getAll('/notes', {
    invitation: DESK_REJECTED_SUBMISSION_ID,
    details: 'replyCount,invitation,original'
  }) : $.Deferred().resolve([]);

  var decisionNotesP = Webfield.getAll('/notes', {
    invitation: DECISION_INVITATION_REGEX,
  });

  var userGroupsP;
  if (!user || _.startsWith(user.id, 'guest_')) {
    userGroupsP = $.Deferred().resolve([]);
  } else {
    userGroupsP = Webfield.getAll('/groups', {
      regex: CONFERENCE_ID + '/.*',
      member: user.id,
      web: true
    }).then(function(groups) {
      if (!groups || !groups.length) {
        return [];
      }

      return groups.map(function(g) { return g.id; });
    });
  }

  return $.when(notesP, decisionNotesP, withdrawnNotesP, deskRejectedNotesP, userGroupsP);
}

function renderConferenceHeader() {
  Webfield.ui.venueHeader(HEADER);
  Webfield.ui.spinner('#notes', { inline: true });
}

function getElementId(decision) {
  if (!decision) return decision;
  return decision.replace(/\W/g, '-')
    .replace('(', '')
    .replace(')', '')
    .toLowerCase();
}

function renderConferenceTabs() {
  sections.push({
    heading: 'Your Consoles',
    id: 'your-consoles',
  });
  var tabNames = new Set();
  for (var decision in DECISION_HEADING_MAP) {
    tabNames.add(DECISION_HEADING_MAP[decision]);
  }
  var tabArray = Array.from(tabNames);
  tabArray.forEach(function(tabName) {
    sections.push({
      heading: tabName,
      id: getElementId(tabName)
    });
  })

  Webfield.ui.tabPanel(sections, {
    container: '#notes',
    hidden: true
  });
}

function createConsoleLinks(allGroups) {
  var uniqueGroups = _.sortedUniq(allGroups.sort());

  return uniqueGroups.map(function(group) {
    var groupName = group.split('/').pop();
    if (groupName.slice(-1) === 's') {
      groupName = groupName.slice(0, -1);
    }

    return [
      '<li class="note invitation-link">',
        '<a href="/group?id=' + group + '">' + groupName.replace(/_/g, ' ') + ' Console</a>',
      '</li>'
    ].join('');
  });
}

function groupNotesByDecision(notes, decisionNotes, withdrawnNotes, deskRejectedNotes) {
  // Categorize notes into buckets defined by DECISION_HEADING_MAP
  var notesDict = _.keyBy(notes, 'id');

  var papersByDecision = {};
  for (var decision in DECISION_HEADING_MAP) {
    papersByDecision[getElementId(DECISION_HEADING_MAP[decision])] = [];
  }

  decisionNotes.forEach(function(d) {
    var tabName = DECISION_HEADING_MAP[d.content.decision];
    if (tabName) {
      var decisionKey = getElementId(tabName);
      if (notesDict[d.forum] && papersByDecision[decisionKey]) {
        papersByDecision[decisionKey].push(notesDict[d.forum]);
      }
    }

  });

  if (papersByDecision['withdrawn-rejected-submissions']) {
    papersByDecision['withdrawn-rejected-submissions'] = papersByDecision['withdrawn-rejected-submissions'].concat(withdrawnNotes.concat(deskRejectedNotes));
  }
  return papersByDecision;
}

function renderContent(notes, decisionNotes, withdrawnNotes, deskRejectedNotes, userGroups) {
  var papersByDecision = groupNotesByDecision(notes, decisionNotes, withdrawnNotes, deskRejectedNotes);

  // Your Consoles Tab
  if (userGroups && userGroups.length) {
    var consoleLinks = createConsoleLinks(userGroups);
    $('#your-consoles').html('<ul class="list-unstyled submissions-list">' +
      consoleLinks.join('\n') + '</ul>');

    $('.tabs-container a[href="#your-consoles"]').parent().show();
  } else {
    $('.tabs-container a[href="#your-consoles"]').parent().hide();
  }

  // Register event handlers
  $('#group-container').on('shown.bs.tab', 'ul.nav-tabs li a', function(e) {
    var containerSelector = $(e.target).attr('href');
    var containerId = containerSelector.substring(1);
    if (!papersByDecision.hasOwnProperty(containerId) || !$(containerSelector).length) {
      return;
    }

    setTimeout(function() {
      Webfield.ui.searchResults(
        papersByDecision[containerId],
        Object.assign({}, paperDisplayOptions, { showTags: false, container: containerSelector })
      );
    }, 150);
  });

  $('#group-container').on('hidden.bs.tab', 'ul.nav-tabs li a', function(e) {
    var containerSelector = $(e.target).attr('href');
    var containerId = containerSelector.substring(1);
    if (!papersByDecision.hasOwnProperty(containerId) || !$(containerSelector).length) {
      return;
    }

    Webfield.ui.spinner(containerSelector, { inline: true });
  });

  $('#notes > .spinner-container').remove();
  $('.tabs-container').show();
}

// Go!
main();

});
//# sourceURL=webfieldCode.js</script><script src="./poster_files/index-8d5ab195e774e3fc0705.js.下载"></script></body></html>
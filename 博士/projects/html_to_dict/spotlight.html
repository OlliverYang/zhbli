<!DOCTYPE html>
<!-- saved from url=(0079)https://openreview.net/group?id=ICLR.cc/2021/Conference#spotlight-presentations -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin="true"><link rel="stylesheet" href="./ICLR 2021 Conference _ OpenReview_files/css"><link rel="stylesheet" href="./ICLR 2021 Conference _ OpenReview_files/bootstrap.min.css"><link rel="icon" href="https://openreview.net/favicon.ico"><script type="text/javascript" async="" src="./ICLR 2021 Conference _ OpenReview_files/analytics.js.下载"></script><script async="" src="./ICLR 2021 Conference _ OpenReview_files/js"></script><script>window.dataLayer = window.dataLayer || [];
            function gtag() { dataLayer.push(arguments); }
            gtag('js', new Date());
            gtag('config', 'UA-108703919-1', {
              page_path: window.location.pathname + window.location.search,
              transport_type: 'beacon'
            });</script><meta name="viewport" content="width=device-width, initial-scale=1"><meta property="og:image" content="https://openreview.net/images/openreview_logo_512.png"><meta property="og:type" content="website"><meta property="og:site_name" content="OpenReview"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@openreviewnet"><title>ICLR 2021 Conference | OpenReview</title><meta name="description" content="Welcome to the OpenReview homepage for ICLR 2021 Conference"><meta property="og:title" content="ICLR 2021 Conference"><meta property="og:description" content="Welcome to the OpenReview homepage for ICLR 2021 Conference"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/styles.38b7829b.chunk.css" as="style"><link rel="stylesheet" href="./ICLR 2021 Conference _ OpenReview_files/styles.38b7829b.chunk.css" data-n-g=""><noscript data-n-css="true"></noscript><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/main-6b56d8b97b908e519b08.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/webpack-d7b2fb72fb7257504a38.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/framework.b11cd6ab3c62dae3dfb8.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/29107295.c7a36c5cb4964dc936e4.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/b637e9a5.d9354c88856ddbaf0be2.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/42e8bd60.2605cce5b4c2063fa3d3.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/55e0983c962a44019f922501f82fb92be7dc6038.b1fe41cb81db81e91237.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/181a3372a136420bacd8143b1a96dfd629357ddc.7ec9ffd88886a5000e00.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/a070395ba8e9275b849fcd4310593532831ac4b7.8744cf13ed507c350a25.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/1220f4ffb4828c6d46a13072dc033abc74519993.3d3dd7c19e661b4a6bfa.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/styles.c669c7da917090bc8543.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/_app-0232e1e8985093b3faa8.js.下载" as="script"><link rel="preload" href="./ICLR 2021 Conference _ OpenReview_files/group-11f7765f7db1e915182b.js.下载" as="script"><script src="./ICLR 2021 Conference _ OpenReview_files/tex-chtml-full.js.下载" async="" crossorigin="anonymous"></script><style type="text/css">.simpread-theme-root{font-size:62.5%!important}sr-rd-content,sr-rd-desc,sr-rd-title{width:100%}sr-rd-title{display:-webkit-box;margin:1em 0 .5em;overflow:hidden;text-overflow:ellipsis;text-rendering:optimizelegibility;-webkit-line-clamp:3;-webkit-box-orient:vertical}sr-rd-content{text-align:left;word-break:break-word}sr-rd-desc{text-align:justify;line-height:2.4;margin:0 0 1.2em;box-sizing:border-box}sr-rd-content{font-size:25.6px;font-size:1.6rem;line-height:1.6}sr-rd-content h1,sr-rd-content h1 *,sr-rd-content h2,sr-rd-content h2 *,sr-rd-content h3,sr-rd-content h3 *,sr-rd-content h4,sr-rd-content h4 *,sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{word-break:break-all}sr-rd-content div,sr-rd-content p{display:block;float:inherit;line-height:1.6;font-size:25.6px;font-size:1.6rem}sr-rd-content div,sr-rd-content p,sr-rd-content pre,sr-rd-content sr-blockquote{margin:0 0 1.2em;word-break:break-word}sr-rd-content a{padding:0 5px;vertical-align:baseline;vertical-align:initial}sr-rd-content a,sr-rd-content a:link{color:inherit;font-size:inherit;font-weight:inherit;border:none}sr-rd-content a:hover{background:transparent}sr-rd-content img{margin:10px;padding:5px;max-width:100%;background:#fff;border:1px solid #bbb;box-shadow:1px 1px 3px #d4d4d4}sr-rd-content figcaption{text-align:center;font-size:14px}sr-rd-content sr-blockquote{display:block;position:relative;padding:15px 25px;text-align:left;line-height:inherit}sr-rd-content sr-blockquote:before{position:absolute}sr-rd-content sr-blockquote *{margin:0;font-size:inherit}sr-rd-content table{width:100%;margin:0 0 1.2em;word-break:keep-all;word-break:normal;overflow:auto;border:none}sr-rd-content table td,sr-rd-content table th{border:none}sr-rd-content ul{margin:0 0 1.2em;margin-left:1.3em;padding:0;list-style:disc}sr-rd-content ol{list-style:decimal;margin:0;padding:0}sr-rd-content ol li,sr-rd-content ul li{font-size:inherit;list-style:disc;margin:0 0 1.2em}sr-rd-content ol li{list-style:decimal;margin-left:1.3em}sr-rd-content ol li *,sr-rd-content ul li *{margin:0;text-align:left;text-align:initial}sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em}sr-rd-content li ul{list-style:circle}sr-rd-content pre{font-family:Consolas,Monaco,Andale Mono,Source Code Pro,Liberation Mono,Courier,monospace;display:block;padding:15px;line-height:1.5;word-break:break-all;word-wrap:break-word;white-space:pre;overflow:auto}sr-rd-content pre,sr-rd-content pre *,sr-rd-content pre div{font-size:17.6px;font-size:1.1rem}sr-rd-content li pre code,sr-rd-content p pre code,sr-rd-content pre{background-color:transparent;border:none}sr-rd-content pre code{margin:0;padding:0}sr-rd-content pre code,sr-rd-content pre code *{font-size:17.6px;font-size:1.1rem}sr-rd-content pre p{margin:0;padding:0;color:inherit;font-size:inherit;line-height:inherit}sr-rd-content li code,sr-rd-content p code{margin:0 4px;padding:2px 4px;font-size:17.6px;font-size:1.1rem}sr-rd-content mark{margin:0 5px;padding:2px;background:#fffdd1;border-bottom:1px solid #ffedce}.sr-rd-content-img{width:90%;height:auto}.sr-rd-content-img-load{width:48px;height:48px;margin:0;padding:0;border-style:none;border-width:0;background-repeat:no-repeat;background-image:url(data:image/gif;base64,R0lGODlhMAAwAPcAAAAAABMTExUVFRsbGx0dHSYmJikpKS8vLzAwMDc3Nz4+PkJCQkRERElJSVBQUFdXV1hYWFxcXGNjY2RkZGhoaGxsbHFxcXZ2dnl5eX9/f4GBgYaGhoiIiI6OjpKSkpaWlpubm56enqKioqWlpampqa6urrCwsLe3t7q6ur6+vsHBwcfHx8vLy8zMzNLS0tXV1dnZ2dzc3OHh4eXl5erq6u7u7vLy8vf39/n5+f///wEBAQQEBA4ODhkZGSEhIS0tLTk5OUNDQ0pKSk1NTV9fX2lpaXBwcHd3d35+foKCgoSEhIuLi4yMjJGRkZWVlZ2dnaSkpKysrLOzs7u7u7y8vMPDw8bGxsnJydvb293d3eLi4ubm5uvr6+zs7Pb29gYGBg8PDyAgICcnJzU1NTs7O0ZGRkxMTFRUVFpaWmFhYWVlZWtra21tbXNzc3V1dXh4eIeHh4qKipCQkJSUlJiYmJycnKampqqqqrW1tcTExMrKys7OztPT09fX19jY2Ojo6PPz8/r6+hwcHCUlJTQ0NDg4OEFBQU9PT11dXWBgYGZmZm9vb3Jycnp6en19fYCAgIWFhaurq8DAwMjIyM3NzdHR0dTU1ODg4OTk5Onp6fDw8PX19fv7+xgYGB8fHz8/P0VFRVZWVl5eXmpqanR0dImJiaCgoKenp6+vr9/f3+fn5+3t7fHx8QUFBQgICBYWFioqKlVVVWJiYo+Pj5eXl6ioqLa2trm5udbW1vT09C4uLkdHR1FRUVtbW3x8fJmZmcXFxc/Pz42Njb+/v+/v7/j4+EtLS5qamri4uL29vdDQ0N7e3jIyMpOTk6Ojo7GxscLCwisrK1NTU1lZWW5ubkhISAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACH/C05FVFNDQVBFMi4wAwEAAAAh/i1NYWRlIGJ5IEtyYXNpbWlyYSBOZWpjaGV2YSAod3d3LmxvYWRpbmZvLm5ldCkAIfkEAAoA/wAsAAAAADAAMAAABv/AnHBILBqPyKRySXyNSC+mdFqEAAARqpaIux0dVwduq2VJLN7iI3ys0cZkosogIJSKODBAXLzJYjJpcTkuCAIBDTRceg5GNDGAcIM5GwKWHkWMkjk2kDI1k0MzCwEBCTBEeg9cM5AzoUQjAwECF5KaQzWQMYKwNhClBStDjEM4fzGKZCxRRioFpRA2OXlsQrqAvUM300gsCgofr0UWhwMjQhgHBxhjfpCgeDMtLtpCOBYG+g4lvS8JAQZoEHKjRg042GZsylHjBYuHMY7gyHBAn4EDE1ZI8tCAhL1tNLoJsQGDxYoVEJHcOPHAooEEGSLmKKjlWIuHKF/ES0IjxAL/lwxCfFRCwwVKlC4UTomxIYFFaVtKomzBi8yKCetMkKnxEIZIMjdKdBi6ZIYyWAthSZGUVu0RGRsyyJ07V0SoGC3yutCrN40KcIADK6hAlgmLE4hNIF58QlmKBYIDV2g75bBixouVydCAAUOGzp87h6AsBQa9vfTy0uuFA86Y1m5jyyaDQwUJ0kpexMC95AWHBw9YkJlBYoSKs1RmhJDgoIGDDIWN1BZBvUSLr0psmKDgoLuDCSZ4G4FhgrqIESZeFMbBAsOD7g0ifJBxT7wkGyxImB+Bgr7EEA8418ADGrhARAodtKCEDNYRQYNt+wl3RAfNOWBBCr3MkMEEFZxg3YwkLXjQQQg7URPDCSNQN8wRMEggwQjICUECBRNQoIIQKYAAQgpCvOABBx2ksNANLpRQQolFuCBTETBYQOMHaYxwwQV2UVMCkPO1MY4WN3wwwQQWNJPDCJ2hI4QMH3TQQXixsVDBlyNIIiUGZuKopgdihmLDBjVisOWYGFxQJ0MhADkCdnGcQCMFHsZyAQZVDhEikCtOIsMFNXKAHZmQ9kFCBxyAEGNUmFYgIREiTDmoEDCICMKfccQAgghpiRDoqtSkcAKsk7RlK51IiAcLCZ2RMJsWRbkw6rHMFhEEACH5BAAKAP8ALAAAAAAwADAAAAf/gDmCg4SFhoeIiYqLhFhRUViMkpOFEwICE5SahDg4hjgSAQJEh16em4ctRklehkQBAaSFXhMPVaiFVwoGPyeFOK+xp4MkOzoCVLiDL7sGEF2cwbKDW0A6Oj0tyoNOBt5PhUQCwoRL1zpI29QO3gxZhNLDLz7XP1rqg1E/3kmDwLDTcBS5tgMcPkG0vCW4MkjaICoBrgmxgcrFO0NWEnib0OofORtDrvGYcqhTIhcOHIjgYgiJtx9RcuBQEiSIEkFPjOnIZMiGFi3DCiVRQFTClFaDsDDg1UQQDhs2kB4x1uPFrC1ZsrL8tCQIUQVBMLgY9uSBFKSGvEABwoSQFy5Z/7NqgVZqygSvRIU0uSeTrqIuSHF00RI3yxa0iLqIePBVwYMoQSX5LKyF4qQsTIR8NYJYEla5XSIzwnHFSBAGtzZ5IcylsyYvJ564lmz5oO3buAttabKEie/fS5bE3LYFi/Hjx7MgtZKyefMhQzCIpvTiipUr2LNjp8vcuXck0ydVt649O90tTIIrUbKEfXsS4T0jn6+ck0x/8XPr34/Dyon8iRimDhZOFFGBC6hwMcUULfhFCRckGFHEBEUwAeAvLUhxwglUYDFbXRgUMeEEGExxYSFaULHhhlUApQgOLSwh4gQTGCECXyYtMowNL6i44hVcTIcDCRXQOEEFTVg1SPAVT0SSyBZVKClIFy1MIYWGUzhpyBM0FpGEFYhxscQRSKTmiTwkiCBFbTJt4d+GCB6CxRFHROGgTFLQiYQ2OVxBAgkM5ZAFFCKIECgnWVBBBZuFvMBXIVkkcQQGIpwiRXBSOFVFoSRsVYgNd0qCwxMYHJHERTlcykSmgkBYaBUnStICEhhgIMUwly7BqiBXFAoFqurY0ASdS3iaam+75mCDFIWe8KEmVJSKQWqD5JpsDi8QCoWUymwxJgZOMGrtL1QUaqc6WShBJreCjItimlEYi4sWUNxqiLu5WCHvNtPhu98iJ/hG0r+MdGFcqAQTHAgAIfkEAAoA/wAsAAAAADAAMAAACP8AcwgcSLCgwYMIEypcSDALHjxZGEqcWNCNAQNvKGokGCjQQTYX2Ry84XHjQT4a5JQk2CakwRtu1OQxWXCPAwVlqhQMBNJAm5UCoxAIcEAnTYF+bipYU4NjSwNsgP5pEIAon6MD6yjYeqdgzzYF5QgIIAAO1oF/0mxFI4NgT5ED/YypuqDtWYFSFmyVMzDQ06gCA7kZO8DO3YGA2mw1c1Xg24FVxIxFA8hkH7sF9TTY+uZGDr8XweYAhKaqGCoH96BG2CeNmihNOTLZugCFQCYOHDARaGcAWdEEZ2QYIMCoQTlmcrep4nlgljM4RQQGBKi5Bt9j+hAEVAcBgO9ngAb/pnMmt4MzcLQPtMOmiviBN6KU4RuYSoMv3wF8UdN8ZxU35jkQAR0zCHRDZQvVUFIfaoCRHwBk3PEeQTVEoUaAa+AxYUI3xEHAg2HE8cdEM8yBRm5mZNCfRDWQkR8Ya6inEUoOoKGHSXZ88UUDVGzI0A0oSGgSIG/UseJhG/k4kZJIolUHHXQ8CeWUGmIFyB9YZvlHDVuWpMcaa6ihRphgihkHkwr9kcWabLbZ3B5hihnnmGowgWZCM7SpZxYIzkDHHHP8CeigUpzFpZaIirfSnU026ihHexi30QyxHZVFHW9k4IdJNeyhhx8IalSDFHC8YWodjA7Uhx6s7iEDozdU/8HEG26YGoekE/3hKat68FGgQoHwMYeptGogxYiBaXRDFp7mwSqoCAUiRQbEZiBCRAPtIQW2CP2hB2aj+cErq+ASZAexcuwBVA11MJFuXytlgQIezBX0x6qscltQFnDEQUWoA1HBhLvq8YECCurNMC8Km+40wx57HNnQrwXJMMfAUngUSBUiiGBUIHs8REWl2wG8pBRMxDEHZhx7XFINVOCBgrpN9iHHwJK2LGkfD6FA8Vk32DFwHSTrTNANMeOhR6oJ6THwuwQZ3VDP+tL0Bx0D33Gk1H3p8VAVJm8kA9ZyVJ0DFR3jmoPCUox81x94rFYQx3WonYMffIR91IRcPxHKUB522DGT3xIBsqbehCceEAAh+QQACgD/ACwAAAAAMAAwAAAI/wBzCBxIsKDBgwgTKlxI8BIVSZcYSpxIkNMjBQo4UNxYkNNBRxgfHdzkkeNBLB3qlBzIqRFGRwY5OVpEyWRBS4kcPJjU0aUCmAXxIDCggKdNgVkQOXDgSFNFn0AHdkFjgKilowOhLHUgpaBPkQTrVDUwB+vATIuWrsHE8itBLAyqOmBrViCVpYfqEITK8lHVH13rCtz0aCmiqzlahhy4olBVRU45YqFbsBKapZA8KlYAdtOaqoRWHKwkaWVBLG7c4IlMcI6DQw8kCQSxaI0IgSV+VI06EBOHHz9EHwShqDikSaYvKYIdSSAnkiU76GaAheAmKIYECAigyLRzKGuKK/9aMwfLyhKOkCPcJOWBXueS0AgKEECAIEbenU+CFL44IyiZOLcJQ5oMmAMWjAxCn3YMSGEgQprg0Yh4azQyRX4KceIBIdvVR4gHAUqECRSMiNcBhgl1IUSHgzBSHUeWeLAGTSZFIoggaKyAIkObSCLFjgkRJgJrghVpJEeaJaakaV1EIgIUUD4JhQgiUIFVS4dspaUDaCBWSSNugNnImGG6AQKQCnWBgA5stulmczl8KWaYYjZy5lFquqmnDnA2KSWUU05p5VFY4rVllxkeyUlJSaJ5ZF2cWEKJowcVaBYmUngwRxYmbXLJJZk8SJEmVMzBQQcclEApQZlk4eolXVD/tMkkdXRgqwd11MSRJp++egmRCGURiQeocjCHJLEmtqpzXVziahagiloQFR5wcKoHUkQ0EBZUUFbpZBVh8iy0yRqEx6kdQIHYQJpIIUIk6yopECaUTFKJtJuI62q5BWECAgiTAJsDJYBymkMWK6xgcBf1UqJtRbxesiOoB2XipAilCUQJHnjoeuAk9krr3LIsSUJlJCHGybHHmtQ7yYtFXjKlCB6r3HFDIFPCL1ab4EGlFERujEcl1lUCcrxYWRIo0pWs3C/Ik3hrUxclUHlhZU5XhEW995qVSdWRPDyQ0EQX1AXIlQjMUSYrGFUQ2Qc5KzKho3Fc9qMTNY0H0ngrCrRJJqH2LXhCAQEAIfkEAAoA/wAsAAAAADAAMAAACP8AcwgcSLCgwYMIEypcSFBVlTyqGEqcSJBTBwdmPFDcWJDTwVIOHHQ4yMkjx4Op6pwySXBDyFIGvZTS8OJkQRikFFXY0xGkA5gFpxj6ZIaPzYGXcioqxaqiS5EFVyn6ZCgUjKMDTShSNGpKQZ9AB5r6RLYO1oGrNGx1FFEgJ58jB6ZyQFYRjbMDq4zaGokgSDMdTFokC8orXoFePGy1cDUHp6dxc7BoQPZNU46p2hZ8YWHrBy8C4SK2QLYBT4MvWLAsmGpDqRSXB3IytXcUC4GR3rzpm8OEoaEaC9L4QPb2wVO633jYs1rVG50m3HopKbAOqE+hUhFkhcqBge8VVrv/NeEouSNTqVie6MBHvOwqFXg7zqPowHcDCRy5d8znQ/I3GqByl2OgLTSdQKloUMh9BoRyQoEIsVJFB/+Vksd+CXFShyEMGlLHKhPRYIIGydWBIUKriHJfAhpoh5kpjtB0EioHHKCIakd5sceFJ7HSASoQHibkkBx5ZKRjSKJ1gglLMumkCcbZ5MUGolRppZWKNAZDBx2UUkqXXX4ZyYkLsQJKAGimKQCaAqAi0JZfesllmPKdtIoha66ZJptu5rDKFCYw2WSgJ+SB1WNXJpqlQmRuZOSjbhEpqUGcpFJTj2/UEdtJNFRxyimaUWTKF1+YkUKjBrGyRySmtJoCR6t8/wLArAGMcilDXrxgwimtnmLCrRPJ5Mmss3pSyoAIcXLJFLzyGgkLsaFK0AuK8EAsAIVEEiRBe/DaaxXI5pAKC+HGpEq0KTTwBbFfKLKtQFX0ekJ626VwwhQupnpJKpesxkodBxAbyn40oIIKH+++cMK9bV3ywgttsZLKxCAWdIkGnXRSRUI0VCycvSeclgMMeeSRryoTX/JuDnucehILC6fg8bgsNJaDF/umUu5ZqgB6gs0js1AzQaukvPJJXuSxcBWbwsCCyRXtC4Mq0i6UysInXHKT0PkKVPTEm9rEir1Qiud0HkALhDK/VaNYhQlT7Oz00AVJzO/RFK3CR9pvPhndNVo0tG0TyXRPKhHNfxue4Sqr4K244QEBACH5BAAKAP8ALAAAAAAwADAAAAj/AHMIHEiwoMGDCBMqXEhwBgsWNBhKnFjwiRo1pihqLMjpIK2LdA7m6rjxoJYRJkgS/KgmZMFctGZhKVkwy4Y3jnBxZOmS4IpYh2TppClwxs03dDQV/Eihp8BVRxw4UKOF6MAUb7KuIMiJliw1TwqikuqgltWBmjxknRVRYFeQBLXIknpk1dmBlBxlNbHyYtiBtKTGUnF3ICdTR45oyAL4a08XaKRuyFVyRtuaGrI+6fgWrMBcGqRGGFoQF6WEM2jRWUFZbFZHp3OYWLKEb44UQB04FUiDjlQXCG3RnjUCl8ocNJbgJJyDk/OBtWI5oFB1YC4TsgwpULABYQoPS2aF/0dVXaCKJzMRcmLhyJZhFm20bzfk4bhhLLXEi6eVwm5z+yKRlMUSQmyngCEUqAAgQblQ8oR44dFByYIJcTKCAwYqgEYtSkm0Sgq0hDcLKhQilMsi8h3iQXkUzWDCLB4wtpEKZRjyBnBEcWJaiRWacktrhQUpZEmcNefWcwJpsoIKS6rApJMqkEbkLItUaWUbbSxyhIwnmWLKCF6G6aNVmjgAy5kFoHkmLO7l0KWXYIp5C5lmrmnnmW0qCeWTT+JIEydUWiloG1sOuRCSziFp6KKGzSDjRppoMAKQJa1CyS23XEYRKoIIgoaCkGKRgi2ksgCpEAGkWsARUirESRYqkP9KqgosSgQTAq+kGkACHmhqECcOyXpLClgAyeNTrWHRRgG6viKECZQShMUtwlLiH2+4XGtQLiMksIRhKqAhiK6CtLGgC6TessIMxzXIAiUzIPRGKwD44GcOmoxgSK4ByLLgKk5mAaAWD7Hg3yozzODfE/QCoIZ9Rh1wwFYIrdJhQZaysEJ6yGWRRVuaHAIAAGCkcJALzG2ExUOUXEyDx5elAMbIQlx81yoas8Diyx8bpsbIrfx1FycurMCCC5TyrCkuPoyMQK00zWA0RAU52jNBS4wMgCN35eKCxsYVpHTVQIzcQ2xEaULJQ9ryBrNBtbgCwCsmn5VLFlB3fDWDFAwUxihBY297bGGB/31oLiMZrnhBAQEAIfkEAAoA/wAsAAAAADAAMAAACP8AcwgcSLCgwYMIEypcSDCTCxeZGEqcWPDOmzd3KGosyOmgnQtv7Bzk1HHjQVW2qJQk+PGCyII3RPxKZbKgql9MmtAsaOeiCIMs2Ci64KfmwEw4mdy5UVDExZcDWUFSNFSV0YEsmGhlQZDTxzc/CdqiusbW1ah2tIqowfIpQVVvqEJidXbgiyZaqbAEKaIkJxFU2QCrO5CTCa1OLg38CvWFBapOVlLMxNbgJSdaTXT06jYHpyZULbw4mMpFwkwlSrhgWpCK1iajc1D59UtvDhVrqEIdWEOEBAlFDwITIcKOrVSSe+cMVnilCaG+rA68QYUNrwa8miBkYYd4cRURBwb/K7FzZDAmtgW60PCA1/UHvyQTvISiO/E7LOh6ln+QdY7LETSA3QNvsMBfVy+Y4J0dJvhxYEKclCCBe+4pYoJ+DLESzB3epTfRDb5gx0sEv0inUSYq2HGHYhux0B4TsdXESSoxahShCv4RpuOOJpHk2Y+S3eBCMEMGY2SR5dUUAkhv+HKRk29owGImKJhggi1YYnklMA8ydAMbCoQp5gJhLmAbSlnacqWatgxm1JdixlmmbUIaeeSdSW70ly++aNCnn3wywSKPhBZaVyYmanQDEyVgaBIrfgTDQmUamaCLLooYuNENqUjKAjDBUVRDLwaUmoAGeUKoigufAsMCRJuG/7BLqaXuEkJ4CdXwAgutBnNJlwfVwJofGiRAqwEPoJAjQanw6ioLqTjKiirLEnTDHbtoJxAnwCiiC60I+HJgs66+UINknFySSrQC3cDKuQJpMEAACdR4gwkN0GrBgaw8pAp/mazLLidvXHqBQHbMK4AFBqniRJhcIcRKtTncoG4q4XHCCwAA8CIQK70EEIAYKhy0K7AIBZzKrwNt3HFJKoghci+OnsXKupdQqjHHHg9kgQABDLDbWar4sfJKO3dMkB8JiLxAokbVILCjSfc8UBNAB8BEXemm4gfUVUuWSQMi68LcVRavvGzYBZVAgAC6lHwWJ5Qd5LLV01kggZuGehZ2d38oE9YLxxH0LdELdthRo+GM5xAQACH5BAAKAP8ALAAAAAAwADAAAAj/AHMIHEiwoMGDCBMqXEiQGAwYxBhKnFgQhTBhKChqLFjsoIklwkwc7LgRYSZgVw7iuSiSowk7l0oWzFRCBEyDJlga5JMBg5IsMgcSMyFCBAqSA3OGLGjjiRufM4IO5GPHJq6CSvEUlISh6zCpA3OhKGrCBsGcS1oKzLSkqxyzYAVeqiqCEkE8ILUmdeMmg924AotJKloi08CVS/TmyKKk6xOkFInBnRmpqCSSaFsWE9E1CVCDl2AkJCZpWBbIAq8UtfP5SqRIKXNQyvBUrVATfD/vxMMb2AzINohGuhoYqaSeSwwPFJxEkfPHB2Gg4I0HBaWIA2FIioqwGIwnkgji/5JTxLmiIpESZroynfcwXLmWM0Q6t4L5IksooeZ4SRJ1FJLEtBEKbtyHwTCTLZQLDMO0d8V+ChUjjHmM2KGcRsRQggIKF1JESQUVOKGbTJmMSFExeAADIWAstjgRSTBCVkwWD2VBIww3cidTMZEoscQSPgL5oxzcEXPFkUgmSdyOGTgwhANQRvkkMAIZmeSVS5ZUDAZRSjnEEKFQmcOMONqIY406yhQJSBe1CRKRLkq0Ypx0DmRDgic+YUJ8QeWSySWX8KmRJAww4IZ+GxVDzCU2ZpGmRLm4ocCkQixhYkLF2DBDo47iOV8koUw6aSgiYJdQLps2egkxJOXiqUE28P95iRxDiBqEIigIWtCiqmYCmTCFiKArQcWYEMoTBFGCQRC2LgFhiTbOMCwuPejQihsCuWoDScL8YAADI4olgahJdDfDJZ4Wo4gO1iKbgxJBBKGEQCV4a0ASqBEjApRZcgQhCjywOwRcRAQQABHZKmKAAQmIWVAWf2lkgxDsBvBVDrkUfDBJVySwsCLDSvVEK+wWAaPGRCCVxMI/lMDiJT+w60OWKBOUBQMLO/CoTBmwq8MSxBb8CsIEPbGwAU7ERckr7BbSYQ4oQ0YMEQsr0O9GwzDdSnpBG0z0WQgYoEBsUkkSiiKeRl1QLhkwQjZYxYRcDBGvHDzSnC0qUrcieNcLmV0JJYjm9+AGBQQAIfkEAAoA/wAsAAAAADAAMAAACP8AcwgcSLCgwYMIEypcSBCQlmWAGEqcWHAFFBErKGqUKEmECEkHA21MCEhZn4OSLoI0mOzElpEFa7RE9rJgx48Gl8lZcqwmzByAJJ04sUIkwZsrB3qpxYTnn58Dlw09scymx4wEW8hhwuQK1IGBVpyQIsnLUY9Jc9R4whWK2a8C/yAbenIgUoLJuMqpCzdHoBZDkdUYuALtQC20mpYwqhHQ24KAWp5oYfQm1kBSuNLScnBLVYQllW1hPLDP1JrKkCFTJrDPTibJDEbesIHzwWVXcisbTNCLUGSfDV5J/IS3wL9yMCiHglBL7ucQCTp/mlBLiRYEl4lAohwDEimkCdb/gPH8SotljyUy/iMliRs3ymkpC2/wj7Lyyv7QXyhpSXcMS5Q1USBatLBCbjBsFMgTGMCXhBTUNYZbC8ZR1AcSSIgQHEw1RLiRJFfs19eIJKoH1nGkBfLHiiy2WOFIJdAioxwy1vhETV4so+OOPPo0UiBLKCLkkERil4MXD/HYI1RAEulkEUaq2OKUL2oUyAm0HHNMllweI4KHJYYp5k+AMBiRgrUkk56VyRjzxRcijHTFA7wkwdpGfRQBBgB8klGlQl4kwcugEBxjG0N/LOEDn3x6ssSaC12pCC9mUCpBCX8qVQsZjAIAhiJ1eZFpb0ZtcQwElFbqhiT7eaHIF4x+/2EMMozJYUwJkB4nCRvMlbYEnYM+cAx9gTzAKAJPnNnaGAF0ksRxgABilAigKPDAhr4ZQSkvTOwnSSedIOGjX0YIEIAnzAXCxKBMCITMAgoosER4NZQggQQJIpSMkTYVEEAAEJxphAEGsCGQFxjEawxWBS3DF0WAQPBvAQwPbIARRiljRrxG5AoTFJ0IIIAbRgVisREEyRHvAieMuMUCIo+Rr0AnSwdBvBGACdMS/wogR0E1E1RLvAo8AZcyB/xrjIcmE4yxeGzEy8vMMElygACelFBQ0xeHJ0m1vPD70woSdGxQ0AQFIoedIwaSKxsEG2xQICKWiEEBBmAw5kRSSQex4d6ADxQQACH5BAAKAP8ALAAAAAAwADAAAAj/AHMIHEiwoMGDCBMqXEhwE5ctmxhKnFgQFx48lShqlEjpYkaDxTYm3JQly8FKFymBpGSFi8iCmihdoVTDYEc8KgtqseMMlcuXAjdVunIFV0iCNz8OLIbCWc+aQAVyIXrl58CkBf04taM0ajFcRCtFHIgSJ8Eaz5ziGRtVYA2ZV7Qg9Yh0q8m2BLMQpaSJLF2pkZwOO6qxGGGCMYn6ufq32DCnkawS5CIXYTEtWvoa1LL3p94ri3Nk4eksZ0MrIEBsQcilZJYtmpcOpbRa4GFcgZ/FzvHVTocOHPAgrKHFdRYubHNwwQUV4ZZhuAhuQdWMA/Bmw0ZuMa6lxmGGhGtA/5vDwXqHSFm+G9S03XV3kZSe/Lb+hFJyhcWIu65NsRgq83MM0xxFDmF2n0RZNNPMM/y9tMluGhWlHl4UWmYbb7xN+NKEhOGCBi8ghhhiIwdS9BhPKDpjhx2RCRSJDjDGKCMzAxYGQiMX4Ihjjjl+ZIeMQOpAI1DFgMCjjhfk2MhHHooo4iGNaCgRNE5tpSJkkhmGYYYVdumlSJrYkUSJCxWDBzRkTomGIIJEAt8iozQT3UZ+XDBIAHgKUWOZzUzgZxt2NKgQF80QIgCeAhAyR5oHOdbIKH5O0AgeezaECigCHCrAIG2E9iBDmxzFhR1tRDqKEldweIEgmQYgyAPQEP/2xAPPkFnMFY6gQpAfcywyAaSjONPoBIgaYsdufoACywEd2BbqUZE8wMsEldl2hRKQTgDChFYccAAHguaQBCyDHKBrDs4sssgTAkHzwCGHzPFdDXjkeNdB0HQ1kBWEwALLBGM5ooACUfLGAS+HoKGvQFuEppEmE/hbyBUDCUzwQLhEAOKYXaLCjL9JEJbEwI0Q9ESI2VG4BS/+gnJvDhYXzPAEh/CyiGRAzeEvLOwSNPLFBOGBMC924IWLAv4+gLPFjhymSSMgRvCySFYgfYBwBcX83RXSprHwRlcswnHWJIMEQgcOt6WlQTE3+iVCHAwc8tsTaTHMMNXSrbdBAQEAIfkEAAoA/wAsAAAAADAAMAAACP8AcwgcSLCgwYMIEypcSPDGqlWcGEqcWDDLlStZKGqUaPEKlo0bOWXKdBDLFSsfDWJRZgNkwRtasmi5ofJkSoKZUOBRscrlQE4xs5AsaNJjQU5X8OBJ0dKnQBtZovYkWPSmQC1KUWR0KpDTlqhaIg6s2lCFUis0uT6NmmWqQLJjleLZohYn2LQ54OawkUIKnmBiNaYIdhBoVLpvL95UpjSFW4Krhh5U0amTBi0GV7FNu8WSJcRbdOKxZPCGshIlHv8MBaC1rhBNu37VonpgFp0q8ObglAUPFCjOrBy8oehLawBfGqQIbGOLboOZrmAemEkFcGfOoBAeXqvQcQA8FJH/psj8Si3s2FGEVZiplI/vPko9Z2hJCvYQUKRYCrzQkqIAxyVQm0KcqIBeLVfERlEKDXzxhTMgbVELFCpIBpINIbyhIEWWbKUWf3UlxMmIu0VEYogLYaGIKKKsyOKLkICo0RVS1FgjHjbiMZUUAfTo44+gDDhRLaUU2UGRpRzZQUol/OhkAKBsSF4tRxqJZAdLvuUiixO8KAok802ElI1k3uiWiSWSKCOKbLaJ0A0ldBDmQgUC5pQViugSjRQgWaJBBiF4SBEWGiRgQDTRTCMlgRm+8YYGUljIXghBGHBoNEGEMGdCVpTiqKMdqLDoQDfgMQ2iiCaQwU2bkipWJlJo//DpG07YaRAnGegZjQG6KGJFYLVQo8KauwXTAR4EZRFCBqQ4moEUMnLCCKoNlKAbFtOAkmlXuw2EBzWKvDFdV8E0IesbUCCkDBmFOCFpDk2wGwSfOUDxBinp5mAFuIo4AyJfkEAyrkFWKHNQMA2QAQopaXUgjTQx5nCDE4oowojBBn0F0g1vFFJIA1cMVIoZ0pQyFiMVN9GqRiiA4nETgZUijRkmDwRFxWsIV1cmiigciqAdkByxQJlkULEGQmrkjMug5Cvyw0MLlMIaFdPrVBbSeKyIpA6bAUlBNpRSMSmCgqRMKIWAgoJBI5dsUDBrUMOIVS4po0EpMsoMMYicQB7hRNk+nVhQ11/f6uZBTZDcweETbWGFFQMzLvlAAQEAIfkEAAoA/wAsAAAAADAAMAAACP8AcwgcSLCgwYMIEypcSLDYjRvFGEqcWPBPqlR/KGpseOOgRYwbN6oINaFjxYsZDWpJZTLkwGQEALiqZfBjSoJd9kyqBMjlwD2CAAAAclPgR0wGYUyatKelTyRCAXA4CZIgJp2TkPocqAWBUB8wCNpsWGmppYhbBz5pJZQC2hxjuS7d0yUtQUDVhAZINjBujhtYw4bMU+lgMh5Ch/SEi3JgqqWTFhe8URfhpB8/OGgdWIyC0FZPBHbBhKnyH8ipDBZLlUyF5IYTAgR4tcDO60oxWzVCiKlsJadw89gaXlh1GwKyAxCAoOItByC2EwKCUbRLpVvDbd2yhPCGiWqvkg//ciOYssYbMJJlv5V1IaZmhMLPJvTh7UQtKtarSGVfIQw3g4T3SjWVTVTMHtklYwlwDBWjAgQECELTRn/ccgtdWwFihwYMSpQKJv25FKJdCkX01ogkGpSKG9RQ04aLL7Y4S4cTWaLCjTjimMdithjg44+D/CjNaxvdIsKRSCJphxYC9fjjkz6GQiRFxSST5JVLCpRKIy3G2KKMNEpkY4457thQDvahmOKabCp0g5FhJnTgWVtV0sgCDKgQkhbNNGPCZhTxWc0nhLYRp2qozMLBLB8kU+BCgNQCAaGESmOHmgjtccwsis7yRFMlqkDBApRWw0FqaGIq0FtdJPNBp7PU/8LfQcU0wwClC7QxCUEmILFrQjA8oedAmJjQzKIcNMOXahpQGoEtr2lBgTShTGjiQCog0QgHRRVjiQiccnALQpVIM8QTRQl0zBDSSDNuDrZwwIEJAu2hbSP0TpbHMccAWtAe3BlkSQTscqguBRN8sKoIjbihAaoVMbnRDRu0C0FxORwzQcJopaKBG26IcChFI7GrsFoTUHCyQCY00ggSe6TYhRvsyiKxuhsfI9YsbjTSzJQh1WKuNKgUdAzCKwukgsuNLLuVFhOY68ajGW+c9F8f9KxZWpbIMkQowxKkMccFWYKEGxvc7BMMsxwT4thXo2lCliQWM6LGKtPaJkIipA8c2t4T/bHHHv4CbjhBAQEAOw==)}.sr-rd-content-center{text-align:center;display:-webkit-box;-webkit-box-align:center;-webkit-box-pack:center;-webkit-box-orient:vertical}.sr-rd-content-center-small{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex}.sr-rd-content-center-small img{margin:0;padding:0;border:0;box-shadow:none}.sr-rd-content-nobeautify{margin:0;padding:0;border:0;box-shadow:0 0 0}sr-rd-mult{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin:0 0 16px;padding:16px 0 24px;width:100%;background-color:#fff;border-radius:4px;box-shadow:0 1px 2px 0 rgba(60,64,67,.3),0 2px 6px 2px rgba(60,64,67,.15)}sr-rd-mult:hover{-webkit-transition:all .45s 0ms;transition:all .45s 0ms;box-shadow:1px 1px 8px rgba(0,0,0,.16)}sr-rd-mult sr-rd-mult-content{padding:0 16px;overflow:auto}sr-rd-mult sr-rd-mult-avatar,sr-rd-mult sr-rd-mult-content{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sr-rd-mult sr-rd-mult-avatar{-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 15px}sr-rd-mult sr-rd-mult-avatar span{display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;max-width:75px;overflow:hidden;text-overflow:ellipsis;text-align:left;font-size:16px;font-size:1rem}sr-rd-mult sr-rd-mult-avatar img{margin-bottom:0;max-width:50px;max-height:50px;width:50px;height:50px;border-radius:50%}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center{margin:0}sr-page{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;width:100%}</style><style type="text/css">sr-rd-theme-github{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{position:relative;margin-top:1em;margin-bottom:1pc;font-weight:700;line-height:1.4;text-align:left;color:#363636}sr-rd-content h1{padding-bottom:.3em;font-size:57.6px;font-size:3.6rem;line-height:1.2}sr-rd-content h2{padding-bottom:.3em;font-size:44.8px;font-size:2.8rem;line-height:1.225}sr-rd-content h3{font-size:38.4px;font-size:2.4rem;line-height:1.43}sr-rd-content h4{font-size:32px;font-size:2rem}sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h6{color:#777}sr-rd-content ol,sr-rd-content ul{list-style-type:disc;padding:0;padding-left:2em}sr-rd-content ol ol,sr-rd-content ul ol{list-style-type:lower-roman}sr-rd-content ol ol ol,sr-rd-content ol ul ol,sr-rd-content ul ol ol,sr-rd-content ul ul ol{list-style-type:lower-alpha}sr-rd-content table{width:100%;overflow:auto;word-break:normal;word-break:keep-all}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{padding:6px 13px;border:1px solid #ddd}sr-rd-content table tr{background-color:#fff;border-top:1px solid #ccc}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{border-left:4px solid #ddd}.simpread-theme-root{background-color:#fff;color:#333}sr-rd-title{font-family:PT Sans,SF UI Display,\.PingFang SC,PingFang SC,Neue Haas Grotesk Text Pro,Arial Nova,Segoe UI,Microsoft YaHei,Microsoft JhengHei,Helvetica Neue,Source Han Sans SC,Noto Sans CJK SC,Source Han Sans CN,Noto Sans SC,Source Han Sans TC,Noto Sans CJK TC,Hiragino Sans GB,sans-serif;font-size:54.4px;font-size:3.4rem;font-weight:700;line-height:1.3}sr-rd-desc{position:relative;margin:0;margin-bottom:30px;padding:25px;padding-left:56px;font-size:28.8px;font-size:1.8rem;color:#777;background-color:rgba(0,0,0,.05);box-sizing:border-box}sr-rd-desc:before{content:"\201C";position:absolute;top:-28px;left:16px;font-size:80px;font-family:Arial;color:rgba(0,0,0,.15)}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#363636;font-weight:400;line-height:1.8}sr-rd-content b *,sr-rd-content strong,sr-rd-content strong * sr-rd-content b{-webkit-animation:none 0s ease 0s 1 normal none running;animation:none 0s ease 0s 1 normal none running;-webkit-backface-visibility:visible;backface-visibility:visible;background:transparent none repeat 0 0/auto auto padding-box border-box scroll;border:medium none currentColor;border-collapse:separate;-o-border-image:none;border-image:none;border-radius:0;border-spacing:0;bottom:auto;box-shadow:none;box-sizing:content-box;caption-side:top;clear:none;clip:auto;color:#000;-webkit-columns:auto;-moz-columns:auto;columns:auto;-webkit-column-count:auto;-moz-column-count:auto;column-count:auto;-webkit-column-fill:balance;-moz-column-fill:balance;column-fill:balance;-webkit-column-gap:normal;-moz-column-gap:normal;column-gap:normal;-webkit-column-rule:medium none currentColor;-moz-column-rule:medium none currentColor;column-rule:medium none currentColor;-webkit-column-span:1;-moz-column-span:1;column-span:1;-webkit-column-width:auto;-moz-column-width:auto;column-width:auto;content:normal;counter-increment:none;counter-reset:none;cursor:auto;direction:ltr;display:inline;empty-cells:show;float:none;font-family:serif;font-size:medium;font-style:normal;font-variant:normal;font-weight:400;font-stretch:normal;line-height:normal;height:auto;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;left:auto;letter-spacing:normal;list-style:disc outside none;margin:0;max-height:none;max-width:none;min-height:0;min-width:0;opacity:1;orphans:2;outline:medium none invert;overflow:visible;overflow-x:visible;overflow-y:visible;padding:0;page-break-after:auto;page-break-before:auto;page-break-inside:auto;-webkit-perspective:none;perspective:none;-webkit-perspective-origin:50% 50%;perspective-origin:50% 50%;position:static;right:auto;-moz-tab-size:8;-o-tab-size:8;tab-size:8;table-layout:auto;text-align:left;text-align-last:auto;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;top:auto;-webkit-transform:none;transform:none;-webkit-transform-origin:50% 50% 0;transform-origin:50% 50% 0;-webkit-transform-style:flat;transform-style:flat;-webkit-transition:none 0s ease 0s;transition:none 0s ease 0s;unicode-bidi:normal;vertical-align:baseline;visibility:visible;white-space:normal;widows:2;width:auto;word-spacing:normal;z-index:auto;all:initial}sr-rd-content a,sr-rd-content a:link{color:#4183c4;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#4183c4;text-decoration:underline}sr-rd-content pre{background-color:#f7f7f7;border-radius:3px}sr-rd-content li code,sr-rd-content p code{background-color:rgba(0,0,0,.04);border-radius:3px}.simpread-multi-root{background:#f8f9fa}</style><style type="text/css">sr-rd-theme-newsprint{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-weight:700}sr-rd-content h1{font-size:48px;font-size:3rem;line-height:1.6em;margin-top:2em}sr-rd-content h2,sr-rd-content h3{font-size:32px;font-size:2rem;line-height:1.15;margin-top:2.285714em;margin-bottom:1.15em}sr-rd-content h3{font-weight:400}sr-rd-content h4{font-size:28.8px;font-size:1.8rem;margin-top:2.67em}sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h1{border-bottom:1px solid;margin-bottom:1.875em;padding-bottom:.8125em}sr-rd-content ol,sr-rd-content ul{margin:0 0 1.5em 1.5em}sr-rd-content ol li{list-style-type:decimal;list-style-position:outside}sr-rd-content ul li{list-style-type:disc;list-style-position:outside}sr-rd-content table{width:100%;margin-bottom:1.5em;font-size:25.6px;font-size:1.6rem}sr-rd-content thead th,tfoot th{padding:.25em .25em .25em .4em;text-transform:uppercase}sr-rd-content th{text-align:left}sr-rd-content td{vertical-align:top;padding:.25em .25em .25em .4em}sr-rd-content thead{background-color:#dadada}sr-rd-content tr:nth-child(2n){background:#e8e7e7}sr-rd-content sr-blockquote{padding:10px 15px;border-left-style:solid;border-left-width:10px;border-color:#d6dbdf;background:none repeat scroll 0 0 rgba(102,128,153,.05);text-align:left}sr-rd-content sr-blockquote:before{content:""}.simpread-multi-root,.simpread-theme-root{background-color:#f3f2ee;color:#2c3e50}sr-rd-title{font-family:PingFang SC,Hiragino Sans GB,Microsoft Yahei,WenQuanYi Micro Hei,sans-serif;line-height:1.5;font-weight:500;font-size:48px;font-size:3rem;color:#07b;border-bottom:1px solid;margin-bottom:1.875em;padding-bottom:.8125em}sr-rd-desc{color:rgba(102,128,153,.6);background-color:rgba(102,128,153,.075);border-radius:4px;margin-bottom:1em;padding:15px;font-size:32px;font-size:2rem;line-height:1.5;text-align:center}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{line-height:1.8;color:#2c3e50}sr-rd-content a,sr-rd-content a:link{color:#08c;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#5ba4e5}sr-rd-content li code,sr-rd-content p code,sr-rd-content pre{background-color:#dadada}sr-rd-mult{background-color:rgba(102,128,153,.075)}</style><style type="text/css">sr-rd-theme-gothic{display:none}sr-rd-content h1{line-height:64px;line-height:4rem;margin:64px 0 28px;margin:4rem 0 1.75rem;padding:20px 30px}sr-rd-content h1,sr-rd-content h2{font-weight:400;text-align:center;text-transform:uppercase}sr-rd-content h2{line-height:48px;line-height:3rem;margin:48px 0 31px;margin:3rem 0 1.9375rem;padding:0 30px}sr-rd-content h3,sr-rd-content h4,sr-rd-content h5{font-weight:400}sr-rd-content h6{font-weight:700}sr-rd-content h1{font-size:57.6px;font-size:3.6rem}sr-rd-content h2{font-size:51.2px;font-size:3.2rem}sr-rd-content h3{font-size:40px;font-size:2.5rem}sr-rd-content h4{font-size:35.2px;font-size:2.2rem}sr-rd-content h5{font-size:30.4px;font-size:1.9rem}sr-rd-content h6{font-size:27.2px;font-size:1.7rem}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{margin-top:1.2em;margin-bottom:.6em;color:#111}sr-rd-content ol,sr-rd-content ul{list-style-type:disc;margin-left:3em}sr-rd-content ol ol,sr-rd-content ul ol{list-style-type:lower-roman}sr-rd-content ol ol ol,sr-rd-content ol ul ol,sr-rd-content ul ol ol,sr-rd-content ul ul ol{list-style-type:lower-alpha}sr-rd-content table{margin-bottom:20px}sr-rd-content table td,sr-rd-content table th{padding:8px;line-height:20px;line-height:1.25rem;vertical-align:top;border-top:1px solid #ddd}sr-rd-content table th{font-weight:700}sr-rd-content table thead th{vertical-align:bottom}sr-rd-content table caption+thead tr:first-child td,sr-rd-content table caption+thead tr:first-child th,sr-rd-content table colgroup+thead tr:first-child td,sr-rd-content table colgroup+thead tr:first-child th,sr-rd-content table thead:first-child tr:first-child td,sr-rd-content table thead:first-child tr:first-child th{border-top:0}sr-rd-content table tbody+tbody{border-top:2px solid #ddd}sr-rd-content sr-blockquote{margin:0 0 17.777px;margin:0 0 1.11111rem;padding:8px 17.777px 0 16.888px;padding:.5rem 1.11111rem 0 1.05556rem;border-left:1px solid gray}sr-rd-content sr-blockquote,sr-rd-content sr-blockquote p{line-height:2;color:#6f6f6f}.simpread-multi-root,.simpread-theme-root{background:#fcfcfc;color:#333}sr-rd-title{font-weight:400;line-height:64px;line-height:4rem;text-align:center;text-transform:uppercase;color:#111;font-size:51.2px;font-size:3.2rem}sr-rd-desc{margin:0 0 17.777px;margin:0 0 1.11111rem;padding:8px 17.777px 0 16.888px;padding:.5rem 1.11111rem 0 1.05556rem;font-size:32px;font-size:2rem;line-height:2;color:#6f6f6f;border-left:1px solid gray}sr-rd-content{font-weight:400;color:#333}sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#333}sr-rd-content a,sr-rd-content a:link{color:#900;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#900;text-decoration:underline}sr-rd-content li code,sr-rd-content p code,sr-rd-content pre{background-color:transparent;border:1px solid #ccc}sr-rd-mult{background-color:#f2f2f2}</style><style type="text/css">sr-rd-theme-engwrite{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{margin:20px 0 10px;padding:0;font-weight:500;-webkit-font-smoothing:antialiased}sr-rd-content h1{font-weight:300;text-align:center;font-size:44.8px;font-size:2.8rem;color:#933d3f}sr-rd-content h2{font-size:38.4px;font-size:2.4rem;border-bottom:1px solid #ccc;color:#000}sr-rd-content h3{font-size:28.8px;font-size:1.8rem}sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h6{color:#777}sr-rd-content ol,sr-rd-content ul{padding-left:30px}sr-rd-content ol li>:first-child,sr-rd-content ol li ol:first-of-type,sr-rd-content ol li ul:first-of-type,sr-rd-content ul li>:first-child,sr-rd-content ul li ol:first-of-type,sr-rd-content ul li ul:first-of-type{margin-top:0}sr-rd-content ol ol,sr-rd-content ol ul,sr-rd-content ul ol,sr-rd-content ul ul{margin-bottom:0}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{border:1px solid #ccc;padding:6px 13px}sr-rd-content table tr{border-top:1px solid #ccc;background-color:#fff}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{text-align:left;border-top:1px dotted #cdc7bc;border-bottom:1px dotted #cdc7bc;background-color:#f8edda;color:#777}sr-blockquote>:first-child{margin-top:0}sr-blockquote>:last-child{margin-bottom:0}.simpread-multi-root,.simpread-theme-root{background-color:#fcf5ed;color:#333}sr-rd-title{font-weight:300;text-align:center;font-size:44.8px;font-size:2.8rem;color:#933d3f}sr-rd-desc{padding:10px;background-color:#f8edda;color:#777;font-size:32px;font-size:2rem;text-align:center;border-top:1px dotted #cdc7bc;border-bottom:1px dotted #cdc7bc}sr-rd-content{padding:20px 0;margin:0 auto}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#333;line-height:1.8}sr-rd-content a,sr-rd-content a:link{color:#ae3737;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{text-decoration:underline}sr-rd-content pre{background-color:transparent;border:1px solid #ccc;border-radius:3px}sr-rd-content li code,sr-rd-content p code{border:1px solid #eaeaea;background-color:#f4ece3;border-radius:3px}sr-rd-mult{background-color:#f8edda}</style><style type="text/css">sr-rd-theme-octopress{display:none}sr-rd-content h1{font-size:56.32px;font-size:3.52rem;line-height:30.72px;line-height:1.92rem}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{text-rendering:optimizelegibility;margin-bottom:20.8px;margin-bottom:1.3rem;font-weight:700}sr-rd-content h2{font-size:38.4px;font-size:2.4rem}sr-rd-content h3{font-size:33.28px;font-size:2.08rem}sr-rd-content h4{font-size:28.8px;font-size:1.8rem}sr-rd-content h5,sr-rd-content h6{font-size:25.6px;font-size:1.6rem}sr-rd-content h1,sr-rd-content h2{padding-top:27.2px;padding-top:1.7rem;padding-bottom:19.2px;padding-bottom:1.2rem;background:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAABCAYAAACsXeyTAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAFUlEQVQIHWNIS0sr/v//PwMMDzY+ADqMahlW4J91AAAAAElFTkSuQmCC") 0 100% repeat-x}sr-rd-content h2{padding-top:20.8px;padding-top:1.3rem;padding-bottom:0}sr-rd-content ul{list-style-type:disc}sr-rd-content ul ul{list-style-type:circle;margin-bottom:0}sr-rd-content ul ul ul{list-style-type:square;margin-bottom:0}sr-rd-content ol{list-style-type:decimal}sr-rd-content ol ol{list-style-type:lower-alpha;margin-bottom:0}sr-rd-content ol ol ol{list-style-type:lower-roman;margin-bottom:0}sr-rd-content ol,sr-rd-content ol ol,sr-rd-content ol ul,sr-rd-content ul,sr-rd-content ul ol,sr-rd-content ul ul{margin-left:1.3em}sr-rd-content ol ol,sr-rd-content ol ul,sr-rd-content ul ol,sr-rd-content ul ul{margin-bottom:0}sr-rd-content table{width:100%;overflow:auto;word-break:normal;word-break:keep-all}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{padding:6px 13px;border:1px solid #ddd}sr-rd-content table tr{background-color:#fff;border-top:1px solid #ccc}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{font-style:italic;font-size:inherit;line-height:2;padding-left:1em;border-left:4px solid hsla(0,0%,67%,.5)}.simpread-multi-root,.simpread-theme-root{background:#f8f8f8 url("data:image/png;base64,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") 0 0;color:#333}sr-rd-title{font-size:56.32px;font-size:3.52rem;line-height:64px;line-height:4rem;font-weight:700;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAABCAYAAACsXeyTAAAACXBIW…sTAAALEwEAmpwYAAAAFUlEQVQIHWNIS0sr/v//PwMMDzY+ADqMahlW4J91AAAAAElFTkSuQmCC) 0 100% repeat-x}sr-rd-desc{font-style:italic;font-size:30.72px;font-size:1.92rem;line-height:2;padding-left:1em;border-left:4px solid hsla(0,0%,67%,.5)}sr-rd-content{margin:0 auto;padding:1em 0}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{line-height:2;color:#333}sr-rd-content a,sr-rd-content a:link{color:#1863a1;text-decoration:underline}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#0181eb;text-decoration:underline}sr-rd-content pre{color:#586e75;background-color:#fdf6e3;border-radius:.4em;border:1px solid #e7dec3}sr-rd-content li code,sr-rd-content p code{color:#555;background-color:transparent;border:1px solid #ddd}sr-rd-mult{background-color:#ededed}</style><style type="text/css">sr-rd-theme-pixyii{display:none}sr-rd-content h1,sr-rd-content h1 *,sr-rd-content h2,sr-rd-content h2 *,sr-rd-content h3,sr-rd-content h3 *,sr-rd-content h4,sr-rd-content h4 *,sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{color:inherit;font-weight:900;line-height:1.2;margin:1em 0}sr-rd-content h1,sr-rd-content h1 *{font-size:62.72px;font-size:3.92rem}sr-rd-content h2,sr-rd-content h2 *{font-size:58.24px;font-size:3.64rem}sr-rd-content h3,sr-rd-content h3 *{font-size:36.4px;font-size:2.275rem}sr-rd-content h4,sr-rd-content h4 *{font-size:29.12px;font-size:1.82rem}sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{font-size:25.168px;font-size:1.573rem}sr-rd-content ol,sr-rd-content ul{font-size:28px;font-size:1.75rem;line-height:24px;line-height:1.5rem}sr-rd-content li{font-size:25.2px;font-size:1.575rem;line-height:1.8;margin:0;position:relative}sr-rd-content table{width:100%;font-size:25.2px;font-size:1.575rem}sr-rd-content table>tbody>tr>td,sr-rd-content table>tbody>tr>th,sr-rd-content table>tfoot>tr>td,sr-rd-content table>tfoot>tr>th,sr-rd-content table>thead>tr>td,sr-rd-content table>thead>tr>th{padding:12px;line-height:1.2;vertical-align:top;border-top:1px solid #333}sr-rd-content table>thead>tr>th{vertical-align:bottom;border-bottom:2px solid #333}sr-rd-content table>caption+thead>tr:first-child>td,sr-rd-content table>caption+thead>tr:first-child>th,sr-rd-content table>colgroup+thead>tr:first-child>td,sr-rd-content table>colgroup+thead>tr:first-child>th,sr-rd-content table>thead:first-child>tr:first-child>td,sr-rd-content table>thead:first-child>tr:first-child>th{border-top:0}sr-rd-content table>tbody+tbody{border-top:2px solid #333}sr-rd-content sr-blockquote{margin:16px 0;margin:1rem 0;padding:1.33em;font-style:italic;border-left:5px solid #7a7a7a;color:#555}.simpread-theme-root{background-color:#fff;color:#555}sr-rd-title{font-family:PingFang SC,Hiragino Sans GB,Microsoft Yahei,WenQuanYi Micro Hei,sans-serif;font-size:67.2px;font-size:4.2rem;font-weight:900;line-height:1.2}sr-rd-desc{margin:16px 0;margin:1rem 0;padding:1.33em;font-style:italic;font-size:32px;font-size:2rem;line-height:2;border-left:5px solid #7a7a7a;color:#555}sr-rd-content{font-size:33.6px;font-size:2.1rem;line-height:1.8;font-weight:400;color:#555}sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#555;font-size:28px;font-size:1.75rem;line-height:1.8;font-weight:300}sr-rd-content b,sr-rd-content b *,sr-rd-content strong,sr-rd-content strong *{font-weight:700}sr-rd-content a,sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover,sr-rd-content a:link{color:#463f5c;text-decoration:underline}sr-rd-content sr-blockquote code{font-size:inherit}sr-rd-content pre{border:1px solid #7a7a7a}sr-rd-content li code,sr-rd-content p code,sr-rd-content pre{color:#7a7a7a;background-color:transparent}.simpread-multi-root{background:#f8f9fa}</style><style type="text/css">sr-rd-theme-monospace{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-weight:700;color:#6363ac}sr-rd-content h1{font-size:35.2px;font-size:2.2rem}sr-rd-content h2{font-size:32px;font-size:2rem}sr-rd-content h3{font-size:28.8px;font-size:1.8rem}sr-rd-content h4{font-size:25.6px;font-size:1.6rem}sr-rd-content h5{font-size:22.4px;font-size:1.4rem}sr-rd-content h6{font-size:20.8px;font-size:1.3rem}sr-rd-content strong{color:#b5302e}sr-rd-content em{font-style:italic;color:#400469}sr-rd-content ol,sr-rd-content ul{list-style-type:none}sr-rd-content ol li,sr-rd-content ul li{margin:0}sr-rd-content table{line-height:25.6px;line-height:1.6rem;border-collapse:collapse;border-spacing:0;empty-cells:show;border:1px solid #cbcbcb}sr-rd-content thead{background-color:#e0e0e0;color:#000;text-align:left;vertical-align:bottom}sr-rd-content td,sr-rd-content th{border-left:1px solid #cbcbcb;border-width:0 0 0 1px;margin:0;overflow:visible;padding:.5em 1em}sr-rd-content sr-blockquote{background-color:hsla(0,0%,50%,.05);border-top-right-radius:5px;border-bottom-right-radius:5px;border-left:8px solid #979797;line-height:2}sr-rd-content sr-blockquote *{line-height:inherit}.simpread-theme-root{color:#333;background:#fff}sr-rd-title{font-size:44.8px;font-size:2.8rem;line-height:1.2;font-weight:700;color:#6363ac}sr-rd-desc{padding:10px;background-color:hsla(0,0%,50%,.05);font-size:28.8px;font-size:1.8rem;text-align:center;border-top-right-radius:5px;border-bottom-right-radius:5px;border-left:8px solid #979797}sr-rd-content{color:#333}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{line-height:1.7}sr-rd-content a,sr-rd-content a:link{color:#005dad;text-decoration:underline}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#fff;background-color:#2a6496;text-decoration:none}sr-rd-content pre{color:#e9eded;background-color:#263238}sr-rd-content li code,sr-rd-content p code{color:#949415;background-color:transparent}.simpread-multi-root{background:#f8f9fa}</style><style type="text/css">sr-rd-theme-night{display:none}sr-rd-content h1{margin-top:2em}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{color:#dedede;font-weight:400;clear:both;-ms-word-wrap:break-word;word-wrap:break-word;margin:0;padding:0}sr-rd-content h1{font-size:57.6px;font-size:3.6rem;line-height:64px;line-height:4rem;margin-bottom:38.4px;margin-bottom:2.4rem;letter-spacing:-1.5px}sr-rd-content h2{font-size:38.4px;font-size:2.4rem;line-height:48px;line-height:3rem}sr-rd-content h2,sr-rd-content h3{margin-bottom:38.4px;margin-bottom:2.4rem;letter-spacing:-1px}sr-rd-content h3{font-size:28.8px;font-size:1.8rem;line-height:38.4px;line-height:2.4rem}sr-rd-content h4{font-size:25.6px;font-size:1.6rem;line-height:35.2px;line-height:2.2rem;margin-bottom:38.4px;margin-bottom:2.4rem}sr-rd-content h5{font-size:16px;font-size:1rem;line-height:20px;line-height:1.25rem;margin-bottom:24px;margin-bottom:1.5rem}sr-rd-content h6{font-size:25.6px;font-size:1.6rem;line-height:25.6px;line-height:1.6rem;margin-bottom:12px;margin-bottom:.75rem;font-weight:700}sr-rd-content ol,sr-rd-content ul{padding:0 0 0 30px;padding:0 0 0 1.875rem}sr-rd-content ul{list-style:square}sr-rd-content ol{list-style:decimal}sr-rd-content ol ol,sr-rd-content ol ul,sr-rd-content ul ol,sr-rd-content ul ul{margin:0}sr-rd-content li div{padding-top:0}sr-rd-content li,sr-rd-content li p{margin:0;position:relative}sr-rd-content table{margin-top:0;margin-bottom:24px;margin-bottom:1.5rem;border-collapse:collapse;border-spacing:0;page-break-inside:auto;text-align:left}sr-rd-content table a{color:#dedede}sr-rd-content thead{display:table-header-group}sr-rd-content table td,sr-rd-content table th{border:1px solid #474d54}sr-rd-content sr-blockquote{margin:0 0 30px 30px;margin:0 0 1.875rem 1.875rem;border-left:2px solid #474d54;padding-left:30px;margin-top:35px;line-height:2}.simpread-multi-root,.simpread-theme-root{background:#363b40;color:#b8bfc6}sr-rd-title{color:#dedede;font-size:50.4px;font-size:3.15rem;line-height:56px;line-height:3.5rem;letter-spacing:-1.5px}sr-rd-desc{margin:35px;margin-left:0;padding-left:30px;padding-left:1.875rem;font-size:32px;font-size:2rem;line-height:2;border-left:2px solid #474d54}sr-rd-content,sr-rd-desc{color:#b8bfc6}sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#b8bfc6;margin-top:0;line-height:2}sr-rd-content a,sr-rd-content a:link{color:#e0e0e0;text-decoration:underline;-webkit-transition:all .2s ease-in-out;transition:all .2s ease-in-out}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#fff}sr-rd-content pre{background-color:transparent;border:1px solid}sr-rd-content li code,sr-rd-content p code{background:rgba(0,0,0,.05)}sr-rd-mult{background-color:#2d3034}panel{background-color:#2e2e2e!important}panel panel-tab span{color:#fff}panel sr-opt-gp sr-opt-label{color:rgba(108,255,240,.8);font-weight:400}panel text-field-float{color:rgba(108,255,240,.8)!important;font-weight:400!important}panel list-field{background-color:transparent!important}panel list-field:hover list-field-name{color:#fff!important;font-weight:700}panel input,panel list-field-name{color:hsla(35,10%,76%,.87)!important}panel list-view{background-color:#2e2e2e!important}panel-tabs{border-bottom-color:#393d40!important}sr-annote{color:#333}</style><style type="text/css">sr-rd-theme-dark{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-weight:700;color:#dbdbfd}sr-rd-content h1{font-size:48px;font-size:3rem}sr-rd-content h2{font-size:44.8px;font-size:2.8rem}sr-rd-content h3{font-size:40px;font-size:2.5rem}sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{color:#549ad8}sr-rd-content h5{font-size:32px;font-size:2rem}sr-rd-content h6{font-size:28.8px;font-size:1.8rem}sr-rd-content strong{color:#ffffc5}sr-rd-content em{color:#c885f5}sr-rd-content table{width:100%;line-height:25.6px;line-height:1.6rem;border-collapse:collapse;border-spacing:0;empty-cells:show;border:1px solid #cbcbcb}sr-rd-content thead{background-color:#263238;color:#f5f5f5;text-align:left;vertical-align:bottom}sr-rd-content table td,sr-rd-content table th{border-left:1px solid #cbcbcb;border-width:0 0 0 1px;margin:0;overflow:visible;padding:.5em 1em}sr-rd-content sr-blockquote{background-color:hsla(0,0%,50%,.05);border-top-right-radius:5px;border-bottom-right-radius:5px;border-left:8px solid #979797;color:#ebebeb}.simpread-multi-root,.simpread-theme-root{color:#ebebeb;background:#222}sr-rd-title{padding-bottom:.3em;font-size:44.8px;font-size:2.8rem;font-weight:700;line-height:1.2;color:#dbdbfd;border-bottom:1px solid #eee}sr-rd-desc{margin:20px;margin-left:0;padding:5px 20px;font-size:28.8px;font-size:1.8rem;background-color:hsla(0,0%,50%,.05);color:#ebebeb;border-top-right-radius:5px;border-bottom-right-radius:5px;border-left:8px solid #979797}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{line-height:1.7;color:#ebebeb}sr-rd-content a,sr-rd-content a:link{color:#8ac9ff;text-decoration:underline}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{background-color:#2a6496;color:#fff;text-decoration:none}sr-rd-content pre{color:#e9eded;background-color:#263238}sr-rd-content li code,sr-rd-content p code{color:#caca16;background-color:transparent}sr-rd-mult{background-color:hsla(0,0%,50%,.1)}panel{background-color:#222!important}panel panel-tab span{color:#fff}panel sr-opt-gp sr-opt-label{color:rgba(108,255,240,.8);font-weight:400}panel text-field-float{color:rgba(108,255,240,.8)!important;font-weight:400!important}panel list-field{background-color:transparent!important}panel list-field:hover list-field-name{color:#fff!important;font-weight:700}panel input,panel list-field-name{color:hsla(35,10%,76%,.87)!important}panel list-view{background-color:#222!important}panel text-field input{color:rgba(108,255,240,.8)!important}panel-tabs{border-bottom-color:#393d40!important}sr-annote{color:#333}</style><style type="text/css">sr-rd-theme-mail{display:none}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{position:relative;margin-top:1em;margin-bottom:1pc;font-weight:700;line-height:1.4;text-align:left;color:#363636}sr-rd-content h1{padding-bottom:.3em;font-size:36p;line-height:1.2}sr-rd-content h2{padding-bottom:.3em;font-size:28p;line-height:1.225}sr-rd-content h3{font-size:24p;line-height:1.43}sr-rd-content h4{font-size:2p}sr-rd-content h5{font-size:16px}sr-rd-content h6{font-size:16px;color:#777}sr-rd-content ol,sr-rd-content ul{list-style-type:disc;padding:0;padding-left:2em}sr-rd-content ol ol,sr-rd-content ul ol{list-style-type:lower-roman}sr-rd-content ol ol ol,sr-rd-content ol ul ol,sr-rd-content ul ol ol,sr-rd-content ul ul ol{list-style-type:lower-alpha}sr-rd-content table{width:100%;overflow:auto;word-break:normal;word-break:keep-all}sr-rd-content table th{font-weight:700}sr-rd-content table td,sr-rd-content table th{padding:6px 13px;border:1px solid #ddd}sr-rd-content table tr{background-color:#fff;border-top:1px solid #ccc}sr-rd-content table tr:nth-child(2n){background-color:#f8f8f8}sr-rd-content sr-blockquote{border-left:4px solid #ddd}.simpread-theme-root{background-color:#fff;color:#333}.sr-header{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:left;-ms-flex-pack:left;justify-content:left;width:100%;margin:10px 0;height:41px;border-bottom:1px solid #e0e0e0;padding-bottom:10px}.sr-header,.sr-header a{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:#195bf7}.sr-header a{display:-webkit-box;display:-ms-flexbox;display:flex;text-decoration:none}.sr-header .sr-name{height:41px;line-height:41px;font-size:20px;font-weight:700;text-decoration:none}.sr-header .sr-logo{display:block;width:41px;height:41px;background-repeat:no-repeat;background-position:50%;background-image:url(https://simpread-1254315611.file.myqcloud.com/favicon/favicon-32x32.png);margin-right:5px}.sr-header .sr-slogan{height:41px;line-height:44px;font-weight:700;font-size:15px}.sr-rd-footer{font-size:14px;text-align:center;color:#363636}.sr-rd-footer-group{display:-webkit-box;display:-ms-flexbox;display:flex;height:20px}.sr-rd-footer-line{width:100%;border-top:1px solid #e0e0e0}.sr-rd-footer-text{min-width:150px;line-height:0;text-align:center}.sr-rd-footer-copywrite{margin:10px 0 0;color:#363636}.sr-rd-footer-copywrite abbr{-webkit-font-feature-settings:normal;font-feature-settings:normal;font-variant:normal;text-decoration:none}.sr-rd-footer-copywrite .second{margin:10px 0}.sr-rd-footer-copywrite .third a:hover{border:none!important}.sr-rd-footer-copywrite .third a:first-child{margin-right:50px}.sr-rd-footer-copywrite .sr-icon{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:33px;height:33px;opacity:.8;-webkit-transition:opacity .5s ease;transition:opacity .5s ease;cursor:pointer}.sr-rd-footer-copywrite .sr-icon:hover{opacity:1}.sr-rd-footer-copywrite a,.sr-rd-footer-copywrite a:link,.sr-rd-footer-copywrite a:visited{margin:0;padding:0;color:inherit;background-color:transparent;font-size:inherit!important;line-height:normal;text-decoration:none;vertical-align:baseline;vertical-align:initial;border:none!important;box-sizing:border-box}.sr-rd-footer-copywrite a:focus,.sr-rd-footer-copywrite a:hover,.sr-rd-footer a:active{color:inherit;text-decoration:none;border-bottom:1px dotted!important}.sr-rd-content-desc{margin:0;padding:0 0 0 1em;color:#363636;line-height:2;font-size:18px;border-left:4px solid hsla(0,0%,67%,.5)}sr-rd-content{font-size:16px;line-height:1.6}sr-rd-content h1,sr-rd-content h1 *,sr-rd-content h2,sr-rd-content h2 *,sr-rd-content h3,sr-rd-content h3 *,sr-rd-content h4,sr-rd-content h4 *,sr-rd-content h5,sr-rd-content h5 *,sr-rd-content h6,sr-rd-content h6 *{word-break:break-all}sr-rd-content div,sr-rd-content p{display:block;float:inherit;line-height:1.6;font-size:16px}sr-rd-content,sr-rd-content *,sr-rd-content div,sr-rd-content p{color:#363636;font-weight:400;line-height:1.8}sr-rd-content strong,sr-rd-content strong *{-webkit-animation:none 0s ease 0s 1 normal none running;animation:none 0s ease 0s 1 normal none running;-webkit-backface-visibility:visible;backface-visibility:visible;background:transparent none repeat 0 0/auto auto padding-box border-box scroll;border:medium none currentColor;border-collapse:separate;-o-border-image:none;border-image:none;border-radius:0;border-spacing:0;bottom:auto;box-shadow:none;box-sizing:content-box;caption-side:top;clear:none;clip:auto;color:#000;-webkit-columns:auto;-moz-columns:auto;columns:auto;-webkit-column-count:auto;-moz-column-count:auto;column-count:auto;-webkit-column-fill:balance;-moz-column-fill:balance;column-fill:balance;-webkit-column-gap:normal;-moz-column-gap:normal;column-gap:normal;-webkit-column-rule:medium none currentColor;-moz-column-rule:medium none currentColor;column-rule:medium none currentColor;-webkit-column-span:1;-moz-column-span:1;column-span:1;-webkit-column-width:auto;-moz-column-width:auto;column-width:auto;content:normal;counter-increment:none;counter-reset:none;cursor:auto;direction:ltr;display:inline;empty-cells:show;float:none;font-family:serif;font-size:medium;font-style:normal;font-variant:normal;font-weight:400;font-stretch:normal;line-height:normal;height:auto;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;left:auto;letter-spacing:normal;list-style:disc outside none;margin:0;max-height:none;max-width:none;min-height:0;min-width:0;opacity:1;orphans:2;outline:medium none invert;overflow:visible;overflow-x:visible;overflow-y:visible;padding:0;page-break-after:auto;page-break-before:auto;page-break-inside:auto;-webkit-perspective:none;perspective:none;-webkit-perspective-origin:50% 50%;perspective-origin:50% 50%;position:static;right:auto;-moz-tab-size:8;-o-tab-size:8;tab-size:8;table-layout:auto;text-align:left;text-align-last:auto;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;top:auto;-webkit-transform:none;transform:none;-webkit-transform-origin:50% 50% 0;transform-origin:50% 50% 0;-webkit-transform-style:flat;transform-style:flat;-webkit-transition:none 0s ease 0s;transition:none 0s ease 0s;unicode-bidi:normal;vertical-align:baseline;visibility:visible;white-space:normal;widows:2;width:auto;word-spacing:normal;z-index:auto;all:initial}sr-rd-content a,sr-rd-content a:link{color:#4183c4;text-decoration:none}sr-rd-content a:active,sr-rd-content a:focus,sr-rd-content a:hover{color:#4183c4;text-decoration:underline}sr-rd-content figure{margin:0;padding:0}sr-rd-content img{display:inline-block;padding:0;height:auto;line-height:100%;max-width:50%;text-decoration:none;vertical-align:text-bottom;border-radius:10px;outline:none}sr-rd-content pre{background-color:#f7f7f7;border-radius:3px}sr-rd-content pre *{font-size:1.1px}sr-rd-content li code,sr-rd-content p code{background-color:rgba(0,0,0,.04);border-radius:3px}.simpread-multi-root{background:#f8f9fa}.sr-rd-mult{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin:0 0 16px;padding:16px 0 24px;width:100%;background-color:#fff;border-bottom:1px solid #e0e0e0}.sr-rd-mult .sr-rd-mult-content{padding:0 16px;overflow:auto}.sr-rd-mult .sr-rd-mult-avatar{margin:0 15px}.sr-rd-mult .sr-rd-mult-avatar span{display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;max-width:75px;overflow:hidden;text-overflow:ellipsis;text-align:left;font-size:16px;font-size:1rem}.sr-rd-mult .sr-rd-mult-avatar img{margin-bottom:0;max-width:50px;max-height:50px;width:50px;height:50px;border-radius:50%}.sr-rd-mult .sr-rd-mult-avatar .sr-rd-content-center{margin:0}sr-rd-content.embed *{font-size:medium}sr-rd-content.embed img{max-width:100%}sr-rd-content.embed a,sr-rd-content.embed a:hover{color:inherit;font-size:medium}sr-rd-content.embed a:hover{background-color:inherit}sr-rd-content.embed .MathJax_Processed,sr-rd-content.embed math{display:none}sr-rd-content.embed pre{color:#000;color:initial;background-color:transparent}sr-rd-content.embed pre,sr-rd-content.embed pre *{font-size:13px!important}</style><style type="text/css">@media (pointer:coarse){sr-read{margin:20px 5%!important;min-width:0!important;max-width:90%!important}sr-rd-title{margin-top:0;font-size:2.7rem}sr-rd-content sr-blockquote,sr-rd-desc{margin:10 0!important;padding:0 0 0 10px!important;width:90%;font-size:1.8rem;font-style:normal;line-height:1.7;text-align:justify}sr-rd-content{font-size:1.75rem;font-weight:300}sr-rd-content figure{margin:0;padding:0;text-align:center}sr-rd-content a,sr-rd-content a:link,sr-rd-content li code,sr-rd-content p code{font-size:inherit}sr-rd-footer{margin-top:20px}sr-blockquote,sr-blockquote *{margin:5px!important;padding:5px!important}sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6,sr-rd-title{font-family:PingFang SC,Verdana,Helvetica Neue,Microsoft Yahei,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#000;font-weight:100;line-height:1.35}sr-rd-content-h1,sr-rd-content-h2,sr-rd-content-h3,sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h1,sr-rd-content h2,sr-rd-content h3,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{margin-top:1.2em;margin-bottom:.6em;line-height:1.35}sr-rd-content-h1,sr-rd-content h1{font-size:1.8em}sr-rd-content-h2,sr-rd-content h2{font-size:1.6em}sr-rd-content-h3,sr-rd-content h3{font-size:1.4em}sr-rd-content-h4,sr-rd-content-h5,sr-rd-content-h6,sr-rd-content h4,sr-rd-content h5,sr-rd-content h6{font-size:1.2em}sr-rd-content-ul,sr-rd-content ul{margin-left:1.3em!important;list-style:disc}sr-rd-content-ol,sr-rd-content ol{list-style:decimal;margin-left:1.9em!important}sr-rd-content-ol ol,sr-rd-content-ol ul,sr-rd-content-ul ol,sr-rd-content-ul ul,sr-rd-content li ol,sr-rd-content li ul{margin-bottom:.8em;margin-left:2em!important}sr-rd-content img{margin:0;padding:0;border:0;max-width:100%!important;height:auto;box-shadow:0 20px 20px -10px rgba(0,0,0,.1)}sr-rd-mult{min-width:0;background-color:#fff;box-shadow:0 1px 6px rgba(32,33,36,.28);border-radius:8px}sr-rd-mult sr-rd-mult-avatar div{margin:0}sr-rd-mult sr-rd-mult-avatar .sr-rd-content-center-small{margin:7px 0!important}sr-rd-mult sr-rd-mult-avatar span{display:block}sr-rd-mult sr-rd-mult-content{padding-left:0}@media only screen and (max-device-width:1024px){.simpread-theme-root,html.simpread-theme-root{font-size:80%!important}sr-rd-mult sr-rd-mult-avatar img{width:50px;height:50px;min-width:50px;min-height:50px}}@media only screen and (max-device-width:414px){.simpread-theme-root,html.simpread-theme-root{font-size:70%!important}sr-rd-mult sr-rd-mult-avatar img{width:30px;height:30px;min-width:30px;min-height:30px}}@media only screen and (max-device-width:320px){.simpread-theme-root,html.simpread-theme-root{font-size:90%!important}sr-rd-content p{margin-bottom:.5em}}}</style><style type="text/css">button[aria-label][data-balloon-pos]{overflow:visible}[aria-label][data-balloon-pos]{position:relative;cursor:pointer}[aria-label][data-balloon-pos]:after{text-indent:0;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;font-weight:400;font-style:normal;text-shadow:none;font-size:12px;background:hsla(0,0%,6%,.95);border-radius:2px;color:#fff;content:attr(aria-label);padding:.5em 1em;white-space:nowrap}[aria-label][data-balloon-pos]:after,[aria-label][data-balloon-pos]:before{opacity:0;pointer-events:none;-webkit-transition:all .18s ease-out .18s;transition:all .18s ease-out .18s;position:absolute;z-index:10}[aria-label][data-balloon-pos]:before{width:0;height:0;border:5px solid transparent;border-top-color:hsla(0,0%,6%,.95);content:""}[aria-label][data-balloon-pos]:hover:after,[aria-label][data-balloon-pos]:hover:before,[aria-label][data-balloon-pos]:not([data-balloon-nofocus]):focus:after,[aria-label][data-balloon-pos]:not([data-balloon-nofocus]):focus:before,[aria-label][data-balloon-pos][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-visible]:before{opacity:1;pointer-events:none}[aria-label][data-balloon-pos].font-awesome:after{font-family:FontAwesome,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif}[aria-label][data-balloon-pos][data-balloon-break]:after{white-space:pre}[aria-label][data-balloon-pos][data-balloon-break][data-balloon-length]:after{white-space:pre-line;word-break:break-word}[aria-label][data-balloon-pos][data-balloon-blunt]:after,[aria-label][data-balloon-pos][data-balloon-blunt]:before{-webkit-transition:none;transition:none}[aria-label][data-balloon-pos][data-balloon-pos=up]:after{margin-bottom:10px}[aria-label][data-balloon-pos][data-balloon-pos=up]:after,[aria-label][data-balloon-pos][data-balloon-pos=up]:before{bottom:100%;left:50%;-webkit-transform:translate(-50%,4px);transform:translate(-50%,4px);-webkit-transform-origin:top;transform-origin:top}[aria-label][data-balloon-pos][data-balloon-pos=up]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=up]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=up][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=up][data-balloon-visible]:before{-webkit-transform:translate(-50%);transform:translate(-50%)}[aria-label][data-balloon-pos][data-balloon-pos=up-left]:after{bottom:100%;left:0;margin-bottom:10px;-webkit-transform:translateY(4px);transform:translateY(4px);-webkit-transform-origin:top;transform-origin:top}[aria-label][data-balloon-pos][data-balloon-pos=up-left]:before{bottom:100%;left:5px;-webkit-transform:translateY(4px);transform:translateY(4px);-webkit-transform-origin:top;transform-origin:top}[aria-label][data-balloon-pos][data-balloon-pos=up-left]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=up-left]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=up-left][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=up-left][data-balloon-visible]:before{-webkit-transform:translate(0);transform:translate(0)}[aria-label][data-balloon-pos][data-balloon-pos=up-right]:after{bottom:100%;right:0;margin-bottom:10px;-webkit-transform:translateY(4px);transform:translateY(4px);-webkit-transform-origin:top;transform-origin:top}[aria-label][data-balloon-pos][data-balloon-pos=up-right]:before{bottom:100%;right:5px;-webkit-transform:translateY(4px);transform:translateY(4px);-webkit-transform-origin:top;transform-origin:top}[aria-label][data-balloon-pos][data-balloon-pos=up-right]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=up-right]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=up-right][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=up-right][data-balloon-visible]:before{-webkit-transform:translate(0);transform:translate(0)}[aria-label][data-balloon-pos][data-balloon-pos=down]:after{left:50%;margin-top:10px;top:100%;-webkit-transform:translate(-50%,-4px);transform:translate(-50%,-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down]:before{width:0;height:0;border:5px solid transparent;border-bottom-color:hsla(0,0%,6%,.95);left:50%;top:100%;-webkit-transform:translate(-50%,-4px);transform:translate(-50%,-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=down]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=down][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=down][data-balloon-visible]:before{-webkit-transform:translate(-50%);transform:translate(-50%)}[aria-label][data-balloon-pos][data-balloon-pos=down-left]:after{left:0;margin-top:10px;top:100%;-webkit-transform:translateY(-4px);transform:translateY(-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down-left]:before{width:0;height:0;border:5px solid transparent;border-bottom-color:hsla(0,0%,6%,.95);left:5px;top:100%;-webkit-transform:translateY(-4px);transform:translateY(-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down-left]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=down-left]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=down-left][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=down-left][data-balloon-visible]:before{-webkit-transform:translate(0);transform:translate(0)}[aria-label][data-balloon-pos][data-balloon-pos=down-right]:after{right:0;margin-top:10px;top:100%;-webkit-transform:translateY(-4px);transform:translateY(-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down-right]:before{width:0;height:0;border:5px solid transparent;border-bottom-color:hsla(0,0%,6%,.95);right:5px;top:100%;-webkit-transform:translateY(-4px);transform:translateY(-4px)}[aria-label][data-balloon-pos][data-balloon-pos=down-right]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=down-right]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=down-right][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=down-right][data-balloon-visible]:before{-webkit-transform:translate(0);transform:translate(0)}[aria-label][data-balloon-pos][data-balloon-pos=left]:after{margin-right:10px;right:100%;top:50%;-webkit-transform:translate(4px,-50%);transform:translate(4px,-50%)}[aria-label][data-balloon-pos][data-balloon-pos=left]:before{width:0;height:0;border:5px solid transparent;border-left-color:hsla(0,0%,6%,.95);right:100%;top:50%;-webkit-transform:translate(4px,-50%);transform:translate(4px,-50%)}[aria-label][data-balloon-pos][data-balloon-pos=left]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=left]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=left][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=left][data-balloon-visible]:before{-webkit-transform:translateY(-50%);transform:translateY(-50%)}[aria-label][data-balloon-pos][data-balloon-pos=right]:after{left:100%;margin-left:10px;top:50%;-webkit-transform:translate(-4px,-50%);transform:translate(-4px,-50%)}[aria-label][data-balloon-pos][data-balloon-pos=right]:before{width:0;height:0;border:5px solid transparent;border-right-color:hsla(0,0%,6%,.95);left:100%;top:50%;-webkit-transform:translate(-4px,-50%);transform:translate(-4px,-50%)}[aria-label][data-balloon-pos][data-balloon-pos=right]:hover:after,[aria-label][data-balloon-pos][data-balloon-pos=right]:hover:before,[aria-label][data-balloon-pos][data-balloon-pos=right][data-balloon-visible]:after,[aria-label][data-balloon-pos][data-balloon-pos=right][data-balloon-visible]:before{-webkit-transform:translateY(-50%);transform:translateY(-50%)}[aria-label][data-balloon-pos][data-balloon-length=small]:after{white-space:normal;width:80px}[aria-label][data-balloon-pos][data-balloon-length=medium]:after{white-space:normal;width:150px}[aria-label][data-balloon-pos][data-balloon-length=large]:after{white-space:normal;width:260px}[aria-label][data-balloon-pos][data-balloon-length=xlarge]:after{white-space:normal;width:380px}@media screen and (max-width:768px){[aria-label][data-balloon-pos][data-balloon-length=xlarge]:after{white-space:normal;width:90vw}}[aria-label][data-balloon-pos]:before{display:none}[aria-label][data-balloon-pos]:after{box-shadow:0 0 10px rgba(0,0,0,.3);border-radius:5px;font-weight:700;font-size:10px}[aria-label][data-balloon-pos][data-balloon-pos=up]:after{line-height:21px}[aria-label][data-balloon-pos][data-balloon-order=downleft]:after{left:120%}[aria-label][data-balloon-pos][data-balloon-order=downright]:after{right:-22px}[aria-label][data-balloon-pos][data-balloon-order=upright]:after{left:10%}</style><style type="text/css">/*!
 * Waves v0.7.5
 * http://fian.my.id/Waves
 * 
 * Copyright 2014-2016 Alfiana E. Sibuea and other contributors
 * Released under the MIT license
 * https://github.com/fians/Waves/blob/master/LICENSE
 */.md-waves-effect{position:relative;cursor:pointer;display:inline-block;overflow:hidden;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;-webkit-tap-highlight-color:transparent}.md-waves-effect .md-waves-ripple{position:absolute;border-radius:50%;width:100px;height:100px;margin-top:-50px;margin-left:-50px;opacity:0;background:rgba(0,0,0,.2);background:-webkit-radial-gradient(rgba(0,0,0,.2) 0,rgba(0,0,0,.3) 40%,rgba(0,0,0,.4) 50%,rgba(0,0,0,.5) 60%,hsla(0,0%,100%,0) 70%);background:radial-gradient(rgba(0,0,0,.2) 0,rgba(0,0,0,.3) 40%,rgba(0,0,0,.4) 50%,rgba(0,0,0,.5) 60%,hsla(0,0%,100%,0) 70%);-webkit-transition:all .5s ease-out;transition:all .5s ease-out;-webkit-transition-property:-webkit-transform,opacity;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transform:scale(0) translate(0);transform:scale(0) translate(0);pointer-events:none}.md-waves-effect.md-waves-light .md-waves-ripple{background:hsla(0,0%,100%,.4);background:-webkit-radial-gradient(hsla(0,0%,100%,.2) 0,hsla(0,0%,100%,.3) 40%,hsla(0,0%,100%,.4) 50%,hsla(0,0%,100%,.5) 60%,hsla(0,0%,100%,0) 70%);background:radial-gradient(hsla(0,0%,100%,.2) 0,hsla(0,0%,100%,.3) 40%,hsla(0,0%,100%,.4) 50%,hsla(0,0%,100%,.5) 60%,hsla(0,0%,100%,0) 70%)}.md-waves-effect.md-waves-classic .md-waves-ripple{background:rgba(0,0,0,.2)}.md-waves-effect.md-waves-classic.md-waves-light .md-waves-ripple{background:hsla(0,0%,100%,.4)}.md-waves-notransition{-webkit-transition:none!important;transition:none!important}.md-waves-button,.md-waves-circle{-webkit-transform:translateZ(0);transform:translateZ(0);-webkit-mask-image:-webkit-radial-gradient(circle,#fff 100%,#000 0)}.md-waves-button,.md-waves-button-input,.md-waves-button:hover,.md-waves-button:visited{white-space:nowrap;vertical-align:middle;cursor:pointer;border:none;outline:none;color:inherit;background-color:transparent;font-size:1em;line-height:1em;text-align:center;text-decoration:none;z-index:1}.md-waves-button{padding:.85em 1.1em;border-radius:.2em}.md-waves-button-input{margin:0;padding:.85em 1.1em}.md-waves-input-wrapper{border-radius:.2em;vertical-align:bottom}.md-waves-input-wrapper.md-waves-button{padding:0}.md-waves-input-wrapper .md-waves-button-input{position:relative;top:0;left:0;z-index:1}.md-waves-circle{text-align:center;width:2.5em;height:2.5em;line-height:2.5em;border-radius:50%}.md-waves-float{-webkit-mask-image:none;box-shadow:0 1px 1.5px 1px rgba(0,0,0,.12);-webkit-transition:all .3s;transition:all .3s}.md-waves-float:active{box-shadow:0 8px 20px 1px rgba(0,0,0,.3)}.md-waves-block{display:block}</style><style type="text/css">.simpread-font{font:300 16px/1.8 -apple-system,PingFang SC,Microsoft Yahei,Lantinghei SC,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;color:#333;text-rendering:optimizelegibility;-webkit-text-size-adjust:100%;-webkit-font-smoothing:antialiased}.simpread-hidden{display:none}.simpread-read-root{display:-webkit-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;margin:0;top:-1000px;left:0;width:100%;z-index:2147483646;overflow-x:hidden;opacity:0;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}.simpread-read-root-show{top:0}.simpread-read-root-hide{top:1000px}sr-read{display:-webkit-flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-flow:column nowrap;flex-flow:column;margin:20px 20%;min-width:400px;min-height:400px;text-align:center}read-process{position:fixed;top:0;left:0;height:3px;width:100%;background-color:#64b5f6;-webkit-transition:width 2s;transition:width 2s;z-index:20000}sr-rd-content-error{display:block;position:relative;margin:0;margin-bottom:30px;padding:25px;background-color:rgba(0,0,0,.05)}sr-rd-footer{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;font-size:14px}sr-rd-footer,sr-rd-footer-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}sr-rd-footer-group{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-rd-footer-line{width:100%;border-top:1px solid #e0e0e0}sr-rd-footer-text{min-width:150px}sr-rd-footer-copywrite{margin:10px 0 0;color:inherit}sr-rd-footer-copywrite abbr{-webkit-font-feature-settings:normal;font-feature-settings:normal;font-variant:normal;text-decoration:none}sr-rd-footer-copywrite .second{margin:10px 0}sr-rd-footer-copywrite .third a:hover{border:none!important}sr-rd-footer-copywrite .third a:first-child{margin-right:50px}sr-rd-footer-copywrite .sr-icon{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:33px;height:33px;opacity:.8;-webkit-transition:opacity .5s ease;transition:opacity .5s ease;cursor:pointer}sr-rd-footer-copywrite .sr-icon:hover{opacity:1}sr-rd-footer-copywrite a,sr-rd-footer-copywrite a:link,sr-rd-footer-copywrite a:visited{margin:0;padding:0;color:inherit;background-color:transparent;font-size:inherit!important;line-height:normal;text-decoration:none;vertical-align:baseline;vertical-align:initial;border:none!important;box-sizing:border-box}sr-rd-footer-copywrite a:focus,sr-rd-footer-copywrite a:hover,sr-rd-footer a:active{color:inherit;text-decoration:none;border-bottom:1px dotted!important}.simpread-blocks{text-decoration:none!important}.simpread-blocks *{margin:0}.simpread-blocks a{padding:0;text-decoration:none!important}.simpread-blocks img{margin:0;padding:0;border:0;background:transparent;box-shadow:none}.simpread-focus-root{display:block;position:fixed;top:0;left:0;right:0;bottom:0;background-color:hsla(0,0%,92%,.9);z-index:2147483645;opacity:0;-webkit-transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms;transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms}.simpread-focus-highlight{position:relative;box-shadow:0 0 0 20px #fff;background-color:#fff;overflow:visible;z-index:2147483646}.sr-controlbar-bg sr-rd-crlbar,.sr-controlbar-bg sr-rd-crlbar fab{z-index:2147483647}sr-rd-crlbar.controlbar{position:fixed;right:0;bottom:0;width:100px;height:100%;opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}sr-rd-crlbar.controlbar:hover{opacity:1}sr-rd-crlbar fap *{box-sizing:border-box}@media (max-height:620px){fab{zoom:.8}}@media (max-height:783px){dialog-gp dialog-content{max-height:580px}dialog-gp dialog-footer{border-top:1px solid #e0e0e0}}.simpread-highlight-selector{cursor:pointer!important}.simpread-highlight-controlbar,.simpread-highlight-selector{background-color:#fafafa!important;outline:3px dashed #1976d2!important;opacity:.8!important;-webkit-transition:opacity .5s ease!important;transition:opacity .5s ease!important}.simpread-highlight-controlbar{position:relative!important}simpread-highlight,sr-snapshot-ctlbar{position:fixed;top:0;left:0;right:0;padding:15px;height:50px;background-color:rgba(50,50,50,.9);box-shadow:0 2px 5px rgba(0,0,0,.26);box-sizing:border-box;z-index:2147483640}simpread-highlight,sr-highlight-ctl,sr-snapshot-ctlbar{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-highlight-ctl{margin:0 5px;width:50px;height:20px;color:#fff;background-color:#1976d2;border-radius:4px;box-shadow:0 3px 1px -2px rgba(0,0,0,.2),0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12);cursor:pointer}toc-bg{position:fixed;left:0;top:0;width:50px;height:200px;font-size:medium}toc-bg:hover{z-index:3}.toc-bg-hidden{opacity:0;-webkit-transition:opacity .5s ease;transition:opacity .5s ease}.toc-bg-hidden:hover{opacity:1;z-index:3}.toc-bg-hidden:hover toc{width:180px}toc *{all:unset}toc{position:fixed;left:0;top:100px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;padding:10px;width:0;max-width:200px;max-height:500px;overflow-x:hidden;overflow-y:hidden;cursor:pointer;border:1px solid hsla(0,0%,62%,.22);-webkit-transition:width .5s;transition:width .5s}toc:hover{overflow-y:auto}toc.mini:hover{width:200px!important}toc::-webkit-scrollbar{width:3px}toc::-webkit-scrollbar-thumb{border-radius:10px;background-color:hsla(36,2%,54%,.5)}toc outline{position:relative;display:-webkit-box;-webkit-line-clamp:1;-webkit-box-orient:vertical;overflow:hidden;text-overflow:ellipsis;padding:2px 0;min-height:21px;line-height:21px;text-align:left}toc outline a,toc outline a:active,toc outline a:focus,toc outline a:visited{display:block;width:100%;color:inherit;font-size:11px;text-decoration:none!important;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}toc outline a:hover{font-weight:700!important}toc outline a.toc-outline-theme-dark,toc outline a.toc-outline-theme-night{color:#fff!important}.toc-level-h1{padding-left:5px}.toc-level-h2{padding-left:15px}.toc-level-h3{padding-left:25px}.toc-level-h4{padding-left:35px}.toc-outline-active{border-left:2px solid #f44336}toc outline active{position:absolute;left:0;top:0;bottom:0;padding:0 0 0 3px;border-left:2px solid #e8e8e8}sr-kbd{background:-webkit-gradient(linear,0 0,0 100%,from(#fff785),to(#ffc542));border:1px solid #e3be23;-o-border-image:none;border-image:none;-o-border-image:initial;border-image:initial;position:absolute;left:0;padding:1px 3px 0;font-size:11px!important;font-weight:700;box-shadow:0 3px 7px 0 rgba(0,0,0,.3);overflow:hidden;border-radius:3px}.sr-kbd-a{position:relative}kbd-mapping{position:fixed;left:5px;bottom:5px;-ms-flex-flow:row;flex-flow:row;width:250px;height:500px;background-color:#fff;border:1px solid hsla(0,0%,62%,.22);box-shadow:0 2px 5px rgba(0,0,0,.26);border-radius:3px}kbd-mapping,kbd-maps{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}kbd-maps{margin:40px 0 20px;width:100%;overflow-x:auto}kbd-maps::-webkit-scrollbar-thumb{background-clip:padding-box;border-radius:10px;border:2px solid transparent;background-color:rgba(85,85,85,.55)}kbd-maps::-webkit-scrollbar{width:10px;-webkit-transition:width .7s cubic-bezier(.4,0,.2,1);transition:width .7s cubic-bezier(.4,0,.2,1)}kbd-mapping kbd-map-title{position:absolute;margin:5px 0;width:100%;font-size:14px;font-weight:700}kbd-maps-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}kbd-maps-title{margin:5px 0;padding-left:53px;font-size:12px;font-weight:700}kbd-map kbd{display:inline-block;padding:3px 5px;font-size:11px;line-height:10px;color:#444d56;vertical-align:middle;background-color:#fafbfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}kbd-map kbd-name{display:inline-block;text-align:right;width:50px}kbd-map kbd-desc{padding-left:3px}sharecard-bg{position:fixed;top:0;left:0;width:100%;height:100%;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(0,0,0,.4);z-index:2147483647}sharecard{max-width:450px;background-color:#64b5f6}sharecard,sharecard-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-head{margin:25px;color:#fff;border-radius:10px;box-shadow:0 2px 6px 0 rgba(0,0,0,.2),0 25px 50px 0 rgba(0,0,0,.15)}sharecard-card{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}sharecard-card,sharecard-top{display:-webkit-box;display:-ms-flexbox;display:flex}sharecard-top{-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:65px;background-color:#fff;color:#878787;font-size:25px;font-weight:500;border-top-left-radius:10px;border-top-right-radius:10px}sharecard-top span.logos{display:block;width:48px;height:48px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAMAAABg3Am1AAABU1BMVEUAAAAnNJMnNZI3Q5onNJInNJMnNJInNJMnNJI8SJ0tOZY/S55EUKAoNJI6RpwoNJNIU6InNJInNJImNJI7SJwmNJJ2fLUiMJFKVaNCTJ9faK1HUaJOWKVSXaUnNJNYY6pye7cmM5JXYKhwebMjMI8mL4719fW9vb0oNZP/UlLz8/QqN5TAwMAnNJPv7+/Pz8/q6+/p6enNzc3Kysry8vMsOJXc3env7/LU1uXo29vR0dHOzs7ExMTwjo73bW37XV3Aj1TCYELl5u3n5+fW2Obn6O7f4OrZ2+g0QJkxPpgvO5bh4uvS1OTP0ePCwsJQW6ZLVqTs7fHd3d3V1dXqv79VX6lET6A1TIxXUIBSgHxWQnpelHf+WVnopkXqbC7j5Ozi4uLDw8NGUaFATJ9SgH3r6+vGyd7BxNva2trX19ejqM2gpczHx8dze7Zha67Z2dlTgH1aXQeSAAAAJnRSTlMA6ff+497Y8NL+/fv49P379sqab/BeOiX06tzVy8m/tKqpalA7G6oKj0EAAAJlSURBVEjHndNXWxpBFIDhcS2ICRLAkt4Dx4WhLk0E6R0MYoIISrWX5P9f5cwSIRC2+T1czMV5n2FnZwn2eWONUqCAv3H2Uf5Ra1hx4+0WEXtDQW0fCPYJ1EffEfIV4CSROAE4jsePoTFsNmTJF/IeIHF2lgCIn57GodlqDWXBK7IwBYatVlMWFAildPKX7I3m74Z9fsCiQChoimoFQAz04Ad2gH1n9fv9n9hgMNDr9euLWD6fLxQKxaLfb7dTSlahbFVdEPwIQtrAihZQgyKCtCagbQe3xh0QFMgy5MR11+ewYY5/qlZ7vT2xu93ULKjbFLpiUxnIIwjgKmVTLDUFXMrAi2NJWCRLIthTBo4xyOLKpwyqU6CuDCI41hFBCVdOhyLw4FgJ1skCAiyl9BSHbCorgo6VJXTru5hrVCQS8Yr5xLzX59YJSFpVFwD9U0BGC3hGdFpATgRupTGe9R9I1b1ePBvXKDyvq/O/44LT4/E4BUbSCAwj8Evq6HlnOBprx6JhJz8Gktc7xeaP9ndY+0coQvCccFBD4JW60UIY50ciLOAODAQRVOeCHm4Q3Xks6uRDY+CQ+AR4T2wMYh6+jMCIQOp78CFoj0H7EQgIuhI3dGaHCrwgADwCPjJvA372GRigCJg49FUdk3D87pq3zp4SA5zc1Zh9DxfwkpjgUg5Mv+lbeE3McC8Lpu7SA3wk2xzcqL2tN5DfIsQC8HB7UamUy6FQOpTO5QKBQDZbKnWSyUzGjdWCwaDA8+7Le4BNgm3qQGWchYh9s5hNq6wVbBlbwhZYOp3OYOA4zmgEypnM2zj8ByIdedKrH8vDAAAAAElFTkSuQmCC");zoom:.8}sharecard-content{padding:15px;max-height:500px;font-size:20px;text-align:justify;background-color:#2196f3;overflow-x:hidden;overflow-y:auto}sharecard-via{padding:10px;font-size:10px;background-color:#2196f3}sharecard-footer{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;padding-right:5px;height:100px;background-color:#fff;color:#878787;font-size:15px;font-weight:500;border-bottom-left-radius:10px;border-bottom-right-radius:10px}sharecard-footer,sharecard-footer div{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sharecard-footer span.qrcode{display:block;width:100px;height:100px;margin:5px;background-repeat:no-repeat;background-position:50%;background-image:url("data:image/png;base64,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")}sharecard-control{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:0 19px;height:80px;background-color:#fff}simpread-snapshot{width:100%;height:100%;cursor:move;z-index:2147483645}simpread-snapshot,sr-mask{position:fixed;left:0;top:0}sr-mask{background-color:rgba(0,0,0,.1)}.simpread-feedback,.simpread-urlscheme{position:fixed;right:20px;bottom:20px;z-index:2147483646}simpread-feedback,simpread-urlscheme{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:20px 20px 0;width:500px;color:rgba(51,51,51,.87);background-color:#fff;border-radius:3px;box-shadow:0 0 2px rgba(0,0,0,.12),0 2px 2px rgba(0,0,0,.26);overflow:hidden;-webkit-transform-origin:bottom;transform-origin:bottom;-webkit-transition:all .6s ease;transition:all .6s ease}simpread-feedback *,simpread-urlscheme *{font-size:12px!important;box-sizing:border-box}simpread-feedback.active,simpread-urlscheme.active{-webkit-animation-name:srFadeInUp;animation-name:srFadeInUp;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback.hide,simpread-urlscheme.hide{-webkit-animation-name:srFadeInDown;animation-name:srFadeInDown;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}simpread-feedback sr-fb-label,simpread-urlscheme sr-urls-label{width:100%}simpread-feedback sr-fb-head,simpread-urlscheme sr-urls-head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin-bottom:5px;width:100%}simpread-feedback sr-fb-content,simpread-urlscheme sr-urls-content{margin-bottom:5px;width:100%}simpread-feedback sr-urls-footer,simpread-urlscheme sr-urls-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-fb-a,simpread-urlscheme sr-urls-a{color:#2163f7;cursor:pointer}simpread-feedback text-field-state,simpread-urlscheme text-field-state{border-top:none rgba(34,101,247,.8)!important;border-left:none rgba(34,101,247,.8)!important;border-right:none rgba(34,101,247,.8)!important;border-bottom:2px solid rgba(34,101,247,.8)!important}simpread-feedback switch,simpread-urlscheme switch{margin-top:0!important}@-webkit-keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@keyframes srFadeInUp{0%{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}to{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}}@-webkit-keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}@keyframes srFadeInDown{0%{opacity:1;-webkit-transform:translateY(0);transform:translateY(0)}to{opacity:0;-webkit-transform:translateY(100px);transform:translateY(100px)}}simpread-feedback sr-fb-head{font-weight:700}simpread-feedback sr-fb-content{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column}simpread-feedback sr-fb-content,simpread-feedback sr-fb-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal}simpread-feedback sr-fb-footer{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;width:100%}simpread-feedback sr-close{position:absolute;right:20px;cursor:pointer;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s;z-index:200}simpread-feedback sr-close:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin-top:10px}simpread-feedback sr-stars i{margin-right:10px;cursor:pointer}simpread-feedback sr-stars i svg{-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s}simpread-feedback sr-stars i svg:hover{-webkit-transform:rotate(-15deg) scale(1.3);transform:rotate(-15deg) scale(1.3)}simpread-feedback sr-stars i.active svg{-webkit-transform:rotate(0) scale(1);transform:rotate(0) scale(1)}simpread-feedback sr-emojis{display:block;height:100px;overflow:hidden}simpread-feedback sr-emoji{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-transition:.3s;transition:.3s}simpread-feedback sr-emoji>svg{margin:15px 0;width:70px;height:70px;-ms-flex-negative:0;flex-shrink:0}simpread-feedback sr-stars-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin:10px 0 20px}</style><style type="text/css">sr-opt-focus,sr-opt-read{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column}sr-opt-focus,sr-opt-gp,sr-opt-read{display:-webkit-flex;-webkit-box-direction:normal;width:100%}sr-opt-gp{position:relative;-webkit-box-orient:horizontal;-ms-flex-flow:row nowrap;flex-flow:row;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;margin-bottom:25px;font-size:15px}sr-opt-gp input,sr-opt-gp textarea{font-family:Inconsolata,Operator Mono,Consolas,Andale Mono WT,Andale Mono,Lucida Console,Lucida Sans Typewriter,DejaVu Sans Mono,Bitstream Vera Sans Mono,Liberation Mono,Nimbus Mono L,Courier New,Courier,monospace!important}sr-opt-gp sr-opt-label{display:block;position:absolute;margin:-8px 0 0;font-size:14px;font-weight:700;color:rgba(0,137,123,.8);-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;pointer-events:none;-webkit-transform:scale(.75) translateY(-8px);transform:scale(.75) translateY(-8px);-webkit-transform-origin:left top 0;transform-origin:left top 0}sr-opt-themes{display:-webkit-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-flow:row nowrap;flex-flow:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;width:100%;margin:8px 0 17px;padding:0}sr-opt-theme{width:40px;height:20px;cursor:pointer;list-style:none;border-radius:3px;border:1px solid #212121;box-sizing:border-box;opacity:1;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms}sr-opt-theme:hover{-webkit-transform:translateY(-1px);transform:translateY(-1px);box-shadow:0 5px 10px rgba(0,0,0,.2)}sr-opt-theme:not(:first-child){margin-left:5px}sr-opt-theme[sr-type=active]{box-shadow:0 5px 10px rgba(0,0,0,.2);border:none}</style><style type="text/css">notify-gp{font:300 14px -apple-system,PingFang SC,Microsoft Yahei,Lantinghei SC,Hiragino Sans GB,Microsoft Sans Serif,WenQuanYi Micro Hei,sans-serif;text-rendering:optimizelegibility;-webkit-text-size-adjust:100%;-webkit-font-smoothing:antialiased;display:-webkit-flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-flow:column nowrap;flex-flow:column;-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end;position:fixed;top:0;right:0;margin:0 15px 0 0;padding:0;text-transform:none;pointer-events:none}notify-gp notify{display:-webkit-flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0;margin-top:15px;padding:14px 24px;min-width:288px;max-width:568px;min-height:48px;color:hsla(0,0%,100%,.9);background-color:#000;box-sizing:border-box;border-radius:4px;pointer-events:auto;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;opacity:0;-webkit-transform:scaleY(0);transform:scaleY(0);-webkit-transform-origin:left top 0;transform-origin:left top 0;-webkit-transition:all .45s cubic-bezier(.23,1,.32,1) 0ms,opacity 1s cubic-bezier(.23,1,.32,1) 0ms;transition:all .45s cubic-bezier(.23,1,.32,1) 0ms,opacity 1s cubic-bezier(.23,1,.32,1) 0ms;box-shadow:0 3px 5px -1px rgba(0,0,0,.2),0 6px 10px 0 rgba(0,0,0,.14),0 1px 18px 0 rgba(0,0,0,.12)}notify-gp notify-title{font-size:13px;font-weight:700}notify-gp notify-content{display:block;font-size:14px;font-weight:400;text-align:left;overflow:hidden}notify-gp notify-content a,notify-gp notify-content a:active,notify-gp notify-content a:link,notify-gp notify-content a:visited{margin:inherit;padding-bottom:5px;color:#fff;font-size:inherit;text-decoration:none;-webkit-transition:color .5s;transition:color .5s}notify-gp notify-content a:hover{margin:0;margin:initial;padding:0;padding:initial;color:inherit;font-size:inherit;text-decoration:none}notify-gp notify-i{display:none;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 10px 0 0;width:24px;height:24px;background-position:50%;background-repeat:no-repeat}notify-gp notify-action,notify-gp notify-cancel{display:none;margin:0 8px;max-width:80px;min-width:56px;height:36px;line-height:34px;color:#bb86fc;font-weight:500;font-size:inherit;text-transform:uppercase;text-align:center;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;cursor:pointer}notify-gp .notify-error notify-action,notify-gp .notify-error notify-cancel,notify-gp .notify-success notify-action,notify-gp .notify-success notify-cancel,notify-gp .notify-warning notify-action,notify-gp .notify-warning notify-cancel{color:#fff}notify-gp notify-action:active,notify-gp notify-cancel:active{border-radius:4px;background-color:rgba(98,0,238,.3)}notify-gp notify-cancel{margin:0}notify-gp notify-a{display:block;position:absolute;top:5px;right:5px;cursor:pointer}notify-gp notify-exit{display:none;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-left:5px;width:36px;height:36px;min-width:36px;min-height:36px;background-color:transparent;border-radius:50%;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;cursor:pointer}notify-gp notify-exit:hover{background-color:hsla(0,0%,100%,.4)}notify-gp notify-exit:active{background-color:hsla(0,0%,100%,.2)}notify-gp notify-a notify-span{display:block;width:16px;height:16px;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAABDklEQVQ4T6VT0VFCQQzcrQA7ECoRK1AqEDugA6ECsQPsADvgVSAlaAlWEGdvkjchczI45Osud9nc7m2IEmY2BfAEYA5A6xsARwAHAB8ktR6DeWNmKwAvXlSxY78i+RabEcDM9gAe/qoq+T3JhXINwDu/Xlgc1zYk13TOn+XZA4C7AvgN4LbkZgJYO+84O5C8N7Odi6n8O8llh+ZGAD3uO5LPDgIvzoDRbBDAV+dputBAXKNecQM5B9CefQlAj0JwVmdLdGSwHI1CFXEgOS8ihia1WRNRdpU9JwlatpWVc0gr3c0xu95IAfdPK2uoHkcrJxANkzTJdPKTf3ROchvJk2n0LxNPfV9vnDVEJ+P8C6jMhLeGEqMKAAAAAElFTkSuQmCC);opacity:.9}notify-gp notify-i.holdon{display:block;margin:0 0 0 24px;width:20px;height:20px;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAQAAAAngNWGAAAATUlEQVR4AWMYSuB/4P+V/1lRRFiBIoEYCoGC//+vAypFKFsHFFkJV4AsAVGKzsOjFFUZHqUElCGUwpRRrpCw1YQ9Qzh4SA5wwlE4hAAAiFGQefYhNJkAAAAASUVORK5CYII=);cursor:pointer}notify-gp .notify-show{opacity:1;-webkit-transform:scaleY(1)!important;transform:scaleY(1)!important}notify-gp .notify-hide{-webkit-animation-name:fadeOutUp;animation-name:fadeOutUp;-webkit-animation-duration:.45s;animation-duration:.45s;-webkit-animation-fill-mode:both;animation-fill-mode:both}notify-gp .notify-success{background-color:#4caf50}notify-gp .notify-warning{background-color:#ffa000}notify-gp .notify-error{background-color:#ef5350}notify-gp .notify-info{background-color:#1976d2}notify-gp .notify-modal{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-flow:column nowrap;flex-flow:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;height:auto;max-height:200px;box-shadow:0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12),0 3px 1px -2px rgba(0,0,0,.2)}notify-gp .notify-modal .notify-modal-content{margin-top:5px;font-size:13px;white-space:normal}notify-gp .notify-modal .notify-modal-content a{margin:0;padding:0;color:inherit;font-size:inherit;text-decoration:underline;cursor:pointer}notify-gp .notify-modal .notify-modal-content a:active,notify-gp .notify-modal .notify-modal-content a:focus,notify-gp .notify-modal .notify-modal-content a:hover,notify-gp .notify-modal .notify-modal-content a:visited{color:inherit}notify-gp .notify-snackbar{position:fixed;bottom:0;left:50%;margin-bottom:5px;-webkit-transform-origin:left bottom 0;transform-origin:left bottom 0}.notify-position-lt-corner{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;margin:0 0 0 15px;left:0;right:auto}.notify-position-lb-corner{margin:0 0 15px 15px;right:auto;left:0}.notify-position-lb-corner,.notify-position-rb-corner{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-flow:column-reverse wrap-reverse;flex-flow:column-reverse wrap-reverse;top:auto;bottom:0}.notify-position-rb-corner{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;margin:0 15px 15px 0;left:auto;right:0}@-webkit-keyframes fadeOutUp{0%{opacity:1}to{margin-top:0;padding:0;height:0;min-height:0;opacity:0;-webkit-transform:scaleY(0);transform:scaleY(0)}}@keyframes fadeOutUp{0%{opacity:1}to{margin-top:0;padding:0;height:0;min-height:0;opacity:0;-webkit-transform:scaleY(0);transform:scaleY(0)}}@media (pointer:coarse){notify-gp{top:auto;bottom:0;left:0;margin:0 10px 10px}notify-gp notify{width:100%;max-width:600px}notify-gp .notify-hide,notify-gp .notify-show{-webkit-transform-origin:bottom!important;transform-origin:bottom!important}notify-gp .notify-snackbar{position:static}}</style><style type="text/css">:root{--sr-annote-color-0:#b4d9fb;--sr-annote-color-1:#ffeb3b;--sr-annote-color-2:#80deea;--sr-annote-color-3:#85d1f6;--sr-annote-color-4:#8cd842;--sr-annote-color-5:#ffb7da}[sr-annote-bg-color]{color:inherit}[sr-annote-bg-color][data-color-type="0"]{background-color:var(--sr-annote-color-0)}[sr-annote-bg-color][data-color-type="1"]{background-color:var(--sr-annote-color-1)}[sr-annote-bg-color][data-color-type="2"]{background-color:var(--sr-annote-color-2)}[sr-annote-bg-color][data-color-type="3"]{background-color:var(--sr-annote-color-3)}[sr-annote-bg-color][data-color-type="4"]{background-color:var(--sr-annote-color-4)}[sr-annote-bg-color][data-color-type="5"]{background-color:var(--sr-annote-color-5)}[sr-annote-bb-color][data-color-type="1"]{border-bottom-color:var(--sr-annote-color-1)}[sr-annote-bb-color][data-color-type="2"]{border-bottom-color:var(--sr-annote-color-2)}[sr-annote-bb-color][data-color-type="3"]{border-bottom-color:var(--sr-annote-color-3)}[sr-annote-bb-color][data-color-type="4"]{border-bottom-color:var(--sr-annote-color-4)}[sr-annote-bb-color][data-color-type="5"]{border-bottom-color:var(--sr-annote-color-5)}[sr-annote-bl-color][data-color-type="1"]{border-left:5px solid var(--sr-annote-color-1)}[sr-annote-bl-color][data-color-type="2"]{border-left:5px solid var(--sr-annote-color-2)}[sr-annote-bl-color][data-color-type="3"]{border-left:5px solid var(--sr-annote-color-3)}[sr-annote-bl-color][data-color-type="4"]{border-left:5px solid var(--sr-annote-color-4)}[sr-annote-bl-color][data-color-type="5"]{border-left:5px solid var(--sr-annote-color-5)}[data-color-style="1"]{background-color:transparent!important;background-repeat:no-repeat;background-size:100% 100%}[data-color-style="1"][data-color-type="1"]{background-color:transparent!important;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaCAMAAAB//6mtAAAAPFBMVEVHcEz/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zv/6zsmGdgxAAAAE3RSTlMAJJGnQ3Bh/ANS6An0u9o1FMp/ufrp4AAAAgZJREFUSMe1Vou2oyAM5BEIbxD+/183gFq99bZ7dnVOe6oWZwgJGRj7DcaY+et5BqtVFcA4eLraoLUFgJJLHzvBvuM00nPIzA4qazXkDuKka0VPIHNeiuee/RXO+qaAFjGFlpQQMi7JORcQ24Z+hRhcWpYYo5SikiQpkuBnldLXQwm5hANT53pxz9v1eZIdpEFCsmpLgXF+tVKGg640ZZrmDxacaNcIzqVE3EIoC7yYnjHuLxQ45cvquDP/RvgRiG4R2uYewlXKvXbo2r8x71NyFI4EZmZaj2vEShFJpnYHQopSAT/lgHEhZY2L+1/yVzAooJyyoEGEgLeEMFWC4K9NwHiVuFXlPSLNWbZ2g/61IuKN9AMyrwEUK5a7yQlLnUnwyr1v1luKCZsaq69kwwcCaC3ZWUZZPsFOkxaUA2pt8pHZE6KyhmnXbi3O1/xDSLkXqM8QH1FYwI+26nV1TwTQljy3ma8inSLYWiN+bp74ZX16hc4mnWkb7P0Jv9FejEB8+wsXlTfvLGIbthVvFN3jAbR0l34WUodzw6avJtIp1M6vya61EuTB1EqT3JW5jYfXiWw6Y626eOP7p5Th4FbrfqiZR4PxBjq5rv/BD8jplR12bcYBw44k4KCVtR9Q/Kszni/2W+MLh66n4WjLwz0LX5vqtDnqH4nitD8M/P10ZQ44q634A66hf2zVV84fAAAAAElFTkSuQmCC)}[data-color-style="1"][data-color-type="2"]{background-color:transparent!important;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaCAMAAAB//6mtAAAANlBMVEVHcEyA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3uqA3urNaunbAAAAEXRSTlMAe+8uaFLb/AQCG0ANkralydBaCTIAAAIBSURBVEjHtVaJduMwCJRA6ED3///sIstn6qbd13hy+NlxRjBYDEp9h+B9WI6JMRIZnSnEyKQPGKKIjOwnwoD6CXLPuHmeeI5RkREi+dKEEeMKuRKFnJPAq98gvKyf0ORiXbc619KaBXDy7r2Pzwq5Ym0TlFqz5BMROb1fxi96CGeDlcPNz855wji1NddaSpOFcl5kMxHvFvEoKtdi4WDbONbjLdxMokgKelWMGZP/olpgGrKWGfA7yrdw0EQpRMlh1Py1CMnYDv/P7MZri8lJQiXTRnnmV4lzG2X9C9bMQVTTkf2FH6voWJvtf8Whb76Um41GDeD6J+BG9Ttk3B8irziXz5AftbFmrcI4JMrNfXqFGnd5su2fZR8omnhJQMOlNp+Ekaap0NT+CHu3cT5GWPsT/M6Np0haW30ketlu1VBQBvpD+gAAygZWjFgeWaFQSmMbJKPhiQL0FtWyzbzO9ss+d66/dQHnfrAJ+U0rv1qNdOn93ndi3dI692Jy86o1cWt0Sb/0QCh5uHw0ptp+xyktfwXAfRitac17l1hQYbbAYpBng+JYzxHZ4SQ1C0zyczwIw8GJ5siRx2ywNWNopPzJymaps4ljJghjIvKKlr0nBlXEcLXYLX8/nYR9lBJDphEvnWaZZQwKjNvZDE6hUFfCqyvNAevis/7A+s+rFf8Do0ByAiavdJkAAAAASUVORK5CYII=)}[data-color-style="1"][data-color-type="3"]{background-color:transparent!important;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaCAMAAAB//6mtAAAANlBMVEVHcEyF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0faF0fYOABKzAAAAEXRSTlMA3e8uaFJ7/AQCGz8Nt5KlzvXM15AAAAIFSURBVEjHtVaNesIgDARCCA0/gfd/2YXSWuvUbd/sfWqlpQdcIBdjXmGJcVmvmSEh+lBxSYkxHPCICRg4TiwD5idon9F5NiKnZNArkf4EhARpg95JSs5ZEc1vsDyMn8HXYqnbUKWUZp0j/fTex3eD3rG2tVaKSNX1JADO74eJqx7K2dzGQSdKbR8YTStVxgSabbWusvkEzwaJoCpLsY4mHU2unbK/AI1FjCUUCZtizJDjN9UWxiFrIdon2f8E2jUrqhSArmHE/HHXZG+7+yPzvW5zCGtLxT2u9/wmc20jrP/CDJnKVkLieOIHERlR6P8FTZ31Xz2Fm32A4Bz1T2AopnpXuG2iaLiWz5Afalm/RWFcMtZGnx5B0k2e2vqH6RUlIK8LCG6Pzcfhdfsb8NIvYe8W5zYC6VfwE41dpKlNLpm9HjfxuBjv+kX6OOdAD7BhgHLJCAVzHscg++CuCEBvyazHLIZqnyTH/tYFiH6wCX0WTNysRrP0re87sZ7SEj2Y3ETzaU90ORw5cAan1OHyyXux/RmnpvwNzj2fRmsh8C1LrJB1M6kj+cQzQTHKQUxKpjleqsLnOMuDZTg44iw5qtYGWhrMN1xDE+/qiRnqqtx5q4iiwfXskZt+q3bLr6uTm3ENQ8YxX7yrZdYyaGHYW3NyJim1IJxdaRZYp2InHtjePFvxF2YVcjg/uCn9AAAAAElFTkSuQmCC)}[data-color-style="1"][data-color-type="4"]{background-color:transparent!important;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaCAMAAAB//6mtAAAAMFBMVEWM2EJHcEyM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKM2EKKz/w5AAAAEHRSTlP/AANRaHvzGy3WqT4Nk77kx+qVTgAAAb9JREFUSMe1VtlyxDAIwwYDvv//b4udY5Nus9NOE71kJocA4SDAXcEb5rWIZiQKFX3OguEFQswqKv6FNxr4iXd/0UvOzsgRRwTUrHmF3clGLsXg3QfAifjwoCjV1hl6qCm12HkAvoF7jzG21lKqVk9WlfK5gkUP42wb2RvpGTHVkYBl0GqdslHW8kMAr0jBMj3y8W9CjCJmCWFVTFTLQQvYxMaB9Lu0r6NxW5Qq+xmBozzUocM/YQWliltfj012RWpsEe4Ax2aSiT/2wKk1y7rQ4T5Uk+pVgVDQwHwbO3fgqnuTvZPa4FbYP0R+CTC0Knh3AEPKawVCNcL9aAFlBPCB4SkQmPpK6SH6jmVKpE8FGKfIRttT9JwIPdBj+jN3dR6cTb8GT0RpOLwIXKHQH6kgZjeHnQ/v/wDf8R8Htww7rxT/wMhXN88PIuWyGk4J3w9vqsPlM1GKVyN/Qe98Na1DkNXRhCbSTILtZC1j3MYHpuMnnc3dzYBrpc0R/XDw6YRjqalzNdheR+d3y5yvW6urcZd9I8I9meG3ZrdyvZ3sT4pYxJEv7rsMrItQ0X17WT7Jw5VQT7Tvy5U/4RRsxRfv9RDt9/UjOgAAAABJRU5ErkJggg==)}[data-color-style="1"][data-color-type="5"]{background-color:transparent!important;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGAAAAAaCAMAAAB//6mtAAAAOVBMVEVHcEz/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r/t9r20adnAAAAEnRSTlMAVQ062Hwb/ARpK/Olk7XoScQnLSnCAAAB6ElEQVRIx7VW2Y7DIAzkMBCDOf//Y9cQmiZt2q12m1EegoTGF/ZYiFdYGONHWV+kRBOL0NryzwQalEV76+1yh/gAu4uLLVoURMkG0EivvZ6QsnRyqxgfse48XqE0xgSuZRMpVcg5MFw7wIUMALXWRBRnPOoXK5wPLZmz5snh5nfj3KGfgSJR6h7UGEfipNZnRhZf0FCCcOTaKNs53AiCQ0hkypox6716rgUnmwspU2d6y/geIdQeApsQzxVfhEJo+S/Mu5SFnCFFeSvrkd9G6GX9JzpBgJpMsccQPBEZqtD+bWHLb9yVexEWjTdPL/HvRkJuIfp7Ewgb03fI77nKeGvY3lgyVvdtC6RnABYjtO+ydyQj7QgAw6E23wT2/Hikdgl7g6LmC21X8DvXXxGPNrrEe263hNzSPf/X5CeEPNqA53O6xEKSalRAoclXFIArPLvMRDgZju+e7UFyXl1oRixTanhKb3ffJeuU1rlHkVsfKG6DTpmHGZhTl1cWICRoZ5xdiAdYqM/dgGqM3abEAIVV/xLqOcZtoTtxgBwAEkUG3hRx6QrO68VYOSLvBnzJTRekeNAzLnVkbnVbMIQcvedYn7restzaD7aTxVotu79lv8uMPUj57bDOV82qRNIfaM+Wq+WXresHSSV3cD8ocu8AAAAASUVORK5CYII=)}[data-color-style="2"]{background-color:transparent!important}[data-color-style="2"][data-color-type="1"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,var(--sr-annote-color-1) 0)}[data-color-style="2"][data-color-type="2"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,var(--sr-annote-color-2) 0)}[data-color-style="2"][data-color-type="3"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,var(--sr-annote-color-3) 0)}[data-color-style="2"][data-color-type="4"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,var(--sr-annote-color-4) 0)}[data-color-style="2"][data-color-type="5"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,var(--sr-annote-color-5) 0)}[data-color-style="3"]{position:relative;background-color:transparent!important}[data-color-style="3"]:after{content:"";position:absolute;left:0;bottom:25px;height:8px;width:58px;border-radius:4px;opacity:.8;transition:all .3s}[data-color-style="3"][data-color-type="1"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 85%,var(--sr-annote-color-1) 0)}[data-color-style="3"][data-color-type="2"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 85%,var(--sr-annote-color-2) 0)}[data-color-style="3"][data-color-type="3"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 85%,var(--sr-annote-color-3) 0)}[data-color-style="3"][data-color-type="4"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 85%,var(--sr-annote-color-4) 0)}[data-color-style="3"][data-color-type="5"]{background-color:transparent!important;background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 85%,var(--sr-annote-color-5) 0)}sr-annote-note{position:relative;bottom:-5px;padding:0 4px;color:#fff;background-color:#333;font-weight:700;font-style:normal;font-family:arial,helvetica,clean,sans-serif;border-radius:5px;opacity:.8;cursor:pointer}sr-annote-note:after{content:"N"}pre.sr-annote+sr-annote-note{bottom:25px;right:25px}sr-annote-note:hover{opacity:1}sr-annote-note sr-annote-note-tip{position:absolute;left:22px;top:0;padding:.5em 1em;max-width:400px;color:#fff;background:#101010;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;font-weight:400;font-style:normal;text-shadow:none;font-size:12px;text-indent:0;white-space:pre;z-index:10;border-radius:5px;box-shadow:0 0 10px rgba(0,0,0,.3);opacity:0;overflow:auto;pointer-events:none;z-index:20000;transition:all .18s ease-out .18s}sr-annote-note:hover sr-annote-note-tip{opacity:1;pointer-events:auto}sr-annote-note sr-annote-note-tip{overflow:hidden}sr-annote-note sr-annote-note-tip:hover{overflow:overlay}sr-annote-note sr-annote-note-tip::-webkit-scrollbar-track{background-color:transparent}sr-annote-note sr-annote-note-tip::-webkit-scrollbar{width:12px}sr-annote-note sr-annote-note-tip::-webkit-scrollbar-thumb{background-clip:padding-box;padding-top:80px;background-color:#ddd;border:3px solid transparent;border-radius:8px}</style><style type="text/css">.sr-annote-hideall{background-color:transparent!important;pointer-events:none}sr-annote-trigger{position:fixed!important;bottom:52px;right:32px;display:-webkit-box!important;display:-ms-flexbox!important;display:flex!important;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 0 15px;padding:0;width:40px!important;height:40px!important;line-height:40px!important;color:#fff;background-color:rgba(245,82,70,.8);border-radius:50%;cursor:pointer;box-shadow:0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12),0 3px 1px -2px rgba(0,0,0,.2);-webkit-transition:all .5s ease-in-out 0ms;transition:all .5s ease-in-out 0ms;overflow:visible!important;overflow:initial!important}sr-annote-trigger.open{right:95px}sr-annote-trigger.off{background-color:#bdbdbd}sr-annote-trigger sr-i{display:-webkit-box!important;display:-ms-flexbox!important;display:flex!important;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;border-radius:50%}sr-annote{padding:6px 0;background-color:transparent;font-size:inherit;cursor:pointer}.sr-annote[data-type=code],.sr-annote[data-type=img]{border-bottom-width:5px;border-bottom-style:solid}sr-annote[data-color-type="0"]{padding:7px 0}sr-annote-floating{position:fixed;color:#fff;background:hsla(0,0%,6%,.95);font-weight:700;border-radius:5px;box-shadow:0 0 10px rgba(0,0,0,.3);opacity:0;-webkit-animation-delay:.2s;animation-delay:.2s;-webkit-animation-duration:.5s;animation-duration:.5s;-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:sr-annote-slideInUp;animation-name:sr-annote-slideInUp;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) .1s;transition:all 1s cubic-bezier(.23,1,.32,1) .1s;z-index:2000}sr-annote-floating.hidden{-webkit-animation-duration:.5s;animation-duration:.5s;-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:sr-annote-slideInDown;animation-name:sr-annote-slideInDown;pointer-events:none}.sr-annote-floatingbar-hiden{display:none}@-webkit-keyframes sr-annote-slideInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0);visibility:visible}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@keyframes sr-annote-slideInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0);visibility:visible}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@-webkit-keyframes sr-annote-slideInDown{0%{opacity:1;visibility:visible}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}@keyframes sr-annote-slideInDown{0%{opacity:1;visibility:visible}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}sr-annote-floatingbar{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;margin:5px}sr-annote-floatingbar,sr-annote-floatingbar sr-anote-fb-item{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-annote-floatingbar sr-anote-fb-item{margin-right:5px;cursor:pointer}sr-annote-floatingbar sr-anote-fb-item:last-child{margin-right:0}sr-annote-floatingbar sr-anote-fb-item{width:20px;height:20px;border-radius:50%;box-sizing:border-box}sr-annote-floatingbar sr-anote-fb-item[type=copy],sr-annote-floatingbar sr-anote-fb-item[type=export],sr-annote-floatingbar sr-anote-fb-item[type=note],sr-annote-floatingbar sr-anote-fb-item[type=remove],sr-annote-floatingbar sr-anote-fb-item[type=style],sr-annote-floatingbar sr-anote-fb-item[type=tag]{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#fff;border-radius:50%}sr-annote-floatingbar sr-anote-fb-item[type=style]{background-color:#f73859!important}sr-annote-floatingbar sr-anote-fb-item[type=export]{background-color:#cc0e74}sr-annote-floatingbar sr-anote-fb-item[type=copy]{background-color:#a78674}sr-annote-floatingbar sr-anote-fb-item[type=remove]{background-color:#f44336}sr-annote-floatingbar sr-anote-fb-item[remove=confirm]{-webkit-transform:rotate(270deg);transform:rotate(270deg);-webkit-transition:all .2s ease-in-out 0s;transition:all .2s ease-in-out 0s}sr-annote-sidebar-bg{position:fixed;top:0;right:0;bottom:180px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;width:256px;font-size:22.4px;font-size:1.4rem;font-weight:500;opacity:0;-webkit-transform:translateX(256px);transform:translateX(256px);-webkit-transition:all .45s cubic-bezier(.23,1,.32,1) 0ms;transition:all .45s cubic-bezier(.23,1,.32,1) 0ms}sr-annote-sidebar-bg.mini{pointer-events:none}sr-annote-sidebar-bg:hover{z-index:2147483647}sr-annote-sidebar-bg.open{opacity:1;-webkit-transform:translateX(0);transform:translateX(0)}sr-annote-sidebar{margin:3px 4px 0;padding-left:20px;height:100%;overflow-x:hidden}sr-annote-sidebar.mini{pointer-events:none}sr-annote-sidebar *{box-sizing:border-box}sr-annote-sidebar{overflow-y:hidden}sr-annote-sidebar:hover{overflow-y:overlay}sr-annote-sidebar::-webkit-scrollbar-track{background-color:transparent}sr-annote-sidebar::-webkit-scrollbar{width:12px}sr-annote-sidebar::-webkit-scrollbar-thumb{padding-top:80px;background-clip:padding-box;background-color:#ddd;border:3px solid transparent;border-radius:8px;border-radius:10px;border:6px solid transparent;background-color:rgba(85,85,85,.55)}sr-annote-sidebar::-webkit-scrollbar{width:0;-webkit-transition:width .7s cubic-bezier(.4,0,.2,1);transition:width .7s cubic-bezier(.4,0,.2,1)}sr-annote-sidebar:hover::-webkit-scrollbar{width:16px}sr-annote-sidebar[srcoll=on]:hover sr-annote-sidebar-card[type=option].mini{-webkit-transform:translateX(-16px);transform:translateX(-16px)}sr-annote-sidebar[srcoll=on]:hover sr-annote-sidebar-card.off{-webkit-transform:translateX(190px);transform:translateX(190px)}sr-annote-sidebar-cards{display:block}sr-annote-sidebar-card{position:relative;display:block!important;margin:12px;color:rgba(51,51,51,.87);background-color:#fff;border-radius:4px;box-shadow:0 2px 5px rgba(0,0,0,.08);-webkit-transition:all .45s cubic-bezier(.23,1,.32,1) 0ms;transition:all .45s cubic-bezier(.23,1,.32,1) 0ms;pointer-events:auto}sr-annote-sidebar-card:hover{box-shadow:0 10px 20px 0 rgba(168,182,191,.6);-webkit-transform:translateY(-1px);transform:translateY(-1px)}sr-annote-sidebar-card:last-child{margin-bottom:30px}sr-annote-sidebar-card.off{display:block;-webkit-transform:translateX(205px);transform:translateX(205px);-webkit-transition:all .25s ease-out;transition:all .25s ease-out}sr-annote-sidebar-card.off:hover{-webkit-transform:translateX(120px)!important;transform:translateX(120px)!important}sr-annote-sidebar-card.hide{display:block;-webkit-transform:translateX(256px);transform:translateX(256px);-webkit-transition:all .25s ease-out;transition:all .25s ease-out}sr-annote-sidebar-card-anchor{position:absolute;left:0;top:0;width:90%;height:100%}sr-annote-sidebar-card-action{position:absolute;top:10px;right:3px;display:block;width:12px;height:12px;line-height:12px;-webkit-transition:all .25s ease-out;transition:all .25s ease-out;cursor:pointer;z-index:20000}sr-annote-sidebar-card-action.open{-webkit-transform:rotate(90deg);transform:rotate(90deg)}sr-annote-sidebar-card[mode=mini]{overflow:hidden}sr-annote-sidebar-card[mode=mini] sr-annote-sidebar-preview{display:block;padding:6px 12px 5px 10px;height:32px;color:#fff;font-size:13px;font-weight:400;white-space:nowrap;text-align:left;text-overflow:ellipsis;text-shadow:1px 1px 3px rgba(0,0,0,.3);-webkit-transition:all .25s ease-out;transition:all .25s ease-out;overflow:hidden}sr-annote-sidebar-card[mode=mini][type=img] sr-annote-sidebar-preview{text-align:center}sr-annote-sidebar-card[mode=mini] sr-annote-sidebar-detail{padding:0 15px;height:0}sr-annote-sidebar-card[mode=mini] sr-annote-sidebar-note,sr-annote-sidebar-card[mode=mini] sr-annote-sidebar-toolbars{display:none}sr-annote-sidebar-card[mode=mini] pre{margin:0!important;padding:0!important;white-space:nowrap;text-overflow:ellipsis;overflow:hidden}sr-annote-sidebar-card[mode=normal] sr-annote-sidebar-preview{display:none}sr-annote-sidebar-card[mode=normal] sr-annote-sidebar-detail{padding:15px;height:auto}sr-annote-sidebar-card[data-color-type="0"]{display:none!important}sr-annote-sidebar-card pre{margin:0!important;padding:0!important;background-color:transparent!important;max-height:200px;font-size:10px;overflow:hidden}sr-annote-sidebar-card input,sr-annote-sidebar-card textarea{font-size:12px!important}sr-annote-sidebar-card img{margin:0;padding:0;max-height:100px;max-width:80%;background:#fff;border:0;border-radius:6px;box-shadow:0 20px 20px -10px rgba(0,0,0,.1)}sr-annote-sidebar-detail{display:block;padding:15px;width:100%;color:#fff;font-size:10px;text-align:justify;border-top-left-radius:4px;border-top-right-radius:4px;-webkit-transition:all .25s ease-out;transition:all .25s ease-out}sr-annote-sidebar-card[type=img] sr-annote-sidebar-detail{text-align:center}sr-annote-sidebar-tags{-ms-flex-wrap:wrap;flex-wrap:wrap;margin-top:15px}sr-annote-sidebar-tag,sr-annote-sidebar-tags{display:-webkit-box;display:-ms-flexbox;display:flex}sr-annote-sidebar-tag{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;padding:4px 12px;color:rgba(0,0,0,.87);background-color:#fff;height:22px;font-size:14px;font-size:.875rem;font-weight:400;white-space:nowrap;border-radius:16px;outline:none;cursor:pointer;overflow:hidden;-webkit-transition:all .2s ease-in-out 0s;transition:all .2s ease-in-out 0s;-webkit-transform-origin:left;transform-origin:left;-webkit-transform:scale(.8);transform:scale(.8)}sr-annote-sidebar-note{display:block;padding:16px;width:100%;background-color:#fff;text-align:left}sr-annote-sidebar-toolbars{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end;padding-right:10px;height:32px;background-color:#fff;border-bottom-left-radius:4px;border-bottom-right-radius:4px}sr-annote-sidebar-toolbar,sr-annote-sidebar-toolbars{-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-annote-sidebar-toolbar{display:-webkit-box!important;display:-ms-flexbox!important;display:flex!important;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin-left:5px;width:20px;height:20px;line-height:20px;border-radius:50%;overflow:visible!important;overflow:initial!important}sr-annote-sidebar-toolbar[remove=confirm]{-webkit-transform:rotate(270deg);transform:rotate(270deg);-webkit-transition:all .2s ease-in-out 0s;transition:all .2s ease-in-out 0s}sr-annote-sidebar-toolbar[remove=confirm] svg path{fill:#f44336}sr-annote-sidebar-card[type=unread]{background-color:#cb63e6}sr-annote-sidebar-card[type=unread] sr-annote-sidebar-detail.title{padding:0;text-align:left;font-size:15px}sr-annote-sidebar-card[type=unread] sr-annote-sidebar-detail.desc{padding:0 0 0 10px;border-left:1px outset #fff;border-top-left-radius:0;border-top-right-radius:0}sr-annote-sidebar-card[type=unread][mode=mini] sr-annote-sidebar-preview{font-size:13px;text-shadow:1px 1px 3px rgba(0,0,0,.3)}sr-annote-sidebar-card[type=option]{height:32px;background-color:transparent;box-shadow:none;overflow:visible;overflow:initial}sr-annote-sidebar-card[type=option]:hover{-webkit-transform:translateY(0);transform:translateY(0)}sr-annote-sidebar-card[type=option].mini{margin-right:0}sr-annote-sidebar-card[type=option] sr-annote-sidebar-card-action,sr-annote-sidebar-card[type=option] sr-annote-sidebar-card-anchor{display:none}sr-annote-sidebar-card[type=option] sr-annote-sidebar-preview{background-color:transparent}sr-annote-sidebar-options{position:absolute;top:0;right:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;padding-right:5px;height:100%;background-color:#3a3a3a;border-left:10px outset #222;border-radius:4px}sr-annote-sidebar-option,sr-annote-sidebar-options{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-annote-sidebar-option{display:-webkit-box!important;display:-ms-flexbox!important;display:flex!important;margin-left:5px;width:20px;height:20px;border-radius:50%;-webkit-transition:all .25s ease-out;transition:all .25s ease-out;cursor:pointer;overflow:visible!important;overflow:initial!important}sr-annote-sidebar-option[off=true],sr-annote-sidebar-option[side=false]:not(:first-child){width:0;margin-left:0}sr-annote-sidebar-option[side=false]:first-child{margin-right:5px}sr-annote-sidebar-option[off=true]~sr-annote-sidebar-option:last-child svg{-webkit-transform:rotate(180deg);transform:rotate(180deg)}sr-annote-sidebar-option[type=drag][state=on] svg path,sr-annote-sidebar-option[type=export][state=on] svg path,sr-annote-sidebar-option[type=goon][state=on] svg path,sr-annote-sidebar-option[type=save][state=on] svg path{fill:#8cd842}sr-annote-sidebar-option[type=collapse][state=on] svg{-webkit-transform:rotate(90deg);transform:rotate(90deg)}sr-annote-sidebar-option[lock=true] svg path{fill:#f55246!important}sr-annote-sidebar-card[type=option]:hover~sr-annote-sidebar-card[type=unread]{z-index:-1}</style><style type="text/css">@-webkit-keyframes fadeInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@keyframes fadeInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@-webkit-keyframes fadeOutDown{0%{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}@keyframes fadeOutDown{0%{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}.sr-alertgp{position:fixed;left:0;top:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;background-color:rgba(51,51,51,.8);z-index:2147483647}.sr-alertgp .alert{min-width:400px;min-height:400px;width:650px;background-color:#fff;border-radius:4px;box-shadow:0 14px 45px rgba(0,0,0,.247059);-webkit-animation-name:fadeInUp;animation-name:fadeInUp;-webkit-animation-duration:.8s;animation-duration:.8s;-webkit-animation-fill-mode:both;animation-fill-mode:both}.sr-alertgp .alert,.sr-alertgp .alert .loading{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.sr-alertgp .alert .loading{position:relative;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:100%;height:35%;background-color:transparent}.alert .loading .progress{display:block;margin:10px auto;max-width:80%;max-height:250px;width:20%}.alert .loading .progress .percentage{fill:#666;font-family:sans-serif;font-size:.5em;text-anchor:middle}.alert .loading .progress .circle-bg{fill:none;stroke:#eee;stroke-width:3.8}.alert .loading .progress .circle{fill:none;stroke-width:2.8;stroke-linecap:round;stroke:#1dba90;-webkit-transition:all .2s ease-in-out;transition:all .2s ease-in-out}@-webkit-keyframes scaleAnimation{0%{opacity:0;-webkit-transform:scale(1.5);transform:scale(1.5)}to{opacity:1;-webkit-transform:scale(1);transform:scale(1)}}@keyframes scaleAnimation{0%{opacity:0;-webkit-transform:scale(1.5);transform:scale(1.5)}to{opacity:1;-webkit-transform:scale(1);transform:scale(1)}}@-webkit-keyframes fadeOut{0%{opacity:1}to{opacity:0}}@keyframes fadeOut{0%{opacity:1}to{opacity:0}}@-webkit-keyframes fadeIn{0%{opacity:0}to{opacity:1}}@keyframes fadeIn{0%{opacity:0}to{opacity:1}}.sr-alertgp .alert .close{position:absolute;top:0;right:0;z-index:2}.sr-alertgp .alert .close:hover{-webkit-transform:rotate(270deg);transform:rotate(270deg);-webkit-transition:all .25s ease-out;transition:all .25s ease-out}.sr-alertgp .alert .sr-alert-icon img{max-width:650px;width:100%;-webkit-transform:scale(.7);transform:scale(.7);-webkit-transition:all .5s ease-out;transition:all .5s ease-out}.sr-alertgp .alert .actionbar{position:relative;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:20px;width:80%;height:50px;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) 0ms;transition:all 1s cubic-bezier(.23,1,.32,1) 0ms}.sr-alertgp .alert .actionbar,.sr-alertgp .alert.notification{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center}.sr-alertgp .alert.notification{width:500px;min-height:350px}.sr-alertgp .alert.notification[data-state=siren]{background-image:-webkit-radial-gradient(circle farthest-corner at 10% 20%,#cd212a 0,#ec5f05 90%);background-image:radial-gradient(circle farthest-corner at 10% 20%,#cd212a 0,#ec5f05 90%)}.sr-alertgp .alert.notification[data-state=lock]{background-image:-webkit-radial-gradient(circle farthest-corner at 10% 20%,#8451a1 0,rgba(132,81,161,.83) 90%);background-image:radial-gradient(circle farthest-corner at 10% 20%,#8451a1 0,rgba(132,81,161,.83) 90%)}.sr-alertgp .alert.notification[data-state=warning]{background-image:-webkit-linear-gradient(left,#f2709c,#ff9472);background-image:linear-gradient(90deg,#f2709c,#ff9472)}.sr-alertgp .alert.notification[data-state=bug]{background-image:-webkit-linear-gradient(bottom,#ad5389,#3c1053);background-image:linear-gradient(0deg,#ad5389,#3c1053)}.sr-alertgp .alert.notification[data-state=safe],.sr-alertgp .alert.notification[data-state=server]{background-color:#8ec5fc;background-image:-webkit-linear-gradient(28deg,#8ec5fc,#e0c3fc);background-image:linear-gradient(62deg,#8ec5fc,#e0c3fc)}.sr-alertgp .alert.notification .sr-alert-icon{position:relative;width:100%}.sr-alertgp .alert.notification .loading .progress{padding:5px;background-color:#fff;border-radius:50%}.sr-alertgp .alert.notification .loading .progress .circle-bg{stroke:transparent}.sr-alertgp .alert.notification .loading .progress .circle{stroke-width:1}.sr-alertgp .alert.notification[data-state=siren] .loading .progress .circle{stroke:#cd212a}.sr-alertgp .alert.notification[data-state=lock] .loading .progress .circle{stroke:#8451a1}.sr-alertgp .alert.notification[data-state=warning] .loading .progress .circle{stroke:#f2709c}.sr-alertgp .alert.notification[data-state=bug] .loading .progress .circle{stroke:#ad5389}.sr-alertgp .alert.notification[data-state=safe] .loading .progress .circle,.sr-alertgp .alert.notification[data-state=server] .loading .progress .circle{stroke:#8ec5fc}.sr-alertgp .alert.notification .content{padding:10px 70px;width:100%;color:#fff;text-align:center;font-size:28.8px;font-size:1.8rem;box-sizing:border-box}.sr-alertgp .alert.notification .flag{position:absolute;left:0;top:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;z-index:100000}.sr-alertgp .alert.notification .flag img{width:50px}.sr-alertgp .alert.notification[data-state=lock] .flag img{width:30px}.sr-alertgp .alert.notification .flag img.swing{-webkit-transform-origin:center;transform-origin:center;-webkit-animation-name:swing;animation-name:swing;-webkit-animation-duration:1s;animation-duration:1s}.sr-alertgp .alert.notification .actionbar{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-pack:distribute;justify-content:space-around;margin:0}.sr-alertgp .alert.notification .return{padding:5px 32px;color:#333;background-color:#fff;font-size:15px;font-weight:700;border-radius:56px}.sr-alertgp .alert.notification .return:hover{box-shadow:0 3px 1px -2px rgba(0,0,0,.2),0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12)}@-webkit-keyframes swing{20%{-webkit-transform:rotate(15deg);transform:rotate(15deg)}40%{-webkit-transform:rotate(-10deg);transform:rotate(-10deg)}60%{-webkit-transform:rotate(5deg);transform:rotate(5deg)}80%{-webkit-transform:rotate(-5deg);transform:rotate(-5deg)}to{-webkit-transform:rotate(0deg);transform:rotate(0deg)}}@keyframes swing{20%{-webkit-transform:rotate(15deg);transform:rotate(15deg)}40%{-webkit-transform:rotate(-10deg);transform:rotate(-10deg)}60%{-webkit-transform:rotate(5deg);transform:rotate(5deg)}80%{-webkit-transform:rotate(-5deg);transform:rotate(-5deg)}to{-webkit-transform:rotate(0deg);transform:rotate(0deg)}}</style><style type="text/css">@-webkit-keyframes fadeInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@keyframes fadeInUp{0%{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}to{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}}@-webkit-keyframes fadeOutDown{0%{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}@keyframes fadeOutDown{0%{opacity:1;-webkit-transform:translateZ(0);transform:translateZ(0)}to{opacity:0;-webkit-transform:translate3d(0,100%,0);transform:translate3d(0,100%,0)}}sr-promo-bg{position:fixed;right:15px;bottom:15px;z-index:2147483646}sr-promo,sr-promo-notice{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:10px;color:rgba(51,51,51,.87);background-color:#fff;border-radius:4px;box-shadow:0 12px 18px -6px rgba(0,0,0,.3);overflow:hidden;-webkit-transform-origin:bottom;transform-origin:bottom;-webkit-transition:all .6s ease;transition:all .6s ease;-webkit-animation-name:fadeInUp;animation-name:fadeInUp;-webkit-animation-duration:.5s;animation-duration:.5s;-webkit-animation-fill-mode:both;animation-fill-mode:both}sr-promo img{width:220px;cursor:pointer}sr-promo-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;padding:10px 0 0;width:100%}sr-promo-a{padding:5px 10px;color:#fff;font-size:12px;font-weight:700;border-radius:2px;box-shadow:0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12),0 3px 1px -2px rgba(0,0,0,.2);cursor:pointer}sr-promo-a.later{background-color:#2196f3}sr-promo-a.cancel{background-color:#757575}sr-promo-tip{font-size:12px;padding:5px 10px;border-radius:2px}sr-promo-notice{position:absolute;top:10px;left:10px;right:10px;height:293px;padding-bottom:0;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;font-size:12px;border-radius:0;box-shadow:none;overflow-y:hidden}sr-promo-notice:hover{overflow-y:overlay}sr-promo-notice-title{font-weight:700;text-align:center;margin-bottom:5px;width:100%;font-size:14px}sr-promo-notice-content{margin-top:5px}</style><style type="text/css">dialog-gp .carousel,welcome .carousel{position:relative;width:100%;height:400px;-webkit-perspective:500px;perspective:500px;-webkit-transform-style:preserve-3d;transform-style:preserve-3d;-webkit-transform-origin:0 50%;transform-origin:0 50%;overflow:hidden}dialog-gp .carousel.carousel-slider,welcome .carousel.carousel-slider{top:0;left:0;height:100%}dialog-gp .carousel.carousel-slider .carousel-item,welcome .carousel.carousel-slider .carousel-item{position:absolute;top:0;left:0;width:100%;height:100%;min-height:400px}dialog-gp .carousel .carousel-item,welcome .carousel .carousel-item{display:none;position:absolute;top:0;left:0;width:200px;height:200px}dialog-gp .carousel .carousel-item>img,welcome .carousel .carousel-item>img{width:100%}dialog-gp .carousel .indicators,welcome .carousel .indicators{position:absolute;margin:0;padding:0;left:0;right:0;bottom:0;text-align:center}dialog-gp .carousel .indicators .indicator-item,welcome .carousel .indicators .indicator-item{display:inline-block;position:relative;margin:14px 4px;height:10px;width:10px;background-color:#e0e0e0;-webkit-transition:background-color .3s;transition:background-color .3s;border-radius:50%;cursor:pointer}dialog-gp .carousel .indicators .indicator-item.active,welcome .carousel .indicators .indicator-item.active{background-color:#4caf50}dialog-gp .carousel .carousel-item:not(.active) .materialboxed,dialog-gp .carousel.scrolling .carousel-item .materialboxed,welcome .carousel .carousel-item:not(.active) .materialboxed,welcome .carousel.scrolling .carousel-item .materialboxed{pointer-events:none}</style><style type="text/css">.simpread-upgrade-root *{box-sizing:border-box}.simpread-upgrade-root{-webkit-transition:all .25s ease-out;transition:all .25s ease-out}.simpread-upgrade-root.open{background-color:rgba(51,51,51,.8)}.simpread-upgrade-root dialog-gp{position:relative}.simpread-upgrade-root dialog-gp .close{position:fixed;top:0;right:0;z-index:2}.simpread-upgrade-root dialog-gp .close:hover{-webkit-transform:rotate(270deg);transform:rotate(270deg);-webkit-transition:all .25s ease-out;transition:all .25s ease-out}.simpread-upgrade-root dialog-content{padding-bottom:80px!important;overflow-y:hidden}.simpread-upgrade-root dialog-content:hover{overflow-y:overlay}.simpread-upgrade-root dialog-content::-webkit-scrollbar-track{background-color:transparent}.simpread-upgrade-root dialog-content::-webkit-scrollbar{width:12px}.simpread-upgrade-root dialog-content::-webkit-scrollbar-thumb{background-clip:padding-box;padding-top:80px;background-color:#ddd;border:3px solid transparent;border-radius:8px}.simpread-upgrade-root .floating{position:absolute;left:0;right:0;bottom:0;height:80px;overflow-y:hidden}.simpread-upgrade-root .floating,.simpread-upgrade-root .floating .billing{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.simpread-upgrade-root .floating .billing{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:1px 40px;color:#fff;background-color:#4dbb7c;font-size:15px;font-weight:400;opacity:0;border-radius:30px;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.simpread-upgrade-root .floating .billing.open{-webkit-animation-name:fadeInUp;animation-name:fadeInUp;-webkit-animation-duration:.3s;animation-duration:.3s;-webkit-animation-fill-mode:both;animation-fill-mode:both}.simpread-upgrade-root .floating .billing .sales{font-size:12px}.simpread-upgrade-root .floating .billing .rate{margin:0 5px}.simpread-upgrade-root .floating .billing .price{margin-left:2px;margin-right:5px}.upgrade{position:relative;color:rgba(51,51,51,.87);font-family:Hiragino Sans GB,Microsoft Yahei;text-shadow:none}.upgrade .head{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.upgrade .head img{margin-bottom:5px;width:60px;border-radius:9px;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.upgrade .head .title{margin:10px 0;font-weight:700;font-size:15px}.upgrade .head .desc{width:70%;text-align:center;font-size:13px}.upgrade .features{position:relative;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-pack:distribute;justify-content:space-around;-webkit-transition:height .2s cubic-bezier(.23,1,.32,1) 0ms;transition:height .2s cubic-bezier(.23,1,.32,1) 0ms}.upgrade .features.init{height:100px}.upgrade .features.init .base,.upgrade .features.init .pro{opacity:0}.upgrade .loading{position:absolute;left:0;top:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;background-color:#fff;z-index:1}.upgrade .loading span{width:50px;height:50px;opacity:.87}.features.error{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:inherit;font-size:14px}.features.error img{margin:10px 0;width:300px}.upgrade .base,.upgrade .pro{margin:20px 20px 13px;width:100%;text-align:center;opacity:1;-webkit-transition:opacity .2s cubic-bezier(.23,1,.32,1) 0ms;transition:opacity .2s cubic-bezier(.23,1,.32,1) 0ms}.upgrade .pricecard{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;border-radius:4px;border:1px solid #eef1f4;position:relative}.upgrade .pro .pricecard{border:2px solid #4dbb7c;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.upgrade .pricecard .mode{margin:20px 10px 10px;font-size:18px;font-weight:700}.upgrade .pricecard .sales{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;height:92px;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center}.upgrade .pricecard .discountrate{position:absolute;top:-12px;left:0;right:0}.upgrade .pricecard .discountrate .rate{padding:3px 10px;color:#fff;background-color:#4dbb7c;font-weight:700;font-size:13px;border-radius:10px}.upgrade .pricecard .desc{font-size:30px}.upgrade .pricecard .desc del{font-size:15px;font-weight:700;text-decoration:line-through}.upgrade .pricecard .price{position:relative;color:#4dbb7c;font-size:30px;font-weight:700}.upgrade .pricecard .message{position:relative;font-size:11px;font-weight:400;background-image:-webkit-linear-gradient(top,hsla(0,0%,100%,0) 50%,#ffeb3b 0);background-image:linear-gradient(180deg,hsla(0,0%,100%,0) 50%,#ffeb3b 0)}.upgrade .pricecard .countdown{margin-top:5px}.upgrade .pricecard .billing{position:relative;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:30px 20%;padding:5px;color:#333;background-color:#e2e2e2;font-size:15px;font-style:normal;font-weight:700;border-radius:4px;cursor:pointer}.upgrade .pricecard .billing,.upgrade .pricecard .billing i{-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms}.upgrade .pricecard .billing i{height:27px;line-height:22px;margin-left:5px}.upgrade .pricecard .billing:hover{box-shadow:0 3px 1px -2px rgba(0,0,0,.2),0 2px 2px 0 rgba(0,0,0,.14),0 1px 5px 0 rgba(0,0,0,.12)}.upgrade .pricecard .billing:hover i{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.upgrade .pricecard .billing:hover .dropdown-price{opacity:1;-webkit-transform:scale(1);transform:scale(1)}.upgrade .pricecard .billing .dropdown-price{position:absolute;left:-41px;top:38px;-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:270px;color:rgba(51,51,51,.87);font-size:12px;text-shadow:none;box-sizing:border-box;border-radius:4px;box-shadow:0 8px 10px 1px rgba(0,0,0,.14),0 3px 14px 2px rgba(0,0,0,.12),0 5px 5px -3px rgba(0,0,0,.2);opacity:0;-webkit-transform:scaleY(0);transform:scaleY(0);-webkit-transform-origin:left top 0;transform-origin:left top 0;-webkit-transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms,-webkit-transform .45s cubic-bezier(.23,1,.32,1) 0ms;transition:opacity 1s cubic-bezier(.23,1,.32,1) 0ms,-webkit-transform .45s cubic-bezier(.23,1,.32,1) 0ms;transition:transform .45s cubic-bezier(.23,1,.32,1) 0ms,opacity 1s cubic-bezier(.23,1,.32,1) 0ms;transition:transform .45s cubic-bezier(.23,1,.32,1) 0ms,opacity 1s cubic-bezier(.23,1,.32,1) 0ms,-webkit-transform .45s cubic-bezier(.23,1,.32,1) 0ms;z-index:1}.upgrade .pricecard .billing .dropdown-price,.upgrade .pricecard .billing .dropdown-price .store{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#fff}.upgrade .pricecard .billing .dropdown-price .store{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;padding:8px 24px 8px 16px;width:100%;-webkit-transition:all 1s cubic-bezier(.23,1,.32,1) 0ms;transition:all 1s cubic-bezier(.23,1,.32,1) 0ms;cursor:pointer}.upgrade .pricecard .billing .dropdown-price .store:hover{background-color:#eee}.upgrade .pricecard .billing .dropdown-price .store:hover i{-webkit-transform:rotate(270deg) translateY(7px);transform:rotate(270deg) translateY(7px)}.upgrade .pricecard .billing .dropdown-price .store .names{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;width:135px}.upgrade .pricecard .billing .dropdown-price .store .des{width:100%;color:rgba(51,51,51,.56);text-align:left;font-size:10px;-webkit-transform:scale(.8) translateX(-17px);transform:scale(.8) translateX(-17px)}.upgrade .pricecard .billing .dropdown-price .store .num{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;width:46px}.upgrade .pricecard .billing .dropdown-price .store .tips{-webkit-transform:scale(.8);transform:scale(.8);color:#4dbb7c}.upgrade .pricecard .billing .dropdown-price .store i{-webkit-transform:rotate(270deg);transform:rotate(270deg)}.upgrade .base[data-enable=false] .pricecard,.upgrade .pro[data-enable=true] .pricecard{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;height:210px}.upgrade .base[data-enable=false] .pricecard .mode,.upgrade .pro[data-enable=true] .pricecard .mode{font-size:25px}.upgrade .base[data-enable=false] .pricecard .billing,.upgrade .base[data-enable=false] .pricecard .sales,.upgrade .pro[data-enable=true] .pricecard .billing .dropdown-price,.upgrade .pro[data-enable=true] .pricecard .billing i,.upgrade .pro[data-enable=true] .pricecard .discountrate,.upgrade .pro[data-enable=true] .pricecard .sales{display:none}.upgrade .pro[data-enable=true] .pricecard .billing{position:absolute;top:-28px;left:0;right:0;display:inherit;margin:10px 20%;border-radius:30px}.upgrade .pro .billing{color:#fff;background-color:#4dbb7c}.upgrade .features.diff{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;margin-top:20px}.upgrade .features.diff,.upgrade .features .feature{-webkit-box-direction:normal;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.upgrade .features .feature{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:unset;-ms-flex-pack:unset;justify-content:unset;margin:14px 20px 0;font-size:15px}.upgrade .features .feature.empty{height:27px}.upgrade .features .icon{margin-right:10px;width:15px}.upgrade .features .label{width:120px;font-size:15px;text-align:left}.upgrade .features a{color:inherit;cursor:auto}.upgrade .features a.active{padding-bottom:5px;border-bottom:1px dotted;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;cursor:pointer}.upgrade .features a.active:hover{color:#4285f4}.upgrade .features .label .remark{margin-left:5px;padding:2px 5px;background-color:#ffeb3b;font-size:12px;font-weight:400;border-radius:4px}.upgrade .features .label .remark.roadmap{background-color:#e2e2e2}.upgrade .ticket{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;font-size:13px}.upgrade .ticket .message{width:70%;text-align:center}.upgrade .ticket .line{margin:7px 0 0;width:100%;height:1px;background-image:-webkit-linear-gradient(.1deg,rgba(255,18,18,0) -2.8%,#e2e2e2 50.8%,rgba(0,159,8,0) 107.9%);background-image:linear-gradient(89.9deg,rgba(255,18,18,0) -2.8%,#e2e2e2 50.8%,rgba(0,159,8,0) 107.9%)}.upgrade .ticket .notice{margin:20px 20%;padding:5px 20px;color:#333;background-color:#e2e2e2;font-size:15px;font-weight:400;border-radius:4px}.upgrade .ticket .content{margin:0 0 13px;width:80%}.upgrade .ticket .content li{margin-bottom:6px}.upgrade .ticket .content li:last-child{margin-bottom:0}.upgrade .ticket .last{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-pack:distribute;justify-content:space-around;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:20px 0;width:100%}.upgrade .carousels{margin:20px 20px 13px}.upgrade .carousel.carousel-slider{height:420px;border-radius:4px;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.upgrade .carousels setion{position:relative}.upgrade .carousels setion img{margin-top:-82px;width:100%}.upgrade .carousels .descr{position:absolute;left:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:80px;width:100%;padding-bottom:10px;background-color:#fff;font-size:17px}.simpread-upgrade-root.mini dialog-gp{border-radius:15px!important}.simpread-upgrade-root.mini dialog-content{padding:0!important;width:650px!important}.simpread-upgrade-root.mini dialog-gp .close{position:absolute}.simpread-upgrade-root.mini .upgrade .carousels{margin:0}.simpread-upgrade-root.mini .upgrade .carousels .descr{padding-bottom:70px;height:130px}.simpread-upgrade-root.mini .upgrade .carousel.carousel-slider{height:450px}.simpread-upgrade-root.mini .floating .billing{margin-bottom:30px;min-height:40px}.simpread-upgrade-root.mini footer{position:absolute;top:199px;left:-60px;right:-60px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;-webkit-box-align:center;-ms-flex-align:center;align-items:center}</style><style type="text/css">.simpread-tipsalert-root dialog-gp{position:absolute}.simpread-tipsalert-root dialog-gp .close{position:absolute;top:0;right:0;z-index:2}.simpread-tipsalert-root dialog-content{padding:0!important;width:650px!important}.simpread-tipsalert-root .details{position:relative;color:rgba(51,51,51,.87);font-family:Hiragino Sans GB,Microsoft Yahei;text-shadow:none}.simpread-tipsalert-root .details .carousel.carousel-slider{height:450px;border-radius:4px;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.simpread-tipsalert-root .details .carousels setion sr-div-center{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;height:321px;box-sizing:border-box}.simpread-tipsalert-root .details .carousels setion sr-div-center.hidden{display:none}.simpread-tipsalert-root .details .carousels setion sr-div-center img{margin-top:20px!important;height:321px;width:auto!important}.simpread-tipsalert-root .details .carousels setion sr-div-center sr-video{position:absolute;left:0;right:0;width:100%;height:321px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;z-index:2}.simpread-tipsalert-root .details .carousels setion .tipsimg.error{width:0!important;height:0!important}.simpread-tipsalert-root .details .carousels setion .tipsimg:after{content:"\F1C5" " Sorry, the image below is broken :(";font-family:Font Awesome\ 5 Free;font-weight:900;position:absolute;top:0;left:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:650px;height:100%;color:#646464;background-color:#fff;z-index:2}.simpread-tipsalert-root .details .carousels setion sr-div-center sr-video+img{opacity:.5}.simpread-tipsalert-root .details .carousels setion img{margin-top:-82px;width:100%}.simpread-tipsalert-root .details .carousels setion video{height:321px}.simpread-tipsalert-root .details .carousels setion video.active{display:block}.simpread-tipsalert-root .details .carousels .descr{position:absolute;left:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:80px;width:100%;padding-bottom:10px;background-color:#fff;font-size:14px}.simpread-tipsalert-root .details .carousels .descr b{margin:0 3px}.simpread-tipsalert-root .details .carousels .descr.large{font-size:17px}.simpread-tipsalert-root .floating{position:absolute;left:0;right:0;bottom:14px;height:80px;overflow-y:hidden}.simpread-tipsalert-root .floating,.simpread-tipsalert-root .floating .docs{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.simpread-tipsalert-root .floating .docs{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;padding:1px 40px;height:30px;color:#fff;background-color:#4dbb7c;font-size:15px;font-weight:400;border-radius:30px;-webkit-transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;transition:all .5s cubic-bezier(.23,1,.32,1) 0ms;box-shadow:0 12px 18px -6px rgba(0,0,0,.3)}.simpread-tipsalert-root footer{position:absolute;top:199px;left:-60px;right:-60px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;-webkit-box-align:center;-ms-flex-align:center;align-items:center}</style><style type="text/css">sr-annote-popup{display:block;width:480px}sr-annote-popup-gp{position:relative;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;margin-bottom:25px}sr-annote-popup-label{color:rgba(0,137,123,.8);font-size:14px;font-weight:700;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;pointer-events:none;-webkit-transition:all .45s cubic-bezier(.23,1,.32,1) 0ms;transition:all .45s cubic-bezier(.23,1,.32,1) 0ms;-webkit-transform:scale(.75) translateY(-8px);transform:scale(.75) translateY(-8px);-webkit-transform-origin:left top 0;transform-origin:left top 0}sr-annote-popup-desc{position:relative;padding:10px 30px;font-size:15px;text-align:justify;color:#555;line-height:1.6;border-bottom:1px solid #e0e0e0}sr-annote-popup-desc:before{content:"\201C";position:absolute;left:-5px;top:-24px;color:rgba(0,137,123,.8);font-family:Arial;font-size:4em}sr-annote-popup-desc sr-annote{background-color:transparent!important}sr-annote-popup-gp[type=img]{-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-annote-popup-gp ol,sr-annote-popup-gp ul{margin:0 0 1.2em;margin-left:1.3em;padding:0;list-style:disc}sr-annote-popup-gp ol li,sr-annote-popup-gp ul li{margin:0 0 1.2em;font-size:15px;list-style:disc}sr-annote-popup-gp a{padding:0 5px;vertical-align:baseline;vertical-align:initial}sr-annote-popup-gp a,sr-annote-popup-gp a:link{color:#463f5c;font-size:inherit;font-weight:inherit;text-decoration:underline;border:none}sr-annote-popup-gp a:hover{background:transparent}sr-annote-popup-gp img{margin:0;padding:0;max-width:50%;height:auto;background:#fff;border:0;border-radius:6px;box-shadow:0 20px 20px -10px rgba(0,0,0,.1)}sr-annote-popup-gp pre{padding:10px!important;background-color:transparent!important;border-radius:6px!important;box-shadow:0 20px 20px -10px rgba(0,0,0,.1);overflow-x:auto}sr-annote-popup auto-complete list-view{max-height:150px!important}sr-annote-popup list-view::-webkit-scrollbar-thumb{background-clip:padding-box;border-radius:10px;border:2px solid transparent;background-color:rgba(85,85,85,.55)}sr-annote-popup list-view::-webkit-scrollbar{width:10px;-webkit-transition:width .7s cubic-bezier(.4,0,.2,1);transition:width .7s cubic-bezier(.4,0,.2,1)}sr-annote-popup-gp sr-annote-floatingbar{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:left;-ms-flex-pack:left;justify-content:left}sr-annote-popup-gp sr-annote-floatingbar sr-anote-fb-item[data-color-type]{display:-webkit-box;display:-ms-flexbox;display:flex;width:30px;height:30px;border-radius:50%}sr-annote-popup-gp sr-annote-floatingbar sr-anote-fb-item[data-color-type],sr-annote-popup-gp sr-anote-item{-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-right:20px}sr-annote-popup-gp sr-anote-item{display:-webkit-box!important;display:-ms-flexbox!important;display:flex!important;padding:0 5px;font-size:13px;font-weight:400;-webkit-transition:all .5s ease-in-out 0ms;transition:all .5s ease-in-out 0ms}sr-annote-popup-gp sr-anote-lock{position:absolute;left:0;top:-3px;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;background-color:hsla(0,0%,100%,.8);z-index:1}sr-annote-popup-gp sr-anote-lock svg{margin-top:3px;cursor:pointer}</style><style type="text/css">.gu-mirror{position:fixed!important;margin:0!important;z-index:9999!important;opacity:.8;-ms-filter:"progid:DXImageTransform.Microsoft.Alpha(Opacity=80)";filter:alpha(opacity=80)}.gu-hide{display:none!important}.gu-unselectable{-webkit-user-select:none!important;-moz-user-select:none!important;-ms-user-select:none!important;user-select:none!important}.gu-transit{opacity:.2;-ms-filter:"progid:DXImageTransform.Microsoft.Alpha(Opacity=20)";filter:alpha(opacity=20)}</style><style type="text/css">sr-search{-webkit-box-orient:vertical;-ms-flex-direction:column;flex-direction:column;padding:10px;border:1px solid #dfe1e5;border-radius:8px}sr-search,sr-search sr-search-header{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-direction:normal;margin-bottom:10px}sr-search sr-search-header{-webkit-box-orient:horizontal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-search sr-search-header img{margin-right:10px;width:22px}sr-search sr-search-header sr-search-span{font-weight:700}sr-search-unreader-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;border-bottom:1px solid #dfe1e5;margin-bottom:5px}sr-search-content{max-height:666px;overflow-x:hidden;overflow-y:auto}sr-search-unreader-title{font-weight:700;font-size:15px;margin-bottom:5px}sr-search-unreader-create{margin-bottom:5px;color:#70757a}sr-search-unreader-desc{margin-bottom:5px}sr-search-unreader-tags{margin-bottom:5px;color:#70757a;font-size:11px;font-style:italic}sr-search-unreader-tag{margin-right:5px}sr-search-paging{width:100%;margin:10px}sr-search-paging,sr-search-paging sr-search-more{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center}sr-search-paging sr-search-more{width:36px;height:36px;box-shadow:0 0 0 1px rgba(0,0,0,.04),0 4px 8px 0 rgba(0,0,0,.2);border-radius:50%;opacity:.9;cursor:pointer}sr-search-paging sr-search-more:hover{opacity:1}sr-search-paging sr-search-more.disable{cursor:no-drop}sr-search-paging sr-search-more svg{width:24px;height:24px;fill:#757575}sr-search-info{text-align:center}</style><link as="script" rel="prefetch" href="./ICLR 2021 Conference _ OpenReview_files/index-8d5ab195e774e3fc0705.js.下载"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/login-9489325c3dba66c15b36.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/about-21ad854b65cdc28a5879.js"><link as="script" rel="prefetch" href="./ICLR 2021 Conference _ OpenReview_files/group-11f7765f7db1e915182b.js.下载"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/venues-41b2f7835ab776406336.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/contact-c0f4dc3837847c913ba0.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/faq-6d1c8a0d3e1b4eea5c70.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/legal/terms-1acd9435966a971c48c8.js"><link as="script" rel="prefetch" href="https://openreview.net/_next/static/chunks/pages/legal/privacy-a259b7f57265e0f174e8.js"><link as="fetch" rel="prefetch" href="https://openreview.net/_next/data/v1.0.9-1-gf539684/faq.json"><style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style id="MJX-CHTML-styles">
mjx-container[jax="CHTML"] {
  line-height: 0;
}

mjx-container [space="1"] {
  margin-left: .111em;
}

mjx-container [space="2"] {
  margin-left: .167em;
}

mjx-container [space="3"] {
  margin-left: .222em;
}

mjx-container [space="4"] {
  margin-left: .278em;
}

mjx-container [space="5"] {
  margin-left: .333em;
}

mjx-container [rspace="1"] {
  margin-right: .111em;
}

mjx-container [rspace="2"] {
  margin-right: .167em;
}

mjx-container [rspace="3"] {
  margin-right: .222em;
}

mjx-container [rspace="4"] {
  margin-right: .278em;
}

mjx-container [rspace="5"] {
  margin-right: .333em;
}

mjx-container [size="s"] {
  font-size: 70.7%;
}

mjx-container [size="ss"] {
  font-size: 50%;
}

mjx-container [size="Tn"] {
  font-size: 60%;
}

mjx-container [size="sm"] {
  font-size: 85%;
}

mjx-container [size="lg"] {
  font-size: 120%;
}

mjx-container [size="Lg"] {
  font-size: 144%;
}

mjx-container [size="LG"] {
  font-size: 173%;
}

mjx-container [size="hg"] {
  font-size: 207%;
}

mjx-container [size="HG"] {
  font-size: 249%;
}

mjx-container [width="full"] {
  width: 100%;
}

mjx-box {
  display: inline-block;
}

mjx-block {
  display: block;
}

mjx-itable {
  display: inline-table;
}

mjx-row {
  display: table-row;
}

mjx-row > * {
  display: table-cell;
}

mjx-mtext {
  display: inline-block;
  text-align: left;
}

mjx-mstyle {
  display: inline-block;
}

mjx-merror {
  display: inline-block;
  color: red;
  background-color: yellow;
}

mjx-mphantom {
  visibility: hidden;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-math {
  display: inline-block;
  text-align: left;
  line-height: 0;
  text-indent: 0;
  font-style: normal;
  font-weight: normal;
  font-size: 100%;
  font-size-adjust: none;
  letter-spacing: normal;
  word-wrap: normal;
  word-spacing: normal;
  white-space: nowrap;
  direction: ltr;
  padding: 1px 0;
}

mjx-container[jax="CHTML"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="CHTML"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="CHTML"][display="true"] mjx-math {
  padding: 0;
}

mjx-container[jax="CHTML"][justify="left"] {
  text-align: left;
}

mjx-container[jax="CHTML"][justify="right"] {
  text-align: right;
}

mjx-mi {
  display: inline-block;
  text-align: left;
}

mjx-mo {
  display: inline-block;
  text-align: left;
}

mjx-stretchy-h {
  display: inline-table;
  width: 100%;
}

mjx-stretchy-h > * {
  display: table-cell;
  width: 0;
}

mjx-stretchy-h > * > mjx-c {
  display: inline-block;
  transform: scalex(1.0000001);
}

mjx-stretchy-h > * > mjx-c::before {
  display: inline-block;
  padding: .001em 0;
  width: initial;
}

mjx-stretchy-h > mjx-ext {
  overflow: hidden;
  width: 100%;
}

mjx-stretchy-h > mjx-ext > mjx-c::before {
  transform: scalex(500);
}

mjx-stretchy-h > mjx-ext > mjx-c {
  width: 0;
}

mjx-stretchy-h > mjx-beg > mjx-c {
  margin-right: -.1em;
}

mjx-stretchy-h > mjx-end > mjx-c {
  margin-left: -.1em;
}

mjx-stretchy-v {
  display: inline-block;
}

mjx-stretchy-v > * {
  display: block;
}

mjx-stretchy-v > mjx-beg {
  height: 0;
}

mjx-stretchy-v > mjx-end > mjx-c {
  display: block;
}

mjx-stretchy-v > * > mjx-c {
  transform: scaley(1.0000001);
  transform-origin: left center;
  overflow: hidden;
}

mjx-stretchy-v > mjx-ext {
  display: block;
  height: 100%;
  box-sizing: border-box;
  border: 0px solid transparent;
  overflow: hidden;
}

mjx-stretchy-v > mjx-ext > mjx-c::before {
  width: initial;
}

mjx-stretchy-v > mjx-ext > mjx-c {
  transform: scaleY(500) translateY(.1em);
  overflow: visible;
}

mjx-mark {
  display: inline-block;
  height: 0px;
}

mjx-mn {
  display: inline-block;
  text-align: left;
}

mjx-msqrt {
  display: inline-block;
  text-align: left;
}

mjx-root {
  display: inline-block;
  white-space: nowrap;
}

mjx-surd {
  display: inline-block;
  vertical-align: top;
}

mjx-sqrt {
  display: inline-block;
  padding-top: .07em;
}

mjx-sqrt > mjx-box {
  border-top: .07em solid;
}

mjx-sqrt.mjx-tall > mjx-box {
  padding-left: .3em;
  margin-left: -.3em;
}

mjx-mroot {
  display: inline-block;
  text-align: left;
}

mjx-msub {
  display: inline-block;
  text-align: left;
}

mjx-msup {
  display: inline-block;
  text-align: left;
}

mjx-msubsup {
  display: inline-block;
  text-align: left;
}

mjx-script {
  display: inline-block;
  padding-right: .05em;
}

mjx-script > * {
  display: block;
}

mjx-munder {
  display: inline-block;
  text-align: left;
}

mjx-over {
  text-align: left;
}

mjx-munder:not([limits="false"]) {
  display: inline-table;
}

mjx-munder > mjx-row {
  text-align: left;
}

mjx-under {
  padding-bottom: .1em;
}

mjx-mover {
  display: inline-block;
  text-align: left;
}

mjx-mover:not([limits="false"]) {
  padding-top: .1em;
}

mjx-mover:not([limits="false"]) > * {
  display: block;
  text-align: left;
}

mjx-munderover {
  display: inline-block;
  text-align: left;
}

mjx-munderover:not([limits="false"]) {
  padding-top: .1em;
}

mjx-munderover:not([limits="false"]) > * {
  display: block;
}

mjx-mmultiscripts {
  display: inline-block;
  text-align: left;
}

mjx-prescripts {
  display: inline-table;
  padding-left: .05em;
}

mjx-scripts {
  display: inline-table;
  padding-right: .05em;
}

mjx-prescripts > mjx-row > mjx-cell {
  text-align: right;
}

mjx-TeXAtom {
  display: inline-block;
  text-align: left;
}

mjx-c {
  display: inline-block;
}

mjx-utext {
  display: inline-block;
  padding: .75em 0 .2em 0;
}

mjx-c::before {
  display: block;
  width: 0;
}

.MJX-TEX {
  font-family: MJXZERO, MJXTEX;
}

.TEX-B {
  font-family: MJXZERO, MJXTEX-B;
}

.TEX-I {
  font-family: MJXZERO, MJXTEX-I;
}

.TEX-MI {
  font-family: MJXZERO, MJXTEX-MI;
}

.TEX-BI {
  font-family: MJXZERO, MJXTEX-BI;
}

.TEX-S1 {
  font-family: MJXZERO, MJXTEX-S1;
}

.TEX-S2 {
  font-family: MJXZERO, MJXTEX-S2;
}

.TEX-S3 {
  font-family: MJXZERO, MJXTEX-S3;
}

.TEX-S4 {
  font-family: MJXZERO, MJXTEX-S4;
}

.TEX-A {
  font-family: MJXZERO, MJXTEX-A;
}

.TEX-C {
  font-family: MJXZERO, MJXTEX-C;
}

.TEX-CB {
  font-family: MJXZERO, MJXTEX-CB;
}

.TEX-FR {
  font-family: MJXZERO, MJXTEX-FR;
}

.TEX-FRB {
  font-family: MJXZERO, MJXTEX-FRB;
}

.TEX-SS {
  font-family: MJXZERO, MJXTEX-SS;
}

.TEX-SSB {
  font-family: MJXZERO, MJXTEX-SSB;
}

.TEX-SSI {
  font-family: MJXZERO, MJXTEX-SSI;
}

.TEX-SC {
  font-family: MJXZERO, MJXTEX-SC;
}

.TEX-T {
  font-family: MJXZERO, MJXTEX-T;
}

.TEX-V {
  font-family: MJXZERO, MJXTEX-V;
}

.TEX-VB {
  font-family: MJXZERO, MJXTEX-VB;
}

mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c {
  font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important;
}

@font-face /* 0 */ {
  font-family: MJXZERO;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff");
}

@font-face /* 1 */ {
  font-family: MJXTEX;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff");
}

@font-face /* 2 */ {
  font-family: MJXTEX-B;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff");
}

@font-face /* 3 */ {
  font-family: MJXTEX-I;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff");
}

@font-face /* 4 */ {
  font-family: MJXTEX-MI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff");
}

@font-face /* 5 */ {
  font-family: MJXTEX-BI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff");
}

@font-face /* 6 */ {
  font-family: MJXTEX-S1;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff");
}

@font-face /* 7 */ {
  font-family: MJXTEX-S2;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff");
}

@font-face /* 8 */ {
  font-family: MJXTEX-S3;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff");
}

@font-face /* 9 */ {
  font-family: MJXTEX-S4;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff");
}

@font-face /* 10 */ {
  font-family: MJXTEX-A;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff");
}

@font-face /* 11 */ {
  font-family: MJXTEX-C;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff");
}

@font-face /* 12 */ {
  font-family: MJXTEX-CB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff");
}

@font-face /* 13 */ {
  font-family: MJXTEX-FR;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff");
}

@font-face /* 14 */ {
  font-family: MJXTEX-FRB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff");
}

@font-face /* 15 */ {
  font-family: MJXTEX-SS;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff");
}

@font-face /* 16 */ {
  font-family: MJXTEX-SSB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff");
}

@font-face /* 17 */ {
  font-family: MJXTEX-SSI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff");
}

@font-face /* 18 */ {
  font-family: MJXTEX-SC;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff");
}

@font-face /* 19 */ {
  font-family: MJXTEX-T;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff");
}

@font-face /* 20 */ {
  font-family: MJXTEX-V;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff");
}

@font-face /* 21 */ {
  font-family: MJXTEX-VB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff");
}

mjx-c.mjx-c20::before {
  padding: 0 0.25em 0 0;
  content: " ";
}

mjx-c.mjx-c25::before {
  padding: 0.75em 0.833em 0.056em 0;
  content: "%";
}

mjx-c.mjx-c28::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: "(";
}

mjx-c.mjx-c29::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: ")";
}

mjx-c.mjx-c2B::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "+";
}

mjx-c.mjx-c2C::before {
  padding: 0.121em 0.278em 0.194em 0;
  content: ",";
}

mjx-c.mjx-c2E::before {
  padding: 0.12em 0.278em 0 0;
  content: ".";
}

mjx-c.mjx-c2F::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "/";
}

mjx-c.mjx-c30::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "0";
}

mjx-c.mjx-c31::before {
  padding: 0.666em 0.5em 0 0;
  content: "1";
}

mjx-c.mjx-c32::before {
  padding: 0.666em 0.5em 0 0;
  content: "2";
}

mjx-c.mjx-c33::before {
  padding: 0.665em 0.5em 0.022em 0;
  content: "3";
}

mjx-c.mjx-c35::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "5";
}

mjx-c.mjx-c3A::before {
  padding: 0.43em 0.278em 0 0;
  content: ":";
}

mjx-c.mjx-c3D::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "=";
}

mjx-c.mjx-c3E::before {
  padding: 0.54em 0.778em 0.04em 0;
  content: ">";
}

mjx-c.mjx-c5C::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "\5C";
}

mjx-c.mjx-c61::before {
  padding: 0.448em 0.5em 0.011em 0;
  content: "a";
}

mjx-c.mjx-c67::before {
  padding: 0.453em 0.5em 0.206em 0;
  content: "g";
}

mjx-c.mjx-c69::before {
  padding: 0.669em 0.278em 0 0;
  content: "i";
}

mjx-c.mjx-c6C::before {
  padding: 0.694em 0.278em 0 0;
  content: "l";
}

mjx-c.mjx-c6D::before {
  padding: 0.442em 0.833em 0 0;
  content: "m";
}

mjx-c.mjx-c6E::before {
  padding: 0.442em 0.556em 0 0;
  content: "n";
}

mjx-c.mjx-c6F::before {
  padding: 0.448em 0.5em 0.01em 0;
  content: "o";
}

mjx-c.mjx-c72::before {
  padding: 0.442em 0.392em 0 0;
  content: "r";
}

mjx-c.mjx-c73::before {
  padding: 0.448em 0.394em 0.011em 0;
  content: "s";
}

mjx-c.mjx-c75::before {
  padding: 0.442em 0.556em 0.011em 0;
  content: "u";
}

mjx-c.mjx-c78::before {
  padding: 0.431em 0.528em 0 0;
  content: "x";
}

mjx-c.mjx-c7A::before {
  padding: 0.431em 0.444em 0 0;
  content: "z";
}

mjx-c.mjx-c7B::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "{";
}

mjx-c.mjx-c7D::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "}";
}

mjx-c.mjx-cA0::before {
  padding: 0 0.25em 0 0;
  content: "\A0";
}

mjx-c.mjx-cB1::before {
  padding: 0.666em 0.778em 0 0;
  content: "\B1";
}

mjx-c.mjx-cD7::before {
  padding: 0.491em 0.778em 0 0;
  content: "\D7";
}

mjx-c.mjx-c398::before {
  padding: 0.705em 0.778em 0.022em 0;
  content: "\398";
}

mjx-c.mjx-c3A9::before {
  padding: 0.704em 0.722em 0 0;
  content: "\3A9";
}

mjx-c.mjx-c2061::before {
  padding: 0 0 0 0;
  content: "";
}

mjx-c.mjx-c210E.TEX-I::before {
  padding: 0.694em 0.576em 0.011em 0;
  content: "h";
}

mjx-c.mjx-c2113::before {
  padding: 0.705em 0.417em 0.02em 0;
  content: "\2113";
}

mjx-c.mjx-c2115.TEX-A::before {
  padding: 0.683em 0.722em 0.02em 0;
  content: "N";
}

mjx-c.mjx-c211D.TEX-A::before {
  padding: 0.683em 0.722em 0 0;
  content: "R";
}

mjx-c.mjx-c2192::before {
  padding: 0.511em 1em 0.011em 0;
  content: "\2192";
}

mjx-c.mjx-c2208::before {
  padding: 0.54em 0.667em 0.04em 0;
  content: "\2208";
}

mjx-c.mjx-c2212::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "\2212";
}

mjx-c.mjx-c2217::before {
  padding: 0.465em 0.5em 0 0;
  content: "\2217";
}

mjx-c.mjx-c221A::before {
  padding: 0.8em 0.853em 0.2em 0;
  content: "\221A";
}

[noIC] mjx-c.mjx-c221A:last-child::before {
  padding-right: 0.833em;
}

mjx-c.mjx-c221E::before {
  padding: 0.442em 1em 0.011em 0;
  content: "\221E";
}

mjx-c.mjx-c2265::before {
  padding: 0.636em 0.778em 0.138em 0;
  content: "\2265";
}

mjx-c.mjx-c22C5::before {
  padding: 0.31em 0.278em 0 0;
  content: "\22C5";
}

mjx-c.mjx-c1D400.TEX-B::before {
  padding: 0.698em 0.869em 0 0;
  content: "A";
}

mjx-c.mjx-c1D431.TEX-B::before {
  padding: 0.444em 0.607em 0 0;
  content: "x";
}

mjx-c.mjx-c1D433.TEX-B::before {
  padding: 0.444em 0.511em 0 0;
  content: "z";
}

mjx-c.mjx-c1D434.TEX-I::before {
  padding: 0.716em 0.75em 0 0;
  content: "A";
}

mjx-c.mjx-c1D439.TEX-I::before {
  padding: 0.68em 0.749em 0 0;
  content: "F";
}

[noIC] mjx-c.mjx-c1D439.TEX-I:last-child::before {
  padding-right: 0.643em;
}

mjx-c.mjx-c1D43A.TEX-I::before {
  padding: 0.705em 0.786em 0.022em 0;
  content: "G";
}

mjx-c.mjx-c1D43C.TEX-I::before {
  padding: 0.683em 0.504em 0 0;
  content: "I";
}

[noIC] mjx-c.mjx-c1D43C.TEX-I:last-child::before {
  padding-right: 0.44em;
}

mjx-c.mjx-c1D43E.TEX-I::before {
  padding: 0.683em 0.889em 0 0;
  content: "K";
}

[noIC] mjx-c.mjx-c1D43E.TEX-I:last-child::before {
  padding-right: 0.849em;
}

mjx-c.mjx-c1D43F.TEX-I::before {
  padding: 0.683em 0.681em 0 0;
  content: "L";
}

mjx-c.mjx-c1D441.TEX-I::before {
  padding: 0.683em 0.888em 0 0;
  content: "N";
}

[noIC] mjx-c.mjx-c1D441.TEX-I:last-child::before {
  padding-right: 0.803em;
}

mjx-c.mjx-c1D442.TEX-I::before {
  padding: 0.704em 0.763em 0.022em 0;
  content: "O";
}

mjx-c.mjx-c1D447.TEX-I::before {
  padding: 0.677em 0.704em 0 0;
  content: "T";
}

[noIC] mjx-c.mjx-c1D447.TEX-I:last-child::before {
  padding-right: 0.584em;
}

mjx-c.mjx-c1D449.TEX-I::before {
  padding: 0.683em 0.769em 0.022em 0;
  content: "V";
}

[noIC] mjx-c.mjx-c1D449.TEX-I:last-child::before {
  padding-right: 0.583em;
}

mjx-c.mjx-c1D44E.TEX-I::before {
  padding: 0.441em 0.529em 0.01em 0;
  content: "a";
}

mjx-c.mjx-c1D44F.TEX-I::before {
  padding: 0.694em 0.429em 0.011em 0;
  content: "b";
}

mjx-c.mjx-c1D450.TEX-I::before {
  padding: 0.442em 0.433em 0.011em 0;
  content: "c";
}

mjx-c.mjx-c1D451.TEX-I::before {
  padding: 0.694em 0.52em 0.01em 0;
  content: "d";
}

mjx-c.mjx-c1D452.TEX-I::before {
  padding: 0.442em 0.466em 0.011em 0;
  content: "e";
}

mjx-c.mjx-c1D453.TEX-I::before {
  padding: 0.705em 0.55em 0.205em 0;
  content: "f";
}

[noIC] mjx-c.mjx-c1D453.TEX-I:last-child::before {
  padding-right: 0.49em;
}

mjx-c.mjx-c1D454.TEX-I::before {
  padding: 0.442em 0.477em 0.205em 0;
  content: "g";
}

mjx-c.mjx-c1D456.TEX-I::before {
  padding: 0.661em 0.345em 0.011em 0;
  content: "i";
}

mjx-c.mjx-c1D458.TEX-I::before {
  padding: 0.694em 0.521em 0.011em 0;
  content: "k";
}

mjx-c.mjx-c1D459.TEX-I::before {
  padding: 0.694em 0.298em 0.011em 0;
  content: "l";
}

mjx-c.mjx-c1D45A.TEX-I::before {
  padding: 0.442em 0.878em 0.011em 0;
  content: "m";
}

mjx-c.mjx-c1D45B.TEX-I::before {
  padding: 0.442em 0.6em 0.011em 0;
  content: "n";
}

mjx-c.mjx-c1D45C.TEX-I::before {
  padding: 0.441em 0.485em 0.011em 0;
  content: "o";
}

mjx-c.mjx-c1D45D.TEX-I::before {
  padding: 0.442em 0.503em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c1D45F.TEX-I::before {
  padding: 0.442em 0.451em 0.011em 0;
  content: "r";
}

mjx-c.mjx-c1D460.TEX-I::before {
  padding: 0.442em 0.469em 0.01em 0;
  content: "s";
}

mjx-c.mjx-c1D461.TEX-I::before {
  padding: 0.626em 0.361em 0.011em 0;
  content: "t";
}

mjx-c.mjx-c1D462.TEX-I::before {
  padding: 0.442em 0.572em 0.011em 0;
  content: "u";
}

mjx-c.mjx-c1D464.TEX-I::before {
  padding: 0.443em 0.716em 0.011em 0;
  content: "w";
}

mjx-c.mjx-c1D465.TEX-I::before {
  padding: 0.442em 0.572em 0.011em 0;
  content: "x";
}

mjx-c.mjx-c1D466.TEX-I::before {
  padding: 0.442em 0.49em 0.205em 0;
  content: "y";
}

mjx-c.mjx-c1D6FD.TEX-I::before {
  padding: 0.705em 0.566em 0.194em 0;
  content: "\3B2";
}

mjx-c.mjx-c1D700.TEX-I::before {
  padding: 0.452em 0.466em 0.022em 0;
  content: "\3B5";
}

mjx-c.mjx-c1D7CE.TEX-B::before {
  padding: 0.654em 0.575em 0.01em 0;
  content: "0";
}

mjx-c.mjx-c1D7E4.TEX-SS::before {
  padding: 0.677em 0.5em 0 0;
  content: "2";
}

mjx-c.mjx-c2D.TEX-MI::before {
  padding: 0.251em 0.358em 0 0;
  content: "-";
}

mjx-c.mjx-c4F.TEX-C::before {
  padding: 0.705em 0.796em 0.022em 0;
  content: "O";
}
</style></head><body><div id="__next"><nav class="navbar navbar-inverse navbar-fixed-top" role="navigation"><div class="container"><div class="navbar-header"><button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="sr-only">Toggle navigation</span><span class="icon-bar"></span><span class="icon-bar"></span><span class="icon-bar"></span></button><a class="navbar-brand home push-link" href="https://openreview.net/"><strong>OpenReview</strong>.net</a></div><div id="navbar" class="navbar-collapse collapse"><form class="navbar-form navbar-left profile-search" role="search"><div class="form-group has-feedback"><input type="text" name="term" class="form-control" value="" placeholder="Search OpenReview..." autocomplete="off"><span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span></div><input type="hidden" name="group" value="all"><input type="hidden" name="content" value="all"><input type="hidden" name="source" value="all"><input type="hidden" name="sort" value="cdate:desc"></form><ul class="nav navbar-nav navbar-right"><li id="user-menu"><a href="https://openreview.net/login">Login</a></li></ul></div></div></nav><div id="or-banner" class="banner" style=""><div class="container"><div class="row"><div class="col-xs-12"><a title="Venue Homepage" href="https://openreview.net/group?id=ICLR.cc/2021"><img class="icon" src="./ICLR 2021 Conference _ OpenReview_files/arrow_left.svg" alt="back arrow">Go to <strong>ICLR 2021</strong> homepage</a></div></div></div></div><div id="flash-message-container" class="alert alert-danger fixed-overlay" role="alert" style="display:none"><div class="container"><div class="row"><div class="col-xs-12"><div class="alert-content"><button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button></div></div></div></div></div><div class="container"><div class="row"><div class="col-xs-12"><main id="content" class="group  "><div id="group-container"><div id="header" class="venue-header" style="display: block;"><h1>International Conference on Learning Representations</h1>
<h3>ICLR 2021</h3>

  <h4>
      <span class="venue-location">
        <span class="glyphicon glyphicon-globe" aria-hidden="true"></span> Vienna, Austria
      </span>
      <span class="venue-date">
        <span class="glyphicon glyphicon-calendar" aria-hidden="true"></span> May 04 2021
      </span>
      <span class="venue-website">
        <span class="glyphicon glyphicon-new-window" aria-hidden="true"></span> <a href="https://iclr.cc/" title="International Conference on Learning Representations Homepage" target="_blank">https://iclr.cc/</a>
      </span>
      <span class="venue-contact">
        <span class="glyphicon glyphicon-envelope" aria-hidden="true"></span> <a href="mailto:iclr2021programchairs@googlegroups.com" target="_blank">iclr2021programchairs@googlegroups.com</a>
      </span>
  </h4>

<div class="description">
    <p class="dark">Please see the venue website for more information.<br> </p><p class="dark"><strong>Abstract Submission End:</strong> Sep 28 2020 03:00PM UTC-0</p><p class="dark"><strong>Paper Submission End:</strong> Oct 2 2020 03:00PM UTC-0</p>
  <p></p>
</div>
</div><div id="invitation"></div><div id="notes">
<div class="tabs-container " style=""><ul class="nav nav-tabs" role="tablist">
    <li role="presentation" style="display: none;">
      <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#your-consoles" aria-controls="your-consoles" role="tab" data-toggle="tab" data-tab-index="0" data-modify-history="true">
        Your Consoles
      </a>
    </li>
    <li role="presentation">
      <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#oral-presentations" aria-controls="oral-presentations" role="tab" data-toggle="tab" data-tab-index="1" data-modify-history="true">
        Oral Presentations
      </a>
    </li>
    <li role="presentation" class="active">
      <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#spotlight-presentations" aria-controls="spotlight-presentations" role="tab" data-toggle="tab" data-tab-index="2" data-modify-history="true" aria-expanded="true">
        Spotlight Presentations
      </a>
    </li>
    <li role="presentation">
      <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#poster-presentations" aria-controls="poster-presentations" role="tab" data-toggle="tab" data-tab-index="3" data-modify-history="true">
        Poster Presentations
      </a>
    </li>
    <li role="presentation">
      <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#withdrawn-rejected-submissions" aria-controls="withdrawn-rejected-submissions" role="tab" data-toggle="tab" data-tab-index="4" data-modify-history="true">
        Withdrawn/Rejected Submissions
      </a>
    </li>
</ul>

<div class="tab-content">
    <div role="tabpanel" class="tab-pane fade  " id="your-consoles">
      
    </div>
    <div role="tabpanel" class="tab-pane fade  " id="oral-presentations">
      
    </div>
    <div role="tabpanel" class="tab-pane fade   active in" id="spotlight-presentations">
      
    <ul class="list-unstyled submissions-list">
    <li class="note " data-id="pBqLS-7KYAF" data-number="525">
      <h4>
        <a href="https://openreview.net/forum?id=pBqLS-7KYAF">
            Sparse Quantized Spectral Clustering
        </a>
      
        
          <a href="https://openreview.net/pdf?id=pBqLS-7KYAF" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhenyu_Liao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhenyu_Liao1">Zhenyu Liao</a>, <a href="https://openreview.net/profile?id=~Romain_Couillet1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Romain_Couillet1">Romain Couillet</a>, <a href="https://openreview.net/profile?id=~Michael_W._Mahoney1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_W._Mahoney1">Michael W. Mahoney</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Feb 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#pBqLS-7KYAF-details-484" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="pBqLS-7KYAF-details-484"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Eigenspectrum, high-dimensional statistic, random matrix theory, spectral clustering</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Given a large data matrix, sparsifying, quantizing, and/or performing other entry-wise nonlinear operations can have numerous benefits, ranging from speeding up iterative algorithms for core numerical linear algebra problems to providing nonlinear filters to design state-of-the-art neural network models. Here, we exploit tools from random matrix theory to make precise statements about how the eigenspectrum of a matrix changes under such nonlinear transformations. In particular, we show that very little change occurs in the informative eigenstructure, even under drastic sparsification/quantization, and consequently that very little downstream performance loss occurs when working with very aggressively sparsified or quantized spectral clustering problems.
      We illustrate how these results depend on the nonlinearity, we characterize a phase transition beyond which spectral clustering becomes possible, and we show when such nonlinear transformations can introduce spurious non-informative eigenvectors.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="HHSEKOnPvaO" data-number="536">
      <h4>
        <a href="https://openreview.net/forum?id=HHSEKOnPvaO">
            Graph-Based Continual Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=HHSEKOnPvaO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Binh_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Binh_Tang1">Binh Tang</a>, <a href="https://openreview.net/profile?id=~David_S._Matteson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_S._Matteson1">David S. Matteson</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#HHSEKOnPvaO-details-81" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HHSEKOnPvaO-details-81"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Despite significant advances, continual learning models still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. Rehearsal approaches alleviate the problem by maintaining and replaying a small episodic memory of previous samples, often implemented as an array of independent memory slots. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=HHSEKOnPvaO&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Vfs_2RnOD0H" data-number="1417">
      <h4>
        <a href="https://openreview.net/forum?id=Vfs_2RnOD0H">
            Dynamic Tensor Rematerialization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Vfs_2RnOD0H" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=jerry96%40cs.washington.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jerry96@cs.washington.edu">Marisa Kirisame</a>, <a href="https://openreview.net/profile?id=~Steven_Lyubomirsky1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Steven_Lyubomirsky1">Steven Lyubomirsky</a>, <a href="https://openreview.net/profile?email=altanh%40cs.washington.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="altanh@cs.washington.edu">Altan Haan</a>, <a href="https://openreview.net/profile?email=jrb%40cs.washington.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jrb@cs.washington.edu">Jennifer Brennan</a>, <a href="https://openreview.net/profile?email=dh63%40cs.washington.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="dh63@cs.washington.edu">Mike He</a>, <a href="https://openreview.net/profile?email=jroesch%40cs.washington.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jroesch@cs.washington.edu">Jared Roesch</a>, <a href="https://openreview.net/profile?id=~Tianqi_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianqi_Chen1">Tianqi Chen</a>, <a href="https://openreview.net/profile?id=~Zachary_Tatlock1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zachary_Tatlock1">Zachary Tatlock</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Vfs_2RnOD0H-details-169" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Vfs_2RnOD0H-details-169"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Rematerialization, Memory-saving, Runtime Systems, Checkpointing</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="0" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi></math></mjx-assistive-mml></mjx-container>-layer linear feedforward network on an  <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="1" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-n"><mjx-c class="mjx-c3A9"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.166em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Ω</mi><mo stretchy="false">(</mo><msqrt><mi>N</mi></msqrt><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> memory budget with only <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="2" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-cal mjx-i"><mjx-c class="mjx-c4F TEX-C"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi data-mjx-variant="-tex-calligraphic" mathvariant="script">O</mi></mrow><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present an online algorithm for rematerialization (recomputing intermediate activations during backpropagation instead of storing them), which enables training under low memory, finding that it is competitive with offline techniques.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Vfs_2RnOD0H&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="F1vEjWK-lH_" data-number="2546">
      <h4>
        <a href="https://openreview.net/forum?id=F1vEjWK-lH_">
            Gradient Vaccine: Investigating and Improving Multi-task Optimization in Massively Multilingual Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=F1vEjWK-lH_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zirui_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zirui_Wang1">Zirui Wang</a>, <a href="https://openreview.net/profile?id=~Yulia_Tsvetkov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yulia_Tsvetkov1">Yulia Tsvetkov</a>, <a href="https://openreview.net/profile?id=~Orhan_Firat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Orhan_Firat1">Orhan Firat</a>, <a href="https://openreview.net/profile?id=~Yuan_Cao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuan_Cao2">Yuan Cao</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#F1vEjWK-lH_-details-675" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="F1vEjWK-lH_-details-675"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multi-task Learning, Multilingual Modeling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Massively multilingual models subsuming tens or even hundreds of languages pose great challenges to multi-task optimization. While it is a common practice to apply a language-agnostic procedure optimizing a joint multilingual task objective, how to properly characterize and take advantage of its underlying problem structure for improving optimization efficiency remains under-explored. In this paper, we attempt to peek into the black-box of multilingual optimization through the lens of loss function geometry. We find that gradient similarity measured along the optimization trajectory is an important signal, which correlates well with not only language proximity but also the overall model performance. Such observation helps us to identify a critical limitation of existing gradient-based multi-task learning methods, and thus we derive a simple and scalable optimization procedure, named Gradient Vaccine, which encourages more geometrically aligned parameter updates for close tasks. Empirically, our method obtains significant model performance gains on multilingual machine translation and XTREME benchmark tasks for multilingual language models. Our work reveals the importance of properly measuring and utilizing language proximity in multilingual optimization, and has broader implications for multi-task learning beyond multilingual modeling.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="87ZwsaQNHPZ" data-number="2578">
      <h4>
        <a href="https://openreview.net/forum?id=87ZwsaQNHPZ">
            CPT: Efficient Deep Neural Network Training via Cyclic Precision
        </a>
      
        
          <a href="https://openreview.net/pdf?id=87ZwsaQNHPZ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yonggan_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yonggan_Fu1">Yonggan Fu</a>, <a href="https://openreview.net/profile?email=hg31%40rice.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="hg31@rice.edu">Han Guo</a>, <a href="https://openreview.net/profile?email=meng.li%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="meng.li@fb.com">Meng Li</a>, <a href="https://openreview.net/profile?email=xy33%40rice.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="xy33@rice.edu">Xin Yang</a>, <a href="https://openreview.net/profile?email=yd31%40rice.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yd31@rice.edu">Yining Ding</a>, <a href="https://openreview.net/profile?id=~Vikas_Chandra2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vikas_Chandra2">Vikas Chandra</a>, <a href="https://openreview.net/profile?id=~Yingyan_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yingyan_Lin1">Yingyan Lin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#87ZwsaQNHPZ-details-628" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="87ZwsaQNHPZ-details-628"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Efficient training, low precision training</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Low-precision deep neural network (DNN) training has gained tremendous attention as reducing precision is one of the most effective knobs for boosting DNNs' training time/energy efficiency. In this paper, we attempt to explore low-precision training from a new perspective as inspired by recent findings in understanding DNN training: we conjecture that DNNs' precision might have a similar effect as the learning rate during DNN training, and advocate dynamic precision along the training trajectory for further boosting the time/energy efficiency of DNN training. Specifically, we propose Cyclic Precision Training (CPT) to cyclically vary the precision between two boundary values which can be identified using a simple precision range test within the first few training epochs. Extensive simulations and ablation studies on five datasets and eleven models demonstrate that CPT's effectiveness is consistent across various models/tasks (including classification and language modeling). Furthermore, through experiments and visualization we show that CPT helps to (1) converge to a wider minima with a lower generalization error and (2) reduce training variance which we believe opens up a new design knob for simultaneously improving the optimization and efficiency of DNN training.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose Cyclic Precision Training towards better accuracy-efficiency trade-offs in DNN training.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="04LZCAxMSco" data-number="2604">
      <h4>
        <a href="https://openreview.net/forum?id=04LZCAxMSco">
            Learning a Latent Simplex in Input Sparsity Time
        </a>
      
        
          <a href="https://openreview.net/pdf?id=04LZCAxMSco" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ainesh_Bakshi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ainesh_Bakshi1">Ainesh Bakshi</a>, <a href="https://openreview.net/profile?id=~Chiranjib_Bhattacharyya1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chiranjib_Bhattacharyya1">Chiranjib Bhattacharyya</a>, <a href="https://openreview.net/profile?id=~Ravi_Kannan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ravi_Kannan1">Ravi Kannan</a>, <a href="https://openreview.net/profile?id=~David_Woodruff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Woodruff1">David Woodruff</a>, <a href="https://openreview.net/profile?id=~Samson_Zhou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Samson_Zhou1">Samson Zhou</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 24 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>7 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#04LZCAxMSco-details-45" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="04LZCAxMSco-details-45"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Latent Simplex, numerical linear algebra, low-rank approximation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We consider the problem of learning a latent <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="3" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>-vertex simplex <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="4" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.41em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mi>d</mi></msup></math></mjx-assistive-mml></mjx-container>, given <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="5" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D400 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.41em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">A</mi></mrow><mo>∈</mo><msup><mrow><mi mathvariant="double-struck">R</mi></mrow><mrow><mi>d</mi><mo>×</mo><mi>n</mi></mrow></msup></math></mjx-assistive-mml></mjx-container>, which can be viewed as <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="6" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container> data points that are formed by randomly perturbing some latent points in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="7" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container>, possibly beyond <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="8" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43E TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>K</mi></math></mjx-assistive-mml></mjx-container>. A large class of latent variable models, such as adversarial clustering, mixed membership stochastic block models, and topic models can be cast in this view of learning a latent simplex. Bhattacharyya and Kannan (SODA 2020) give an algorithm for learning such a <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="9" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>-vertex latent simplex in time roughly <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="10" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="3"><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c7A"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D400 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mi>k</mi><mo>⋅</mo><mtext>nnz</mtext><mo stretchy="false">(</mo><mrow><mi mathvariant="bold">A</mi></mrow><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="11" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c7A"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D400 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>nnz</mtext><mo stretchy="false">(</mo><mrow><mi mathvariant="bold">A</mi></mrow><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> is the number of non-zeros in <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="12" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D400 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">A</mi></mrow></math></mjx-assistive-mml></mjx-container>. We show that the dependence on <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="13" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container> in the running time is unnecessary given a natural assumption about the mass of the top <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="14" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container> singular values of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="15" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D400 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">A</mi></mrow></math></mjx-assistive-mml></mjx-container>, which holds in many of these applications. Further, we show this assumption is necessary, as otherwise an algorithm for learning a latent simplex would imply a better low rank approximation algorithm than what is known. 
      
      We obtain a spectral low-rank approximation to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="16" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D400 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">A</mi></mrow></math></mjx-assistive-mml></mjx-container> in input-sparsity time and show that the column space thus obtained has small <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="17" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-n"><mjx-c class="mjx-c73"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6E"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-n" space="2"><mjx-c class="mjx-c398"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>sin</mi><mo data-mjx-texclass="NONE">⁡</mo><mi mathvariant="normal">Θ</mi></math></mjx-assistive-mml></mjx-container> (angular) distance to the right top-<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="18" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container> singular space of <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="19" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D400 TEX-B"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">A</mi></mrow></math></mjx-assistive-mml></mjx-container>. Our algorithm then selects <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="20" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container> points in the low-rank  subspace with the largest inner product (in absolute value) with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="21" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container> carefully chosen random vectors. By working in the low-rank subspace, we avoid reading the entire matrix in each iteration and thus circumvent the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="22" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-n"><mjx-c class="mjx-c398"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="3"><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c7A"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D400 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">Θ</mi><mo stretchy="false">(</mo><mi>k</mi><mo>⋅</mo><mtext>nnz</mtext><mo stretchy="false">(</mo><mrow><mi mathvariant="bold">A</mi></mrow><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> running time.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We obtain the first input sparsity runtime algorithm for the problem of learning a latent simplex.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=04LZCAxMSco&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="lxHgXYN4bwl" data-number="3094">
      <h4>
        <a href="https://openreview.net/forum?id=lxHgXYN4bwl">
            Expressive Power of Invariant and Equivariant Graph Neural Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=lxHgXYN4bwl" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=waiss.azizian%40ens.fr" class="profile-link" data-toggle="tooltip" data-placement="top" title="waiss.azizian@ens.fr">Waiss Azizian</a>, <a href="https://openreview.net/profile?id=~marc_lelarge1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~marc_lelarge1">marc lelarge</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#lxHgXYN4bwl-details-507" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lxHgXYN4bwl-details-507"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Graph Neural Network, Universality, Approximation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Various classes of Graph Neural Networks (GNN) have been proposed and shown to be successful in a wide range of applications with graph structured data. In this paper, we propose a theoretical framework able to compare the expressive power of these GNN architectures. The current universality theorems only apply to intractable classes of GNNs. Here, we prove the first approximation guarantees for practical GNNs, paving the way for a better understanding of their generalization. Our theoretical results are proved for invariant GNNs computing a graph embedding (permutation of the nodes of the input graph does not affect the output) and equivariant GNNs computing an embedding of the nodes (permutation of the input permutes the output). We show that Folklore Graph Neural Networks (FGNN), which are tensor based GNNs augmented with matrix multiplication are the most expressive architectures proposed so far for a given tensor order. We illustrate our results on the Quadratic Assignment Problem (a NP-Hard combinatorial problem) by showing that FGNNs are able to learn how to solve the problem, leading to much better average performances than existing algorithms (based on spectral, SDP or other GNNs architectures). On a practical side, we also implement masked tensors to handle batches of graphs of varying sizes. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=lxHgXYN4bwl&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="PUkhWz65dy5" data-number="3432">
      <h4>
        <a href="https://openreview.net/forum?id=PUkhWz65dy5">
            Discovering a set of policies for the worst case reward
        </a>
      
        
          <a href="https://openreview.net/pdf?id=PUkhWz65dy5" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tom_Zahavy2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tom_Zahavy2">Tom Zahavy</a>, <a href="https://openreview.net/profile?id=~Andre_Barreto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andre_Barreto1">Andre Barreto</a>, <a href="https://openreview.net/profile?id=~Daniel_J_Mankowitz2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_J_Mankowitz2">Daniel J Mankowitz</a>, <a href="https://openreview.net/profile?id=~Shaobo_Hou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shaobo_Hou1">Shaobo Hou</a>, <a href="https://openreview.net/profile?id=~Brendan_O%26%23x27%3BDonoghue1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brendan_O&#39;Donoghue1">Brendan O'Donoghue</a>, <a href="https://openreview.net/profile?email=iukemaev%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="iukemaev@google.com">Iurii Kemaev</a>, <a href="https://openreview.net/profile?id=~Satinder_Singh2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Satinder_Singh2">Satinder Singh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Feb 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>21 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#PUkhWz65dy5-details-679" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="PUkhWz65dy5-details-679"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=PUkhWz65dy5&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Discovering a set of diverse RL policies by optimising the robustness of the set</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="UcoXdfrORC" data-number="1330">
      <h4>
        <a href="https://openreview.net/forum?id=UcoXdfrORC">
            Model-Based Visual Planning with Self-Supervised Functional Distances
        </a>
      
        
          <a href="https://openreview.net/pdf?id=UcoXdfrORC" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Stephen_Tian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephen_Tian1">Stephen Tian</a>, <a href="https://openreview.net/profile?id=~Suraj_Nair1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Suraj_Nair1">Suraj Nair</a>, <a href="https://openreview.net/profile?id=~Frederik_Ebert1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Frederik_Ebert1">Frederik Ebert</a>, <a href="https://openreview.net/profile?id=~Sudeep_Dasari2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sudeep_Dasari2">Sudeep Dasari</a>, <a href="https://openreview.net/profile?id=~Benjamin_Eysenbach1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Benjamin_Eysenbach1">Benjamin Eysenbach</a>, <a href="https://openreview.net/profile?id=~Chelsea_Finn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chelsea_Finn1">Chelsea Finn</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#UcoXdfrORC-details-699" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="UcoXdfrORC-details-699"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">planning, model learning, distance learning, reinforcement learning, robotics</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A generalist robot must be able to complete a variety of tasks in its environment. One appealing way to specify each task is in terms of a goal observation. However, learning goal-reaching policies with reinforcement learning remains a challenging problem, particularly when hand-engineered reward functions are not available. Learned dynamics models are a promising approach for learning about the environment without rewards or task-directed data, but planning to reach goals with such a model requires a notion of functional similarity between observations and goal states. We present a self-supervised method for model-based visual goal reaching, which uses both a visual dynamics model as well as a dynamical distance function learned using model-free reinforcement learning. Our approach learns entirely using offline, unlabeled data, making it practical to scale to large and diverse datasets. In our experiments, we find that our method can successfully learn models that perform a variety of tasks at test-time, moving objects amid distractors with a simulated robotic arm and even learning to open and close a drawer using a real-world robot. In comparisons, we find that this approach substantially outperforms both model-free and model-based prior methods.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We combine model-based planning with dynamical distance learning to solve visual goal-reaching tasks, using random, unlabeled, experience.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="80FMcTSZ6J0" data-number="568">
      <h4>
        <a href="https://openreview.net/forum?id=80FMcTSZ6J0">
            Noise against noise: stochastic label noise helps combat inherent label noise
        </a>
      
        
          <a href="https://openreview.net/pdf?id=80FMcTSZ6J0" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Pengfei_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pengfei_Chen1">Pengfei Chen</a>, <a href="https://openreview.net/profile?email=gy.chen%40siat.ac.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="gy.chen@siat.ac.cn">Guangyong Chen</a>, <a href="https://openreview.net/profile?email=kourenmu%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="kourenmu@gmail.com">Junjie Ye</a>, <a href="https://openreview.net/profile?id=~jingwei_zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~jingwei_zhao1">jingwei zhao</a>, <a href="https://openreview.net/profile?id=~Pheng-Ann_Heng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pheng-Ann_Heng1">Pheng-Ann Heng</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 04 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#80FMcTSZ6J0-details-359" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="80FMcTSZ6J0-details-359"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Noisy Labels, Robust Learning, SGD noise, Regularization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The noise in stochastic gradient descent (SGD) provides a crucial implicit regularization effect, previously studied in optimization by analyzing the dynamics of parameter updates. In this paper, we are interested in learning with noisy labels, where we have a collection of samples with potential mislabeling. We show that a previously rarely discussed SGD noise, induced by stochastic label noise (SLN), mitigates the effects of inherent label noise. In contrast, the common SGD noise directly applied to model parameters does not. We formalize the differences and connections of SGD noise variants, showing that SLN induces SGD noise dependent on the sharpness of output landscape and the confidence of output probability, which may help escape from sharp minima and prevent overconfidence. SLN not only improves generalization in its simplest form but also boosts popular robust training methods, including sample selection and label correction. Specifically, we present an enhanced algorithm by applying SLN to label correction. Our code is released.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">SGD noise induced by stochastic label noise helps escape sharp minima and prevents overconfidence, hence can mitigate the effects of inherent label noise and improve generalization.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=80FMcTSZ6J0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="qda7-sVg84" data-number="887">
      <h4>
        <a href="https://openreview.net/forum?id=qda7-sVg84">
            Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=qda7-sVg84" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Rishabh_Agarwal2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rishabh_Agarwal2">Rishabh Agarwal</a>, <a href="https://openreview.net/profile?id=~Marlos_C._Machado1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marlos_C._Machado1">Marlos C. Machado</a>, <a href="https://openreview.net/profile?id=~Pablo_Samuel_Castro1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pablo_Samuel_Castro1">Pablo Samuel Castro</a>, <a href="https://openreview.net/profile?id=~Marc_G_Bellemare1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Marc_G_Bellemare1">Marc G Bellemare</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#qda7-sVg84-details-975" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qda7-sVg84-details-975"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement, Generalization, Contrastive learning, Bisimulation, Representation Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Reinforcement learning methods trained on few environments rarely learn policies that generalize to unseen environments. To improve generalization, we incorporate the inherent sequential structure in reinforcement learning into the representation learning process. This approach is orthogonal to recent approaches, which rarely exploit this structure explicitly. Specifically, we introduce a theoretically motivated policy similarity metric (PSM) for measuring behavioral similarity between states. PSM assigns high similarity to states for which the optimal policies in those states as well as in future states are similar. We also present a contrastive representation learning procedure to embed any state similarity metric, which we instantiate with PSM to obtain policy similarity embeddings (PSEs). We demonstrate that PSEs improve generalization on diverse benchmarks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A contrastive representation learning method which encodes behavioural similarity in RL for improving generalization.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=qda7-sVg84&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="5m3SEczOV8L" data-number="562">
      <h4>
        <a href="https://openreview.net/forum?id=5m3SEczOV8L">
            VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=5m3SEczOV8L" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhisheng_Xiao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhisheng_Xiao1">Zhisheng Xiao</a>, <a href="https://openreview.net/profile?id=~Karsten_Kreis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Karsten_Kreis1">Karsten Kreis</a>, <a href="https://openreview.net/profile?id=~Jan_Kautz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jan_Kautz1">Jan Kautz</a>, <a href="https://openreview.net/profile?id=~Arash_Vahdat3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Arash_Vahdat3">Arash Vahdat</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#5m3SEczOV8L-details-724" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5m3SEczOV8L-details-724"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Energy-based Models, Variational Auto-encoder, MCMC</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="23" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></mjx-assistive-mml></mjx-container>256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce an energy-based generative model where the data distribution is defined jointly by a VAE and an energy network.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="MuSYkd1hxRP" data-number="1019">
      <h4>
        <a href="https://openreview.net/forum?id=MuSYkd1hxRP">
            Geometry-Aware Gradient Algorithms for Neural Architecture Search
        </a>
      
        
          <a href="https://openreview.net/pdf?id=MuSYkd1hxRP" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Liam_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Liam_Li1">Liam Li</a>, <a href="https://openreview.net/profile?id=~Mikhail_Khodak1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mikhail_Khodak1">Mikhail Khodak</a>, <a href="https://openreview.net/profile?id=~Nina_Balcan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nina_Balcan1">Nina Balcan</a>, <a href="https://openreview.net/profile?id=~Ameet_Talwalkar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ameet_Talwalkar1">Ameet Talwalkar</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#MuSYkd1hxRP-details-692" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MuSYkd1hxRP-details-692"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">neural architecture search, automated machine learning, weight-sharing, optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Studying the right single-level optimization geometry yields state-of-the-art methods for NAS.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=MuSYkd1hxRP&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="tilovEHA3YS" data-number="1189">
      <h4>
        <a href="https://openreview.net/forum?id=tilovEHA3YS">
            Learning-based Support Estimation in Sublinear Time
        </a>
      
        
          <a href="https://openreview.net/pdf?id=tilovEHA3YS" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=talyaa01%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="talyaa01@gmail.com">Talya Eden</a>, <a href="https://openreview.net/profile?id=~Piotr_Indyk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Piotr_Indyk1">Piotr Indyk</a>, <a href="https://openreview.net/profile?email=shyamsn%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="shyamsn@mit.edu">Shyam Narayanan</a>, <a href="https://openreview.net/profile?id=~Ronitt_Rubinfeld1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ronitt_Rubinfeld1">Ronitt Rubinfeld</a>, <a href="https://openreview.net/profile?email=silwal%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="silwal@mit.edu">Sandeep Silwal</a>, <a href="https://openreview.net/profile?id=~Tal_Wagner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tal_Wagner1">Tal Wagner</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#tilovEHA3YS-details-439" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="tilovEHA3YS-details-439"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">support estimation, sublinear, learning-based, distinct elements, chebyshev polynomial</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We consider the  problem of estimating the number of distinct elements in a large data set (or, equivalently, the support size of the distribution induced by the data set) from a random sample of its elements. The problem occurs in many applications, including biology, genomics, computer systems and linguistics. A line of research spanning the last decade resulted in algorithms that estimate the support up to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="24" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cB1"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D700 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>±</mo><mi>ε</mi><mi>n</mi></math></mjx-assistive-mml></mjx-container> from a sample of size <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="25" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-mi class="mjx-n"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.421em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D700 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-n" space="2"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><msup><mi>log</mi><mn>2</mn></msup><mo data-mjx-texclass="NONE">⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mrow><mo>/</mo></mrow><mi>ε</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>n</mi><mrow><mo>/</mo></mrow><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mi>n</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>, where <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="26" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container> is the data set size.  Unfortunately, this bound is known to be tight, limiting further improvements to the complexity of this problem. In this paper we consider estimation algorithms augmented with a machine-learning-based predictor that, given any element, returns an estimation of  its frequency.  We show that if the predictor is correct up to a constant approximation factor, then the sample complexity can be reduced significantly,  to
      <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" justify="left" role="presentation" tabindex="0" ctxtmenu_counter="27" style="font-size: 113.1%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px;"><mjx-mtext class="mjx-n"><mjx-c class="mjx-cA0"></mjx-c></mjx-mtext><mjx-mi class="mjx-n" space="2"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D700 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-msup space="3"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.413em;"><mjx-texatom size="s" texclass="ORD"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-n"><mjx-c class="mjx-c398"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-n" space="2"><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2061"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D700 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>&nbsp;</mtext><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mrow><mo>/</mo></mrow><mi>ε</mi><mo stretchy="false">)</mo><mo>⋅</mo><msup><mi>n</mi><mrow><mn>1</mn><mo>−</mo><mi mathvariant="normal">Θ</mi><mo stretchy="false">(</mo><mn>1</mn><mrow><mo>/</mo></mrow><mi>log</mi><mo data-mjx-texclass="NONE">⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mrow><mo>/</mo></mrow><mi>ε</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></msup><mo>.</mo></math></mjx-assistive-mml></mjx-container>
      We evaluate the proposed algorithms on a collection of data sets, using the neural-network based estimators from {Hsu et al, ICLR'19} as predictors. Our experiments  demonstrate substantial (up to 3x) improvements in the estimation accuracy compared to the state of the art algorithm.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A learning-based algorithm for support size estimation, which given a sufficiently accurate predictor, improves both provably and empirically over state-of-the-art algorithms that do not use a predictor.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=tilovEHA3YS&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Cri3xz59ga" data-number="1234">
      <h4>
        <a href="https://openreview.net/forum?id=Cri3xz59ga">
            Deciphering and Optimizing Multi-Task Learning: a Random Matrix Approach
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Cri3xz59ga" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Malik_Tiomoko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Malik_Tiomoko1">Malik Tiomoko</a>, <a href="https://openreview.net/profile?id=~Hafiz_Tiomoko_Ali1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hafiz_Tiomoko_Ali1">Hafiz Tiomoko Ali</a>, <a href="https://openreview.net/profile?id=~Romain_Couillet1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Romain_Couillet1">Romain Couillet</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 22 Feb 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Cri3xz59ga-details-995" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Cri3xz59ga-details-995"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Transfer Learning, Multi Task Learning, Random Matrix Theory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This article provides theoretical insights into the inner workings of multi-task and transfer learning methods, by studying the tractable least-square support vector machine multi-task learning (LS-SVM MTL) method, in the limit of large (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="28" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi></math></mjx-assistive-mml></mjx-container>) and numerous (<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="29" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container>) data. By a random matrix analysis applied to a Gaussian mixture data model, the performance of MTL LS-SVM is shown to converge, as <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="30" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo><mjx-mi class="mjx-n" space="4"><mjx-c class="mjx-c221E"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>,</mo><mi>p</mi><mo accent="false" stretchy="false">→</mo><mi mathvariant="normal">∞</mi></math></mjx-assistive-mml></mjx-container>, to a deterministic limit involving simple (small-dimensional) statistics of the data.
      
      We prove (i) that the standard MTL LS-SVM algorithm is in general strongly biased and may dramatically fail (to the point that individual single-task LS-SVMs may outperform the MTL approach, even for quite resembling tasks): our analysis provides a simple method to correct these biases, and that we reveal (ii) the sufficient statistics at play in the method, which can be efficiently estimated, even for quite small datasets. The latter result is exploited to automatically optimize the hyperparameters without resorting to any cross-validation procedure. 
      
      Experiments on popular datasets demonstrate that our improved MTL LS-SVM method is computationally-efficient and outperforms sometimes much more elaborate state-of-the-art multi-task and transfer learning techniques.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper provides a theoretical analysis of Multi Task Learning schemes for large dimensional data</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Cri3xz59ga&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="5k8F6UU39V" data-number="1115">
      <h4>
        <a href="https://openreview.net/forum?id=5k8F6UU39V">
            Autoregressive Entity Retrieval
        </a>
      
        
          <a href="https://openreview.net/pdf?id=5k8F6UU39V" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Nicola_De_Cao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicola_De_Cao1">Nicola De Cao</a>, <a href="https://openreview.net/profile?id=~Gautier_Izacard1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gautier_Izacard1">Gautier Izacard</a>, <a href="https://openreview.net/profile?id=~Sebastian_Riedel1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sebastian_Riedel1">Sebastian Riedel</a>, <a href="https://openreview.net/profile?id=~Fabio_Petroni2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fabio_Petroni2">Fabio Petroni</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#5k8F6UU39V-details-103" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="5k8F6UU39V-details-103"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">entity retrieval, document retrieval, autoregressive language model, entity linking, end-to-end entity linking, entity disambiguation, constrained beam search</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. This enables us to mitigate the aforementioned technical issues since: (i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at https://github.com/facebookresearch/GENRE.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We address entity retrieval by generating their unique name identifiers, left to right, in an autoregressive fashion, and conditioned on the context showing SOTA results in more than 20 datasets with a tiny fraction of the memory of recent systems.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="b9PoimzZFJ" data-number="2500">
      <h4>
        <a href="https://openreview.net/forum?id=b9PoimzZFJ">
            Systematic generalisation with group invariant predictions
        </a>
      
        
          <a href="https://openreview.net/pdf?id=b9PoimzZFJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Faruk_Ahmed1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Faruk_Ahmed1">Faruk Ahmed</a>, <a href="https://openreview.net/profile?id=~Yoshua_Bengio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoshua_Bengio1">Yoshua Bengio</a>, <a href="https://openreview.net/profile?id=~Harm_van_Seijen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Harm_van_Seijen1">Harm van Seijen</a>, <a href="https://openreview.net/profile?id=~Aaron_Courville3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aaron_Courville3">Aaron Courville</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#b9PoimzZFJ-details-180" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="b9PoimzZFJ-details-180"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Systematic generalisation, invariance penalty, semantic anomaly detection</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We consider situations where the presence of dominant simpler correlations with the target variable in a training set can cause an SGD-trained neural network to be less reliant on more persistently correlating complex features. When the non-persistent, simpler correlations correspond to non-semantic background factors, a neural network trained on this data can exhibit dramatic failure upon encountering systematic distributional shift, where the correlating background features are recombined with different objects. We perform an empirical study on three synthetic datasets, showing that group invariance methods across inferred partitionings of the training set can lead to significant improvements at such test-time situations. We also suggest a simple invariance penalty, showing with experiments on our setups that it can perform better than alternatives. We find that even without assuming access to any systematically shifted validation sets, one can still find improvements over an ERM-trained reference model.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Invariance penalties across splits of a biased dataset can improve systematic generalisation</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="R4aWTjmrEKM" data-number="1701">
      <h4>
        <a href="https://openreview.net/forum?id=R4aWTjmrEKM">
            Iterative Empirical Game Solving via Single Policy Best Response
        </a>
      
        
          <a href="https://openreview.net/pdf?id=R4aWTjmrEKM" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Max_Smith1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Max_Smith1">Max Smith</a>, <a href="https://openreview.net/profile?id=~Thomas_Anthony1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thomas_Anthony1">Thomas Anthony</a>, <a href="https://openreview.net/profile?id=~Michael_Wellman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Wellman1">Michael Wellman</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#R4aWTjmrEKM-details-561" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="R4aWTjmrEKM-details-561"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Empirical Game Theory, Reinforcement Learning, Multiagent Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Policy-Space Response Oracles (PSRO) is a general algorithmic framework for learning policies in multiagent systems by interleaving empirical game analysis with deep reinforcement learning (DRL).
      At each iteration, DRL is invoked to train a best response to a mixture of opponent policies.
      The repeated application of DRL poses an expensive computational burden as we look to apply this algorithm to more complex domains.
      We introduce two variations of PSRO designed to reduce the amount of simulation required during DRL training.
      Both algorithms modify how PSRO adds new policies to the empirical game, based on learned responses to a single opponent policy.
      The first, Mixed-Oracles, transfers knowledge from previous iterations of DRL, requiring training only against the opponent's newest policy.
      The second, Mixed-Opponents, constructs a pure-strategy opponent by mixing existing strategy's action-value estimates, instead of their policies.
      Learning against a single policy mitigates conflicting experiences on behalf of a learner facing an unobserved distribution of opponents.
      We empirically demonstrate that these algorithms substantially reduce the amount of simulation during training required by PSRO, while producing equivalent or better solutions to the game.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=R4aWTjmrEKM&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">On each epoch, train against a single opponent policy rather than a distribution; reducing variance and focusing training on salient strategic knowledge.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="_WnwtieRHxM" data-number="2906">
      <h4>
        <a href="https://openreview.net/forum?id=_WnwtieRHxM">
            Understanding the role of importance weighting for deep learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=_WnwtieRHxM" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Da_Xu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Da_Xu2">Da Xu</a>, <a href="https://openreview.net/profile?email=yeyt%40berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yeyt@berkeley.edu">Yuting Ye</a>, <a href="https://openreview.net/profile?id=~Chuanwei_Ruan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuanwei_Ruan1">Chuanwei Ruan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#_WnwtieRHxM-details-560" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_WnwtieRHxM-details-560"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Importance Weighting, Deep Learning, Implicit Bias, Gradient Descent, Learning Theory</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The recent paper by Byrd &amp; Lipton (2019), based on empirical observations, raises a major concern on the impact of importance weighting for the over-parameterized deep learning models. They observe that as long as the model can separate the training data, the impact of importance weighting diminishes as the training proceeds. Nevertheless, there lacks a rigorous characterization of this phenomenon. In this paper, we provide formal characterizations and theoretical justifications on the role of importance weighting with respect to the implicit bias of gradient descent and margin-based learning theory. We reveal both the optimization dynamics and generalization performance under deep learning models. Our work not only explains the various novel phenomenons observed for importance weighting in deep learning, but also extends to the studies where the weights are being optimized as part of the model, which applies to a number of topics under active research.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We study the theoretical properties of importance weighting for deep learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=_WnwtieRHxM&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="37nvvqkCo5" data-number="358">
      <h4>
        <a href="https://openreview.net/forum?id=37nvvqkCo5">
            Long-tail learning via logit adjustment
        </a>
      
        
          <a href="https://openreview.net/pdf?id=37nvvqkCo5" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Aditya_Krishna_Menon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aditya_Krishna_Menon1">Aditya Krishna Menon</a>, <a href="https://openreview.net/profile?id=~Sadeep_Jayasumana1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sadeep_Jayasumana1">Sadeep Jayasumana</a>, <a href="https://openreview.net/profile?id=~Ankit_Singh_Rawat1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ankit_Singh_Rawat1">Ankit Singh Rawat</a>, <a href="https://openreview.net/profile?email=himj%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="himj@google.com">Himanshu Jain</a>, <a href="https://openreview.net/profile?id=~Andreas_Veit1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_Veit1">Andreas Veit</a>, <a href="https://openreview.net/profile?id=~Sanjiv_Kumar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanjiv_Kumar1">Sanjiv Kumar</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#37nvvqkCo5-details-956" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="37nvvqkCo5-details-956"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">long-tail learning, class imbalance</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Real-world classification problems typically exhibit an imbalanced or long-tailed label distribution, wherein many labels have only a few associated samples. This poses a challenge for generalisation on such labels, and also  makes naive learning biased towards dominant labels. In this paper,  we present a statistical framework that unifies and generalises several recent proposals to cope with these challenges. Our framework revisits the classic idea of logit adjustment based on the label frequencies, which encourages a large relative margin between logits of rare positive versus dominant negative labels. This yields two techniques  for long-tail learning, where such adjustment is either applied post-hoc to a trained model, or enforced in the loss during training. These techniques are statistically grounded, and practically effective on four real-world datasets with long-tailed label distributions. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Adjusting classifier logits based on class priors, either post-hoc or during training, can improve performance on rare classes.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=37nvvqkCo5&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="6s7ME_X5_Un" data-number="1049">
      <h4>
        <a href="https://openreview.net/forum?id=6s7ME_X5_Un">
            DDPNOpt: Differential Dynamic Programming Neural Optimizer
        </a>
      
        
          <a href="https://openreview.net/pdf?id=6s7ME_X5_Un" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Guan-Horng_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guan-Horng_Liu1">Guan-Horng Liu</a>, <a href="https://openreview.net/profile?id=~Tianrong_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianrong_Chen1">Tianrong Chen</a>, <a href="https://openreview.net/profile?id=~Evangelos_Theodorou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Evangelos_Theodorou1">Evangelos Theodorou</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#6s7ME_X5_Un-details-633" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6s7ME_X5_Un-details-633"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning training, optimal control, trajectory optimization, differential dynamica programming</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Interpretation of Deep Neural Networks (DNNs) training as an optimal control problem with nonlinear dynamical systems has received considerable attention recently, yet the algorithmic development remains relatively limited. In this work, we make an attempt along this line by reformulating the training procedure from the trajectory optimization perspective. We first show that most widely-used algorithms for training DNNs can be linked to the Differential Dynamic Programming (DDP), a celebrated second-order method rooted in the Approximate Dynamic Programming. In this vein, we propose a new class of optimizer, DDP Neural Optimizer (DDPNOpt), for training feedforward and convolution networks. DDPNOpt features layer-wise feedback policies which improve convergence and reduce sensitivity to hyper-parameter over existing methods. It outperforms other optimal-control inspired training methods in both convergence and complexity, and is competitive against state-of-the-art first and second order methods. We also observe DDPNOpt has surprising benefit in preventing gradient vanishing. Our work opens up new avenues for principled algorithmic design built upon the optimal control theory.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a new class of optimal-control-theoretic training methods, DDPNOpt, that performs a distinct backward pass inherited with Bellman optimality and generates layer-wise feedback policies to robustify training over existing training methods</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ZPa2SyGcbwh" data-number="2811">
      <h4>
        <a href="https://openreview.net/forum?id=ZPa2SyGcbwh">
            Learning with Feature-Dependent Label Noise: A Progressive Approach
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ZPa2SyGcbwh" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yikai_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yikai_Zhang1">Yikai Zhang</a>, <a href="https://openreview.net/profile?id=~Songzhu_Zheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Songzhu_Zheng1">Songzhu Zheng</a>, <a href="https://openreview.net/profile?id=~Pengxiang_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pengxiang_Wu1">Pengxiang Wu</a>, <a href="https://openreview.net/profile?id=~Mayank_Goswami1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mayank_Goswami1">Mayank Goswami</a>, <a href="https://openreview.net/profile?id=~Chao_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chao_Chen1">Chao Chen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 27 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ZPa2SyGcbwh-details-659" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ZPa2SyGcbwh-details-659"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Noisy Label, Deep Learning, Classification</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Label noise is frequently observed in real-world large-scale datasets. The noise is introduced due to a variety of reasons; it is heterogeneous and feature-dependent. Most existing approaches to handling noisy labels fall into two categories: they either assume an ideal feature-independent noise, or remain heuristic without theoretical guarantees. In this paper, we propose to target a new family of feature-dependent label noise, which is much more general than commonly used i.i.d. label noise and encompasses a broad spectrum of noise patterns. Focusing on this general noise family, we propose a progressive label correction algorithm that iteratively corrects labels and refines the model. We provide theoretical guarantees showing that for a wide variety of (unknown) noise patterns, a classifier trained with this strategy converges to be consistent with the Bayes classifier. In experiments, our method outperforms SOTA baselines and is robust to various noise types and levels.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a progressive label correction approach for noisy label learning task.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="dyaIRud1zXg" data-number="309">
      <h4>
        <a href="https://openreview.net/forum?id=dyaIRud1zXg">
            Information Laundering for Model Privacy
        </a>
      
        
          <a href="https://openreview.net/pdf?id=dyaIRud1zXg" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=wang8740%40umn.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="wang8740@umn.edu">Xinran Wang</a>, <a href="https://openreview.net/profile?email=yu.xiang%40utah.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yu.xiang@utah.edu">Yu Xiang</a>, <a href="https://openreview.net/profile?email=0618johnny%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="0618johnny@gmail.com">Jun Gao</a>, <a href="https://openreview.net/profile?id=~Jie_Ding2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jie_Ding2">Jie Ding</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#dyaIRud1zXg-details-514" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="dyaIRud1zXg-details-514"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Adversarial Attack, Machine Learning, Model privacy, Privacy-utility tradeoff, Security</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this work, we propose information laundering, a novel framework for enhancing model privacy. Unlike data privacy that concerns the protection of raw data information, model privacy aims to protect an already-learned model that is to be deployed for public use. The private model can be obtained from general learning methods, and its deployment means that it will return a deterministic or random response for a given input query. An information-laundered model consists of probabilistic components that deliberately maneuver the intended input and output for queries of the model, so the model's adversarial acquisition is less likely. Under the proposed framework, we develop an information-theoretic principle to quantify the fundamental tradeoffs between model utility and privacy leakage and derive the optimal design.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose information laundering, a novel framework for enhancing model privacy.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="OthEq8I5v1" data-number="339">
      <h4>
        <a href="https://openreview.net/forum?id=OthEq8I5v1">
            Mutual Information State Intrinsic Control
        </a>
      
        
          <a href="https://openreview.net/pdf?id=OthEq8I5v1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Rui_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rui_Zhao1">Rui Zhao</a>, <a href="https://openreview.net/profile?id=~Yang_Gao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Gao1">Yang Gao</a>, <a href="https://openreview.net/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="https://openreview.net/profile?id=~Volker_Tresp1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Volker_Tresp1">Volker Tresp</a>, <a href="https://openreview.net/profile?id=~Wei_Xu13" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wei_Xu13">Wei Xu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 14 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#OthEq8I5v1-details-581" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="OthEq8I5v1-details-581"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Intrinsically Motivated Reinforcement Learning, Intrinsic Reward, Intrinsic Motivation, Deep Reinforcement Learning, Reinforcement Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Reinforcement learning has been shown to be highly successful at many challenging tasks. However, success heavily relies on well-shaped rewards. Intrinsically motivated RL attempts to remove this constraint by defining an intrinsic reward function. Motivated by the self-consciousness concept in psychology, we make a natural assumption that the agent knows what constitutes itself, and propose a new intrinsic objective that encourages the agent to have maximum control on the environment. We mathematically formalize this reward as the mutual information between the agent state and the surrounding state under the current agent policy. With this new intrinsic motivation, we are able to outperform previous methods, including being able to complete the pick-and-place task for the first time without using any task reward. A video showing experimental results is available at https://youtu.be/AUCwc9RThpk.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Motivated by the self-consciousness concept in psychology, we propose a new intrinsic objective that encourages the agent to have maximum control on the environment.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=OthEq8I5v1&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="2m0g1wEafh" data-number="3614">
      <h4>
        <a href="https://openreview.net/forum?id=2m0g1wEafh">
            Benefit of deep learning with non-convex noisy gradient descent: Provable excess risk bound and superiority to kernel methods
        </a>
      
        
          <a href="https://openreview.net/pdf?id=2m0g1wEafh" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Taiji_Suzuki1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Taiji_Suzuki1">Taiji Suzuki</a>, <a href="https://openreview.net/profile?email=shunta_akiyama%40mist.i.u-tokyo.ac.jp" class="profile-link" data-toggle="tooltip" data-placement="top" title="shunta_akiyama@mist.i.u-tokyo.ac.jp">Shunta Akiyama</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#2m0g1wEafh-details-342" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="2m0g1wEafh-details-342"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Excess risk, minimax optimal rate, local Rademacher complexity, fast learning rate, kernel method, linear estimator</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Establishing a theoretical analysis that explains why deep learning can outperform shallow learning such as kernel methods is one of the biggest issues in the deep learning literature. Towards answering this question, we evaluate excess risk of a deep learning estimator trained by a noisy gradient descent with ridge regularization on a mildly overparameterized neural network, 
      and discuss its superiority to a class of linear estimators that includes neural tangent kernel approach, random feature model, other kernel methods, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="31" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></mjx-assistive-mml></mjx-container>-NN estimator and so on. We consider a teacher-student regression model, and eventually show that {\it any} linear estimator can be outperformed by deep learning in a sense of the minimax optimal rate especially for a high dimension setting. The obtained excess bounds are so-called fast learning rate which is faster than <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="32" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D442 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class="mjx-n"><mjx-c class="mjx-c221A"></mjx-c></mjx-mo></mjx-surd><mjx-box style="padding-top: 0.281em;"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mrow><mo>/</mo></mrow><msqrt><mi>n</mi></msqrt><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> that is obtained by usual Rademacher complexity analysis. This discrepancy is induced by the non-convex geometry of the model and the noisy gradient descent used for neural network training provably reaches a near global optimal solution even though the loss landscape is highly non-convex. Although the noisy gradient descent does not employ any explicit or implicit sparsity inducing regularization, it shows a preferable generalization performance that dominates linear estimators.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="8yKEo06dKNo" data-number="2273">
      <h4>
        <a href="https://openreview.net/forum?id=8yKEo06dKNo">
            How Does Mixup Help With Robustness and Generalization?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=8yKEo06dKNo" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=linjun.zhang%40rutgers.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="linjun.zhang@rutgers.edu">Linjun Zhang</a>, <a href="https://openreview.net/profile?id=~Zhun_Deng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhun_Deng1">Zhun Deng</a>, <a href="https://openreview.net/profile?id=~Kenji_Kawaguchi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kenji_Kawaguchi1">Kenji Kawaguchi</a>, <a href="https://openreview.net/profile?id=~Amirata_Ghorbani2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amirata_Ghorbani2">Amirata Ghorbani</a>, <a href="https://openreview.net/profile?id=~James_Zou1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_Zou1">James Zou</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#8yKEo06dKNo-details-16" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8yKEo06dKNo-details-16"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Mixup, adversarial robustness, generalization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Mixup is a popular data augmentation technique based on on convex combinations of pairs of examples and their labels. This simple technique has shown to substantially improve both the model's robustness as well as the generalization of the trained model. However,  it is not well-understood why such improvement occurs. In this paper, we provide theoretical analysis to demonstrate how using Mixup in training helps model robustness and generalization. For robustness, we show that minimizing the Mixup loss corresponds to approximately minimizing an upper bound of the adversarial loss. This explains why models obtained by Mixup training exhibits robustness to several kinds of adversarial attacks such as Fast Gradient Sign Method (FGSM). For generalization, we prove that Mixup augmentation corresponds to a specific type of data-adaptive regularization which reduces overfitting. Our analysis provides new insights and a framework to understand Mixup.
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A theoretical point of view for Mixup training</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="hvdKKV2yt7T" data-number="1964">
      <h4>
        <a href="https://openreview.net/forum?id=hvdKKV2yt7T">
            Dataset Inference: Ownership Resolution in Machine Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=hvdKKV2yt7T" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Pratyush_Maini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pratyush_Maini1">Pratyush Maini</a>, <a href="https://openreview.net/profile?id=~Mohammad_Yaghini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mohammad_Yaghini1">Mohammad Yaghini</a>, <a href="https://openreview.net/profile?id=~Nicolas_Papernot1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicolas_Papernot1">Nicolas Papernot</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#hvdKKV2yt7T-details-843" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="hvdKKV2yt7T-details-843"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">model ownership, model extraction, MLaaS</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">With increasingly more data and computation involved in their training,  machine learning models constitute valuable intellectual property. This has spurred interest in model stealing, which is made more practical by advances in learning with partial, little, or no supervision. Existing defenses focus on inserting unique watermarks in a model's decision surface, but this is insufficient:  the watermarks are not sampled from the training distribution and thus are not always preserved during model stealing. In this paper, we make the key observation that knowledge contained in the stolen model's training set is what is common to all stolen copies. The adversary's goal, irrespective of the attack employed, is always to extract this knowledge or its by-products. This gives the original model's owner a strong advantage over the adversary: model owners have access to the original training data. We thus introduce <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="33" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D453 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D450 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">dataset inference</mtext></math></mjx-assistive-mml></mjx-container>, the process of identifying whether a suspected model copy has private knowledge from the original model's dataset, as a defense against model stealing. We develop an approach for dataset inference that combines statistical testing with the ability to estimate the distance of multiple data points to the decision boundary. Our experiments on CIFAR10, SVHN, CIFAR100 and ImageNet show that model owners can claim with confidence greater than 99% that their model (or dataset as a matter of fact) was stolen, despite only exposing 50 of the stolen model's training points. Dataset inference defends against state-of-the-art attacks even when the adversary is adaptive. Unlike prior work, it does not require retraining or overfitting the defended model.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce 'Dataset Inference' as a new method towards resolving model ownership.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=hvdKKV2yt7T&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="JBAa9we1AL" data-number="2257">
      <h4>
        <a href="https://openreview.net/forum?id=JBAa9we1AL">
            Individually Fair Gradient Boosting
        </a>
      
        
          <a href="https://openreview.net/pdf?id=JBAa9we1AL" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=ahsvargo%40umich.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="ahsvargo@umich.edu">Alexander Vargo</a>, <a href="https://openreview.net/profile?email=zhangfan4%40shanghaitech.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhangfan4@shanghaitech.edu.cn">Fan Zhang</a>, <a href="https://openreview.net/profile?id=~Mikhail_Yurochkin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mikhail_Yurochkin1">Mikhail Yurochkin</a>, <a href="https://openreview.net/profile?id=~Yuekai_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuekai_Sun1">Yuekai Sun</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#JBAa9we1AL-details-476" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JBAa9we1AL-details-476"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Algorithmic fairness, boosting, non-smooth models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We consider the task of enforcing individual fairness in gradient boosting. Gradient boosting is a popular method for machine learning from tabular data, which arise often in applications where algorithmic fairness is a concern. At a high level, our approach is a functional gradient descent on a (distributionally) robust loss function that encodes our intuition of algorithmic fairness for the ML task at hand. Unlike prior approaches to individual fairness that only work with smooth ML models, our approach also works with non-smooth models such as decision trees. We show that our algorithm converges globally and generalizes. We also demonstrate the efficacy of our algorithm on three ML problems susceptible to algorithmic bias.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an algorithm for training individually fair gradient boosted decision trees classifiers.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=JBAa9we1AL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="sSjqmfsk95O" data-number="1061">
      <h4>
        <a href="https://openreview.net/forum?id=sSjqmfsk95O">
            Large Scale Image Completion via Co-Modulated Generative Adversarial Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=sSjqmfsk95O" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Shengyu_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shengyu_Zhao1">Shengyu Zhao</a>, <a href="https://openreview.net/profile?id=~Jonathan_Cui1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Cui1">Jonathan Cui</a>, <a href="https://openreview.net/profile?email=simon1727%40qq.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="simon1727@qq.com">Yilun Sheng</a>, <a href="https://openreview.net/profile?email=dongyue8%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="dongyue8@gmail.com">Yue Dong</a>, <a href="https://openreview.net/profile?email=liangx%40rdfz.cn" class="profile-link" data-toggle="tooltip" data-placement="top" title="liangx@rdfz.cn">Xiao Liang</a>, <a href="https://openreview.net/profile?email=echang%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="echang@microsoft.com">Eric I-Chao Chang</a>, <a href="https://openreview.net/profile?id=~Yan_Xu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yan_Xu2">Yan Xu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#sSjqmfsk95O-details-601" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="sSjqmfsk95O-details-601"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">image completion, generative adversarial networks, co-modulation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Numerous task-specific variants of conditional generative adversarial networks have been developed for image completion. Yet, a serious limitation remains that all existing algorithms tend to fail when handling large-scale missing regions. To overcome this challenge, we propose a generic new approach that bridges the gap between image-conditional and recent modulated unconditional generative architectures via co-modulation of both conditional and stochastic style representations. Also, due to the lack of good quantitative metrics for image completion, we propose the new Paired/Unpaired Inception Discriminative Score (P-IDS/U-IDS), which robustly measures the perceptual fidelity of inpainted images compared to real images via linear separability in a feature space. Experiments demonstrate superior performance in terms of both quality and diversity over state-of-the-art methods in free-form image completion and easy generalization to image-to-image translation. Code is available at https://github.com/zsyzzsoft/co-mod-gan.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Bridging the gap between between image-conditional and unconditional GAN architectures via co-modulation</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="o_V-MjyyGV_" data-number="1664">
      <h4>
        <a href="https://openreview.net/forum?id=o_V-MjyyGV_">
            Self-Supervised Policy Adaptation during Deployment
        </a>
      
        
          <a href="https://openreview.net/pdf?id=o_V-MjyyGV_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Nicklas_Hansen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicklas_Hansen1">Nicklas Hansen</a>, <a href="https://openreview.net/profile?email=jangirrishabh%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="jangirrishabh@gmail.com">Rishabh Jangir</a>, <a href="https://openreview.net/profile?id=~Yu_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Sun1">Yu Sun</a>, <a href="https://openreview.net/profile?id=~Guillem_Aleny%C3%A01" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Guillem_Alenyà1">Guillem Alenyà</a>, <a href="https://openreview.net/profile?id=~Pieter_Abbeel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pieter_Abbeel2">Pieter Abbeel</a>, <a href="https://openreview.net/profile?id=~Alexei_A_Efros1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexei_A_Efros1">Alexei A Efros</a>, <a href="https://openreview.net/profile?id=~Lerrel_Pinto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lerrel_Pinto1">Lerrel Pinto</a>, <a href="https://openreview.net/profile?id=~Xiaolong_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaolong_Wang3">Xiaolong Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#o_V-MjyyGV_-details-705" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="o_V-MjyyGV_-details-705"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">reinforcement learning, robotics, self-supervised learning, generalization, sim2real</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In most real world scenarios, a policy trained by reinforcement learning in one environment needs to be deployed in another, potentially quite different environment. However, generalization across different environments is known to be hard. A natural solution would be to keep training after deployment in the new environment, but this cannot be done if the new environment offers no reward signal. Our work explores the use of self-supervision to allow the policy to continue training after deployment without using any rewards. While previous methods explicitly anticipate changes in the new environment, we assume no prior knowledge of those changes yet still obtain significant improvements. Empirical evaluations are performed on diverse simulation environments from DeepMind Control suite and ViZDoom, as well as real robotic manipulation tasks in  continuously changing environments, taking observations from an uncalibrated camera. Our method improves generalization in 31 out of 36 environments across various tasks and outperforms domain randomization on a majority of environments. Webpage and implementation: https://nicklashansen.github.io/PAD/.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Generalization across enviroments is known to be hard. We propose a self-supervised method for policy adaptation during deployment that assumes no prior knowledge of the test environment, yet still obtains significant improvements.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="6Tm1mposlrM" data-number="1839">
      <h4>
        <a href="https://openreview.net/forum?id=6Tm1mposlrM">
            Sharpness-aware Minimization for Efficiently Improving Generalization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=6Tm1mposlrM" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Pierre_Foret1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pierre_Foret1">Pierre Foret</a>, <a href="https://openreview.net/profile?email=akleiner%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="akleiner@google.com">Ariel Kleiner</a>, <a href="https://openreview.net/profile?id=~Hossein_Mobahi2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hossein_Mobahi2">Hossein Mobahi</a>, <a href="https://openreview.net/profile?id=~Behnam_Neyshabur1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Behnam_Neyshabur1">Behnam Neyshabur</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>21 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#6Tm1mposlrM-details-46" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6Tm1mposlrM-details-46"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Sharpness Minimization, Generalization, Regularization, Training Method, Deep Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by the connection between geometry of the loss landscape and generalization---including a generalization bound that we prove here---we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness.  In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-{10, 100}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several.  Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Motivated by the connection between geometry of the loss landscape and generalization, we introduce a procedure for simultaneously minimizing loss value and loss sharpness.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="3Aoft6NWFej" data-number="1810">
      <h4>
        <a href="https://openreview.net/forum?id=3Aoft6NWFej">
            PMI-Masking: Principled masking of correlated spans
        </a>
      
        
          <a href="https://openreview.net/pdf?id=3Aoft6NWFej" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yoav_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoav_Levine1">Yoav Levine</a>, <a href="https://openreview.net/profile?email=barakl%40ai21.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="barakl@ai21.com">Barak Lenz</a>, <a href="https://openreview.net/profile?email=opherl%40ai21.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="opherl@ai21.com">Opher Lieber</a>, <a href="https://openreview.net/profile?id=~Omri_Abend1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Omri_Abend1">Omri Abend</a>, <a href="https://openreview.net/profile?id=~Kevin_Leyton-Brown1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kevin_Leyton-Brown1">Kevin Leyton-Brown</a>, <a href="https://openreview.net/profile?id=~Moshe_Tennenholtz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Moshe_Tennenholtz1">Moshe Tennenholtz</a>, <a href="https://openreview.net/profile?id=~Yoav_Shoham1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoav_Shoham1">Yoav Shoham</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#3Aoft6NWFej-details-808" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3Aoft6NWFej-details-808"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Language modeling, BERT, pointwise mutual information</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Masking tokens uniformly at random constitutes a common flaw in the pretraining of Masked Language Models (MLMs) such as BERT. We show that such uniform masking allows an MLM to minimize its training objective by latching onto shallow local signals, leading to pretraining inefficiency and suboptimal downstream performance. To address this flaw, we propose PMI-Masking, a principled masking strategy based on the concept of Pointwise Mutual Information (PMI), which jointly masks a token n-gram if it exhibits high collocation over the corpus. PMI-Masking motivates, unifies, and improves upon prior more heuristic approaches that attempt to address the drawback of random uniform token masking, such as whole-word masking, entity/phrase masking, and random-span masking. Specifically, we show experimentally that PMI-Masking reaches the performance of prior masking approaches in half the training time, and consistently improves performance at the end of pretraining.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Joint masking of correlated tokens significantly speeds up and improves BERT's pretraining </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=3Aoft6NWFej&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="RLRXCV6DbEJ" data-number="884">
      <h4>
        <a href="https://openreview.net/forum?id=RLRXCV6DbEJ">
            Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images
        </a>
      
        
          <a href="https://openreview.net/pdf?id=RLRXCV6DbEJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Rewon_Child1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rewon_Child1">Rewon Child</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#RLRXCV6DbEJ-details-702" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="RLRXCV6DbEJ-details-702"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">VAE, generative modeling, deep learning, likelihood-based models</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present a hierarchical VAE that, for the first time, generates samples quickly <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="34" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">and</mtext></math></mjx-assistive-mml></mjx-container> outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We argue deeper VAEs should perform better, implement one, and show it outperforms all PixelCNN-based autoregressive models in likelihood, while being substantially more efficient.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=RLRXCV6DbEJ&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="uCQfPZwRaUu" data-number="1629">
      <h4>
        <a href="https://openreview.net/forum?id=uCQfPZwRaUu">
            Data-Efficient Reinforcement Learning with Self-Predictive Representations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=uCQfPZwRaUu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Max_Schwarzer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Max_Schwarzer1">Max Schwarzer</a>, <a href="https://openreview.net/profile?id=~Ankesh_Anand1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ankesh_Anand1">Ankesh Anand</a>, <a href="https://openreview.net/profile?id=~Rishab_Goel3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rishab_Goel3">Rishab Goel</a>, <a href="https://openreview.net/profile?id=~R_Devon_Hjelm1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~R_Devon_Hjelm1">R Devon Hjelm</a>, <a href="https://openreview.net/profile?id=~Aaron_Courville3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aaron_Courville3">Aaron Courville</a>, <a href="https://openreview.net/profile?id=~Philip_Bachman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philip_Bachman1">Philip Bachman</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#uCQfPZwRaUu-details-264" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uCQfPZwRaUu-details-264"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Reinforcement Learning, Self-Supervised Learning, Representation Learning, Sample Efficiency</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">While deep reinforcement learning excels at solving tasks where large amounts of data can be collected through virtually unlimited interaction with the environment, learning from limited interaction remains a key challenge. We posit that an agent can learn more efficiently if we augment reward maximization with self-supervised objectives based on structure in its visual input and sequential interaction with the environment.  Our method, Self-Predictive Representations (SPR), trains an agent to predict its own latent state representations multiple steps into the future. We compute target representations for future states using an encoder which is an exponential moving average of the agent’s parameters and we make predictions using a learned transition model.  On its own,  this future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. We further improve performance by adding data augmentation to the future prediction loss, which forces the agent’s representations to be consistent across multiple views of an observation.  Our full self-supervised objective, which combines future prediction and data augmentation, achieves a median human-normalized score of 0.415 on Atari in a setting limited to 100k steps of environment interaction, which represents a 55% relative improvement over the previous state-of-the-art. Notably, even in this limited data regime, SPR exceeds expert human scores on 7 out of 26 games. We’ve made the code associated with this work available at https://github.com/mila-iqia/spr.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a temporal, self-supervised objective for RL agents and show that it significantly improves data efficiency in a setting limited to just 2h of gameplay on Atari. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="w_7JMpGZRh0" data-number="2576">
      <h4>
        <a href="https://openreview.net/forum?id=w_7JMpGZRh0">
            Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration
        </a>
      
        
          <a href="https://openreview.net/pdf?id=w_7JMpGZRh0" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xavier_Puig1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xavier_Puig1">Xavier Puig</a>, <a href="https://openreview.net/profile?id=~Tianmin_Shu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianmin_Shu1">Tianmin Shu</a>, <a href="https://openreview.net/profile?id=~Shuang_Li5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shuang_Li5">Shuang Li</a>, <a href="https://openreview.net/profile?id=~Zilin_Wang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zilin_Wang2">Zilin Wang</a>, <a href="https://openreview.net/profile?id=~Yuan-Hong_Liao2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuan-Hong_Liao2">Yuan-Hong Liao</a>, <a href="https://openreview.net/profile?id=~Joshua_B._Tenenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_B._Tenenbaum1">Joshua B. Tenenbaum</a>, <a href="https://openreview.net/profile?id=~Sanja_Fidler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanja_Fidler1">Sanja Fidler</a>, <a href="https://openreview.net/profile?id=~Antonio_Torralba1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Antonio_Torralba1">Antonio Torralba</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#w_7JMpGZRh0-details-513" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="w_7JMpGZRh0-details-513"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">social perception, human-AI collaboration, theory of mind, multi-agent platform, virtual environment</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In this paper, we introduce Watch-And-Help (WAH), a challenge for testing social intelligence in agents. In WAH, an AI agent needs to help a human-like agent perform a complex household task efficiently. To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration). For this challenge, we build VirtualHome-Social, a multi-agent household environment, and provide a benchmark including both planning and learning based baselines. We evaluate the performance of AI agents with the human-like agent as well as and with real humans using objective metrics and subjective user ratings. Experimental results demonstrate that our challenge and virtual environment enable a systematic evaluation on the important aspects of machine social intelligence at scale.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce Watch-And-Help (WAH), a challenge for testing social intelligence in agents.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="6puCSjH3hwA" data-number="751">
      <h4>
        <a href="https://openreview.net/forum?id=6puCSjH3hwA">
            A Good Image Generator Is What You Need for High-Resolution Video Synthesis
        </a>
      
        
          <a href="https://openreview.net/pdf?id=6puCSjH3hwA" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yu_Tian2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Tian2">Yu Tian</a>, <a href="https://openreview.net/profile?id=~Jian_Ren2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jian_Ren2">Jian Ren</a>, <a href="https://openreview.net/profile?id=~Menglei_Chai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Menglei_Chai1">Menglei Chai</a>, <a href="https://openreview.net/profile?id=~Kyle_Olszewski1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kyle_Olszewski1">Kyle Olszewski</a>, <a href="https://openreview.net/profile?id=~Xi_Peng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xi_Peng1">Xi Peng</a>, <a href="https://openreview.net/profile?id=~Dimitris_N._Metaxas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dimitris_N._Metaxas1">Dimitris N. Metaxas</a>, <a href="https://openreview.net/profile?id=~Sergey_Tulyakov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Tulyakov1">Sergey Tulyakov</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#6puCSjH3hwA-details-613" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="6puCSjH3hwA-details-613"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">high-resolution video generation, contrastive learning, cross-domain video generation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Image and video synthesis are closely related areas aiming at generating content from noise. While rapid progress has been demonstrated in improving image-based models to handle large resolutions, high-quality renderings, and wide variations in image content, achieving comparable video generation results remains problematic. We present a framework that leverages contemporary image generators to render high-resolution videos. We frame the video synthesis problem as discovering a trajectory in the latent space of a pre-trained and fixed image generator. Not only does such a framework render high-resolution videos, but it also is an order of magnitude more computationally efficient. We introduce a motion generator that discovers the desired trajectory, in which content and motion are disentangled. With such a representation, our framework allows for a broad range of applications, including content and motion manipulation. Furthermore, we introduce a new task, which we call cross-domain video synthesis, in which the image and motion generators are trained on disjoint datasets belonging to different domains. This allows for generating moving objects for which the desired video data is not available. Extensive experiments on various datasets demonstrate the advantages of our methods over existing video generation techniques. Code will be released at https://github.com/snap-research/MoCoGAN-HD.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=6puCSjH3hwA&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Reuse a pre-trained image generator for high-resolution video synthesis</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="v9c7hr9ADKx" data-number="298">
      <h4>
        <a href="https://openreview.net/forum?id=v9c7hr9ADKx">
            UPDeT: Universal Multi-agent RL via Policy Decoupling with Transformers
        </a>
      
        
          <a href="https://openreview.net/pdf?id=v9c7hr9ADKx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Siyi_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siyi_Hu1">Siyi Hu</a>, <a href="https://openreview.net/profile?id=~Fengda_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fengda_Zhu1">Fengda Zhu</a>, <a href="https://openreview.net/profile?id=~Xiaojun_Chang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaojun_Chang3">Xiaojun Chang</a>, <a href="https://openreview.net/profile?id=~Xiaodan_Liang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaodan_Liang2">Xiaodan Liang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 07 Feb 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#v9c7hr9ADKx-details-282" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="v9c7hr9ADKx-details-282"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multi-agent Reinforcement Learning, Transfer Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent advances in multi-agent reinforcement learning have been largely limited in training one model from scratch for every new task. The limitation is due to the restricted model architecture related to fixed input and output dimensions. This hinders the experience accumulation and transfer of the learned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs 6 multi-agent games).  In this paper, we make the first attempt to explore a universal multi-agent reinforcement learning pipeline, designing one single architecture to fit tasks with the requirement of different observation and action configurations. Unlike previous RNN-based models, we utilize a transformer-based model to generate a flexible policy by decoupling the policy distribution from the intertwined input observation with an importance weight measured by the merits of the self-attention mechanism. Compared to a standard transformer block, the proposed model, named as Universal Policy Decoupling Transformer (UPDeT), further relaxes the action restriction and makes the multi-agent task's decision process more explainable. UPDeT is general enough to be plugged into any multi-agent reinforcement learning pipeline and equip them with strong generalization abilities that enables the handling of multiple tasks at a time. Extensive experiments on large-scale SMAC multi-agent competitive games demonstrate that the proposed UPDeT-based multi-agent reinforcement learning achieves significant results relative to state-of-the-art approaches, demonstrating advantageous transfer capability in terms of both performance and training speed (10 times faster).</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="yHeg4PbFHh" data-number="1310">
      <h4>
        <a href="https://openreview.net/forum?id=yHeg4PbFHh">
            BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration
        </a>
      
        
          <a href="https://openreview.net/pdf?id=yHeg4PbFHh" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Augustus_Odena1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Augustus_Odena1">Augustus Odena</a>, <a href="https://openreview.net/profile?id=~Kensen_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kensen_Shi1">Kensen Shi</a>, <a href="https://openreview.net/profile?id=~David_Bieber1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~David_Bieber1">David Bieber</a>, <a href="https://openreview.net/profile?id=~Rishabh_Singh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rishabh_Singh1">Rishabh Singh</a>, <a href="https://openreview.net/profile?id=~Charles_Sutton1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Charles_Sutton1">Charles Sutton</a>, <a href="https://openreview.net/profile?id=~Hanjun_Dai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hanjun_Dai1">Hanjun Dai</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#yHeg4PbFHh-details-234" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="yHeg4PbFHh-details-234"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Program Synthesis</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We use a learned model to guide a bottom-up program synthesis search to efficiently synthesize spreadsheet programs.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="zQTezqCCtNx" data-number="175">
      <h4>
        <a href="https://openreview.net/forum?id=zQTezqCCtNx">
            Improving Adversarial Robustness via Channel-wise Activation Suppressing
        </a>
      
        
          <a href="https://openreview.net/pdf?id=zQTezqCCtNx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yang_Bai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Bai1">Yang Bai</a>, <a href="https://openreview.net/profile?id=~Yuyuan_Zeng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuyuan_Zeng1">Yuyuan Zeng</a>, <a href="https://openreview.net/profile?id=~Yong_Jiang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yong_Jiang3">Yong Jiang</a>, <a href="https://openreview.net/profile?id=~Shu-Tao_Xia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shu-Tao_Xia1">Shu-Tao Xia</a>, <a href="https://openreview.net/profile?id=~Xingjun_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xingjun_Ma1">Xingjun Ma</a>, <a href="https://openreview.net/profile?id=~Yisen_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yisen_Wang1">Yisen Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#zQTezqCCtNx-details-73" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zQTezqCCtNx-details-73"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Adversarial robustness, channel suppressing, activation strategy.</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The study of adversarial examples and their activations have attracted significant attention for secure and robust learning with deep neural networks (DNNs).  Different from existing works, in this paper, we highlight two new characteristics of adversarial examples from the channel-wise activation perspective:  1) the activation magnitudes of adversarial examples are higher than that of natural examples; and 2) the channels are activated more uniformly by adversarial examples than natural examples. We find that, while the state-of-the-art defense adversarial training has addressed the first issue of high activation magnitude via training on adversarial examples, the second issue of uniform activation remains.  This motivates us to suppress redundant activations from being activated by adversarial perturbations during the adversarial training process, via a Channel-wise Activation Suppressing (CAS) training strategy.  We show that CAS can train a model that inherently suppresses adversarial activations, and can be easily applied to existing defense methods to further improve their robustness. Our work provides a simplebut generic training strategy for robustifying the intermediate layer activations of DNNs.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Training with Channel-wise Activation Suppressing (CAS) can help imrove the robustness of adversarial training.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=zQTezqCCtNx&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="30EvkP2aQLD" data-number="2298">
      <h4>
        <a href="https://openreview.net/forum?id=30EvkP2aQLD">
            What are the Statistical Limits of Offline RL with Linear Function Approximation?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=30EvkP2aQLD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ruosong_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruosong_Wang1">Ruosong Wang</a>, <a href="https://openreview.net/profile?id=~Dean_Foster1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dean_Foster1">Dean Foster</a>, <a href="https://openreview.net/profile?id=~Sham_M._Kakade1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sham_M._Kakade1">Sham M. Kakade</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 29 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#30EvkP2aQLD-details-891" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="30EvkP2aQLD-details-891"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">batch reinforcement learning, function approximation, lower bound, representation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Offline reinforcement learning seeks to utilize offline (observational) data to guide the learning of (causal) sequential decision making strategies. The hope is that offline reinforcement learning coupled with function approximation methods (to deal with the curse of dimensionality) can provide a means to help alleviate the excessive sample complexity burden in modern sequential decision making problems. However, the extent to which this broader approach can be effective is not well understood, where the literature largely consists of sufficient conditions.
      
      This work focuses on the basic question of what are necessary representational and distributional conditions that permit provable sample-efficient offline reinforcement learning. Perhaps surprisingly, our main result shows that even if: i) we have realizability in that the true value function of \emph{every} policy is linear in a given set of features and 2) our off-policy data has good  coverage over all features (under a strong spectral condition), any algorithm still (information-theoretically) requires a number of offline samples that is exponential in the problem horizon to non-trivially estimate the value of \emph{any} given policy. Our results highlight that sample-efficient offline policy evaluation is not possible unless significantly stronger conditions hold; such conditions include either having low distribution shift (where the offline data distribution is close to the distribution of the policy to be evaluated) or significantly stronger representational conditions (beyond realizability).</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Exponential lower bounds for batch RL with linear function approximation. </span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="iAmZUo0DxC0" data-number="1169">
      <h4>
        <a href="https://openreview.net/forum?id=iAmZUo0DxC0">
            Unlearnable Examples: Making Personal Data Unexploitable
        </a>
      
        
          <a href="https://openreview.net/pdf?id=iAmZUo0DxC0" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hanxun_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hanxun_Huang1">Hanxun Huang</a>, <a href="https://openreview.net/profile?id=~Xingjun_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xingjun_Ma1">Xingjun Ma</a>, <a href="https://openreview.net/profile?id=~Sarah_Monazam_Erfani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sarah_Monazam_Erfani1">Sarah Monazam Erfani</a>, <a href="https://openreview.net/profile?id=~James_Bailey1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~James_Bailey1">James Bailey</a>, <a href="https://openreview.net/profile?id=~Yisen_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yisen_Wang1">Yisen Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Feb 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#iAmZUo0DxC0-details-535" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="iAmZUo0DxC0-details-535"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Unlearnable Examples, Data Protection, Adversarial Machine Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The volume of "free" data on the internet has been key to the current success of deep learning. However, it also raises privacy concerns about the unauthorized exploitation of personal data for training commercial models. It is thus crucial to develop methods to prevent unauthorized data exploitation. This paper raises the question: can data be made unlearnable for deep learning models? We present a type of error-minimizing noise that can indeed make training examples unlearnable. Error-minimizing noise is intentionally generated to reduce the error of one or more of the training example(s) close to zero, which can trick the model into believing there is "nothing" to learn from these example(s). The noise is restricted to be imperceptible to human eyes, and thus does not affect normal data utility. We empirically verify the effectiveness of error-minimizing noise in both sample-wise and class-wise forms. We also demonstrate its flexibility under extensive experimental settings and practicability in a case study of face recognition. Our work establishes an important ﬁrst step towards making personal data unexploitable to deep learning models.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a type of error-minimizing noise that can make training examples unlearnable to deep learning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=iAmZUo0DxC0&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="roNqYL0_XP" data-number="3177">
      <h4>
        <a href="https://openreview.net/forum?id=roNqYL0_XP">
            Learning Mesh-Based Simulation with Graph Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=roNqYL0_XP" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tobias_Pfaff1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tobias_Pfaff1">Tobias Pfaff</a>, <a href="https://openreview.net/profile?id=~Meire_Fortunato1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Meire_Fortunato1">Meire Fortunato</a>, <a href="https://openreview.net/profile?id=~Alvaro_Sanchez-Gonzalez1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alvaro_Sanchez-Gonzalez1">Alvaro Sanchez-Gonzalez</a>, <a href="https://openreview.net/profile?id=~Peter_Battaglia1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peter_Battaglia1">Peter Battaglia</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Jan 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#roNqYL0_XP-details-275" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="roNqYL0_XP-details-275"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">graph networks, simulation, mesh, physics</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Mesh-based simulations are central to modeling complex physical systems in many disciplines across science and engineering. Mesh representations support powerful numerical integration methods and their resolution can be adapted to strike favorable trade-offs between accuracy and efficiency. However, high-dimensional scientific simulations are very expensive to run, and solvers and parameters must often be tuned individually to each system studied.
      Here we introduce MeshGraphNets, a framework for learning mesh-based simulations using graph neural networks. Our model can be trained to pass messages on a mesh graph and to adapt the mesh discretization during forward simulation. Our results show it can accurately predict the dynamics of a wide range of physical systems, including aerodynamics, structural mechanics, and cloth. The model's adaptivity supports learning resolution-independent dynamics and can scale to more complex state spaces at test time. Our method is also highly efficient, running 1-2 orders of magnitude faster than the simulation on which it is trained. Our approach broadens the range of problems on which neural network simulators can operate and promises to improve the efficiency of complex, scientific modeling tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a general method for learning the dynamics of complex physics systems accurately and efficiently on meshes</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="S0UdquAnr9k" data-number="346">
      <h4>
        <a href="https://openreview.net/forum?id=S0UdquAnr9k">
            Locally Free Weight Sharing for Network Width Search
        </a>
      
        
          <a href="https://openreview.net/pdf?id=S0UdquAnr9k" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xiu_Su1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiu_Su1">Xiu Su</a>, <a href="https://openreview.net/profile?id=~Shan_You3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shan_You3">Shan You</a>, <a href="https://openreview.net/profile?id=~Tao_Huang5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Huang5">Tao Huang</a>, <a href="https://openreview.net/profile?id=~Fei_Wang9" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fei_Wang9">Fei Wang</a>, <a href="https://openreview.net/profile?id=~Chen_Qian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chen_Qian1">Chen Qian</a>, <a href="https://openreview.net/profile?id=~Changshui_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Changshui_Zhang1">Changshui Zhang</a>, <a href="https://openreview.net/profile?id=~Chang_Xu4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chang_Xu4">Chang Xu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 03 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#S0UdquAnr9k-details-178" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="S0UdquAnr9k-details-178"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Searching for network width is an effective way to slim deep neural networks with hardware budgets. With this aim, a one-shot supernet is usually leveraged as a performance evaluator to rank the performance \wrt~different width. Nevertheless, current methods mainly follow a manually fixed weight sharing pattern, which is limited to distinguish the performance gap of different width. In this paper, to better evaluate each width, we propose a locally free weight sharing strategy (CafeNet) accordingly. In CafeNet, weights are more freely shared, and each width is jointly indicated by its base channels and free channels, where free channels are supposed to locate freely in a local zone to better represent each width. Besides, we propose to further reduce the search space by leveraging our introduced FLOPs-sensitive bins. As a result, our CafeNet can be trained stochastically and get optimized within a min-min strategy. Extensive experiments on ImageNet, CIFAR-10, CelebA and MS COCO dataset have verified our superiority comparing to other state-of-the-art baselines. For example, our method can further boost the benchmark NAS network EfficientNet-B0 by 0.41\% via searching its width more delicately.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">One-shot locally free weight sharing supernet for searching optimal network width</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=S0UdquAnr9k&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9OHFhefeB86" data-number="81">
      <h4>
        <a href="https://openreview.net/forum?id=9OHFhefeB86">
            Graph Convolution with Low-rank Learnable Local Filters
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9OHFhefeB86" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xiuyuan_Cheng1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiuyuan_Cheng1">Xiuyuan Cheng</a>, <a href="https://openreview.net/profile?id=~Zichen_Miao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zichen_Miao1">Zichen Miao</a>, <a href="https://openreview.net/profile?id=~Qiang_Qiu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qiang_Qiu1">Qiang Qiu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9OHFhefeB86-details-887" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9OHFhefeB86-details-887"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Geometric variations like rotation, scaling, and viewpoint changes pose a significant challenge to visual understanding. One common solution is to directly model certain intrinsic structures, e.g., using landmarks. However, it then becomes non-trivial to build effective deep models, especially when the underlying non-Euclidean grid is irregular and coarse. Recent deep models using graph convolutions provide an appropriate framework to handle such non-Euclidean data, but many of them, particularly those based on global graph Laplacians, lack expressiveness to capture local features required for representation of signals lying on the non-Euclidean grid. The current paper introduces a new type of graph convolution with learnable low-rank local filters, which is provably more expressive than previous spectral graph convolution methods. The model also provides a unified framework for both spectral and spatial graph convolutions. To improve model robustness, regularization by local graph Laplacians is introduced. The representation stability against input graph data perturbation is theoretically proved, making use of the graph filter locality and the local graph regularization. Experiments on spherical mesh data, real-world facial expression recognition/skeleton-based action recognition data, and data with simulated graph noise show the empirical advantage of the proposed model.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Graph convolution model for landmark data with local graph regularization and provable graph signal representation expressiveness and stability.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=9OHFhefeB86&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="HgLO8yalfwc" data-number="2310">
      <h4>
        <a href="https://openreview.net/forum?id=HgLO8yalfwc">
            Regularized Inverse Reinforcement Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=HgLO8yalfwc" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Wonseok_Jeon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wonseok_Jeon1">Wonseok Jeon</a>, <a href="https://openreview.net/profile?id=~Chen-Yang_Su1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chen-Yang_Su1">Chen-Yang Su</a>, <a href="https://openreview.net/profile?id=~Paul_Barde1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Paul_Barde1">Paul Barde</a>, <a href="https://openreview.net/profile?id=~Thang_Doan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Thang_Doan1">Thang Doan</a>, <a href="https://openreview.net/profile?id=~Derek_Nowrouzezahrai1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Derek_Nowrouzezahrai1">Derek Nowrouzezahrai</a>, <a href="https://openreview.net/profile?id=~Joelle_Pineau1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joelle_Pineau1">Joelle Pineau</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#HgLO8yalfwc-details-809" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="HgLO8yalfwc-details-809"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">inverse reinforcement learning, reward learning, regularized markov decision processes, reinforcement learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Inverse Reinforcement Learning (IRL) aims to facilitate a learner’s ability to imitate expert behavior by acquiring reward functions that explain the expert’s decisions. Regularized IRLapplies strongly convex regularizers to the learner’s policy in order to avoid the expert’s behavior being rationalized by arbitrary constant rewards, also known as degenerate solutions. We propose tractable solutions, and practical methods to obtain them, for regularized IRL. Current methods are restricted to the maximum-entropy IRL framework, limiting them to Shannon-entropy regularizers, as well as proposing solutions that are intractable in practice.  We present theoretical backing for our proposed IRL method’s applicability to both discrete and continuous controls, empirically validating our performance on a variety of tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose tractable solutions of regularized IRL and algorithms to acquire those solutions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="WznmQa42ZAx" data-number="2026">
      <h4>
        <a href="https://openreview.net/forum?id=WznmQa42ZAx">
            Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking
        </a>
      
        
          <a href="https://openreview.net/pdf?id=WznmQa42ZAx" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Michael_Sejr_Schlichtkrull1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Sejr_Schlichtkrull1">Michael Sejr Schlichtkrull</a>, <a href="https://openreview.net/profile?id=~Nicola_De_Cao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nicola_De_Cao1">Nicola De Cao</a>, <a href="https://openreview.net/profile?id=~Ivan_Titov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ivan_Titov1">Ivan Titov</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#WznmQa42ZAx-details-361" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WznmQa42ZAx-details-361"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Graph neural networks, interpretability, sparse stochastic gates, semantic role labeling, question answering</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Graph neural networks (GNNs) have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs (e.g. syntactic trees or co-reference structures) contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="35" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c30"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mn>0</mn></msub></math></mjx-assistive-mml></mjx-container> norm. We use our technique as an attribution method to analyze GNN models for two tasks -- question answering and semantic role labeling -- providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a novel post-hoc interpretation method for graph neural networks, and apply it to analyse two models from the NLP literature.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="VqzVhqxkjH1" data-number="1415">
      <h4>
        <a href="https://openreview.net/forum?id=VqzVhqxkjH1">
            Deep Neural Network Fingerprinting by Conferrable Adversarial Examples
        </a>
      
        
          <a href="https://openreview.net/pdf?id=VqzVhqxkjH1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Nils_Lukas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nils_Lukas1">Nils Lukas</a>, <a href="https://openreview.net/profile?id=~Yuxuan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuxuan_Zhang1">Yuxuan Zhang</a>, <a href="https://openreview.net/profile?id=~Florian_Kerschbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Kerschbaum1">Florian Kerschbaum</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 02 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#VqzVhqxkjH1-details-66" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="VqzVhqxkjH1-details-66"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Fingerprinting, Adversarial Examples, Transferability, Conferrability</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In Machine Learning as a Service, a provider trains a deep neural network and gives many users access. The hosted (source) model is susceptible to model stealing attacks, where an adversary derives a surrogate model from API access to the source model. For post hoc detection of such attacks, the provider needs a robust method to determine whether a suspect model is a surrogate of their model. We propose a fingerprinting method for deep neural network classifiers that extracts a set of inputs from the source model so that only surrogates agree with the source model on the classification of such inputs. These inputs are a subclass of transferable adversarial examples which we call conferrable adversarial examples that exclusively transfer with a target label from a source model to its surrogates. We propose a new method to generate these conferrable adversarial examples. We present an extensive study on the irremovability of our fingerprint against fine-tuning, weight pruning, retraining, retraining with different architectures, three model extraction attacks from related work, transfer learning, adversarial training, and two new adaptive attacks. Our fingerprint is robust against distillation, related model extraction attacks, and even transfer learning when the attacker has no access to the model provider's dataset. Our fingerprint is the first method that reaches a ROC AUC of 1.0 in verifying surrogates, compared to a ROC AUC of 0.63 by previous fingerprints. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Proposal of a new property called "conferrability" for adversarial examples that we use as a method for DNN fingerprinting robust to model extraction.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="uXl3bZLkr3c" data-number="1015">
      <h4>
        <a href="https://openreview.net/forum?id=uXl3bZLkr3c">
            Tent: Fully Test-Time Adaptation by Entropy Minimization
        </a>
      
        
          <a href="https://openreview.net/pdf?id=uXl3bZLkr3c" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Dequan_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dequan_Wang1">Dequan Wang</a>, <a href="https://openreview.net/profile?id=~Evan_Shelhamer2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Evan_Shelhamer2">Evan Shelhamer</a>, <a href="https://openreview.net/profile?id=~Shaoteng_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shaoteng_Liu1">Shaoteng Liu</a>, <a href="https://openreview.net/profile?id=~Bruno_Olshausen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bruno_Olshausen1">Bruno Olshausen</a>, <a href="https://openreview.net/profile?id=~Trevor_Darrell2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Trevor_Darrell2">Trevor Darrell</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#uXl3bZLkr3c-details-38" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="uXl3bZLkr3c-details-38"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning, unsupervised learning, domain adaptation, self-supervision, robustness</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent): we optimize the model for confidence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Deep networks can generalize better during testing by adapting to feedback from their own predictions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=uXl3bZLkr3c&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="zDy_nQCXiIj" data-number="635">
      <h4>
        <a href="https://openreview.net/forum?id=zDy_nQCXiIj">
            GAN "Steerability" without optimization 
        </a>
      
        
          <a href="https://openreview.net/pdf?id=zDy_nQCXiIj" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Nurit_Spingarn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nurit_Spingarn1">Nurit Spingarn</a>, <a href="https://openreview.net/profile?id=~Ron_Banner1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ron_Banner1">Ron Banner</a>, <a href="https://openreview.net/profile?id=~Tomer_Michaeli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tomer_Michaeli1">Tomer Michaeli</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 11 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#zDy_nQCXiIj-details-660" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zDy_nQCXiIj-details-660"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Generative Adversarial Network, semantic directions in latent space, nonlinear walk</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent research has shown remarkable success in revealing "steering" directions in the latent spaces of pre-trained GANs. These directions correspond to semantically meaningful image transformations (e.g., shift, zoom, color manipulations), and have the same interpretable effect across all categories that the GAN can generate. Some methods focus on user-specified transformations, while others discover transformations in an unsupervised manner. However, all existing techniques rely on an optimization procedure to expose those directions, and offer no control over the degree of allowed interaction between different transformations. In this paper, we show that "steering" trajectories can be computed in closed form directly from the generator's weights without any form of training or optimization. This applies to user-prescribed geometric transformations, as well as to unsupervised discovery of more complex effects. Our approach allows determining both linear and nonlinear trajectories, and has many advantages over previous methods. In particular, we can control whether one transformation is allowed to come on the expense of another (e.g., zoom-in with or without allowing translation to keep the object centered). Moreover, we can determine the natural end-point of the trajectory, which corresponds to the largest extent to which a transformation can be applied without incurring degradation. Finally, we show how transferring attributes between images can be achieved without optimization, even across different categories.
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Extracting linear &amp; nonlinear semantic directions in GAN latent space without any required optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="MLSvqIHRidA" data-number="458">
      <h4>
        <a href="https://openreview.net/forum?id=MLSvqIHRidA">
            Contrastive Divergence Learning is a Time Reversal Adversarial Game
        </a>
      
        
          <a href="https://openreview.net/pdf?id=MLSvqIHRidA" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Omer_Yair1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Omer_Yair1">Omer Yair</a>, <a href="https://openreview.net/profile?id=~Tomer_Michaeli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tomer_Michaeli1">Tomer Michaeli</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#MLSvqIHRidA-details-562" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="MLSvqIHRidA-details-562"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Unsupervised learning, energy based model, adversarial learning, contrastive divergence, noise contrastive estimation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Contrastive divergence (CD) learning is a classical method for fitting unnormalized statistical models to data samples. Despite its wide-spread use, the convergence properties of this algorithm are still not well understood. The main source of difficulty is an unjustified approximation which has been used to derive the gradient of the loss. In this paper, we present an alternative derivation of CD that does not require any approximation and sheds new light on the objective that is actually being optimized by the algorithm. Specifically, we show that CD is an adversarial learning procedure, where a discriminator attempts to classify whether a Markov chain generated from the model has been time-reversed. Thus, although predating generative adversarial networks (GANs) by more than a decade, CD is, in fact, closely related to these techniques. Our derivation settles well with previous observations, which have concluded that CD's update steps cannot be expressed as the gradients of any fixed objective function. In addition, as a byproduct, our derivation reveals a simple correction that can be used as an alternative to Metropolis-Hastings rejection, which is required when the underlying Markov chain is inexact (e.g., when using Langevin dynamics with a large step).</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present an alternative derivation of the classical Contrastive divergence method, which reveals that it is in fact an adversarial learning procedure.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=MLSvqIHRidA&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="LGgdb4TS4Z" data-number="2141">
      <h4>
        <a href="https://openreview.net/forum?id=LGgdb4TS4Z">
            Topology-Aware Segmentation Using Discrete Morse Theory
        </a>
      
        
          <a href="https://openreview.net/pdf?id=LGgdb4TS4Z" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xiaoling_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaoling_Hu1">Xiaoling Hu</a>, <a href="https://openreview.net/profile?id=~Yusu_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yusu_Wang1">Yusu Wang</a>, <a href="https://openreview.net/profile?id=~Li_Fuxin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Li_Fuxin1">Li Fuxin</a>, <a href="https://openreview.net/profile?id=~Dimitris_Samaras3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dimitris_Samaras3">Dimitris Samaras</a>, <a href="https://openreview.net/profile?id=~Chao_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chao_Chen1">Chao Chen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#LGgdb4TS4Z-details-866" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LGgdb4TS4Z-details-866"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Topology, Morse theory, Image segmentation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">In the segmentation of fine-scale structures from natural and biomedical images, per-pixel accuracy is not the only metric of concern. Topological correctness, such as vessel connectivity and membrane closure, is crucial for downstream analysis tasks. In this paper, we propose a new approach to train deep image segmentation networks for better topological accuracy. In particular, leveraging the power of discrete Morse theory (DMT), we identify global structures, including 1D skeletons and 2D patches, which are important for topological accuracy. Trained with a novel loss based on these global structures, the network performance is significantly improved especially near topologically challenging locations (such as weak spots of connections and membranes). On diverse datasets, our method achieves superior performance on both the DICE score and topological metrics.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper proposes a loss based on discrete Morse theory to train deep image segmentation networks for better topological accuracy.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Ut1vF_q_vC" data-number="952">
      <h4>
        <a href="https://openreview.net/forum?id=Ut1vF_q_vC">
            Are Neural Rankers still Outperformed by Gradient Boosted Decision Trees?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Ut1vF_q_vC" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhen_Qin5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhen_Qin5">Zhen Qin</a>, <a href="https://openreview.net/profile?email=lyyanle%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="lyyanle@google.com">Le Yan</a>, <a href="https://openreview.net/profile?id=~Honglei_Zhuang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Honglei_Zhuang1">Honglei Zhuang</a>, <a href="https://openreview.net/profile?id=~Yi_Tay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Tay1">Yi Tay</a>, <a href="https://openreview.net/profile?id=~Rama_Kumar_Pasumarthi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rama_Kumar_Pasumarthi1">Rama Kumar Pasumarthi</a>, <a href="https://openreview.net/profile?id=~Xuanhui_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuanhui_Wang1">Xuanhui Wang</a>, <a href="https://openreview.net/profile?email=bemike%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="bemike@google.com">Michael Bendersky</a>, <a href="https://openreview.net/profile?email=najork%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="najork@google.com">Marc Najork</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Ut1vF_q_vC-details-255" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Ut1vF_q_vC-details-255"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Learning to Rank, benchmark, neural network, gradient boosted decision trees</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Despite the success of neural models on many major machine learning problems, their effectiveness on traditional Learning-to-Rank (LTR) problems is still not widely acknowledged. We first validate this concern by showing that most recent neural LTR models are, by a large margin, inferior to the best publicly available Gradient Boosted Decision Trees (GBDT) in terms of their reported ranking accuracy on benchmark datasets. This unfortunately was somehow overlooked in recent neural LTR papers. We then investigate why existing neural LTR models under-perform and identify several of their weaknesses. Furthermore, we propose a unified framework comprising of counter strategies to ameliorate the existing weaknesses of neural models. Our models are the first to be able to perform equally well, comparing with the best tree-based baseline, while outperforming recently published neural LTR models by a large margin. Our results can also serve as a benchmark to facilitate future improvement of neural LTR models.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="lVgB2FUbzuQ" data-number="2637">
      <h4>
        <a href="https://openreview.net/forum?id=lVgB2FUbzuQ">
            Predicting Infectiousness for Proactive Contact Tracing
        </a>
      
        
          <a href="https://openreview.net/pdf?id=lVgB2FUbzuQ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yoshua_Bengio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoshua_Bengio1">Yoshua Bengio</a>, <a href="https://openreview.net/profile?id=~Prateek_Gupta2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Prateek_Gupta2">Prateek Gupta</a>, <a href="https://openreview.net/profile?id=~Tegan_Maharaj1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tegan_Maharaj1">Tegan Maharaj</a>, <a href="https://openreview.net/profile?id=~Nasim_Rahaman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nasim_Rahaman1">Nasim Rahaman</a>, <a href="https://openreview.net/profile?id=~Martin_Weiss4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Weiss4">Martin Weiss</a>, <a href="https://openreview.net/profile?id=~Tristan_Deleu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tristan_Deleu1">Tristan Deleu</a>, <a href="https://openreview.net/profile?id=~Eilif_Benjamin_Muller1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Eilif_Benjamin_Muller1">Eilif Benjamin Muller</a>, <a href="https://openreview.net/profile?id=~Meng_Qu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Meng_Qu2">Meng Qu</a>, <a href="https://openreview.net/profile?email=victor.schmidt%40mila.quebec" class="profile-link" data-toggle="tooltip" data-placement="top" title="victor.schmidt@mila.quebec">victor schmidt</a>, <a href="https://openreview.net/profile?id=~Pierre-luc_St-charles1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pierre-luc_St-charles1">Pierre-luc St-charles</a>, <a href="https://openreview.net/profile?email=halsdurf%40uottawa.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="halsdurf@uottawa.ca">hannah alsdurf</a>, <a href="https://openreview.net/profile?id=~Olexa_Bilaniuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Olexa_Bilaniuk1">Olexa Bilaniuk</a>, <a href="https://openreview.net/profile?email=david.buckeridge%40mcgill.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="david.buckeridge@mcgill.ca">david buckeridge</a>, <a href="https://openreview.net/profile?email=gaetan.marceau.caron%40mila.quebec" class="profile-link" data-toggle="tooltip" data-placement="top" title="gaetan.marceau.caron@mila.quebec">gaetan caron</a>, <a href="https://openreview.net/profile?email=pierre.luc.carrier%40mila.quebec" class="profile-link" data-toggle="tooltip" data-placement="top" title="pierre.luc.carrier@mila.quebec">pierre luc carrier</a>, <a href="https://openreview.net/profile?id=~Joumana_Ghosn1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joumana_Ghosn1">Joumana Ghosn</a>, <a href="https://openreview.net/profile?email=satya.ortiz-gagne%40mila.quebec" class="profile-link" data-toggle="tooltip" data-placement="top" title="satya.ortiz-gagne@mila.quebec">satya ortiz gagne</a>, <a href="https://openreview.net/profile?id=~Christopher_Pal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christopher_Pal1">Christopher Pal</a>, <a href="https://openreview.net/profile?id=~Irina_Rish1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Irina_Rish1">Irina Rish</a>, <a href="https://openreview.net/profile?id=~Bernhard_Sch%C3%B6lkopf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernhard_Schölkopf1">Bernhard Schölkopf</a>, <a href="https://openreview.net/profile?email=abhinav.sharma%40mcgill.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="abhinav.sharma@mcgill.ca">abhinav sharma</a>, <a href="https://openreview.net/profile?id=~Jian_Tang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jian_Tang1">Jian Tang</a>, <a href="https://openreview.net/profile?email=andrew.williams%40umontreal.ca" class="profile-link" data-toggle="tooltip" data-placement="top" title="andrew.williams@umontreal.ca">andrew williams</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#lVgB2FUbzuQ-details-855" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="lVgB2FUbzuQ-details-855"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">covid-19, contact tracing, distributed inference, set transformer, deepset, epidemiology, applications, domain randomization, retraining, simulation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The COVID-19 pandemic has spread rapidly worldwide, overwhelming manual contact tracing in many countries and resulting in widespread lockdowns for emergency containment. Large-scale digital contact tracing (DCT) has emerged as a potential solution to resume economic and social activity while minimizing spread of the virus. Various DCT methods have been proposed, each making trade-offs be-tween privacy, mobility restrictions, and public health. The most common approach, binary contact tracing (BCT), models infection as a binary event, informed only by an individual’s test results, with corresponding binary recommendations that either all or none of the individual’s contacts quarantine. BCT ignores the inherent uncertainty in contacts and the infection process, which could be used to tailor messaging to high-risk individuals, and prompt proactive testing or earlier warnings. It also does not make use of observations such as symptoms or pre-existing medical conditions, which could be used to make more accurate infectiousness predictions. In this paper, we use a recently-proposed COVID-19 epidemiological simulator to develop and test methods that can be deployed to a smartphone to locally and proactively predict an individual’s infectiousness (risk of infecting others) based on their contact history and other information, while respecting strong privacy constraints. Predictions are used to provide personalized recommendations to the individual via an app, as well as to send anonymized messages to the individual’s contacts, who use this information to better predict their own infectiousness, an approach we call proactive contact tracing (PCT). Similarly to other works, we find that compared to no tracing, all DCT methods tested are able to reduce spread of the disease and thus save lives, even at low adoption rates, strongly supporting a role for DCT methods in managing the pandemic. Further, we find a deep-learning based PCT method which improves over BCT for equivalent average mobility, suggesting PCT could help in safe re-opening and second-wave prevention.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Proposes a framework called Proactive Contact Tracing which uses distributed inference of expected Covid-19 infectiousness to provide individualized, private recommendations.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="yr1mzrH3IC" data-number="2011">
      <h4>
        <a href="https://openreview.net/forum?id=yr1mzrH3IC">
            Regularization Matters in Policy Optimization - An Empirical Study on Continuous Control
        </a>
      
        
          <a href="https://openreview.net/pdf?id=yr1mzrH3IC" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhuang_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhuang_Liu1">Zhuang Liu</a>, <a href="https://openreview.net/profile?id=~Xuanlin_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuanlin_Li1">Xuanlin Li</a>, <a href="https://openreview.net/profile?id=~Bingyi_Kang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bingyi_Kang1">Bingyi Kang</a>, <a href="https://openreview.net/profile?id=~Trevor_Darrell2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Trevor_Darrell2">Trevor Darrell</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 24 Feb 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>21 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#yr1mzrH3IC-details-799" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="yr1mzrH3IC-details-799"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Policy Optimization, Regularization, Continuous Control, Deep Reinforcement Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep Reinforcement Learning (Deep RL) has been receiving increasingly more attention  thanks to its encouraging performance on a variety of control tasks. Yet, conventional regularization techniques in training neural networks (e.g., <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="36" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> regularization, dropout) have been largely ignored in RL methods, possibly because agents are typically trained and evaluated in the same environment, and because the deep RL community focuses more on high-level algorithm designs. In this work, we present the first comprehensive study of regularization techniques with multiple policy optimization algorithms on continuous control tasks. Interestingly, we find conventional regularization techniques on the policy networks can often bring large improvement, especially on harder tasks. Our findings are shown to be robust against training hyperparameter variations. We also compare these techniques with the more widely used entropy regularization. In addition, we study regularizing different components and find that only regularizing the policy network is typically the best. We further analyze why regularization may help generalization in RL from four perspectives - sample complexity, reward distribution, weight norm, and noise robustness. We hope our study provides guidance for future practices in regularizing policy optimization algorithms. Our code is available at https://github.com/xuanlinli17/iclr2021_rlreg .</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that conventional regularization methods (e.g., <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="37" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container>), which have been largely ignored in RL methods, can be very effective in policy optimization on continuous control tasks; we also analyze why they can help from several perspectives.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=yr1mzrH3IC&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="O-XJwyoIF-k" data-number="2176">
      <h4>
        <a href="https://openreview.net/forum?id=O-XJwyoIF-k">
            Minimum Width for Universal Approximation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=O-XJwyoIF-k" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sejun_Park1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sejun_Park1">Sejun Park</a>, <a href="https://openreview.net/profile?id=~Chulhee_Yun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chulhee_Yun1">Chulhee Yun</a>, <a href="https://openreview.net/profile?id=~Jaeho_Lee3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jaeho_Lee3">Jaeho Lee</a>, <a href="https://openreview.net/profile?id=~Jinwoo_Shin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jinwoo_Shin1">Jinwoo Shin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#O-XJwyoIF-k-details-713" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="O-XJwyoIF-k-details-713"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">universal approximation, neural networks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The universal approximation property of width-bounded networks has been studied as a dual of classical universal approximation results on depth-bounded networks. However, the critical width enabling the universal approximation has not been exactly characterized in terms of the input dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="38" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mi>x</mi></msub></math></mjx-assistive-mml></mjx-container> and the output dimension <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="39" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>d</mi><mi>y</mi></msub></math></mjx-assistive-mml></mjx-container>. In this work, we provide the first definitive result in this direction for networks using the ReLU activation functions: The minimum width required for the universal approximation of the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="40" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msup><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: 0.363em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>L</mi><mi>p</mi></msup></math></mjx-assistive-mml></mjx-container> functions is exactly <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="41" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c78"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c7B"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D465 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-msub space="2"><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D451 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c7D"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo data-mjx-texclass="OP" movablelimits="true">max</mo><mo fence="false" stretchy="false">{</mo><msub><mi>d</mi><mi>x</mi></msub><mo>+</mo><mn>1</mn><mo>,</mo><msub><mi>d</mi><mi>y</mi></msub><mo fence="false" stretchy="false">}</mo></math></mjx-assistive-mml></mjx-container>. We also prove that the same conclusion does not hold for the uniform approximation with ReLU, but does hold with an additional threshold activation function. Our proof technique can be also used to derive a tighter upper bound on the minimum width required for the universal approximation using networks with general activation functions.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We establish the tight bound on width for the universal approximability of neural network.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="ks5nebunVn_" data-number="1717">
      <h4>
        <a href="https://openreview.net/forum?id=ks5nebunVn_">
            Towards Robustness Against Natural Language Word Substitutions
        </a>
      
        
          <a href="https://openreview.net/pdf?id=ks5nebunVn_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xinshuai_Dong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xinshuai_Dong1">Xinshuai Dong</a>, <a href="https://openreview.net/profile?id=~Anh_Tuan_Luu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anh_Tuan_Luu2">Anh Tuan Luu</a>, <a href="https://openreview.net/profile?id=~Rongrong_Ji5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rongrong_Ji5">Rongrong Ji</a>, <a href="https://openreview.net/profile?id=~Hong_Liu9" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hong_Liu9">Hong Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#ks5nebunVn_-details-838" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="ks5nebunVn_-details-838"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Natural Language Processing, Adversarial Defense</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Robustness against word substitutions has a well-defined and widely acceptable form, i.e., using semantically similar words as substitutions, and thus it is considered as a fundamental stepping-stone towards broader robustness in natural language processing. Previous defense methods capture word substitutions in vector space by using either l_2-ball or hyper-rectangle, which results in perturbation sets that are not inclusive enough or unnecessarily large, and thus impedes mimicry of worst cases for robust training. In this paper, we introduce a novel Adversarial Sparse Convex Combination (ASCC) method. We model the word substitution attack space as a convex hull and leverages a regularization term to enforce perturbation towards an actual substitution, thus aligning our modeling better with the discrete textual space. Based on  ASCC method, we further propose ASCC-defense, which leverages ASCC to generate worst-case perturbations and incorporates adversarial training towards robustness. Experiments show that ASCC-defense outperforms the current state-of-the-arts in terms of robustness on two prevailing NLP tasks, i.e., sentiment analysis and natural language inference, concerning several attacks across multiple model architectures. Besides, we also envision a new class of defense towards robustness in NLP, where our robustly trained word vectors can be plugged into a normally trained model and enforce its robustness without applying any other defense techniques.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Capture adversarial word substitutions in the vector space using convex hull towards robustness.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="p-NZIuwqhI4" data-number="1317">
      <h4>
        <a href="https://openreview.net/forum?id=p-NZIuwqhI4">
            On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers
        </a>
      
        
          <a href="https://openreview.net/pdf?id=p-NZIuwqhI4" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Kenji_Kawaguchi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kenji_Kawaguchi1">Kenji Kawaguchi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Feb 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#p-NZIuwqhI4-details-254" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="p-NZIuwqhI4-details-254"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Implicit Deep Learning, Deep Equilibrium Models, Gradient Descent, Learning Theory, Non-Convex Optimization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A deep equilibrium model uses implicit layers, which are implicitly defined through an equilibrium point of an infinite sequence of computation. It avoids any explicit computation of the infinite sequence by finding an equilibrium point directly via root-finding and by computing gradients via implicit differentiation. In this paper, we analyze the gradient dynamics of deep equilibrium models with nonlinearity only on weight matrices and non-convex objective functions of weights for regression and classification. Despite non-convexity, convergence to global optimum at a linear rate is guaranteed without any assumption on the width of the models, allowing the width to be smaller than the output dimension and the number of data points. Moreover, we prove a relation between the gradient dynamics of the deep implicit layer and the dynamics of trust region Newton method of a shallow explicit layer. This mathematically proven relation along with our numerical observation suggests the importance of understanding implicit bias of implicit layers and an open problem on the topic. Our proofs deal with implicit layers, weight tying and nonlinearity on weights, and differ from those in the related literature.
      
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We analyze gradient dynamics of a simple deep equilibrium model and mathematically prove its theoretical properties. </span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="US-TP-xnXI" data-number="2229">
      <h4>
        <a href="https://openreview.net/forum?id=US-TP-xnXI">
            Structured Prediction as Translation between Augmented Natural Languages
        </a>
      
        
          <a href="https://openreview.net/pdf?id=US-TP-xnXI" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Giovanni_Paolini1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Giovanni_Paolini1">Giovanni Paolini</a>, <a href="https://openreview.net/profile?id=~Ben_Athiwaratkun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ben_Athiwaratkun1">Ben Athiwaratkun</a>, <a href="https://openreview.net/profile?email=kronej%40amazon.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="kronej@amazon.com">Jason Krone</a>, <a href="https://openreview.net/profile?id=~Jie_Ma3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jie_Ma3">Jie Ma</a>, <a href="https://openreview.net/profile?id=~Alessandro_Achille1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alessandro_Achille1">Alessandro Achille</a>, <a href="https://openreview.net/profile?id=~RISHITA_ANUBHAI2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~RISHITA_ANUBHAI2">RISHITA ANUBHAI</a>, <a href="https://openreview.net/profile?id=~Cicero_Nogueira_dos_Santos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cicero_Nogueira_dos_Santos1">Cicero Nogueira dos Santos</a>, <a href="https://openreview.net/profile?id=~Bing_Xiang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bing_Xiang2">Bing Xiang</a>, <a href="https://openreview.net/profile?id=~Stefano_Soatto1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefano_Soatto1">Stefano Soatto</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#US-TP-xnXI-details-114" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="US-TP-xnXI-details-114"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">language models, few-shot learning, transfer learning, structured prediction, generative modeling, sequence to sequence, multi-task learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks, and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a unified text-to-text approach to handle a variety of structured prediction tasks in a single model, allowing seamless multi-task training and providing extra benefits on low-resource scenarios. </span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="g-wu9TMPODo" data-number="1136">
      <h4>
        <a href="https://openreview.net/forum?id=g-wu9TMPODo">
            How Benign is Benign Overfitting ?
        </a>
      
        
          <a href="https://openreview.net/pdf?id=g-wu9TMPODo" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Amartya_Sanyal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Amartya_Sanyal1">Amartya Sanyal</a>, <a href="https://openreview.net/profile?id=~Puneet_K._Dokania1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Puneet_K._Dokania1">Puneet K. Dokania</a>, <a href="https://openreview.net/profile?id=~Varun_Kanade1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Varun_Kanade1">Varun Kanade</a>, <a href="https://openreview.net/profile?id=~Philip_Torr1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philip_Torr1">Philip Torr</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#g-wu9TMPODo-details-963" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="g-wu9TMPODo-details-963"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">benign overfitting, adversarial robustness, memorization, generalization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We investigate two causes for adversarial vulnerability in deep neural networks: bad data and (poorly) trained models. When trained with SGD, deep neural networks essentially achieve zero training error, even in the presence of label noise, while also exhibiting good generalization on natural test data, something referred to as benign overfitting (Bartlett et al., 2020; Chatterji &amp; Long, 2020).  However, these models are vulnerable to adversarial attacks. We identify label noise as one of the causes for adversarial vulnerability, and provide theoretical and empirical evidence in support of this. Surprisingly, we find several instances of label noise in datasets such as MNIST and CIFAR, and that robustly trained models incur training error on some of these, i.e. they don’t fit the noise. However, removing noisy labels alone does not suffice to achieve adversarial robustness. We conjecture that in part sub-optimal representation learning is also responsible for adversarial vulnerability. By means of simple theoretical setups, we show how the choice of representation can drastically affect adversarial robustness.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Interpolating label noise hurts adversarial robustness</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=g-wu9TMPODo&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xvxPuCkCNPO" data-number="240">
      <h4>
        <a href="https://openreview.net/forum?id=xvxPuCkCNPO">
            Correcting experience replay for multi-agent communication
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xvxPuCkCNPO" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sanjeevan_Ahilan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanjeevan_Ahilan1">Sanjeevan Ahilan</a>, <a href="https://openreview.net/profile?id=~Peter_Dayan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Peter_Dayan1">Peter Dayan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xvxPuCkCNPO-details-361" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xvxPuCkCNPO-details-361"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">multi-agent reinforcement learning, experience replay, communication, relabelling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We consider the problem of learning to communicate using multi-agent reinforcement learning (MARL). A common approach is to learn off-policy, using data sampled from a replay buffer. However, messages received in the past may not accurately reflect the current communication policy of each agent, and this complicates learning. We therefore introduce a 'communication correction' which accounts for the non-stationarity of observed communication induced by multi-agent learning. It works by relabelling the received message to make it likely under the communicator's current policy, and thus be a better reflection of the receiver's current environment. To account for cases in which agents are both senders and receivers, we introduce an ordered relabelling scheme. Our correction is computationally efficient and can be integrated with a range of off-policy algorithms. We find in our experiments that it substantially improves the ability of communicating MARL systems to learn across a variety of cooperative and competitive tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We improve multi-agent learning by relabelling past experience to better reflect current communication policies </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="LSFCEb3GYU7" data-number="2264">
      <h4>
        <a href="https://openreview.net/forum?id=LSFCEb3GYU7">
            Emergent Symbols through Binding in External Memory
        </a>
      
        
          <a href="https://openreview.net/pdf?id=LSFCEb3GYU7" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Taylor_Whittington_Webb1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Taylor_Whittington_Webb1">Taylor Whittington Webb</a>, <a href="https://openreview.net/profile?email=sinha.ishan%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="sinha.ishan@gmail.com">Ishan Sinha</a>, <a href="https://openreview.net/profile?id=~Jonathan_Cohen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jonathan_Cohen1">Jonathan Cohen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 10 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#LSFCEb3GYU7-details-80" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LSFCEb3GYU7-details-80"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">abstract rules, out-of-distribution generalization, external memory, indirection, variable binding</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A key aspect of human intelligence is the ability to infer abstract rules directly from high-dimensional sensory data, and to do so given only a limited amount of training experience. Deep neural network algorithms have proven to be a powerful tool for learning directly from high-dimensional data, but currently lack this capacity for data-efficient induction of abstract rules, leading some to argue that symbol-processing mechanisms will be necessary to account for this capacity. In this work, we take a step toward bridging this gap by introducing the Emergent Symbol Binding Network (ESBN), a recurrent network augmented with an external memory that enables a form of variable-binding and indirection. This binding mechanism allows symbol-like representations to emerge through the learning process without the need to explicitly incorporate symbol-processing machinery, enabling the ESBN to learn rules in a manner that is abstracted away from the particular entities to which those rules apply. Across a series of tasks, we show that this architecture displays nearly perfect generalization of learned rules to novel entities given only a limited number of training examples, and outperforms a number of other competitive neural network architectures.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a new architecture,  the Emergent Symbol Binding Network, that enables rapid learning of abstract rules and strong generalization of those rules to novel entities.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="opHLcXxYTC_" data-number="2659">
      <h4>
        <a href="https://openreview.net/forum?id=opHLcXxYTC_">
            Influence Estimation for Generative Adversarial Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=opHLcXxYTC_" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Naoyuki_Terashita1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Naoyuki_Terashita1">Naoyuki Terashita</a>, <a href="https://openreview.net/profile?id=~Hiroki_Ohashi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hiroki_Ohashi1">Hiroki Ohashi</a>, <a href="https://openreview.net/profile?email=yuichi.nonaka.zy%40hitachi.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yuichi.nonaka.zy@hitachi.com">Yuichi Nonaka</a>, <a href="https://openreview.net/profile?email=takashi.kanemaru.kf%40hitachi.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="takashi.kanemaru.kf@hitachi.com">Takashi Kanemaru</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#opHLcXxYTC_-details-929" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="opHLcXxYTC_-details-929"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">influence, generative adversarial networks, data cleansing</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Identifying harmful instances, whose absence in a training dataset improves model performance, is important for building better machine learning models. 
      Although previous studies have succeeded in estimating harmful instances under supervised settings, they cannot be trivially extended to generative adversarial networks (GANs).
      This is because previous approaches require that (i) the absence of a training instance directly affects the loss value and that (ii) the change in the loss directly measures the harmfulness of the instance for the performance of a model. 
      In GAN training, however, neither of the requirements is satisfied. 
      This is because, (i) the generator’s loss is not directly affected by the training instances as they are not part of the generator's training steps, and (ii) the values of GAN's losses normally do not capture the generative performance of a model.
      To this end, (i) we propose an influence estimation method that uses the Jacobian of the gradient of the generator's loss with respect to the discriminator’s parameters (and vice versa) to trace how the absence of an instance in the discriminator’s training affects the generator’s parameters, and (ii) we propose a novel evaluation scheme, in which we assess harmfulness of each training instance on the basis of how GAN evaluation metric (e.g., inception score) is expected to change due to the removal of the instance.
      We experimentally verified that our influence estimation method correctly inferred the changes in GAN evaluation metrics.
      We also demonstrated that the removal of the identified harmful instances effectively improved the model’s generative performance with respect to various GAN evaluation metrics.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an influence estimation method which predicts how GAN's output changes if an training instance is abesent, and propose to evaluate harmfulness of the instance by estimating how its absence improves GAN evaluation metric.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xCcdBRQEDW" data-number="2543">
      <h4>
        <a href="https://openreview.net/forum?id=xCcdBRQEDW">
            PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable Physics
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xCcdBRQEDW" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Zhiao_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhiao_Huang1">Zhiao Huang</a>, <a href="https://openreview.net/profile?id=~Yuanming_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuanming_Hu1">Yuanming Hu</a>, <a href="https://openreview.net/profile?id=~Tao_Du1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tao_Du1">Tao Du</a>, <a href="https://openreview.net/profile?email=elegyhunter%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="elegyhunter@gmail.com">Siyuan Zhou</a>, <a href="https://openreview.net/profile?id=~Hao_Su1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Su1">Hao Su</a>, <a href="https://openreview.net/profile?id=~Joshua_B._Tenenbaum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joshua_B._Tenenbaum1">Joshua B. Tenenbaum</a>, <a href="https://openreview.net/profile?id=~Chuang_Gan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chuang_Gan1">Chuang Gan</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xCcdBRQEDW-details-627" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xCcdBRQEDW-details-627"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Soft Body, Differentiable Physics, Benchmark</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Simulated virtual environments serve as one of the main driving forces behind developing and evaluating skill learning algorithms. However, existing environments typically only simulate rigid body physics. Additionally, the simulation process usually does not provide gradients that might be useful for planning and control optimizations. We introduce a new differentiable physics benchmark called PasticineLab, which includes a diverse collection of soft body manipulation tasks. In each task, the agent uses manipulators to deform the plasticine into a desired configuration. The underlying physics engine supports differentiable elastic and plastic deformation using the DiffTaichi system, posing many under-explored challenges to robotic agents. We evaluate several existing reinforcement learning (RL) methods and gradient-based methods on this benchmark. Experimental results suggest that 1) RL-based approaches struggle to solve most of the tasks efficiently;  2) gradient-based approaches, by optimizing open-loop control sequences with the built-in differentiable physics engine, can rapidly find a solution within tens of iterations, but still fall short on multi-stage tasks that require long-term planning. We expect that PlasticineLab will encourage the development of novel algorithms that combine differentiable physics and RL for more complex physics-based skill learning tasks. PlasticineLab will be made publicly available.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a soft-body manipulation benchmark with differentiable physics support.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="8PS8m9oYtNy" data-number="3381">
      <h4>
        <a href="https://openreview.net/forum?id=8PS8m9oYtNy">
            Implicit Normalizing Flows
        </a>
      
        
          <a href="https://openreview.net/pdf?id=8PS8m9oYtNy" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Cheng_Lu5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Cheng_Lu5">Cheng Lu</a>, <a href="https://openreview.net/profile?id=~Jianfei_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jianfei_Chen1">Jianfei Chen</a>, <a href="https://openreview.net/profile?id=~Chongxuan_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chongxuan_Li1">Chongxuan Li</a>, <a href="https://openreview.net/profile?id=~Qiuhao_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qiuhao_Wang1">Qiuhao Wang</a>, <a href="https://openreview.net/profile?id=~Jun_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jun_Zhu2">Jun Zhu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#8PS8m9oYtNy-details-124" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="8PS8m9oYtNy-details-124"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Normalizing flows, deep generative models, probabilistic inference, implicit functions</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Normalizing flows define a probability distribution by an explicit invertible transformation <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="42" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D433 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D431 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold">z</mi></mrow><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mrow><mi mathvariant="bold">x</mi></mrow><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>. In this work, we present implicit normalizing flows (ImpFlows), which generalize normalizing flows by allowing the mapping to be implicitly defined by the roots of an equation <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="43" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D433 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo><mjx-texatom space="2" texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D431 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mn class="mjx-b"><mjx-c class="mjx-c1D7CE TEX-B"></mjx-c></mjx-mn></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mo stretchy="false">(</mo><mrow><mi mathvariant="bold">z</mi></mrow><mo>,</mo><mrow><mi mathvariant="bold">x</mi></mrow><mo stretchy="false">)</mo><mo>=</mo><mrow><mn mathvariant="bold">0</mn></mrow></math></mjx-assistive-mml></mjx-container>. ImpFlows build on residual flows (ResFlows) with a proper balance between expressiveness and tractability. Through theoretical analysis, we show that the function space of ImpFlow is strictly richer than that of ResFlows. Furthermore, for any ResFlow with a fixed number of blocks, there exists some function that ResFlow has a non-negligible approximation error. However, the function is exactly representable by a single-block ImpFlow. We propose a scalable algorithm to train and draw samples from ImpFlows. Empirically, we evaluate ImpFlow on several classification and density modeling tasks, and ImpFlow outperforms ResFlow with a comparable amount of parameters on all the benchmarks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We generalize normalizing flows, allowing the mapping to be implicitly defined by the roots of an equation and enlarging the expressiveness power while retaining the tractability.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=8PS8m9oYtNy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="EqoXe2zmhrh" data-number="649">
      <h4>
        <a href="https://openreview.net/forum?id=EqoXe2zmhrh">
            Support-set bottlenecks for video-text representation learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=EqoXe2zmhrh" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Mandela_Patrick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mandela_Patrick1">Mandela Patrick</a>, <a href="https://openreview.net/profile?id=~Po-Yao_Huang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Po-Yao_Huang1">Po-Yao Huang</a>, <a href="https://openreview.net/profile?id=~Yuki_Asano1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yuki_Asano1">Yuki Asano</a>, <a href="https://openreview.net/profile?id=~Florian_Metze1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Metze1">Florian Metze</a>, <a href="https://openreview.net/profile?id=~Alexander_G_Hauptmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_G_Hauptmann1">Alexander G Hauptmann</a>, <a href="https://openreview.net/profile?id=~Joao_F._Henriques1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Joao_F._Henriques1">Joao F. Henriques</a>, <a href="https://openreview.net/profile?id=~Andrea_Vedaldi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrea_Vedaldi1">Andrea Vedaldi</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#EqoXe2zmhrh-details-790" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EqoXe2zmhrh-details-790"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">video representation learning, multi-modal learning, video-text learning, contrastive learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The dominant paradigm for learning video-text representations – noise contrastive learning – increases the similarity of the representations of pairs of samples that are known to be related, such as text and video from the same sample, and pushes away the representations of all other pairs. We posit that this last behaviour is too strict, enforcing dissimilar representations even for samples that are semantically-related – for example, visually similar videos or ones that share the same depicted action. In this paper, we propose a novel method that alleviates this by leveraging a generative model to naturally push these related samples together: each sample’s caption must be reconstructed as a weighted combination of a support set of visual representations. This simple idea ensures that representations are not overly-specialized to individual samples, are reusable across the dataset, and results in representations that explicitly encode semantics shared between samples, unlike noise contrastive learning. Our proposed method outperforms others by a large margin on MSR-VTT, VATEX, ActivityNet, and MSVD for video-to-text and text-to-video retrieval.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We use a generative objective to improve the instance discrimination limitations of contrastive learning to set new state-of-the-art results in text-to-video retrieval</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="LmUJqB1Cz8" data-number="3169">
      <h4>
        <a href="https://openreview.net/forum?id=LmUJqB1Cz8">
            Winning the L2RPN Challenge: Power Grid Management via Semi-Markov Afterstate Actor-Critic
        </a>
      
        
          <a href="https://openreview.net/pdf?id=LmUJqB1Cz8" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Deunsol_Yoon1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Deunsol_Yoon1">Deunsol Yoon</a>, <a href="https://openreview.net/profile?id=~Sunghoon_Hong2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sunghoon_Hong2">Sunghoon Hong</a>, <a href="https://openreview.net/profile?id=~Byung-Jun_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Byung-Jun_Lee1">Byung-Jun Lee</a>, <a href="https://openreview.net/profile?id=~Kee-Eung_Kim4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kee-Eung_Kim4">Kee-Eung Kim</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#LmUJqB1Cz8-details-353" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LmUJqB1Cz8-details-353"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">power grid management, deep reinforcement learning, graph neural network</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Safe and reliable electricity transmission in power grids is crucial for modern society. It is thus quite natural that there has been a growing interest in the automatic management of power grids, exempliﬁed by the Learning to Run a Power Network Challenge (L2RPN), modeling the problem as a reinforcement learning (RL) task. However, it is highly challenging to manage a real-world scale power grid, mostly due to the massive scale of its state and action space. In this paper, we present an off-policy actor-critic approach that effectively tackles the unique challenges in power grid management by RL, adopting the hierarchical policy together with the afterstate representation. Our agent ranked ﬁrst in the latest challenge (L2RPN WCCI 2020), being able to avoid disastrous situations while maintaining the highest level of operational efﬁciency in every test scenarios. This paper provides a formal description of the algorithmic aspect of our approach, as well as further experimental studies on diverse power grids.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We  present  an  off-policy  actor-critic  approach  that  effectively tackles  the  unique  challenges  in  power  grid  management  by  reinforcement learning,  adopting  the hierarchical policy together with the afterstate representation. </span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="KUDUoRsEphu" data-number="1754">
      <h4>
        <a href="https://openreview.net/forum?id=KUDUoRsEphu">
            Learning Incompressible Fluid Dynamics from Scratch - Towards Fast, Differentiable Fluid Models that Generalize
        </a>
      
        
          <a href="https://openreview.net/pdf?id=KUDUoRsEphu" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Nils_Wandel2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nils_Wandel2">Nils Wandel</a>, <a href="https://openreview.net/profile?id=~Michael_Weinmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Weinmann1">Michael Weinmann</a>, <a href="https://openreview.net/profile?id=~Reinhard_Klein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Reinhard_Klein1">Reinhard Klein</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 02 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#KUDUoRsEphu-details-13" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="KUDUoRsEphu-details-13"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Unsupervised Learning, Fluid Dynamics, U-Net</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Fast and stable fluid simulations are an essential prerequisite for applications ranging from computer-generated imagery to computer-aided design in research and development. However, solving the partial differential equations of incompressible fluids is a challenging task and traditional numerical approximation schemes come at high computational costs. Recent deep learning based approaches promise vast speed-ups but do not generalize to new fluid domains, require fluid simulation data for training, or rely on complex pipelines that outsource major parts of the fluid simulation to traditional methods.
      
      In this work, we propose a novel physics-constrained training approach that generalizes to new fluid domains, requires no fluid simulation data, and allows convolutional neural networks to map a fluid state from time-point t to a subsequent state at time t+dt in a single forward pass. This simplifies the pipeline to train and evaluate neural fluid models. After training, the framework yields models that are capable of fast fluid simulations and can handle various fluid phenomena including the Magnus effect and Kármán vortex streets. We present an interactive real-time demo to show the speed and generalization capabilities of our trained models. Moreover, the trained neural networks are efficient differentiable fluid solvers as they offer a differentiable update step to advance the fluid simulation in time. We exploit this fact in a proof-of-concept optimal control experiment. Our models significantly outperform a recent differentiable fluid solver in terms of computational speed and accuracy.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present an unsupervised training framework for incompressible fluid dynamics that allows neural networks to perform fast, accurate, differentiable fluid simulations and generalize to new domain geometries.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=KUDUoRsEphu&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="qYda4oLEc1" data-number="1296">
      <h4>
        <a href="https://openreview.net/forum?id=qYda4oLEc1">
            The Traveling Observer Model: Multi-task Learning Through Spatial Variable Embeddings
        </a>
      
        
          <a href="https://openreview.net/pdf?id=qYda4oLEc1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Elliot_Meyerson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Elliot_Meyerson1">Elliot Meyerson</a>, <a href="https://openreview.net/profile?id=~Risto_Miikkulainen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Risto_Miikkulainen1">Risto Miikkulainen</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#qYda4oLEc1-details-876" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="qYda4oLEc1-details-876"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Multi-task, Many-task, Multi-domain, Cross-domain, Variable Embeddings, Task Embeddings, Tabular, Analogies</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper frames a general prediction system as an observer traveling around a continuous space, measuring values at some locations, and predicting them at others. The observer is completely agnostic about any particular task being solved; it cares only about measurement locations and their values. This perspective leads to a machine learning framework in which seemingly unrelated tasks can be solved by a single model, by embedding their input and output variables into a shared space. An implementation of the framework is developed in which these variable embeddings are learned jointly with internal model parameters. In experiments, the approach is shown to (1) recover intuitive locations of variables in space and time, (2) exploit regularities across related datasets with completely disjoint input and output spaces, and (3) exploit regularities across seemingly unrelated tasks, outperforming task-specific single-task models and multi-task learning alternatives. The results suggest that even seemingly unrelated tasks may originate from similar underlying processes, a fact that the traveling observer model can use to make better predictions.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Learn a single model across "unrelated" tasks by embedding their input and output variables in a shared space.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="wpSWuz_hyqA" data-number="643">
      <h4>
        <a href="https://openreview.net/forum?id=wpSWuz_hyqA">
            Grounded Language Learning Fast and Slow
        </a>
      
        
          <a href="https://openreview.net/pdf?id=wpSWuz_hyqA" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Felix_Hill1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Felix_Hill1">Felix Hill</a>, <a href="https://openreview.net/profile?id=~Olivier_Tieleman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Olivier_Tieleman1">Olivier Tieleman</a>, <a href="https://openreview.net/profile?email=tamaravg%40google.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="tamaravg@google.com">Tamara von Glehn</a>, <a href="https://openreview.net/profile?id=~Nathaniel_Wong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nathaniel_Wong1">Nathaniel Wong</a>, <a href="https://openreview.net/profile?id=~Hamza_Merzic1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hamza_Merzic1">Hamza Merzic</a>, <a href="https://openreview.net/profile?id=~Stephen_Clark1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephen_Clark1">Stephen Clark</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#wpSWuz_hyqA-details-3" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wpSWuz_hyqA-details-3"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">language, cognition, fast-mapping, grounding, word-learning, memory, meta-learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent work has shown that large text-based neural language models acquire a surprising propensity for one-shot learning. Here, we show that an agent situated in a simulated 3D world, and endowed with a novel dual-coding external memory, can exhibit similar one-shot word learning when trained with conventional RL algorithms. After a single introduction to a novel object via visual perception and language ("This is a dax"), the agent can manipulate the object as instructed ("Put the dax on the bed"), combining short-term, within-episode knowledge of the nonsense word with long-term lexical and motor knowledge. We find that, under certain training conditions and with a particular memory writing mechanism, the agent's one-shot word-object binding generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects. We further show how dual-coding memory can be exploited as a signal for intrinsic motivation, stimulating the agent to seek names for objects that may be useful later. Together, the results demonstrate that deep neural networks can exploit meta-learning, episodic memory and an explicitly multi-modal environment to account for 'fast-mapping', a fundamental pillar of human cognitive development and a potentially transformative capacity for artificial agents.   </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A language-learning agent with dual-coding external memory meta-learns to combine fast-mapped and semantic lexical knowledge to execute instructions in one-shot.. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="D9I3drBz4UC" data-number="148">
      <h4>
        <a href="https://openreview.net/forum?id=D9I3drBz4UC">
            Long-tailed Recognition by Routing Diverse Distribution-Aware Experts
        </a>
      
        
          <a href="https://openreview.net/pdf?id=D9I3drBz4UC" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Xudong_Wang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xudong_Wang4">Xudong Wang</a>, <a href="https://openreview.net/profile?id=~Long_Lian1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Long_Lian1">Long Lian</a>, <a href="https://openreview.net/profile?id=~Zhongqi_Miao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhongqi_Miao1">Zhongqi Miao</a>, <a href="https://openreview.net/profile?id=~Ziwei_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziwei_Liu1">Ziwei Liu</a>, <a href="https://openreview.net/profile?id=~Stella_Yu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stella_Yu2">Stella Yu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 21 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#D9I3drBz4UC-details-387" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="D9I3drBz4UC-details-387"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Natural data are often long-tail distributed over semantic classes. Existing recognition methods tend to focus on gaining performance on tail classes, often at the expense of losing performance on head classes and with increased classifier variance. The low tail performance manifests itself in large inter-class confusion and high classifier variance. We aim to reduce both the bias and the variance of a long-tailed classifier by RoutIng Diverse Experts (RIDE), consisting of three components: 1) a shared architecture for multiple classifiers (experts); 2) a distribution-aware diversity loss that encourages more diverse decisions for classes with fewer training instances; and 3) an expert routing module that dynamically assigns more ambiguous instances to additional experts.  With on-par computational complexity, RIDE significantly outperforms the state-of-the-art methods by 5% to 7% on all the benchmarks including CIFAR100-LT, ImageNet-LT, and iNaturalist 2018. RIDE is also a universal framework that can be applied to different backbone networks and integrated into various long-tailed algorithms and training mechanisms for consistent performance gains. Our code is publicly available at https://github.com/frank-xwang/RIDE-LongTailRecognition.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="YTWGvpFOQD-" data-number="1641">
      <h4>
        <a href="https://openreview.net/forum?id=YTWGvpFOQD-">
            Differentially Private Learning Needs Better Features (or Much More Data)
        </a>
      
        
          <a href="https://openreview.net/pdf?id=YTWGvpFOQD-" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Florian_Tramer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Florian_Tramer1">Florian Tramer</a>, <a href="https://openreview.net/profile?id=~Dan_Boneh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dan_Boneh1">Dan Boneh</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Feb 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#YTWGvpFOQD--details-688" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YTWGvpFOQD--details-688"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Differential Privacy, Privacy, Deep Learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We demonstrate that differentially private machine learning has not yet reached its ''AlexNet moment'' on many canonical vision tasks: linear models trained on handcrafted features significantly outperform end-to-end deep neural networks for moderate privacy budgets.
      To exceed the performance of handcrafted features, we show that private learning requires either much more private data, or access to features learned on public data from a similar domain.
      Our work introduces simple yet strong baselines for differentially private learning that can inform the evaluation of future progress in this area.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Linear models with handcrafted features outperform end-to-end CNNs for differentially private learning</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="GJwMHetHc73" data-number="1953">
      <h4>
        <a href="https://openreview.net/forum?id=GJwMHetHc73">
            Unsupervised Object Keypoint Learning using Local Spatial Predictability
        </a>
      
        
          <a href="https://openreview.net/pdf?id=GJwMHetHc73" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Anand_Gopalakrishnan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anand_Gopalakrishnan1">Anand Gopalakrishnan</a>, <a href="https://openreview.net/profile?id=~Sjoerd_van_Steenkiste1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sjoerd_van_Steenkiste1">Sjoerd van Steenkiste</a>, <a href="https://openreview.net/profile?id=~J%C3%BCrgen_Schmidhuber1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jürgen_Schmidhuber1">Jürgen Schmidhuber</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 08 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#GJwMHetHc73-details-388" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GJwMHetHc73-details-388"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">unsupervised representation learning, object-keypoint representations, visual saliency</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose PermaKey, a novel approach to representation learning based on object keypoints. It leverages the predictability of local image regions from spatial neighborhoods to identify salient regions that correspond to object parts, which are then converted to keypoints. Unlike prior approaches, it utilizes predictability to discover object keypoints, an intrinsic property of objects. This ensures that it does not overly bias keypoints to focus on characteristics that are not unique to objects, such as movement, shape, colour etc.  We demonstrate the efficacy of PermaKey on Atari where it learns keypoints corresponding to the most salient object parts and is robust to certain visual distractors. Further, on downstream RL tasks in the Atari domain we demonstrate how agents equipped with our keypoints outperform those using competing alternatives, even on challenging environments with moving backgrounds or distractor objects.
      </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose PermaKey, a novel method for learning object keypoint representations that leverages local predictability as a measure of objectness.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="JiYq3eqTKY" data-number="3171">
      <h4>
        <a href="https://openreview.net/forum?id=JiYq3eqTKY">
            On Statistical Bias In Active Learning: How and When to Fix It
        </a>
      
        
          <a href="https://openreview.net/pdf?id=JiYq3eqTKY" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sebastian_Farquhar1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sebastian_Farquhar1">Sebastian Farquhar</a>, <a href="https://openreview.net/profile?id=~Yarin_Gal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yarin_Gal1">Yarin Gal</a>, <a href="https://openreview.net/profile?id=~Tom_Rainforth1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tom_Rainforth1">Tom Rainforth</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#JiYq3eqTKY-details-516" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="JiYq3eqTKY-details-516"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Active Learning, Monte Carlo, Risk Estimation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Active learning is a powerful tool when labelling data is expensive, but it introduces a bias because the training data no longer follows the population distribution. We formalize this bias and investigate the situations in which it can be harmful and sometimes even helpful. We further introduce novel corrective weights to remove bias when doing so is beneficial. Through this, our work not only provides a useful mechanism that can improve the active learning approach, but also an explanation for the empirical successes of various existing approaches which ignore this bias. In particular, we show that this bias can be actively helpful when training overparameterized models---like neural networks---with relatively modest dataset sizes.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We formalize the bias introduced by active learning and investigate the situations in which it can be harmful and sometimes even helpful, further introducing novel corrective weights to remove it when doing so is beneficial.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="0N8jUH4JMv6" data-number="1468">
      <h4>
        <a href="https://openreview.net/forum?id=0N8jUH4JMv6">
            Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time
        </a>
      
        
          <a href="https://openreview.net/pdf?id=0N8jUH4JMv6" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Tolga_Ergen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tolga_Ergen1">Tolga Ergen</a>, <a href="https://openreview.net/profile?id=~Mert_Pilanci3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mert_Pilanci3">Mert Pilanci</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#0N8jUH4JMv6-details-44" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0N8jUH4JMv6-details-44"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Convex optimization, non-convex optimization, group sparsity, <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="44" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>1</mn></msub></math></mjx-assistive-mml></mjx-container> norm, convex duality, polynomial time, deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study training of Convolutional Neural Networks (CNNs) with ReLU activations and introduce exact convex optimization formulations with a polynomial complexity with respect to the number of data samples, the number of neurons, and data dimension. More specifically, we develop a convex analytic framework utilizing semi-infinite duality to obtain equivalent convex optimization problems for several two- and three-layer CNN architectures. We first prove that two-layer CNNs can be globally optimized via an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="45" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> norm regularized convex program. We then show that multi-layer circular CNN training problems with a single ReLU layer are equivalent to an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="46" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>1</mn></msub></math></mjx-assistive-mml></mjx-container> regularized convex program that encourages sparsity in the spectral domain. We also extend these results to three-layer CNNs with two ReLU layers. Furthermore, we present extensions of our approach to different pooling methods, which elucidates the implicit architectural bias as convex regularizers.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We study the training problem for various CNN architectures with ReLU activations and introduce equivalent finite dimensional convex formulations that can be used to globally optimize these architectures.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=0N8jUH4JMv6&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Tp7kI90Htd" data-number="146">
      <h4>
        <a href="https://openreview.net/forum?id=Tp7kI90Htd">
            Generalization in data-driven models of primary visual cortex
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Tp7kI90Htd" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Konstantin-Klemens_Lurz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Konstantin-Klemens_Lurz1">Konstantin-Klemens Lurz</a>, <a href="https://openreview.net/profile?email=mohammad.bashiri%40uni-tuebingen.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="mohammad.bashiri@uni-tuebingen.de">Mohammad Bashiri</a>, <a href="https://openreview.net/profile?email=konstantin-friedrich.willeke%40uni-tuebingen.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="konstantin-friedrich.willeke@uni-tuebingen.de">Konstantin Willeke</a>, <a href="https://openreview.net/profile?email=akshay-kumar.jagadish%40student.uni-tuebingen.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="akshay-kumar.jagadish@student.uni-tuebingen.de">Akshay Jagadish</a>, <a href="https://openreview.net/profile?email=eric.wang2%40bcm.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="eric.wang2@bcm.edu">Eric Wang</a>, <a href="https://openreview.net/profile?id=~Edgar_Y._Walker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Edgar_Y._Walker1">Edgar Y. Walker</a>, <a href="https://openreview.net/profile?id=~Santiago_A_Cadena1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Santiago_A_Cadena1">Santiago A Cadena</a>, <a href="https://openreview.net/profile?email=taliah.muhammad%40bcm.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="taliah.muhammad@bcm.edu">Taliah Muhammad</a>, <a href="https://openreview.net/profile?id=~Erick_Cobos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Erick_Cobos1">Erick Cobos</a>, <a href="https://openreview.net/profile?id=~Andreas_S._Tolias1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andreas_S._Tolias1">Andreas S. Tolias</a>, <a href="https://openreview.net/profile?id=~Alexander_S_Ecker1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alexander_S_Ecker1">Alexander S Ecker</a>, <a href="https://openreview.net/profile?id=~Fabian_H._Sinz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fabian_H._Sinz1">Fabian H. Sinz</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Tp7kI90Htd-details-681" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Tp7kI90Htd-details-681"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">neuroscience, cognitive science, multitask learning, transfer learning, representation learning, network architecture, computational biology, visual perception</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep neural networks (DNN) have set new standards at predicting responses of neural populations to visual input.  Most such DNNs consist of a convolutional network (core) shared across all neurons which learns a representation of neural computation in visual cortex and a neuron-specific readout that linearly combines the relevant features in this representation. The goal of this paper is to test whether such a representation is indeed generally characteristic for visual cortex, i.e. generalizes between animals of a species, and what factors contribute to obtaining such a generalizing core. To push all non-linear computations into the core where the generalizing cortical features should be learned, we devise a novel readout that reduces the number of parameters per neuron in the readout by up to two orders of magnitude compared to the previous state-of-the-art. It does so by taking advantage of retinotopy and learns a Gaussian distribution over the neuron’s receptive field position.  With this new readout we train our network on neural responses from mouse primary visual cortex (V1) and obtain a gain in performance of 7% compared to the previous state-of-the-art network.  We then investigate whether the convolutional core indeed captures general cortical features by using the core in transfer learning to a different animal.  When transferring a core trained on thousands of neurons from various animals and scans we exceed the performance of training directly on that animal by 12%, and outperform a commonly used VGG16 core pre-trained on imagenet by 33%. In addition, transfer learning with our data-driven core is more data-efficient than direct training, achieving the same performance with only 40% of the data. Our model with its novel readout thus sets a new state-of-the-art for neural response prediction in mouse visual cortex from natural images, generalizes between animals, and captures better characteristic cortical features than current task-driven pre-training approaches such as VGG16.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a novel network architecture which sets a new state of the art at predicting neural responses to visual input and successfully learns generalizing features of mouse visual cortex (V1).</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Tp7kI90Htd&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="YmqAnY0CMEy" data-number="1327">
      <h4>
        <a href="https://openreview.net/forum?id=YmqAnY0CMEy">
            Mathematical Reasoning via Self-supervised Skip-tree Training
        </a>
      
        
          <a href="https://openreview.net/pdf?id=YmqAnY0CMEy" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Markus_Norman_Rabe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Markus_Norman_Rabe1">Markus Norman Rabe</a>, <a href="https://openreview.net/profile?id=~Dennis_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dennis_Lee1">Dennis Lee</a>, <a href="https://openreview.net/profile?id=~Kshitij_Bansal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kshitij_Bansal1">Kshitij Bansal</a>, <a href="https://openreview.net/profile?id=~Christian_Szegedy1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Christian_Szegedy1">Christian Szegedy</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#YmqAnY0CMEy-details-851" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="YmqAnY0CMEy-details-851"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">self-supervised learning, mathematics, reasoning, theorem proving, language modeling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We demonstrate that self-supervised language modeling applied to mathematical formulas enables logical reasoning. To measure the logical reasoning abilities of language models, we formulate several evaluation (downstream) tasks, such as inferring types, suggesting missing assumptions and completing equalities. For training language models for formal mathematics, we propose a novel skip-tree task. We find that models trained on the skip-tree task show surprisingly strong mathematical reasoning abilities, and outperform models trained on standard skip-sequence tasks. We also analyze the models' ability to formulate new conjectures by measuring how often the predictions are provable and useful in other proofs.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We demonstrate that self-supervised language modeling applied to mathematical formulas enables logical reasoning.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="rcQdycl0zyk" data-number="1018">
      <h4>
        <a href="https://openreview.net/forum?id=rcQdycl0zyk">
            Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="47" style="font-size: 121.3%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mrow><mo>/</mo></mrow><mi>n</mi></math></mjx-assistive-mml></mjx-container> Parameters
        </a>
      
        
          <a href="https://openreview.net/pdf?id=rcQdycl0zyk" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Aston_Zhang2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aston_Zhang2">Aston Zhang</a>, <a href="https://openreview.net/profile?id=~Yi_Tay1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yi_Tay1">Yi Tay</a>, <a href="https://openreview.net/profile?id=~SHUAI_Zhang5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~SHUAI_Zhang5">SHUAI Zhang</a>, <a href="https://openreview.net/profile?id=~Alvin_Chan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alvin_Chan1">Alvin Chan</a>, <a href="https://openreview.net/profile?id=~Anh_Tuan_Luu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anh_Tuan_Luu2">Anh Tuan Luu</a>, <a href="https://openreview.net/profile?id=~Siu_Hui1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Siu_Hui1">Siu Hui</a>, <a href="https://openreview.net/profile?id=~Jie_Fu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jie_Fu2">Jie Fu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 30 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#rcQdycl0zyk-details-537" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rcQdycl0zyk-details-537"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">hypercomplex representation learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent works have demonstrated reasonable success of representation learning in hypercomplex space. Specifically, “fully-connected layers with quaternions” (quaternions are 4D hypercomplex numbers), which replace real-valued matrix multiplications in fully-connected layers with Hamilton products of quaternions, both enjoy parameter savings with only 1/4 learnable parameters and achieve comparable performance in various applications. However, one key caveat is that hypercomplex space only exists at very few predefined dimensions (4D, 8D, and 16D). This restricts the flexibility of models that leverage hypercomplex multiplications. To this end, we propose parameterizing hypercomplex multiplications, allowing models to learn multiplication rules from data regardless of whether such rules are predefined. As a result, our method not only subsumes the Hamilton product, but also learns to operate on any arbitrary <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="48" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container>D hypercomplex space, providing more architectural flexibility using arbitrarily <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="49" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mrow><mo>/</mo></mrow><mi>n</mi></math></mjx-assistive-mml></mjx-container> learnable parameters compared with the fully-connected layer counterpart. Experiments of applications to the LSTM and transformer models on natural language inference, machine translation, text style transfer, and subject verb agreement demonstrate architectural flexibility and effectiveness of the proposed approach.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose to parameterize hypercomplex multiplications using arbitrarily <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="50" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mrow><mo>/</mo></mrow><mi>n</mi></math></mjx-assistive-mml></mjx-container> learnable parameters compared with the fully-connected layer counterpart.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="QYjO70ACDK" data-number="2506">
      <h4>
        <a href="https://openreview.net/forum?id=QYjO70ACDK">
            Distributional Sliced-Wasserstein and Applications to Generative Modeling
        </a>
      
        
          <a href="https://openreview.net/pdf?id=QYjO70ACDK" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Khai_Nguyen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Khai_Nguyen1">Khai Nguyen</a>, <a href="https://openreview.net/profile?id=~Nhat_Ho1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nhat_Ho1">Nhat Ho</a>, <a href="https://openreview.net/profile?email=v.tungph4%40vinai.io" class="profile-link" data-toggle="tooltip" data-placement="top" title="v.tungph4@vinai.io">Tung Pham</a>, <a href="https://openreview.net/profile?id=~Hung_Bui1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hung_Bui1">Hung Bui</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Feb 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#QYjO70ACDK-details-163" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QYjO70ACDK-details-163"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Deep generative models, Sliced Wasserstein, Optimal Transport</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Sliced-Wasserstein distance (SW) and its variant, Max Sliced-Wasserstein distance (Max-SW), have been used widely in the recent years due to their fast computation and scalability even when the probability measures lie in a very high dimensional space. However, SW requires many unnecessary projection samples to approximate its value while Max-SW only uses the most important projection, which ignores the information of other useful directions. In order to account for these weaknesses, we propose a novel distance, named Distributional Sliced-Wasserstein distance (DSW), that finds an optimal distribution over projections that can balance between exploring distinctive projecting directions and the informativeness of projections themselves. We show that the DSW is a generalization of Max-SW, and it can be computed efficiently by searching for the optimal push-forward measure over a set of probability measures over the unit sphere satisfying certain regularizing constraints that favor distinct directions. Finally, we conduct extensive experiments with large-scale datasets to demonstrate the favorable performances of the proposed distances over the previous sliced-based distances in generative modeling applications.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A new optimal transport distance based on slicing approach and its applications to generative modeling.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=QYjO70ACDK&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9EsrXMzlFQY" data-number="874">
      <h4>
        <a href="https://openreview.net/forum?id=9EsrXMzlFQY">
            Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9EsrXMzlFQY" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yu_Sun11" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Sun11">Yu Sun</a>, <a href="https://openreview.net/profile?email=jiaming.liu%40wustl.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jiaming.liu@wustl.edu">Jiaming Liu</a>, <a href="https://openreview.net/profile?email=yiran.s%40wustl.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="yiran.s@wustl.edu">Yiran Sun</a>, <a href="https://openreview.net/profile?id=~Brendt_Wohlberg2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Brendt_Wohlberg2">Brendt Wohlberg</a>, <a href="https://openreview.net/profile?id=~Ulugbek_Kamilov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ulugbek_Kamilov1">Ulugbek Kamilov</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 11 Feb 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9EsrXMzlFQY-details-498" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9EsrXMzlFQY-details-498"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Regularization by denoising, Computational imaging, asynchronous parallel algorithm, Deep denoising priors</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new{asynchronous RED (Async-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of Async-RED is further reduced by using a random subset of measurements at every iteration. We present a complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate Async-RED on image recovery using pre-trained deep denoisers as priors.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Our work develops a novel deep-regularized asynchronous parallel method with provable convergence guarantees for solving large-scale inverse problems.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=9EsrXMzlFQY&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="eMP1j9efXtX" data-number="1325">
      <h4>
        <a href="https://openreview.net/forum?id=eMP1j9efXtX">
            DeepAveragers: Offline Reinforcement Learning By Solving Derived Non-Parametric MDPs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=eMP1j9efXtX" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Aayam_Kumar_Shrestha1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aayam_Kumar_Shrestha1">Aayam Kumar Shrestha</a>, <a href="https://openreview.net/profile?id=~Stefan_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stefan_Lee1">Stefan Lee</a>, <a href="https://openreview.net/profile?id=~Prasad_Tadepalli1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Prasad_Tadepalli1">Prasad Tadepalli</a>, <a href="https://openreview.net/profile?id=~Alan_Fern1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alan_Fern1">Alan Fern</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#eMP1j9efXtX-details-115" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eMP1j9efXtX-details-115"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Offline Reinforcement Learning, Planning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We study an approach to offline reinforcement learning (RL) based on optimally solving  finitely-represented  MDPs  derived  from  a  static  dataset  of  experience. This approach can be applied on top of any learned representation and has the potential to easily support multiple solution objectives as well as zero-shot adjustment to changing environments and goals.  Our main contribution is to introduce the Deep Averagers with Costs MDP (DAC-MDP) and to investigate its solutions for offline RL.  DAC-MDPs are a non-parametric model that can leverage deep representations and account for limited data by introducing costs for exploiting under-represented parts of the model.  In theory, we show conditions that allow for lower-bounding the performance of DAC-MDP solutions. We also investigate the empirical behavior in a number of environments, including those with image-based observations. Overall, the experiments demonstrate that the framework can work in practice and scale to large complex offline RL problems.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">The paper introduces and investigates an offline RL approach based on optimally solving a finite-state MDP that is derived from the experience dataset using any latent state representation. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=eMP1j9efXtX&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="1YLJDvSx6J4" data-number="1954">
      <h4>
        <a href="https://openreview.net/forum?id=1YLJDvSx6J4">
            Learning from Protein Structure with Geometric Vector Perceptrons
        </a>
      
        
          <a href="https://openreview.net/pdf?id=1YLJDvSx6J4" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Bowen_Jing1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bowen_Jing1">Bowen Jing</a>, <a href="https://openreview.net/profile?id=~Stephan_Eismann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stephan_Eismann1">Stephan Eismann</a>, <a href="https://openreview.net/profile?email=psuriana%40stanford.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="psuriana@stanford.edu">Patricia Suriana</a>, <a href="https://openreview.net/profile?id=~Raphael_John_Lamarre_Townshend1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Raphael_John_Lamarre_Townshend1">Raphael John Lamarre Townshend</a>, <a href="https://openreview.net/profile?id=~Ron_Dror1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ron_Dror1">Ron Dror</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#1YLJDvSx6J4-details-507" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="1YLJDvSx6J4-details-507"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">structural biology, graph neural networks, proteins, geometric deep learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Learning on 3D structures of large biomolecules is emerging as a distinct area in machine learning, but there has yet to emerge a unifying network architecture that simultaneously leverages the geometric and relational aspects of the problem domain. To address this gap, we introduce geometric vector perceptrons, which extend standard dense layers to operate on collections of Euclidean vectors. Graph neural networks equipped with such layers are able to perform both geometric and relational reasoning on efficient representations of macromolecules. We demonstrate our approach on two important problems in learning from protein structure: model quality assessment and computational protein design. Our approach improves over existing classes of architectures on both problems, including state-of-the-art convolutional neural networks and graph neural networks. We release our code at https://github.com/drorlab/gvp.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a novel graph neural network layer to learn from the structure of macromolecules.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="zrT3HcsWSAt" data-number="2279">
      <h4>
        <a href="https://openreview.net/forum?id=zrT3HcsWSAt">
            Behavioral Cloning from Noisy Demonstrations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=zrT3HcsWSAt" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Fumihiro_Sasaki2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Fumihiro_Sasaki2">Fumihiro Sasaki</a>, <a href="https://openreview.net/profile?email=ryohta.yamashina%40jp.ricoh.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="ryohta.yamashina@jp.ricoh.com">Ryota Yamashina</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#zrT3HcsWSAt-details-284" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zrT3HcsWSAt-details-284"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Imitation Learning, Inverse Reinforcement Learning, Noisy Demonstrations</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We consider the problem of learning an optimal expert behavior policy given noisy demonstrations that contain observations from both optimal and non-optimal expert behaviors. Popular imitation learning algorithms, such as generative adversarial imitation learning, assume that (clear) demonstrations are given from optimal expert policies but not the non-optimal ones, and thus often fail to imitate the optimal expert behaviors given the noisy demonstrations. Prior works that address the problem require (1) learning policies through environment interactions in the same fashion as reinforcement learning, and (2) annotating each demonstration with confidence scores or rankings. However, such environment interactions and annotations in real-world settings take impractically long training time and a significant human effort. In this paper, we propose an imitation learning algorithm to address the problem without any environment interactions and annotations associated with the non-optimal demonstrations. The proposed algorithm learns ensemble policies with a generalized behavioral cloning (BC) objective function where we exploit another policy already learned by BC. Experimental results show that the proposed algorithm can learn behavior policies that are much closer to the optimal policies than ones learned by BC.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose an imitation learning algorithm to learn from non-optimal (noisy) demonstrations without any environment interactions and annotations associated with the demonstrations.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="0zvfm-nZqQs" data-number="741">
      <h4>
        <a href="https://openreview.net/forum?id=0zvfm-nZqQs">
            Undistillable: Making A Nasty Teacher That CANNOT teach students
        </a>
      
        
          <a href="https://openreview.net/pdf?id=0zvfm-nZqQs" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Haoyu_Ma1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haoyu_Ma1">Haoyu Ma</a>, <a href="https://openreview.net/profile?id=~Tianlong_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tianlong_Chen1">Tianlong Chen</a>, <a href="https://openreview.net/profile?id=~Ting-Kuei_Hu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ting-Kuei_Hu1">Ting-Kuei Hu</a>, <a href="https://openreview.net/profile?id=~Chenyu_You1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chenyu_You1">Chenyu You</a>, <a href="https://openreview.net/profile?id=~Xiaohui_Xie2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaohui_Xie2">Xiaohui Xie</a>, <a href="https://openreview.net/profile?id=~Zhangyang_Wang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhangyang_Wang1">Zhangyang Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 13 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#0zvfm-nZqQs-details-639" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="0zvfm-nZqQs-details-639"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">knowledge distillation, avoid knowledge leaking</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Knowledge Distillation (KD) is a widely used technique to transfer knowledge from pre-trained teacher models to  (usually more lightweight) student models. However, in certain situations, this technique is more of a curse than a blessing. For instance, KD poses a potential risk of exposing intellectual properties (IPs): even if a trained machine learning model is released in ``black boxes'' (e.g., as executable software or APIs without open-sourcing code), it can still be replicated by KD through imitating input-output behaviors. To prevent this unwanted effect of KD, this paper introduces and investigates a concept called <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="51" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D466 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D447 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D450 TEX-I"></mjx-c><mjx-c class="mjx-c210E TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">Nasty Teacher</mtext></math></mjx-assistive-mml></mjx-container>: a specially trained teacher network that yields nearly the same performance as a normal one, but would significantly degrade the performance of student models learned by imitating it. We propose a simple yet effective algorithm to build the nasty teacher, called <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="52" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D453 TEX-I"></mjx-c><mjx-c class="mjx-c2D TEX-MI"></mjx-c><mjx-c class="mjx-c1D462 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D45F TEX-I"></mjx-c><mjx-c class="mjx-c1D45A TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D454 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D458 TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D464 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c><mjx-c class="mjx-c1D454 TEX-I"></mjx-c><mjx-c class="mjx-c1D452 TEX-I"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c1D451 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D460 TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D459 TEX-I"></mjx-c><mjx-c class="mjx-c1D44E TEX-I"></mjx-c><mjx-c class="mjx-c1D461 TEX-I"></mjx-c><mjx-c class="mjx-c1D456 TEX-I"></mjx-c><mjx-c class="mjx-c1D45C TEX-I"></mjx-c><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mtext></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathvariant="italic">self-undermining knowledge distillation</mtext></math></mjx-assistive-mml></mjx-container>. Specifically, we aim to maximize the difference between the output of the nasty teacher and a normal pre-trained network. Extensive experiments on several datasets demonstrate that our method is effective on both standard KD and data-free KD, providing the desirable KD-immunity to model owners for the first time. We hope our preliminary study can draw more awareness and interest in this new practical problem of both social and legal importance. Our codes and pre-trained models can be found at: <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="53" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mtext class="mjx-n" style="color: red;"><mjx-c class="mjx-c5C"></mjx-c><mjx-c class="mjx-c75"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c6C"></mjx-c></mjx-mtext><mjx-texatom texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D454 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D462 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44F TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c2E"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="2"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D434 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D43A TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45C TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D462 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D460 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D461 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D466 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D447 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D450 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c210E TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D452 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45F TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext mathcolor="red">\url</mtext><mrow><mi>h</mi><mi>t</mi><mi>t</mi><mi>p</mi><mi>s</mi><mo>:</mo><mrow><mo>/</mo></mrow><mrow><mo>/</mo></mrow><mi>g</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>u</mi><mi>b</mi><mo>.</mo><mi>c</mi><mi>o</mi><mi>m</mi><mrow><mo>/</mo></mrow><mi>V</mi><mi>I</mi><mi>T</mi><mi>A</mi><mo>−</mo><mi>G</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>p</mi><mrow><mo>/</mo></mrow><mi>N</mi><mi>a</mi><mi>s</mi><mi>t</mi><mi>y</mi><mo>−</mo><mi>T</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>h</mi><mi>e</mi><mi>r</mi></mrow></math></mjx-assistive-mml></mjx-container>.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose the Nasty Teacher, a defensive approach to prevent unauthorized cloning from a teacher model through knowledge distillation. </span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="WiGQBFuVRv" data-number="336">
      <h4>
        <a href="https://openreview.net/forum?id=WiGQBFuVRv">
            Multivariate Probabilistic Time Series Forecasting via Conditioned Normalizing Flows
        </a>
      
        
          <a href="https://openreview.net/pdf?id=WiGQBFuVRv" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=kashif.rasul%40zalando.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="kashif.rasul@zalando.de">Kashif Rasul</a>, <a href="https://openreview.net/profile?id=~Abdul-Saboor_Sheikh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Abdul-Saboor_Sheikh1">Abdul-Saboor Sheikh</a>, <a href="https://openreview.net/profile?email=ingmar.schuster%40zalando.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="ingmar.schuster@zalando.de">Ingmar Schuster</a>, <a href="https://openreview.net/profile?id=~Urs_M_Bergmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Urs_M_Bergmann1">Urs M Bergmann</a>, <a href="https://openreview.net/profile?id=~Roland_Vollgraf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roland_Vollgraf1">Roland Vollgraf</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 30 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#WiGQBFuVRv-details-488" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="WiGQBFuVRv-details-488"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">time series, normalizing flows, attention, probabilistic multivariate forecasting</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Time series forecasting is often fundamental to scientific and engineering problems and enables decision making. With ever increasing data set sizes, a trivial solution to scale up predictions is to assume independence between interacting time series. However, modeling statistical dependencies can improve accuracy and enable analysis of interaction effects. Deep learning methods are well suited for this problem, but multi-variate models often assume a simple parametric distribution and do not scale to high dimensions. In this work we model the multi-variate temporal dynamics of time series via an autoregressive deep learning model, where the data distribution  is represented by a conditioned normalizing flow. This combination retains the power of autoregressive models, such as good performance in extrapolation into the future, with the flexibility of flows as a general purpose high-dimensional distribution model, while remaining computationally tractable. We show that it improves over the state-of-the-art for standard metrics on many real-world data sets with several thousand interacting time-series.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">SOTA Multivariate probabilistic time series forecasting using RNNs or Attention to model the dynamics and normalizing flows for the emission model.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="GY6-6sTvGaf" data-number="1429">
      <h4>
        <a href="https://openreview.net/forum?id=GY6-6sTvGaf">
            Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels
        </a>
      
        
          <a href="https://openreview.net/pdf?id=GY6-6sTvGaf" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Denis_Yarats1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Denis_Yarats1">Denis Yarats</a>, <a href="https://openreview.net/profile?id=~Ilya_Kostrikov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ilya_Kostrikov1">Ilya Kostrikov</a>, <a href="https://openreview.net/profile?id=~Rob_Fergus1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Rob_Fergus1">Rob Fergus</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 08 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#GY6-6sTvGaf-details-140" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="GY6-6sTvGaf-details-140"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training.  The approach leverages input perturbations commonly used in computer vision tasks to transform input examples, as well as regularizing the value function and policy.  Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC’s performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Hafner et al., 2019; Lee et al., 2019; Hafner et al., 2018) methods and recently proposed contrastive learning (Srinivas et al., 2020).  Our approach, which we dub DrQ: Data-regularized Q, can be combined with any model-free reinforcement learning algorithm. We further demonstrate this by applying it to DQN and significantly improve its data-efficiency on the Atari 100k benchmark.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">The first successful demonstration that image augmentation can be applied to image-based Deep RL to achieve SOTA performance.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=GY6-6sTvGaf&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="_0kaDkv3dVf" data-number="2323">
      <h4>
        <a href="https://openreview.net/forum?id=_0kaDkv3dVf">
            HW-NAS-Bench: Hardware-Aware Neural Architecture Search Benchmark
        </a>
      
        
          <a href="https://openreview.net/pdf?id=_0kaDkv3dVf" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Chaojian_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chaojian_Li1">Chaojian Li</a>, <a href="https://openreview.net/profile?id=~Zhongzhi_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhongzhi_Yu1">Zhongzhi Yu</a>, <a href="https://openreview.net/profile?id=~Yonggan_Fu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yonggan_Fu1">Yonggan Fu</a>, <a href="https://openreview.net/profile?id=~Yongan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yongan_Zhang1">Yongan Zhang</a>, <a href="https://openreview.net/profile?id=~Yang_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Zhao1">Yang Zhao</a>, <a href="https://openreview.net/profile?id=~Haoran_You1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Haoran_You1">Haoran You</a>, <a href="https://openreview.net/profile?id=~Qixuan_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Qixuan_Yu1">Qixuan Yu</a>, <a href="https://openreview.net/profile?id=~Yue_Wang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yue_Wang3">Yue Wang</a>, <a href="https://openreview.net/profile?email=hc.onioncc%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="hc.onioncc@gmail.com">Cong Hao</a>, <a href="https://openreview.net/profile?id=~Yingyan_Lin1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yingyan_Lin1">Yingyan Lin</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#_0kaDkv3dVf-details-361" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_0kaDkv3dVf-details-361"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Hardware-Aware Neural Architecture Search, AutoML, Benchmark</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">HardWare-aware Neural Architecture Search (HW-NAS) has recently gained tremendous attention by automating the design of deep neural networks deployed in more resource-constrained daily life devices. Despite its promising performance, developing optimal HW-NAS solutions can be prohibitively challenging as it requires cross-disciplinary knowledge in the algorithm, micro-architecture, and device-specific compilation. First, to determine the hardware-cost to be incorporated into the NAS process, existing works mostly adopt either pre-collected hardware-cost look-up tables or device-specific hardware-cost models. The former can be time-consuming due to the required knowledge of the device’s compilation method and how to set up the measurement pipeline, while building the latter is often a barrier for non-hardware experts like NAS researchers. Both of them limit the development of HW-NAS innovations and impose a barrier-to-entry to non-hardware experts. Second, similar to generic NAS, it can be notoriously difficult to benchmark HW-NAS algorithms due to their significant required computational resources and the differences in adopted search spaces, hyperparameters, and hardware devices. To this end, we develop HW-NAS-Bench, the first public dataset for HW-NAS research which aims to democratize HW-NAS research to non-hardware experts and make HW-NAS research more reproducible and accessible. To design HW-NAS-Bench, we carefully collected the measured/estimated hardware performance (e.g., energy cost and latency) of all the networks in the search spaces of both NAS-Bench-201 and FBNet, on six hardware devices that fall into three categories (i.e., commercial edge devices, FPGA, and ASIC). Furthermore, we provide a comprehensive analysis of the collected measurements in HW-NAS-Bench to provide insights for HW-NAS research. Finally, we demonstrate exemplary user cases to (1) show that HW-NAS-Bench allows non-hardware experts to perform HW-NAS by simply querying our pre-measured dataset and (2) verify that dedicated device-specific HW-NAS can indeed lead to optimal accuracy-cost trade-offs. The codes and all collected data are available at https://github.com/RICE-EIC/HW-NAS-Bench.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A Hardware-Aware Neural Architecture Search Benchmark</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="q3KSThy2GwB" data-number="3731">
      <h4>
        <a href="https://openreview.net/forum?id=q3KSThy2GwB">
            Practical Real Time Recurrent Learning with a Sparse Approximation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=q3KSThy2GwB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jacob_Menick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jacob_Menick1">Jacob Menick</a>, <a href="https://openreview.net/profile?id=~Erich_Elsen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Erich_Elsen1">Erich Elsen</a>, <a href="https://openreview.net/profile?id=~Utku_Evci1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Utku_Evci1">Utku Evci</a>, <a href="https://openreview.net/profile?id=~Simon_Osindero1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Simon_Osindero1">Simon Osindero</a>, <a href="https://openreview.net/profile?id=~Karen_Simonyan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Karen_Simonyan1">Karen Simonyan</a>, <a href="https://openreview.net/profile?id=~Alex_Graves1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_Graves1">Alex Graves</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#q3KSThy2GwB-details-578" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="q3KSThy2GwB-details-578"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">recurrent neural networks, backpropagation, biologically plausible, forward mode, real time recurrent learning, rtrl, bptt</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recurrent neural networks are usually trained with backpropagation through time, which requires storing a complete history of network states, and prohibits updating the weights "online" (after every timestep). Real Time Recurrent Learning (RTRL) eliminates the need for history storage and allows for online weight updates, but does so at the expense of computational costs that are quartic in the state size. This renders RTRL training intractable for all but the smallest networks, even ones that are made highly sparse.
      We introduce the Sparse n-step Approximation (SnAp) to the RTRL influence matrix. SnAp only tracks the influence of a parameter on hidden units that are reached by the computation graph within <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="54" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi></math></mjx-assistive-mml></mjx-container> timesteps of the recurrent core. SnAp with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="55" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>=</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container> is no more expensive than backpropagation but allows training on arbitrarily long sequences. We find that it substantially outperforms other RTRL approximations with comparable costs such as Unbiased Online Recurrent Optimization. For highly sparse networks, SnAp with <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="56" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>n</mi><mo>=</mo><mn>2</mn></math></mjx-assistive-mml></mjx-container> remains tractable and can outperform backpropagation through time in terms of learning speed when updates are done online.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show how to make RTRL efficient with sparse RNNs, a sparse approximation, or both.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="QtTKTdVrFBB" data-number="1925">
      <h4>
        <a href="https://openreview.net/forum?id=QtTKTdVrFBB">
            Random Feature Attention
        </a>
      
        
          <a href="https://openreview.net/pdf?id=QtTKTdVrFBB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Hao_Peng4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Hao_Peng4">Hao Peng</a>, <a href="https://openreview.net/profile?id=~Nikolaos_Pappas1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Nikolaos_Pappas1">Nikolaos Pappas</a>, <a href="https://openreview.net/profile?id=~Dani_Yogatama2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dani_Yogatama2">Dani Yogatama</a>, <a href="https://openreview.net/profile?id=~Roy_Schwartz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Roy_Schwartz1">Roy Schwartz</a>, <a href="https://openreview.net/profile?id=~Noah_Smith1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Noah_Smith1">Noah Smith</a>, <a href="https://openreview.net/profile?id=~Lingpeng_Kong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lingpeng_Kong1">Lingpeng Kong</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 25 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#QtTKTdVrFBB-details-570" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QtTKTdVrFBB-details-570"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Attention, transformers, machine translation, language modeling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep. While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length. We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers. RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism. Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines. In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets. Our analysis shows that RFA’s efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a random-feature-based attention that scales linearly in sequence length, and performs on par with strong transformer baselines on language modeling and machine translation.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=QtTKTdVrFBB&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="rumv7QmLUue" data-number="1732">
      <h4>
        <a href="https://openreview.net/forum?id=rumv7QmLUue">
            A Gradient Flow Framework For Analyzing Network Pruning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=rumv7QmLUue" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Ekdeep_Singh_Lubana1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ekdeep_Singh_Lubana1">Ekdeep Singh Lubana</a>, <a href="https://openreview.net/profile?id=~Robert_Dick1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Robert_Dick1">Robert Dick</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#rumv7QmLUue-details-540" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="rumv7QmLUue-details-540"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Network pruning, Gradient flow, Early pruning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent network pruning methods focus on pruning models early-on in training. To estimate the impact of removing a parameter, these methods use importance measures that were originally designed to prune trained models. Despite lacking justification for their use early-on in training, such measures result in surprisingly low accuracy loss. To better explain this behavior, we develop a general framework that uses gradient flow to unify state-of-the-art importance measures through the norm of model parameters. We use this framework to determine the relationship between pruning measures and evolution of model parameters, establishing several results related to pruning models early-on in training: (i) magnitude-based pruning removes parameters that contribute least to reduction in loss, resulting in models that converge faster than magnitude-agnostic methods; (ii) loss-preservation based pruning preserves first-order model evolution dynamics and its use is therefore justified for pruning minimally trained models; and (iii) gradient-norm based pruning affects second-order model evolution dynamics, such that increasing gradient norm via pruning can produce poorly performing models. We validate our claims on several VGG-13, MobileNet-V1, and ResNet-56 models trained on CIFAR-10/CIFAR-100.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper establishes the relationship between regularly used importance measures for network pruning and evolution of model parameters under gradient flow, thus providing useful insights into pruning early-on in training.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=rumv7QmLUue&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="mLcmdlEUxy-" data-number="2010">
      <h4>
        <a href="https://openreview.net/forum?id=mLcmdlEUxy-">
            Recurrent Independent Mechanisms
        </a>
      
        
          <a href="https://openreview.net/pdf?id=mLcmdlEUxy-" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Anirudh_Goyal1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anirudh_Goyal1">Anirudh Goyal</a>, <a href="https://openreview.net/profile?id=~Alex_Lamb1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Alex_Lamb1">Alex Lamb</a>, <a href="https://openreview.net/profile?id=~Jordan_Hoffmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jordan_Hoffmann1">Jordan Hoffmann</a>, <a href="https://openreview.net/profile?id=~Shagun_Sodhani1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shagun_Sodhani1">Shagun Sodhani</a>, <a href="https://openreview.net/profile?id=~Sergey_Levine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sergey_Levine1">Sergey Levine</a>, <a href="https://openreview.net/profile?id=~Yoshua_Bengio1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yoshua_Bengio1">Yoshua Bengio</a>, <a href="https://openreview.net/profile?id=~Bernhard_Sch%C3%B6lkopf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bernhard_Schölkopf1">Bernhard Schölkopf</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#mLcmdlEUxy--details-740" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="mLcmdlEUxy--details-740"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">modular representations, better generalization, learning mechanisms</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We explore the hypothesis that learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes that only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and compete with each other so they are updated only at time steps where they are most relevant.  We show that this leads to specialization amongst the RIMs, which in turn allows for remarkably improved generalization on tasks where some factors of variation differ systematically between training and evaluation.
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Learning recurrent mechanisms which operate independently, and sparingly interact  can lead to better generalization to out of distribution samples.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=mLcmdlEUxy-&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="XJk19XzGq2J" data-number="2825">
      <h4>
        <a href="https://openreview.net/forum?id=XJk19XzGq2J">
            The Intrinsic Dimension of Images and Its Impact on Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=XJk19XzGq2J" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Phil_Pope1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Phil_Pope1">Phil Pope</a>, <a href="https://openreview.net/profile?id=~Chen_Zhu2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chen_Zhu2">Chen Zhu</a>, <a href="https://openreview.net/profile?id=~Ahmed_Abdelkader1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ahmed_Abdelkader1">Ahmed Abdelkader</a>, <a href="https://openreview.net/profile?id=~Micah_Goldblum1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Micah_Goldblum1">Micah Goldblum</a>, <a href="https://openreview.net/profile?id=~Tom_Goldstein1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tom_Goldstein1">Tom Goldstein</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#XJk19XzGq2J-details-540" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="XJk19XzGq2J-details-540"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">generalization, dimension, manifold, ImageNet, CIFAR</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">It is widely believed that natural image data exhibits low-dimensional structure despite the high dimensionality of conventional pixel representations.  This idea underlies a common intuition for the remarkable success of deep learning in computer vision. In this work, we apply dimension estimation tools to popular datasets and investigate the role of low-dimensional structure in deep learning.  We find that common natural image datasets indeed have very low intrinsic dimension relative to the high number of pixels in the images.  Additionally, we find that low dimensional datasets are easier for neural networks to learn, and models solving these tasks generalize better from training to test data.   Along the way,  we develop a technique for validating our dimension estimation tools on synthetic data generated by GANs allowing us to actively manipulate the intrinsic dimension by controlling the image generation process. Code for our experiments may be found  \href{https://github.com/ppope/dimensions}{here}.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We measure the dimensionality of common used datasets, and experimentally investigate whether the links between dimensionality and learning that have been identified in the manifold learning literature describe the behaviors of deep neural networks.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="eNdiU_DbM9" data-number="94">
      <h4>
        <a href="https://openreview.net/forum?id=eNdiU_DbM9">
            Uncertainty Sets for Image Classifiers using Conformal Prediction
        </a>
      
        
          <a href="https://openreview.net/pdf?id=eNdiU_DbM9" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Anastasios_Nikolas_Angelopoulos1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Anastasios_Nikolas_Angelopoulos1">Anastasios Nikolas Angelopoulos</a>, <a href="https://openreview.net/profile?email=stephenbates%40eecs.berkeley.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="stephenbates@eecs.berkeley.edu">Stephen Bates</a>, <a href="https://openreview.net/profile?id=~Michael_Jordan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_Jordan1">Michael Jordan</a>, <a href="https://openreview.net/profile?id=~Jitendra_Malik2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jitendra_Malik2">Jitendra Malik</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 16 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#eNdiU_DbM9-details-548" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="eNdiU_DbM9-details-548"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">classification, predictive uncertainty, conformal inference, computer vision, imagenet</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Convolutional image classifiers can achieve high predictive accuracy, but quantifying their uncertainty remains an unresolved challenge, hindering their deployment in consequential settings. Existing uncertainty quantification techniques, such as Platt scaling, attempt to calibrate the network’s probability estimates, but they do not have formal guarantees. We present an algorithm that modifies any classifier to output a predictive set containing the true label with a user-specified probability, such as 90%. The algorithm is simple and fast like Platt scaling, but provides a formal finite-sample coverage guarantee for every model and dataset. Our method modifies an existing conformal prediction algorithm to give more stable predictive sets by regularizing the small scores of unlikely classes after Platt scaling. In experiments on both Imagenet and Imagenet-V2 with ResNet-152 and other classifiers, our scheme outperforms existing approaches, achieving coverage with sets that are often factors of 5 to 10 smaller than a stand-alone Platt scaling baseline.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We quantify uncertainty for image classifiers using prediction sets, with detailed experiments on Imagenet Val and V2.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=eNdiU_DbM9&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Rhsu5qD36cL" data-number="95">
      <h4>
        <a href="https://openreview.net/forum?id=Rhsu5qD36cL">
            Sequential Density Ratio Estimation for Simultaneous Optimization of Speed and Accuracy
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Rhsu5qD36cL" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Akinori_F_Ebihara1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Akinori_F_Ebihara1">Akinori F Ebihara</a>, <a href="https://openreview.net/profile?email=miyagawataik%40nec.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="miyagawataik@nec.com">Taiki Miyagawa</a>, <a href="https://openreview.net/profile?email=k-sakurai-bq%40nec.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="k-sakurai-bq@nec.com">Kazuyuki Sakurai</a>, <a href="https://openreview.net/profile?email=h-imaoka_cb%40nec.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="h-imaoka_cb@nec.com">Hitoshi Imaoka</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 06 Feb 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>23 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Rhsu5qD36cL-details-549" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Rhsu5qD36cL-details-549"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Sequential probability ratio test, Early classification, Density ratio estimation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Classifying sequential data as early and as accurately as possible is a challenging yet critical problem, especially when a sampling cost is high. One algorithm that achieves this goal is the sequential probability ratio test (SPRT), which is known as Bayes-optimal: it can keep the expected number of data samples as small as possible, given the desired error upper-bound. However, the original SPRT makes two critical assumptions that limit its application in real-world scenarios: (i) samples are independently and identically distributed, and (ii) the likelihood of the data being derived from each class can be calculated precisely. Here, we propose the SPRT-TANDEM, a deep neural network-based SPRT algorithm that overcomes the above two obstacles. The SPRT-TANDEM sequentially estimates the log-likelihood ratio of two alternative hypotheses by leveraging a novel Loss function for Log-Likelihood Ratio estimation (LLLR) while allowing correlations up to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="57" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c2115 TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>N</mi><mo stretchy="false">(</mo><mo>∈</mo><mrow><mi mathvariant="double-struck">N</mi></mrow><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>  preceding samples. In tests on one original and two public video databases, Nosaic MNIST, UCF101, and SiW, the SPRT-TANDEM achieves statistically significantly better classification accuracy than other baseline classifiers, with a smaller number of data samples. The code and Nosaic MNIST are publicly available at https://github.com/TaikiMiyagawa/SPRT-TANDEM.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">With a novel sequential density estimation algorithm, we relax critical assumptions of the classical Sequential Probability Ratio Test to be applicable in various real-world scenarios.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Rhsu5qD36cL&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="O7ms4LFdsX" data-number="2714">
      <h4>
        <a href="https://openreview.net/forum?id=O7ms4LFdsX">
            Disentangled Recurrent Wasserstein Autoencoder 
        </a>
      
        
          <a href="https://openreview.net/pdf?id=O7ms4LFdsX" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Jun_Han4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jun_Han4">Jun Han</a>, <a href="https://openreview.net/profile?id=~Martin_Renqiang_Min1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Martin_Renqiang_Min1">Martin Renqiang Min</a>, <a href="https://openreview.net/profile?id=~Ligong_Han1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ligong_Han1">Ligong Han</a>, <a href="https://openreview.net/profile?id=~Li_Erran_Li1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Li_Erran_Li1">Li Erran Li</a>, <a href="https://openreview.net/profile?id=~Xuan_Zhang3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xuan_Zhang3">Xuan Zhang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>8 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#O7ms4LFdsX-details-584" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="O7ms4LFdsX-details-584"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Sequential  Representation Learning, Disentanglement, Recurrent Generative Model</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Learning disentangled representations leads to interpretable models and facilitates data generation with style transfer, which has been extensively studied on static data such as images in an unsupervised learning framework. However, only a few works have explored unsupervised disentangled sequential representation learning due to challenges of generating sequential data. In this paper, we propose recurrent Wasserstein Autoencoder (R-WAE), a new framework for generative modeling of sequential data. R-WAE disentangles the representation of an input sequence into static and dynamic factors (i.e., time-invariant and time-varying parts). Our theoretical analysis shows that, R-WAE minimizes an upper bound of a penalized form of the Wasserstein distance between model distribution and sequential data distribution, and simultaneously maximizes the mutual information between input data and different disentangled latent factors, respectively. This is superior to (recurrent) VAE which does not explicitly enforce mutual information maximization between input data and disentangled latent representations. When the number of actions in sequential data is available as weak supervision information, R-WAE is extended to learn a categorical latent representation of actions to improve its disentanglement. Experiments on a variety of datasets show that our models outperform other baselines with the same settings in terms of disentanglement and unconditional video generation both quantitatively and qualitatively.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose the first recurrent Wasserstein Autoencoder for learning disentangled representations of sequential data with theoretical analysis.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=O7ms4LFdsX&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="EGdFhBzmAwB" data-number="3178">
      <h4>
        <a href="https://openreview.net/forum?id=EGdFhBzmAwB">
            Generalization bounds via distillation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=EGdFhBzmAwB" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Daniel_Hsu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Hsu1">Daniel Hsu</a>, <a href="https://openreview.net/profile?id=~Ziwei_Ji1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ziwei_Ji1">Ziwei Ji</a>, <a href="https://openreview.net/profile?id=~Matus_Telgarsky1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matus_Telgarsky1">Matus Telgarsky</a>, <a href="https://openreview.net/profile?id=~Lan_Wang4" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lan_Wang4">Lan Wang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 01 Apr 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>14 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#EGdFhBzmAwB-details-952" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="EGdFhBzmAwB-details-952"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Generalization, statistical learning theory, theory, distillation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">This paper theoretically investigates the following empirical phenomenon: given a high-complexity network with poor generalization bounds, one can distill it into a network with nearly identical predictions but low complexity and vastly smaller generalization bounds.  The main contribution is an analysis showing that the original network inherits this good generalization bound from its distillation, assuming the use of well-behaved data augmentation.  This bound is presented both in an abstract and in a concrete form, the latter complemented by a reduction technique to handle modern computation graphs featuring convolutional layers, fully-connected layers, and skip connections, to name a few.  To round out the story, a (looser) classical uniform convergence analysis of compression is also presented, as well as a variety of experiments on cifar and mnist demonstrating similar generalization performance between the original network and its distillation.  
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper provides a suite of mathematical tools to bound the generalization error of networks which possess low-complexity distillations.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="SRDuJssQud" data-number="1107">
      <h4>
        <a href="https://openreview.net/forum?id=SRDuJssQud">
            Neural Approximate Sufficient Statistics for Implicit Models
        </a>
      
        
          <a href="https://openreview.net/pdf?id=SRDuJssQud" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yanzhi_Chen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yanzhi_Chen1">Yanzhi Chen</a>, <a href="https://openreview.net/profile?id=~Dinghuai_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dinghuai_Zhang1">Dinghuai Zhang</a>, <a href="https://openreview.net/profile?id=~Michael_U._Gutmann1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_U._Gutmann1">Michael U. Gutmann</a>, <a href="https://openreview.net/profile?id=~Aaron_Courville3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aaron_Courville3">Aaron Courville</a>, <a href="https://openreview.net/profile?id=~Zhanxing_Zhu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Zhanxing_Zhu1">Zhanxing Zhu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>20 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#SRDuJssQud-details-370" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="SRDuJssQud-details-370"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">likelihood-free inference, bayesian inference, mutual information, representation learning, summary statistics</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We consider the fundamental problem of how to automatically construct summary statistics for implicit generative models where the evaluation of the likelihood function is intractable but sampling data from the model is possible. The idea is to frame the task of constructing sufficient statistics as learning mutual information maximizing representations of the data with the help of deep neural networks. The infomax learning procedure does not need to estimate any density or density ratio. We apply our approach to both traditional approximate Bayesian computation and recent neural likelihood methods, boosting their performance on a range of tasks. </span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We learn low-dimensional near-sufficient statistics for implicit models by infomax principle without estimating the density or even the density ratio.</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=SRDuJssQud&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="9xC2tWEwBD" data-number="962">
      <h4>
        <a href="https://openreview.net/forum?id=9xC2tWEwBD">
            A Panda? No, It's a Sloth: Slowdown Attacks on Adaptive Multi-Exit Neural Network Inference
        </a>
      
        
          <a href="https://openreview.net/pdf?id=9xC2tWEwBD" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Sanghyun_Hong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sanghyun_Hong1">Sanghyun Hong</a>, <a href="https://openreview.net/profile?id=~Yigitcan_Kaya1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yigitcan_Kaya1">Yigitcan Kaya</a>, <a href="https://openreview.net/profile?email=modoranu.ionut.vlad%40hotmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="modoranu.ionut.vlad@hotmail.com">Ionuț-Vlad Modoranu</a>, <a href="https://openreview.net/profile?id=~Tudor_Dumitras1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Tudor_Dumitras1">Tudor Dumitras</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 26 Feb 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>13 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#9xC2tWEwBD-details-879" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="9xC2tWEwBD-details-879"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Slowdown attacks, efficient inference, input-adaptive multi-exit neural networks, adversarial examples</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent increases in the computational demands of deep neural networks (DNNs), combined with the observation that most input samples require only simple models, have sparked interest in input-adaptive multi-exit architectures, such as MSDNets or Shallow-Deep Networks. These architectures enable faster inferences and could bring DNNs to low-power devices, e.g., in the Internet of Things (IoT). However, it is unknown if the computational savings provided by this approach are robust against adversarial pressure. In particular, an adversary may aim to slowdown adaptive DNNs by increasing their average inference time—a threat analogous to the denial-of-service attacks from the Internet. In this paper, we conduct a systematic evaluation of this threat by experimenting with three generic multi-exit DNNs (based on VGG16, MobileNet, and ResNet56) and a custom multi-exit architecture, on two popular image classification benchmarks (CIFAR-10 and Tiny ImageNet). To this end, we show that adversarial example-crafting techniques can be modified to cause slowdown, and we propose a metric for comparing their impact on different architectures. We show that a slowdown attack reduces the efficacy of multi-exit DNNs by 90–100%, and it amplifies the latency by 1.5–5× in a typical IoT deployment. We also show that it is possible to craft universal, reusable perturbations and that the attack can be effective in realistic black-box scenarios, where the attacker has limited knowledge about the victim. Finally, we show that adversarial training provides limited protection against slowdowns. These results suggest that further research is needed for defending multi-exit architectures against this emerging threat. Our code is available at https://github.com/sanghyun-hong/deepsloth. </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Is the computational savings provided by the input-adaptive 'multi-exit architectures' robust against adversarial perturbations? No.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Pbj8H_jEHYv" data-number="2647">
      <h4>
        <a href="https://openreview.net/forum?id=Pbj8H_jEHYv">
            Orthogonalizing Convolutional Layers with the Cayley Transform
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Pbj8H_jEHYv" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Asher_Trockman1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Asher_Trockman1">Asher Trockman</a>, <a href="https://openreview.net/profile?id=~J_Zico_Kolter1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~J_Zico_Kolter1">J Zico Kolter</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Pbj8H_jEHYv-details-972" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Pbj8H_jEHYv-details-972"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">orthogonal layers, Lipschitz constrained networks, adversarial robustness</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recent work has highlighted several advantages of enforcing orthogonality in the weight layers of deep networks, such as maintaining the stability of activations, preserving gradient norms, and enhancing adversarial robustness by enforcing low Lipschitz constants. Although numerous methods exist for enforcing the orthogonality of fully-connected layers, those for convolutional layers are more heuristic in nature, often focusing on penalty methods or limited classes of convolutions. In this work, we propose and evaluate an alternative approach to directly parameterize convolutional layers that are constrained to be orthogonal. Specifically, we propose to apply the Cayley transform to a skew-symmetric convolution in the Fourier domain, so that the inverse convolution needed by the Cayley transform can be computed efficiently. We compare our method to previous Lipschitz-constrained and orthogonal convolutional layers and show that it indeed preserves orthogonality to a high degree even for large convolutions. Applied to the problem of certified adversarial robustness, we show that networks incorporating the layer outperform existing deterministic methods for certified defense against <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="58" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container>-norm-bounded adversaries, while scaling to larger architectures than previously investigated.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xTJEN-ggl1b" data-number="476">
      <h4>
        <a href="https://openreview.net/forum?id=xTJEN-ggl1b">
            LambdaNetworks: Modeling long-range Interactions without Attention
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xTJEN-ggl1b" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Irwan_Bello1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Irwan_Bello1">Irwan Bello</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xTJEN-ggl1b-details-982" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xTJEN-ggl1b-details-982"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">deep learning, neural networks, attention, transformer, vision, image classification</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We present lambda layers -- an alternative framework to self-attention -- for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Lambda layers capture such interactions by transforming available contexts into linear functions, termed lambdas, and applying these linear functions to each input separately. Similar to linear attention, lambda layers bypass expensive attention maps, but in contrast, they model both content and position-based interactions which enables their application to large structured inputs such as images. The resulting neural network architectures, LambdaNetworks, significantly outperform their convolutional and attentional counterparts on ImageNet classification, COCO object detection and instance segmentation, while being more computationally efficient. Additionally, we design LambdaResNets, a family of hybrid architectures across different scales, that considerably improves the speed-accuracy tradeoff of image classification models. LambdaResNets reach excellent accuracies on ImageNet while being 3.2 - 4.4x faster than the popular EfficientNets on modern machine learning accelerators. In large-scale semi-supervised training with an additional 130M pseudo-labeled images, LambdaResNets achieve up to 86.7% ImageNet accuracy while being 9.5x faster than EfficientNet NoisyStudent and 9x faster than a Vision Transformer with comparable accuracies.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Scalable framework for capturing long-range interactions between input and structured contextual information, which leads to strong improvements in vision tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="m1CD7tPubNy" data-number="64">
      <h4>
        <a href="https://openreview.net/forum?id=m1CD7tPubNy">
            Mind the Pad -- CNNs Can Develop Blind Spots
        </a>
      
        
          <a href="https://openreview.net/pdf?id=m1CD7tPubNy" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Bilal_Alsallakh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bilal_Alsallakh1">Bilal Alsallakh</a>, <a href="https://openreview.net/profile?email=narine%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="narine@fb.com">Narine Kokhlikyan</a>, <a href="https://openreview.net/profile?email=vivekm%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="vivekm@fb.com">Vivek Miglani</a>, <a href="https://openreview.net/profile?email=junyuan%40nyu.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="junyuan@nyu.edu">Jun Yuan</a>, <a href="https://openreview.net/profile?email=orionr%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="orionr@fb.com">Orion Reblitz-Richardson</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>9 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#m1CD7tPubNy-details-140" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="m1CD7tPubNy-details-140"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">CNN, convolution, spatial bias, blind spots, foveation, padding, exposition, debugging, visualization</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">We show how feature maps in convolutional networks are susceptible to spatial bias. Due to a combination of architectural choices, the activation at certain locations is systematically elevated or weakened. The major source of this bias is the padding mechanism. Depending on several aspects of convolution arithmetic, this mechanism can apply the padding unevenly, leading to asymmetries in the learned weights. We demonstrate how such bias can be detrimental to certain tasks such as small object detection: the activation is suppressed if the stimulus lies in the impacted area, leading to blind spots and misdetection. We explore alternative padding methods and propose solutions for analyzing and mitigating spatial bias.
      </span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">The padding mechanism in CNNs can induce harmful spatial bias in the learned weights and in the feature maps, which can be mitigated with careful architectural choices.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=m1CD7tPubNy&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="wS0UFjsNYjn" data-number="1492">
      <h4>
        <a href="https://openreview.net/forum?id=wS0UFjsNYjn">
            Meta-GMVAE: Mixture of Gaussian VAE for Unsupervised Meta-Learning
        </a>
      
        
          <a href="https://openreview.net/pdf?id=wS0UFjsNYjn" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Dong_Bok_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dong_Bok_Lee1">Dong Bok Lee</a>, <a href="https://openreview.net/profile?id=~Dongchan_Min1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dongchan_Min1">Dongchan Min</a>, <a href="https://openreview.net/profile?id=~Seanie_Lee1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Seanie_Lee1">Seanie Lee</a>, <a href="https://openreview.net/profile?id=~Sung_Ju_Hwang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Sung_Ju_Hwang1">Sung Ju Hwang</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>27 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#wS0UFjsNYjn-details-841" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="wS0UFjsNYjn-details-841"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Unsupervised Learning, Meta-Learning, Unsupervised Meta-learning, Variational Autoencoders</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Unsupervised learning aims to learn meaningful representations from unlabeled data which can captures its intrinsic structure, that can be transferred to downstream tasks. Meta-learning, whose objective is to learn to generalize across tasks such that the learned model can rapidly adapt to a novel task, shares the spirit of unsupervised learning in that the both seek to learn more effective and efficient learning procedure than learning from scratch. The fundamental difference of the two is that the most meta-learning approaches are supervised, assuming full access to the labels. However, acquiring labeled dataset for meta-training not only is costly as it requires human efforts in labeling but also limits its applications to pre-defined task distributions. In this paper, we propose a principled unsupervised meta-learning model, namely Meta-GMVAE, based on Variational Autoencoder (VAE) and set-level variational inference. Moreover, we introduce a mixture of Gaussian (GMM) prior, assuming that each modality represents each class-concept in a randomly sampled episode, which we optimize with Expectation-Maximization (EM). Then, the learned model can be used for downstream few-shot classification tasks, where we obtain task-specific parameters by performing semi-supervised EM on the latent representations of the support and query set, and predict labels of the query set by computing aggregated posteriors. We validate our model on Omniglot and Mini-ImageNet datasets by evaluating its performance on downstream few-shot classification tasks. The results show that our model obtain impressive performance gains over existing unsupervised meta-learning baselines, even outperforming supervised MAML on a certain setting.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">we propose a principled unsupervised meta-learning model which meta-learns a set-level variational posterior, by matching it with multi-modal prior distribution obtained by EM.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=wS0UFjsNYjn&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="zWy1uxjDdZJ" data-number="1934">
      <h4>
        <a href="https://openreview.net/forum?id=zWy1uxjDdZJ">
            Fast Geometric Projections for Local Robustness Certification
        </a>
      
        
          <a href="https://openreview.net/pdf?id=zWy1uxjDdZJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Aymeric_Fromherz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aymeric_Fromherz1">Aymeric Fromherz</a>, <a href="https://openreview.net/profile?id=~Klas_Leino1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Klas_Leino1">Klas Leino</a>, <a href="https://openreview.net/profile?id=~Matt_Fredrikson1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Matt_Fredrikson1">Matt Fredrikson</a>, <a href="https://openreview.net/profile?id=~Bryan_Parno1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Bryan_Parno1">Bryan Parno</a>, <a href="https://openreview.net/profile?id=~Corina_Pasareanu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Corina_Pasareanu1">Corina Pasareanu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>18 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#zWy1uxjDdZJ-details-507" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zWy1uxjDdZJ-details-507"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">verification, robustness, safety</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Local robustness ensures that a model classifies all inputs within an <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="59" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D45D TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mi>p</mi></msub></math></mjx-assistive-mml></mjx-container>-ball consistently, which precludes various forms of adversarial inputs.
      In this paper, we present a fast procedure for checking local robustness in feed-forward neural networks with piecewise-linear activation functions.
      Such networks partition the input space into a set of convex polyhedral regions in which the network’s behavior is linear; 
      hence, a systematic search for decision boundaries within the regions around a given input is sufficient for assessing robustness.
      Crucially, we show how the regions around a point can be analyzed using simple geometric projections, thus admitting an efficient, highly-parallel GPU implementation that excels particularly for the <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="60" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c2113"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>ℓ</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> norm, where previous work has been less effective.
      Empirically we find this approach to be far more precise than many approximate verification approaches, while at the same time performing multiple orders of magnitude faster than complete verifiers, and scaling to much deeper networks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We present a fast, scalable procedure for checking local robustness in neural networks</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="NECTfffOvn1" data-number="1239">
      <h4>
        <a href="https://openreview.net/forum?id=NECTfffOvn1">
            Fidelity-based Deep Adiabatic Scheduling
        </a>
      
        
          <a href="https://openreview.net/pdf?id=NECTfffOvn1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?email=eliovify%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="eliovify@gmail.com">Eli Ovits</a>, <a href="https://openreview.net/profile?id=~Lior_Wolf1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lior_Wolf1">Lior Wolf</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 15 Feb 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#NECTfffOvn1-details-192" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NECTfffOvn1-details-192"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Adiabatic quantum computation is a form of computation that acts by slowly interpolating a quantum system between an easy to prepare initial state and a final state that represents a solution to a given computational problem. The choice of the interpolation schedule is critical to the performance: if at a certain time point, the evolution is too rapid, the system has a high probability to transfer to a higher energy state, which does not represent a solution to the problem. On the other hand, an evolution that is too slow leads to a loss of computation time and increases the probability of failure due to decoherence. In this work, we train deep neural models to produce optimal schedules that are conditioned on the problem at hand.  We consider two types of problem representation: the Hamiltonian form, and the Quadratic Unconstrained Binary Optimization (QUBO) form. A novel loss function that scores schedules according to their approximated success probability is introduced. We benchmark our approach on random QUBO problems, Grover search, 3-SAT, and MAX-CUT problems and show that our approach outperforms, by a sizable margin, the linear schedules as well as alternative approaches that were very recently proposed.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A new loss for applying supervised deep learning to the problem of scheduling adiabatic quantum computations</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="NeRdBeTionN" data-number="3009">
      <h4>
        <a href="https://openreview.net/forum?id=NeRdBeTionN">
            On Self-Supervised Image Representations for GAN Evaluation
        </a>
      
        
          <a href="https://openreview.net/pdf?id=NeRdBeTionN" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Stanislav_Morozov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stanislav_Morozov1">Stanislav Morozov</a>, <a href="https://openreview.net/profile?id=~Andrey_Voynov1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrey_Voynov1">Andrey Voynov</a>, <a href="https://openreview.net/profile?id=~Artem_Babenko1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Artem_Babenko1">Artem Babenko</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 28 Jan 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>12 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#NeRdBeTionN-details-710" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="NeRdBeTionN-details-710"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">GAN, evaluation, embedding</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">The embeddings from CNNs pretrained on Imagenet classification are de-facto standard image representations for assessing GANs via FID, Precision and Recall measures. Despite broad previous criticism of their usage for non-Imagenet domains, these embeddings are still the top choice in most of the GAN literature.
      
      In this paper, we advocate the usage of the state-of-the-art self-supervised representations to evaluate GANs on the established non-Imagenet benchmarks. These representations, typically obtained via contrastive learning, are shown to provide better transfer to new tasks and domains, therefore, can serve as more universal embeddings of natural images. With extensive comparison of the recent GANs on the common datasets, we show that self-supervised representations produce a more reasonable ranking of models in terms of FID/Precision/Recall, while the ranking with classification-pretrained embeddings often can be misleading.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We show that the state-of-the-art self-supervised representations should be used when comparing GANs on the non-Imagenet datasets</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="zv-typ1gPxA" data-number="3719">
      <h4>
        <a href="https://openreview.net/forum?id=zv-typ1gPxA">
            Retrieval-Augmented Generation for Code Summarization via Hybrid GNN
        </a>
      
        
          <a href="https://openreview.net/pdf?id=zv-typ1gPxA" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Shangqing_Liu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shangqing_Liu1">Shangqing Liu</a>, <a href="https://openreview.net/profile?id=~Yu_Chen5" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yu_Chen5">Yu Chen</a>, <a href="https://openreview.net/profile?id=~Xiaofei_Xie1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Xiaofei_Xie1">Xiaofei Xie</a>, <a href="https://openreview.net/profile?email=jingkai001%40e.ntu.edu.sg" class="profile-link" data-toggle="tooltip" data-placement="top" title="jingkai001@e.ntu.edu.sg">Jing Kai Siow</a>, <a href="https://openreview.net/profile?id=~Yang_Liu36" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yang_Liu36">Yang Liu</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 23 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>11 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#zv-typ1gPxA-details-212" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="zv-typ1gPxA-details-212"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Code Summarization, Graph Neural Network, Retrieval, Generation</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Source code summarization aims to generate natural language summaries from structured code snippets for better understanding code functionalities. However, automatic code summarization is challenging due to the complexity of the source code and the language gap between the source code and natural language summaries. Most previous approaches either rely on retrieval-based (which can take advantage of similar examples seen from the retrieval database, but have low generalization performance) or generation-based methods (which have better generalization performance, but cannot take advantage of similar examples).
      This paper proposes a novel retrieval-augmented mechanism to combine the benefits of both worlds.
      Furthermore, to mitigate the limitation of Graph Neural Networks (GNNs) on capturing global graph structure information of source code, we propose a novel attention-based dynamic graph to complement the static graph representation of the source code, and design a hybrid message passing GNN for capturing both the local and global structural information. To evaluate the proposed approach, we release a new challenging benchmark, crawled from diversified large-scale open-source C projects (total 95k+ unique functions in the dataset). Our method achieves the state-of-the-art performance, improving existing methods by 1.42, 2.44 and 1.29 in terms of BLEU-4, ROUGE-L and METEOR.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper proposes a novel retrieval-augmented mechanism to augment the code semantics with hybrid graph neural network for source code summarization.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="xppLmXCbOw1" data-number="3479">
      <h4>
        <a href="https://openreview.net/forum?id=xppLmXCbOw1">
            Self-supervised Visual Reinforcement Learning with Object-centric Representations
        </a>
      
        
          <a href="https://openreview.net/pdf?id=xppLmXCbOw1" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Andrii_Zadaianchuk1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Andrii_Zadaianchuk1">Andrii Zadaianchuk</a>, <a href="https://openreview.net/profile?id=~Maximilian_Seitzer1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maximilian_Seitzer1">Maximilian Seitzer</a>, <a href="https://openreview.net/profile?id=~Georg_Martius1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Georg_Martius1">Georg Martius</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>15 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#xppLmXCbOw1-details-39" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="xppLmXCbOw1-details-39"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">self-supervision, autonomous learning, object-centric representations, visual reinforcement learning</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Autonomous agents need large repertoires of skills to act reasonably on new tasks that they have not seen before. However, acquiring these skills using only a stream of high-dimensional, unstructured, and unlabeled observations is a tricky challenge for any autonomous agent. Previous methods have used variational autoencoders to encode a scene into a low-dimensional vector that can be used as a goal for an agent to discover new skills. Nevertheless, in compositional/multi-object environments it is difficult to disentangle all the factors of variation into such a fixed-length representation of the whole scene. We propose to use object-centric representations as a modular and structured observation space, which is learned with a compositional generative world model.
      We show that the structure in the representations in combination with goal-conditioned attention policies helps the autonomous agent to discover and learn useful skills. These skills can be further combined to address compositional tasks like the manipulation of several different objects.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">The combination of object-centric representations and goal-conditioned attention policies helps autonomous agents to learn useful multi-task policies in visual multi-object environments</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="_XYzwxPIQu6" data-number="2968">
      <h4>
        <a href="https://openreview.net/forum?id=_XYzwxPIQu6">
            Identifying nonlinear dynamical systems with multiple time scales and long-range dependencies
        </a>
      
        
          <a href="https://openreview.net/pdf?id=_XYzwxPIQu6" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Dominik_Schmidt1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dominik_Schmidt1">Dominik Schmidt</a>, <a href="https://openreview.net/profile?id=~Georgia_Koppe1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Georgia_Koppe1">Georgia Koppe</a>, <a href="https://openreview.net/profile?email=zahra.monfared%40zi-mannheim.de" class="profile-link" data-toggle="tooltip" data-placement="top" title="zahra.monfared@zi-mannheim.de">Zahra Monfared</a>, <a href="https://openreview.net/profile?email=max.beutelspacher%40mailbox.org" class="profile-link" data-toggle="tooltip" data-placement="top" title="max.beutelspacher@mailbox.org">Max Beutelspacher</a>, <a href="https://openreview.net/profile?id=~Daniel_Durstewitz1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Daniel_Durstewitz1">Daniel Durstewitz</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 12 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#_XYzwxPIQu6-details-83" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="_XYzwxPIQu6-details-83"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">nonlinear dynamical systems, recurrent neural networks, attractors, computational neuroscience, vanishing gradient problem, LSTM</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A main theoretical interest in biology and physics is to identify the nonlinear dynamical system (DS) that generated observed time series. Recurrent Neural Networks (RNN) are, in principle, powerful enough to approximate any underlying DS, but in their vanilla form suffer from the exploding vs. vanishing gradients problem. Previous attempts to alleviate this problem resulted either in more complicated, mathematically less tractable RNN architectures, or strongly limited the dynamical expressiveness of the RNN. 
      Here we address this issue by suggesting a simple regularization scheme for vanilla RNN with ReLU activation which enables them to solve long-range dependency problems and express slow time scales, while retaining a simple mathematical structure which makes their DS properties partly analytically accessible. We prove two theorems that establish a tight connection between the regularized RNN dynamics and their gradients, illustrate on DS benchmarks that our regularization approach strongly eases the reconstruction of DS which harbor widely differing time scales, and show that our method is also en par with other long-range architectures like LSTMs on several tasks.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We introduce a novel regularization for ReLU-based vanilla RNN that mitigates the exploding vs. vanishing gradient problem while retaining a simple mathematical structure that makes the RNN's dynamical systems properties partly analytically tractable</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Oos98K9Lv-k" data-number="1005">
      <h4>
        <a href="https://openreview.net/forum?id=Oos98K9Lv-k">
            Neural Topic Model via Optimal Transport
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Oos98K9Lv-k" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~He_Zhao1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~He_Zhao1">He Zhao</a>, <a href="https://openreview.net/profile?id=~Dinh_Phung2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dinh_Phung2">Dinh Phung</a>, <a href="https://openreview.net/profile?id=~Viet_Huynh1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Viet_Huynh1">Viet Huynh</a>, <a href="https://openreview.net/profile?id=~Trung_Le2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Trung_Le2">Trung Le</a>, <a href="https://openreview.net/profile?id=~Wray_Buntine1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Wray_Buntine1">Wray Buntine</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>5 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Oos98K9Lv-k-details-921" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Oos98K9Lv-k-details-921"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">topic modelling, optimal transport, document analysis</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Recently, Neural Topic Models (NTMs) inspired by variational autoencoders have obtained increasingly research interest due to their promising results on text analysis. However, it is usually hard for existing NTMs to achieve good document representation and coherent/diverse topics at the same time. Moreover, they often degrade their performance severely on short documents. The requirement of reparameterisation could also comprise their training quality and model flexibility. To address these shortcomings, we present a new neural topic model via the theory of optimal transport (OT). Specifically, we propose to learn the topic distribution of a document by directly minimising its OT distance to the document's word distributions. Importantly, the cost matrix of the OT distance models the weights between topics and words, which is constructed by the distances between topics and words in an embedding space. Our proposed model can be trained efficiently with a differentiable loss. Extensive experiments show that our framework significantly outperforms the state-of-the-art NTMs on discovering more coherent and diverse topics and deriving better document representations for both regular and short texts.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">This paper presents a neural topic model via optimal transport, which can discover more coherent and diverse topics and derive better document representations for both regular and short texts.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="bnY0jm4l59" data-number="2408">
      <h4>
        <a href="https://openreview.net/forum?id=bnY0jm4l59">
            Memory Optimization for Deep Networks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=bnY0jm4l59" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Aashaka_Shah1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Aashaka_Shah1">Aashaka Shah</a>, <a href="https://openreview.net/profile?id=~Chao-Yuan_Wu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chao-Yuan_Wu1">Chao-Yuan Wu</a>, <a href="https://openreview.net/profile?email=jaya%40cs.utexas.edu" class="profile-link" data-toggle="tooltip" data-placement="top" title="jaya@cs.utexas.edu">Jayashree Mohan</a>, <a href="https://openreview.net/profile?id=~Vijay_Chidambaram1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Vijay_Chidambaram1">Vijay Chidambaram</a>, <a href="https://openreview.net/profile?id=~Philipp_Kraehenbuehl1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Philipp_Kraehenbuehl1">Philipp Kraehenbuehl</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 17 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#bnY0jm4l59-details-219" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="bnY0jm4l59-details-219"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">memory optimized training, memory efficient training, checkpointing, deep network training</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="61" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c><mjx-c class="mjx-c32"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>32</mn><mo>×</mo></math></mjx-assistive-mml></mjx-container> over the last five years, the total available memory only grew by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="62" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c2E"></mjx-c><mjx-c class="mjx-c35"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2.5</mn><mo>×</mo></math></mjx-assistive-mml></mjx-container>. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="63" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c33"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>3</mn><mo>×</mo></math></mjx-assistive-mml></mjx-container> for various PyTorch models, with a 9-16<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="64" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mi class="mjx-n"><mjx-c class="mjx-c25"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">%</mi></math></mjx-assistive-mml></mjx-container> overhead in computation. For the same computation cost, MONeT requires 1.2-1.8<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="65" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>×</mo></math></mjx-assistive-mml></mjx-container> less memory than current state-of-the-art automated checkpointing frameworks. Our code will be made publicly available upon acceptance.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">MONeT reduces the memory footprint of training while minimizing compute overhead by jointly optimizing checkpointing with operator optimizations.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="QfTXQiGYudJ" data-number="44">
      <h4>
        <a href="https://openreview.net/forum?id=QfTXQiGYudJ">
            Stabilized Medical Image Attacks
        </a>
      
        
          <a href="https://openreview.net/pdf?id=QfTXQiGYudJ" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Gege_Qi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Gege_Qi1">Gege Qi</a>, <a href="https://openreview.net/profile?id=~Lijun_GONG2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lijun_GONG2">Lijun GONG</a>, <a href="https://openreview.net/profile?id=~Yibing_Song1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yibing_Song1">Yibing Song</a>, <a href="https://openreview.net/profile?id=~Kai_Ma2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Kai_Ma2">Kai Ma</a>, <a href="https://openreview.net/profile?id=~Yefeng_Zheng2" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yefeng_Zheng2">Yefeng Zheng</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 22 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>4 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#QfTXQiGYudJ-details-838" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="QfTXQiGYudJ-details-838"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">Healthcare, Biometrics</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Convolutional Neural Networks (CNNs) have advanced existing medical systems for automatic disease diagnosis. However, a threat to these systems arises that adversarial attacks make CNNs vulnerable. Inaccurate diagnosis results make a negative influence on human healthcare. There is a need to investigate potential adversarial attacks to robustify deep medical diagnosis systems. On the other side, there are several modalities of medical images (e.g., CT, fundus, and endoscopic image) of which each type is significantly different from others. It is more challenging to generate adversarial perturbations for different types of medical images. In this paper, we propose an image-based medical adversarial attack method to consistently produce adversarial perturbations on medical images. The objective function of our method consists of a loss deviation term and a loss stabilization term. The loss deviation term increases the divergence between the CNN prediction of an adversarial example and its ground truth label. Meanwhile, the loss stabilization term ensures similar CNN predictions of this example and its smoothed input. From the perspective of the whole iterations for perturbation generation, the proposed loss stabilization term exhaustively searches the perturbation space to smooth the single spot for local optimum escape. We further analyze the KL-divergence of the proposed loss function and find that the loss stabilization term makes the perturbations updated towards a fixed objective spot while deviating from the ground truth. This stabilization ensures the proposed medical attack effective for different types of medical images while producing perturbations in small variance. Experiments on several medical image analysis benchmarks including the recent COVID-19 dataset show the stability of the proposed method.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">We propose a stabilized adversarial attack method for medical image analysis.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="LwEQnp6CYev" data-number="423">
      <h4>
        <a href="https://openreview.net/forum?id=LwEQnp6CYev">
            Quantifying Differences in Reward Functions
        </a>
      
        
          <a href="https://openreview.net/pdf?id=LwEQnp6CYev" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Adam_Gleave1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Adam_Gleave1">Adam Gleave</a>, <a href="https://openreview.net/profile?id=~Michael_D_Dennis1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Michael_D_Dennis1">Michael D Dennis</a>, <a href="https://openreview.net/profile?id=~Shane_Legg1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Shane_Legg1">Shane Legg</a>, <a href="https://openreview.net/profile?id=~Stuart_Russell1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Stuart_Russell1">Stuart Russell</a>, <a href="https://openreview.net/profile?id=~Jan_Leike1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Jan_Leike1">Jan Leike</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>17 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#LwEQnp6CYev-details-689" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="LwEQnp6CYev-details-689"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">rl, irl, reward learning, distance, benchmarks</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">For many tasks, the reward function is inaccessible to introspection or too complex to be specified procedurally, and must instead be learned from user data. Prior work has evaluated learned reward functions by evaluating policies optimized for the learned reward. However, this method cannot distinguish between the learned reward function failing to reflect user preferences and the policy optimization process failing to optimize the learned reward. Moreover, this method can only tell us about behavior in the evaluation environment, but the reward may incentivize very different behavior in even a slightly different deployment environment. To address these problems, we introduce the Equivalent-Policy Invariant Comparison (EPIC) distance to quantify the difference between two reward functions directly, without a policy optimization step. We prove EPIC is invariant on an equivalence class of reward functions that always induce the same optimal policy. Furthermore, we find EPIC can be efficiently approximated and is more robust than baselines to the choice of coverage distribution. Finally, we show that EPIC distance bounds the regret of optimal policies even under different transition dynamics, and we confirm empirically that it predicts policy training success. Our source code is available at https://github.com/HumanCompatibleAI/evaluating-rewards.</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">A theoretically principled distance measure on reward functions that is quick to compute and predicts policy training performance.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="kHSu4ebxFXY" data-number="3014">
      <h4>
        <a href="https://openreview.net/forum?id=kHSu4ebxFXY">
            MARS: Markov Molecular Sampling for Multi-objective Drug Discovery
        </a>
      
        
          <a href="https://openreview.net/pdf?id=kHSu4ebxFXY" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Yutong_Xie3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yutong_Xie3">Yutong Xie</a>, <a href="https://openreview.net/profile?id=~Chence_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Chence_Shi1">Chence Shi</a>, <a href="https://openreview.net/profile?email=zhouhao.nlp%40bytedance.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="zhouhao.nlp@bytedance.com">Hao Zhou</a>, <a href="https://openreview.net/profile?email=yuwei.yang%40bytedance.com" class="profile-link" data-toggle="tooltip" data-placement="top" title="yuwei.yang@bytedance.com">Yuwei Yang</a>, <a href="https://openreview.net/profile?id=~Weinan_Zhang1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Weinan_Zhang1">Weinan Zhang</a>, <a href="https://openreview.net/profile?id=~Yong_Yu1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Yong_Yu1">Yong Yu</a>, <a href="https://openreview.net/profile?id=~Lei_Li11" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Lei_Li11">Lei Li</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 18 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>19 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#kHSu4ebxFXY-details-142" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="kHSu4ebxFXY-details-142"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">drug discovery, molecular graph generation, MCMC sampling</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Searching for novel molecules with desired chemical properties is crucial in drug discovery. Existing work focuses on developing neural models to generate either molecular sequences or chemical graphs. However, it remains a big challenge to find novel and diverse compounds satisfying several properties. In this paper, we propose MARS, a method for multi-objective drug molecule discovery. MARS is based on the idea of generating the chemical candidates by iteratively editing fragments of molecular graphs. To search for high-quality candidates, it employs Markov chain Monte Carlo sampling (MCMC) on molecules with an annealing scheme and an adaptive proposal. To further improve sample efficiency, MARS uses a graph neural network (GNN) to represent and select candidate edits, where the GNN is trained on-the-fly with samples from MCMC. Experiments show that MARS achieves state-of-the-art performance in various multi-objective settings where molecular bio-activity, drug-likeness, and synthesizability are considered. Remarkably, in the most challenging setting where all four objectives are simultaneously optimized, our approach outperforms previous methods significantly in comprehensive evaluations. The code is available at https://github.com/yutxie/mars.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">In this paper, we propose a self-adaptive MCMC sampling method (MARS) to generate molecules targeting multiple objectives for drug discovery for multi-objective drug discovery.</span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="Jnspzp-oIZE" data-number="3192">
      <h4>
        <a href="https://openreview.net/forum?id=Jnspzp-oIZE">
            Gauge Equivariant Mesh CNNs: Anisotropic convolutions on geometric graphs
        </a>
      
        
          <a href="https://openreview.net/pdf?id=Jnspzp-oIZE" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Pim_De_Haan1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Pim_De_Haan1">Pim De Haan</a>, <a href="https://openreview.net/profile?id=~Maurice_Weiler1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Maurice_Weiler1">Maurice Weiler</a>, <a href="https://openreview.net/profile?id=~Taco_Cohen1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Taco_Cohen1">Taco Cohen</a>, <a href="https://openreview.net/profile?id=~Max_Welling1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Max_Welling1">Max Welling</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 19 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>16 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#Jnspzp-oIZE-details-818" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="Jnspzp-oIZE-details-818"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">symmetry, equivariance, mesh, geometric, convolution</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">A common approach to define convolutions on meshes is to interpret them as a graph and apply graph convolutional networks (GCNs).  Such GCNs utilize isotropic kernels and are therefore insensitive to the relative orientation of vertices and thus to the geometry of the mesh as a whole. We propose Gauge Equivariant Mesh CNNs which generalize GCNs to apply anisotropic gauge equivariant kernels. Since the resulting features carry orientation information, we introduce a geometric message passing scheme defined by parallel transporting features over mesh edges. Our experiments validate the significantly improved expressivity of the proposed model over conventional GCNs and other methods.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">One-sentence Summary:</strong>
            <span class="note-content-value ">Expressive anisotropic mesh convolution without having to pick arbitrary kernel orientation by using gauge equivariance</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=Jnspzp-oIZE&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
    <li class="note " data-id="3UDSdyIcBDA" data-number="2889">
      <h4>
        <a href="https://openreview.net/forum?id=3UDSdyIcBDA">
            RMSprop converges with proper hyper-parameter
        </a>
      
        
          <a href="https://openreview.net/pdf?id=3UDSdyIcBDA" class="pdf-link" title="Download PDF" target="_blank"><img src="./ICLR 2021 Conference _ OpenReview_files/pdf_icon_blue.svg"></a>
        
        
      </h4>
      
      
      
      <div class="note-authors">
        <a href="https://openreview.net/profile?id=~Naichen_Shi1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Naichen_Shi1">Naichen Shi</a>, <a href="https://openreview.net/profile?id=~Dawei_Li3" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Dawei_Li3">Dawei Li</a>, <a href="https://openreview.net/profile?id=~Mingyi_Hong1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Mingyi_Hong1">Mingyi Hong</a>, <a href="https://openreview.net/profile?id=~Ruoyu_Sun1" class="profile-link" data-toggle="tooltip" data-placement="top" title="~Ruoyu_Sun1">Ruoyu Sun</a>
      </div>
      
      <div class="note-meta-info">
        <span class="date">28 Sept 2020 (modified: 28 Mar 2021)</span>
          <span class="item">ICLR 2021 Spotlight</span>
          <span class="readers">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span>
        
          <span>10 Replies</span>
        
        
      </div>
      
        <a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#3UDSdyIcBDA-details-202" class="note-contents-toggle" role="button" data-toggle="collapse" aria-expanded="false">Show details</a><div class="collapse" id="3UDSdyIcBDA-details-202"><div class="note-contents-collapse"><ul class="list-unstyled note-content">
          <li>
            <strong class="note-content-field">Keywords:</strong>
            <span class="note-content-value ">RMSprop, convergence, hyperparameter</span>
          </li>
          <li>
            <strong class="note-content-field">Abstract:</strong>
            <span class="note-content-value ">Despite the existence of divergence examples, RMSprop remains 
      one of the most popular algorithms in machine learning. Towards closing the gap between theory and practice, we prove that RMSprop converges with proper choice of hyper-parameters under certain conditions. More specifically, we prove that when the hyper-parameter <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="66" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>β</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container> is close enough to <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="67" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn></math></mjx-assistive-mml></mjx-container>, RMSprop and its random shuffling version converge to a bounded region in general, and to critical points in the interpolation regime. It is worth mentioning that our results do not depend on  ``bounded gradient"  assumption, which is often the key assumption utilized by existing theoretical work for Adam-type adaptive gradient method. Removing this assumption allows us to establish a phase transition from divergence to non-divergence for RMSprop. 
      
      Finally, based on our theory, we conjecture that in practice there is a critical threshold <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="68" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-msubsup><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.271em;"><mjx-mo class="mjx-ss" size="s"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo><mjx-spacer style="margin-top: 0.18em;"></mjx-spacer><mjx-mn class="mjx-ss" size="s"><mjx-c class="mjx-c1D7E4 TEX-SS"></mjx-c></mjx-mn></mjx-script></mjx-msubsup></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mi>β</mi><mn mathvariant="sans-serif">2</mn><mo mathvariant="sans-serif">∗</mo></msubsup></mrow></math></mjx-assistive-mml></mjx-container>, such that RMSprop generates reasonably good results only if <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" role="presentation" tabindex="0" ctxtmenu_counter="69" style="font-size: 113.1%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3E"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2265"></mjx-c></mjx-mo><mjx-texatom space="4" texclass="ORD"><mjx-msubsup><mjx-mi class="mjx-i" noic="true"><mjx-c class="mjx-c1D6FD TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.271em;"><mjx-mo class="mjx-ss" size="s"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo><mjx-spacer style="margin-top: 0.18em;"></mjx-spacer><mjx-mn class="mjx-ss" size="s"><mjx-c class="mjx-c1D7E4 TEX-SS"></mjx-c></mjx-mn></mjx-script></mjx-msubsup></mjx-texatom></mjx-math><mjx-assistive-mml role="presentation" unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>1</mn><mo>&gt;</mo><msub><mi>β</mi><mn>2</mn></msub><mo>≥</mo><mrow><msubsup><mi>β</mi><mn mathvariant="sans-serif">2</mn><mo mathvariant="sans-serif">∗</mo></msubsup></mrow></math></mjx-assistive-mml></mjx-container>. We provide empirical evidence for such a phase transition in our numerical experiments.</span>
          </li>
          <li>
            <strong class="note-content-field">Code Of Ethics:</strong>
            <span class="note-content-value ">I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics</span>
          </li>
          <li>
            <strong class="note-content-field">Supplementary Material:</strong>
            <span class="note-content-value "><a href="https://openreview.net/attachment?id=3UDSdyIcBDA&amp;name=supplementary_material" class="attachment-download-link" title="Download Supplementary Material" target="_blank"><span class="glyphicon glyphicon-download-alt" aria-hidden="true"></span> &nbsp;zip</a></span>
          </li>
      </ul>
      </div></div>
      
      
      
      
    </li>
</ul>
</div>
    <div role="tabpanel" class="tab-pane fade  " id="poster-presentations">
      
    </div>
    <div role="tabpanel" class="tab-pane fade  " id="withdrawn-rejected-submissions">
      
    </div>
</div>
</div></div></div></main></div></div></div><footer class="sitemap"><div class="container"><div class="row hidden-xs"><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/about">About OpenReview</a></li><li><a href="https://openreview.net/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="https://openreview.net/venues">All Venues</a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/contact">Contact</a></li><li><a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-sm-4"><ul class="list-unstyled"><li><a href="https://openreview.net/faq">Frequently Asked Questions</a></li><li><a href="https://openreview.net/legal/terms">Terms of Service</a></li><li><a href="https://openreview.net/legal/privacy">Privacy Policy</a></li></ul></div></div><div class="row visible-xs-block"><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://openreview.net/about">About OpenReview</a></li><li><a href="https://openreview.net/group?id=OpenReview.net/Support">Hosting a Venue</a></li><li><a href="https://openreview.net/venues">All Venues</a></li><li><a class="join-the-team" href="https://codeforscience.org/jobs?job=OpenReview-Developer" target="_blank" rel="noopener noreferrer"><strong>Join the Team</strong></a></li></ul></div><div class="col-xs-6"><ul class="list-unstyled"><li><a href="https://openreview.net/faq">Frequently Asked Questions</a></li><li><a href="https://openreview.net/contact">Contact</a></li><li><a href="https://openreview.net/group?id=ICLR.cc/2021/Conference#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li><li><a href="https://openreview.net/legal/terms">Terms of Service</a></li><li><a href="https://openreview.net/legal/privacy">Privacy Policy</a></li></ul></div></div></div></footer><footer class="sponsor"><div class="container"><div class="row"><div class="col-sm-10 col-sm-offset-1"><p class="text-center">OpenReview is created by the<!-- --> <a href="http://www.iesl.cs.umass.edu/" target="_blank" rel="noopener noreferrer">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.</p></div></div></div></footer><div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog"><div class="modal-dialog "><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button><h3 class="modal-title">Send Feedback</h3></div><div class="modal-body"><p>Enter your feedback below and we'll get back to you as soon as possible.</p><form><div class="form-group"><input type="email" name="from" class="form-control" placeholder="Email" required=""></div><div class="form-group"><input type="text" name="subject" class="form-control" placeholder="Subject"></div><div class="form-group"><textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea></div></form></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button><button type="button" class="btn btn-primary">Send</button></div></div></div></div><div id="bibtex-modal" class="modal fade" tabindex="-1" role="dialog"><div class="modal-dialog "><div class="modal-content"><div class="modal-header"><button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button><h3 class="modal-title">BibTeX Record</h3></div><div class="modal-body"><pre class="bibtex-content"></pre><em class="instructions">Click anywhere on the box above to highlight complete record</em></div><div class="modal-footer"><button type="button" class="btn btn-default" data-dismiss="modal">Done</button></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"groupId":"ICLR.cc/2021/Conference","webfieldCode":"// Webfield Code for ICLR.cc/2021/Conference\nwindow.user = {\"id\":\"guest_1617527366270\",\"isGuest\":true};\n$(function() {\n  var args = {\"id\":\"ICLR.cc/2021/Conference\"};\n  var group = {\"id\":\"ICLR.cc/2021/Conference\",\"cdate\":1587911460292,\"ddate\":null,\"tcdate\":1587911460292,\"tmdate\":1611607761057,\"tddate\":null,\"signatures\":[\"ICLR.cc/2021/Conference\"],\"signatories\":[\"ICLR.cc/2021/Conference\"],\"readers\":[\"everyone\"],\"nonreaders\":[],\"writers\":[\"ICLR.cc/2021/Conference\"],\"members\":[\"ICLR.cc/2021/Conference/Program_Chairs\",\"OpenReview.net/Support\"],\"details\":{\"writable\":false}};\n  var document = null;\n  var window = null;\n  var model = {\n    tokenPayload: function() {\n      return { user: user }\n    }\n  };\n\n  $('#group-container').empty();\n  \n\n  // ------------------------------------\n// Venue homepage template\n//\n// This webfield displays the conference header (#header), the submit button (#invitation),\n// and a tabbed interface for viewing various types of notes.\n// ------------------------------------\n\n// Constants\nvar PARENT_GROUP_ID = 'ICLR.cc/2021';\nvar CONFERENCE_ID = 'ICLR.cc/2021/Conference';\nvar BLIND_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Blind_Submission';\nvar WITHDRAWN_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Withdrawn_Submission';\nvar DESK_REJECTED_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Desk_Rejected_Submission';\nvar DECISION_INVITATION_REGEX = 'ICLR.cc/2021/Conference/Paper.*/-/Decision';\nvar DECISION_HEADING_MAP = {\n  \"Accept (Oral)\": \"Oral Presentations\", \n  \"Accept (Spotlight)\": \"Spotlight Presentations\", \n  \"Accept (Poster)\": \"Poster Presentations\", \n  \"Reject\": \"Withdrawn/Rejected Submissions\"\n  \n};\nvar PAGE_SIZE = 25;\n\nvar HEADER = {\"title\": \"International Conference on Learning Representations\", \"subtitle\": \"ICLR 2021\", \"location\": \"Vienna, Austria\", \"date\": \"May 04 2021\", \"website\": \"https://iclr.cc/\", \"instructions\": \"\u003cp class='dark'\u003ePlease see the venue website for more information.\u003cbr\u003e \u003cp class='dark'\u003e\u003cstrong\u003eAbstract Submission End:\u003c/strong\u003e Sep 28 2020 03:00PM UTC-0\u003c/p\u003e\u003cp class='dark'\u003e\u003cstrong\u003ePaper Submission End:\u003c/strong\u003e Oct 2 2020 03:00PM UTC-0\u003c/p\u003e\", \"deadline\": \"\", \"contact\": \"iclr2021programchairs@googlegroups.com\", \"reviewers_name\": \"Reviewers\", \"area_chairs_name\": \"Area_Chairs\", \"reviewers_id\": \"ICLR.cc/2021/Conference/Reviewers\", \"authors_id\": \"ICLR.cc/2021/Conference/Authors\", \"program_chairs_id\": \"ICLR.cc/2021/Conference/Program_Chairs\", \"area_chairs_id\": \"ICLR.cc/2021/Conference/Area_Chairs\", \"submission_id\": \"ICLR.cc/2021/Conference/-/Submission\", \"blind_submission_id\": \"ICLR.cc/2021/Conference/-/Blind_Submission\", \"withdrawn_submission_id\": \"ICLR.cc/2021/Conference/-/Withdrawn_Submission\", \"desk_rejected_submission_id\": \"ICLR.cc/2021/Conference/-/Desk_Rejected_Submission\", \"public\": true};\n\nvar paperDisplayOptions = {\n  pdfLink: true,\n  replyCount: true,\n  showContents: true\n};\n\nvar sections = [];\n\n// Main is the entry point to the webfield code and runs everything\nfunction main() {\n  if (args \u0026\u0026 args.referrer) {\n    OpenBanner.referrerLink(args.referrer);\n  } else if (PARENT_GROUP_ID.length){\n    OpenBanner.venueHomepageLink(PARENT_GROUP_ID);\n  }\n  Webfield.ui.setup('#group-container', CONFERENCE_ID);  // required\n\n  renderConferenceHeader();\n\n  renderConferenceTabs();\n\n  load().then(renderContent).then(Webfield.ui.done);\n}\n\n// Load makes all the API calls needed to get the data to render the page\nfunction load() {\n  var notesP = Webfield.getAll('/notes', {\n    invitation: BLIND_SUBMISSION_ID,\n    details: 'replyCount,invitation,original'\n  });\n\n  var withdrawnNotesP = WITHDRAWN_SUBMISSION_ID ? Webfield.getAll('/notes', {\n    invitation: WITHDRAWN_SUBMISSION_ID,\n    details: 'replyCount,invitation,original'\n  }) : $.Deferred().resolve([]);\n\n  var deskRejectedNotesP = DESK_REJECTED_SUBMISSION_ID ? Webfield.getAll('/notes', {\n    invitation: DESK_REJECTED_SUBMISSION_ID,\n    details: 'replyCount,invitation,original'\n  }) : $.Deferred().resolve([]);\n\n  var decisionNotesP = Webfield.getAll('/notes', {\n    invitation: DECISION_INVITATION_REGEX,\n  });\n\n  var userGroupsP;\n  if (!user || _.startsWith(user.id, 'guest_')) {\n    userGroupsP = $.Deferred().resolve([]);\n  } else {\n    userGroupsP = Webfield.getAll('/groups', {\n      regex: CONFERENCE_ID + '/.*',\n      member: user.id,\n      web: true\n    }).then(function(groups) {\n      if (!groups || !groups.length) {\n        return [];\n      }\n\n      return groups.map(function(g) { return g.id; });\n    });\n  }\n\n  return $.when(notesP, decisionNotesP, withdrawnNotesP, deskRejectedNotesP, userGroupsP);\n}\n\nfunction renderConferenceHeader() {\n  Webfield.ui.venueHeader(HEADER);\n  Webfield.ui.spinner('#notes', { inline: true });\n}\n\nfunction getElementId(decision) {\n  if (!decision) return decision;\n  return decision.replace(/\\W/g, '-')\n    .replace('(', '')\n    .replace(')', '')\n    .toLowerCase();\n}\n\nfunction renderConferenceTabs() {\n  sections.push({\n    heading: 'Your Consoles',\n    id: 'your-consoles',\n  });\n  var tabNames = new Set();\n  for (var decision in DECISION_HEADING_MAP) {\n    tabNames.add(DECISION_HEADING_MAP[decision]);\n  }\n  var tabArray = Array.from(tabNames);\n  tabArray.forEach(function(tabName) {\n    sections.push({\n      heading: tabName,\n      id: getElementId(tabName)\n    });\n  })\n\n  Webfield.ui.tabPanel(sections, {\n    container: '#notes',\n    hidden: true\n  });\n}\n\nfunction createConsoleLinks(allGroups) {\n  var uniqueGroups = _.sortedUniq(allGroups.sort());\n\n  return uniqueGroups.map(function(group) {\n    var groupName = group.split('/').pop();\n    if (groupName.slice(-1) === 's') {\n      groupName = groupName.slice(0, -1);\n    }\n\n    return [\n      '\u003cli class=\"note invitation-link\"\u003e',\n        '\u003ca href=\"/group?id=' + group + '\"\u003e' + groupName.replace(/_/g, ' ') + ' Console\u003c/a\u003e',\n      '\u003c/li\u003e'\n    ].join('');\n  });\n}\n\nfunction groupNotesByDecision(notes, decisionNotes, withdrawnNotes, deskRejectedNotes) {\n  // Categorize notes into buckets defined by DECISION_HEADING_MAP\n  var notesDict = _.keyBy(notes, 'id');\n\n  var papersByDecision = {};\n  for (var decision in DECISION_HEADING_MAP) {\n    papersByDecision[getElementId(DECISION_HEADING_MAP[decision])] = [];\n  }\n\n  decisionNotes.forEach(function(d) {\n    var tabName = DECISION_HEADING_MAP[d.content.decision];\n    if (tabName) {\n      var decisionKey = getElementId(tabName);\n      if (notesDict[d.forum] \u0026\u0026 papersByDecision[decisionKey]) {\n        papersByDecision[decisionKey].push(notesDict[d.forum]);\n      }\n    }\n\n  });\n\n  if (papersByDecision['withdrawn-rejected-submissions']) {\n    papersByDecision['withdrawn-rejected-submissions'] = papersByDecision['withdrawn-rejected-submissions'].concat(withdrawnNotes.concat(deskRejectedNotes));\n  }\n  return papersByDecision;\n}\n\nfunction renderContent(notes, decisionNotes, withdrawnNotes, deskRejectedNotes, userGroups) {\n  var papersByDecision = groupNotesByDecision(notes, decisionNotes, withdrawnNotes, deskRejectedNotes);\n\n  // Your Consoles Tab\n  if (userGroups \u0026\u0026 userGroups.length) {\n    var consoleLinks = createConsoleLinks(userGroups);\n    $('#your-consoles').html('\u003cul class=\"list-unstyled submissions-list\"\u003e' +\n      consoleLinks.join('\\n') + '\u003c/ul\u003e');\n\n    $('.tabs-container a[href=\"#your-consoles\"]').parent().show();\n  } else {\n    $('.tabs-container a[href=\"#your-consoles\"]').parent().hide();\n  }\n\n  // Register event handlers\n  $('#group-container').on('shown.bs.tab', 'ul.nav-tabs li a', function(e) {\n    var containerSelector = $(e.target).attr('href');\n    var containerId = containerSelector.substring(1);\n    if (!papersByDecision.hasOwnProperty(containerId) || !$(containerSelector).length) {\n      return;\n    }\n\n    setTimeout(function() {\n      Webfield.ui.searchResults(\n        papersByDecision[containerId],\n        Object.assign({}, paperDisplayOptions, { showTags: false, container: containerSelector })\n      );\n    }, 150);\n  });\n\n  $('#group-container').on('hidden.bs.tab', 'ul.nav-tabs li a', function(e) {\n    var containerSelector = $(e.target).attr('href');\n    var containerId = containerSelector.substring(1);\n    if (!papersByDecision.hasOwnProperty(containerId) || !$(containerSelector).length) {\n      return;\n    }\n\n    Webfield.ui.spinner(containerSelector, { inline: true });\n  });\n\n  $('#notes \u003e .spinner-container').remove();\n  $('.tabs-container').show();\n}\n\n// Go!\nmain();\n\n});\n//# sourceURL=webfieldCode.js","query":{"id":"ICLR.cc/2021/Conference"}}},"page":"/group","query":{"id":"ICLR.cc/2021/Conference"},"buildId":"v1.0.9-1-gf539684","isFallback":false,"gip":true,"head":[["meta",{"charSet":"utf-8"}],["meta",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["meta",{"property":"og:image","content":"https://openreview.net/images/openreview_logo_512.png"}],["meta",{"property":"og:type","content":"website"}],["meta",{"property":"og:site_name","content":"OpenReview"}],["meta",{"name":"twitter:card","content":"summary"}],["meta",{"name":"twitter:site","content":"@openreviewnet"}],["title",{"children":"ICLR 2021 Conference | OpenReview"}],["meta",{"name":"description","content":"Welcome to the OpenReview homepage for ICLR 2021 Conference"}],["meta",{"property":"og:title","content":"ICLR 2021 Conference"}],["meta",{"property":"og:description","content":"Welcome to the OpenReview homepage for ICLR 2021 Conference"}]]}</script><script nomodule="" src="./ICLR 2021 Conference _ OpenReview_files/polyfills-f9ac226678fc858257de.js.下载"></script><script src="./ICLR 2021 Conference _ OpenReview_files/main-6b56d8b97b908e519b08.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/webpack-d7b2fb72fb7257504a38.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/framework.b11cd6ab3c62dae3dfb8.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/29107295.c7a36c5cb4964dc936e4.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/b637e9a5.d9354c88856ddbaf0be2.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/42e8bd60.2605cce5b4c2063fa3d3.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/55e0983c962a44019f922501f82fb92be7dc6038.b1fe41cb81db81e91237.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/181a3372a136420bacd8143b1a96dfd629357ddc.7ec9ffd88886a5000e00.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/a070395ba8e9275b849fcd4310593532831ac4b7.8744cf13ed507c350a25.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/1220f4ffb4828c6d46a13072dc033abc74519993.3d3dd7c19e661b4a6bfa.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/styles.c669c7da917090bc8543.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/_app-0232e1e8985093b3faa8.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/group-11f7765f7db1e915182b.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/_buildManifest.js.下载" async=""></script><script src="./ICLR 2021 Conference _ OpenReview_files/_ssgManifest.js.下载" async=""></script><script> </script><script>// Webfield Code for ICLR.cc/2021/Conference
window.user = {"id":"guest_1617527366270","isGuest":true};
$(function() {
  var args = {"id":"ICLR.cc/2021/Conference"};
  var group = {"id":"ICLR.cc/2021/Conference","cdate":1587911460292,"ddate":null,"tcdate":1587911460292,"tmdate":1611607761057,"tddate":null,"signatures":["ICLR.cc/2021/Conference"],"signatories":["ICLR.cc/2021/Conference"],"readers":["everyone"],"nonreaders":[],"writers":["ICLR.cc/2021/Conference"],"members":["ICLR.cc/2021/Conference/Program_Chairs","OpenReview.net/Support"],"details":{"writable":false}};
  var document = null;
  var window = null;
  var model = {
    tokenPayload: function() {
      return { user: user }
    }
  };

  $('#group-container').empty();
  

  // ------------------------------------
// Venue homepage template
//
// This webfield displays the conference header (#header), the submit button (#invitation),
// and a tabbed interface for viewing various types of notes.
// ------------------------------------

// Constants
var PARENT_GROUP_ID = 'ICLR.cc/2021';
var CONFERENCE_ID = 'ICLR.cc/2021/Conference';
var BLIND_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Blind_Submission';
var WITHDRAWN_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Withdrawn_Submission';
var DESK_REJECTED_SUBMISSION_ID = 'ICLR.cc/2021/Conference/-/Desk_Rejected_Submission';
var DECISION_INVITATION_REGEX = 'ICLR.cc/2021/Conference/Paper.*/-/Decision';
var DECISION_HEADING_MAP = {
  "Accept (Oral)": "Oral Presentations", 
  "Accept (Spotlight)": "Spotlight Presentations", 
  "Accept (Poster)": "Poster Presentations", 
  "Reject": "Withdrawn/Rejected Submissions"
  
};
var PAGE_SIZE = 25;

var HEADER = {"title": "International Conference on Learning Representations", "subtitle": "ICLR 2021", "location": "Vienna, Austria", "date": "May 04 2021", "website": "https://iclr.cc/", "instructions": "<p class='dark'>Please see the venue website for more information.<br> <p class='dark'><strong>Abstract Submission End:</strong> Sep 28 2020 03:00PM UTC-0</p><p class='dark'><strong>Paper Submission End:</strong> Oct 2 2020 03:00PM UTC-0</p>", "deadline": "", "contact": "iclr2021programchairs@googlegroups.com", "reviewers_name": "Reviewers", "area_chairs_name": "Area_Chairs", "reviewers_id": "ICLR.cc/2021/Conference/Reviewers", "authors_id": "ICLR.cc/2021/Conference/Authors", "program_chairs_id": "ICLR.cc/2021/Conference/Program_Chairs", "area_chairs_id": "ICLR.cc/2021/Conference/Area_Chairs", "submission_id": "ICLR.cc/2021/Conference/-/Submission", "blind_submission_id": "ICLR.cc/2021/Conference/-/Blind_Submission", "withdrawn_submission_id": "ICLR.cc/2021/Conference/-/Withdrawn_Submission", "desk_rejected_submission_id": "ICLR.cc/2021/Conference/-/Desk_Rejected_Submission", "public": true};

var paperDisplayOptions = {
  pdfLink: true,
  replyCount: true,
  showContents: true
};

var sections = [];

// Main is the entry point to the webfield code and runs everything
function main() {
  if (args && args.referrer) {
    OpenBanner.referrerLink(args.referrer);
  } else if (PARENT_GROUP_ID.length){
    OpenBanner.venueHomepageLink(PARENT_GROUP_ID);
  }
  Webfield.ui.setup('#group-container', CONFERENCE_ID);  // required

  renderConferenceHeader();

  renderConferenceTabs();

  load().then(renderContent).then(Webfield.ui.done);
}

// Load makes all the API calls needed to get the data to render the page
function load() {
  var notesP = Webfield.getAll('/notes', {
    invitation: BLIND_SUBMISSION_ID,
    details: 'replyCount,invitation,original'
  });

  var withdrawnNotesP = WITHDRAWN_SUBMISSION_ID ? Webfield.getAll('/notes', {
    invitation: WITHDRAWN_SUBMISSION_ID,
    details: 'replyCount,invitation,original'
  }) : $.Deferred().resolve([]);

  var deskRejectedNotesP = DESK_REJECTED_SUBMISSION_ID ? Webfield.getAll('/notes', {
    invitation: DESK_REJECTED_SUBMISSION_ID,
    details: 'replyCount,invitation,original'
  }) : $.Deferred().resolve([]);

  var decisionNotesP = Webfield.getAll('/notes', {
    invitation: DECISION_INVITATION_REGEX,
  });

  var userGroupsP;
  if (!user || _.startsWith(user.id, 'guest_')) {
    userGroupsP = $.Deferred().resolve([]);
  } else {
    userGroupsP = Webfield.getAll('/groups', {
      regex: CONFERENCE_ID + '/.*',
      member: user.id,
      web: true
    }).then(function(groups) {
      if (!groups || !groups.length) {
        return [];
      }

      return groups.map(function(g) { return g.id; });
    });
  }

  return $.when(notesP, decisionNotesP, withdrawnNotesP, deskRejectedNotesP, userGroupsP);
}

function renderConferenceHeader() {
  Webfield.ui.venueHeader(HEADER);
  Webfield.ui.spinner('#notes', { inline: true });
}

function getElementId(decision) {
  if (!decision) return decision;
  return decision.replace(/\W/g, '-')
    .replace('(', '')
    .replace(')', '')
    .toLowerCase();
}

function renderConferenceTabs() {
  sections.push({
    heading: 'Your Consoles',
    id: 'your-consoles',
  });
  var tabNames = new Set();
  for (var decision in DECISION_HEADING_MAP) {
    tabNames.add(DECISION_HEADING_MAP[decision]);
  }
  var tabArray = Array.from(tabNames);
  tabArray.forEach(function(tabName) {
    sections.push({
      heading: tabName,
      id: getElementId(tabName)
    });
  })

  Webfield.ui.tabPanel(sections, {
    container: '#notes',
    hidden: true
  });
}

function createConsoleLinks(allGroups) {
  var uniqueGroups = _.sortedUniq(allGroups.sort());

  return uniqueGroups.map(function(group) {
    var groupName = group.split('/').pop();
    if (groupName.slice(-1) === 's') {
      groupName = groupName.slice(0, -1);
    }

    return [
      '<li class="note invitation-link">',
        '<a href="/group?id=' + group + '">' + groupName.replace(/_/g, ' ') + ' Console</a>',
      '</li>'
    ].join('');
  });
}

function groupNotesByDecision(notes, decisionNotes, withdrawnNotes, deskRejectedNotes) {
  // Categorize notes into buckets defined by DECISION_HEADING_MAP
  var notesDict = _.keyBy(notes, 'id');

  var papersByDecision = {};
  for (var decision in DECISION_HEADING_MAP) {
    papersByDecision[getElementId(DECISION_HEADING_MAP[decision])] = [];
  }

  decisionNotes.forEach(function(d) {
    var tabName = DECISION_HEADING_MAP[d.content.decision];
    if (tabName) {
      var decisionKey = getElementId(tabName);
      if (notesDict[d.forum] && papersByDecision[decisionKey]) {
        papersByDecision[decisionKey].push(notesDict[d.forum]);
      }
    }

  });

  if (papersByDecision['withdrawn-rejected-submissions']) {
    papersByDecision['withdrawn-rejected-submissions'] = papersByDecision['withdrawn-rejected-submissions'].concat(withdrawnNotes.concat(deskRejectedNotes));
  }
  return papersByDecision;
}

function renderContent(notes, decisionNotes, withdrawnNotes, deskRejectedNotes, userGroups) {
  var papersByDecision = groupNotesByDecision(notes, decisionNotes, withdrawnNotes, deskRejectedNotes);

  // Your Consoles Tab
  if (userGroups && userGroups.length) {
    var consoleLinks = createConsoleLinks(userGroups);
    $('#your-consoles').html('<ul class="list-unstyled submissions-list">' +
      consoleLinks.join('\n') + '</ul>');

    $('.tabs-container a[href="#your-consoles"]').parent().show();
  } else {
    $('.tabs-container a[href="#your-consoles"]').parent().hide();
  }

  // Register event handlers
  $('#group-container').on('shown.bs.tab', 'ul.nav-tabs li a', function(e) {
    var containerSelector = $(e.target).attr('href');
    var containerId = containerSelector.substring(1);
    if (!papersByDecision.hasOwnProperty(containerId) || !$(containerSelector).length) {
      return;
    }

    setTimeout(function() {
      Webfield.ui.searchResults(
        papersByDecision[containerId],
        Object.assign({}, paperDisplayOptions, { showTags: false, container: containerSelector })
      );
    }, 150);
  });

  $('#group-container').on('hidden.bs.tab', 'ul.nav-tabs li a', function(e) {
    var containerSelector = $(e.target).attr('href');
    var containerId = containerSelector.substring(1);
    if (!papersByDecision.hasOwnProperty(containerId) || !$(containerSelector).length) {
      return;
    }

    Webfield.ui.spinner(containerSelector, { inline: true });
  });

  $('#notes > .spinner-container').remove();
  $('.tabs-container').show();
}

// Go!
main();

});
//# sourceURL=webfieldCode.js</script><script src="./ICLR 2021 Conference _ OpenReview_files/index-8d5ab195e774e3fc0705.js.下载"></script></body></html>
# 今日头条实习

## 论文

1. 介绍你的论文。
   1. 对抗性dropout是怎么做的。
2. 跟踪领域有什么新的研究方向？

## 计算机视觉

1. 检测网络的回归损失为什么用L1损失不用L2损失。
2. 深度学习如何防止梯度爆炸/消失等现象？
   1. 在目标函数中添加对于参数的惩罚项以减小模型的capacity，常见的有l2正则化（用于减小权重大小）和l1正则化（使权重更稀疏）（对于每一个元素的绝对值的求和）
   2. 数据扩增（平移图像，添加噪声，旋转，色调偏移）
   3. 多任务学习
   4. 早停
   5. 稀疏表示（表征（隐藏层）更稀疏）
   6. 通过整合多个模型来减小泛化误差
   7. Dropout
   8. Adversarial Training

## 编程基础

1. python是值传递还是指针传递？
   1. 参数传递：把值传过去。数字、字符或者元组等不可变对象类型都属于**值传递**。
   2. 值传递：把地址传过去。字典dict或者列表list等可变对象类型属于**引用传递**。
2. 线程和进程的区别？
   1. 进程有独立的地址空间。
   2. 线程之间没有单独的地址空间。

## 代码

1. 手写kmeans算法。

## 收尾

1. 你有什么要问我的？

#  阿里巴巴实习

## 论文

1. 如何解决出画面问题？全局搜索。全局搜索慢怎么办？与局部搜索结合。
2. 你做多目标跟踪吗？不，做单目标，不过多目标也了解一些，如。。。
3. 介绍sort，deepsort，center track。

## 项目

1. 项目精度怎么样？取决于数据。

## 计算机视觉

1. 不同距离如汉明距离，L2距离等的区别。
   1. 欧氏距离：直线距离。
   2. 曼哈顿距离：实际驾驶距离。
   3. 余弦距离
   4. 汉明距离：两个等长字符串在对应位置上不同字符的数目。
2. 牛顿法和随机梯度的区别：牛顿法是二阶收敛，梯度下降是一阶收敛。梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。

## 代码

1. 有几种方法可以反转链表。

## 收尾

1. 可否到杭州实习？
2. 你有什么要问我的？有什么落地场景？部署怎么办？黑暗场景怎么办？处理流视频还是正常视频？是否有转正机会？基本都是转正的，校招很少。

# 自己整理

## 深度学习

### 归一化层

batchNorm是在batch上，对NHW做归一化。出发点：不同的batch，分布不一样。

layerNorm在通道方向上，对CHW归一化：对深度网络的某一层的所有神经元的输入按以下公式进行normalize操作

instanceNorm在图像像素上，对HW做归一化

GroupNorm将channel分组，然后再做归一化；

### 优化算法

#### Momentum

Momentum的思想就是模拟物体运动的惯性：当我们跑步时转弯，我们最终的前进方向是由我们之前的方向和转弯的方向共同决定的。Momentum在每次更新时，保留[一部分上次的更新方向](https://blog.csdn.net/qsczse943062710/article/details/76763739)

#### 共轭梯度法

### 数据不平衡问题

1. 采样
2. 权重
3. 数据扩增
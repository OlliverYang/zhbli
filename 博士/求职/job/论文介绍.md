## 全局时空感知：长期跟踪系统

### 现有算法的缺点

孪生跟踪器倾向于使用局部搜索机制：在以先前帧的目标位置为中心的小邻域内搜索目标，以确定其当前位置。

但是，局部搜索机制存在一些缺陷。 首先，如果前一帧中目标位置的预测由于挑战性的照明变化，运动模糊等原因而偏离了预测，则可能会导致不可逆的累积误差，因为当前帧中生成的搜索区域可能不会覆盖导致在随后的帧中完全失败。 第二，基于局部搜索机制的跟踪器很难满足长期跟踪的需求。 在长期跟踪情况下，目标经常重新进入并重新退出屏幕。 由于当目标离开并重新进入屏幕时，跟踪器无法设置正确的搜索区域，因此由于错误的搜索区域而没有覆盖目标，跟踪器通常无法检索目标。

### 本文的解决方案

本文提出的跟踪器通过在在整个图像平面上搜索目标，来减少累积误差并提高鲁棒性。

在跟踪过程中，我们的跟踪器始终能够感知整个图像上的目标。 因此，即使跟踪器由于具有挑战性的目标外观变化而犯了一个错误，一旦目标的外观恢复正常，仍然可以及时检索目标。 尤其是在长期视野外消失的情况下，当目标离开屏幕时跟踪器无法在完整图像中找到目标，而当目标从任何位置重新进入屏幕时，我们的跟踪器可以继续工作。

此外，我们还添加了基于CNN的轨迹预测模块，可利用目标的时间运动信息来减轻干扰因素的干扰。这两个空间和时间模块利用高级外观信息和互补轨迹信息来提高跟踪的鲁棒性。

与简单设计的手工策略不同，我们提出的基于CNN的运动模型经过端到端训练，可以使用其历史轨迹信息和当前目标外观信息来预测目标当前位置分布。

### Method

我们将视觉对象跟踪任务分解为首先提取候选目标，然后使用运动模型消除干扰物。因此，我们可以通过设计更精确的跟踪组件和更鲁棒的运动模型来获得更好的性能。

（1）我们使用整个图像而不是小的图像块作为跟踪器的输入，以为其提供全局空间信息。 （2）为了更好地感知全局空间信息，我们提出了一个两阶段跟踪组件，该组件能够检测在视觉上类似于ground truth的候选目标。 （3）为了感知时间信息，我们提出了一种运动模型，该模型能够通过预测位置分布以获得最终的跟踪结果来排除干扰物。

网络基于Faster RCNN构建。输入包括完整的搜索图像和模板图像。利用逐通道互相关将搜索特征和模板特征进行融合。统合特征送入RPN网络得到若干个候选框。接下来，在融合特征上执行ROI Align得到每个候选目标的7*7特征，然后进行分类和边框回归，确定topK个表观与真实物体接近的目标，送入运动模型。

运动模型的作用是通过学习物体的位置分布，剔除干扰物。

为了对历史轨迹信息进行建模，将物体的位置信息转换为2维高斯热图，并根据时间顺序串接，以获得通道为k的轨迹张量。我们的运动模型不仅利用历史轨迹信息进行预测，而且还考虑了当前帧的出现信息。为此，将当前帧的RGB图像与轨迹张量连接起来，以获得通道尺寸为（3 + k）的张量作为轨迹预测网络的输入。网络的输出是表示当前帧位置分布的热图，得分最高的候选目标就是当前帧的跟踪结果。

## 孪生跟踪器的端到端时间聚合

### Introduction

在孪生跟踪器中，视频中的目标由于运动模糊，遮挡等原因导致表观变差，可能导致学习到的特征可能没有那么大的判别力。

以前的工作尝试过很多方法来改进特征表示。但是，大多数算法都基于仅从当前帧裁剪的特征执行跟踪，这限制了暹罗跟踪器的功能。

实际上，视频具有关于目标的丰富信息，并且这种时间信息是视频理解和跟踪的重要基础。

在本文中，我们介绍了一种新的孪生跟踪器，通过聚合来自相邻帧的时间信息来改善每帧的特征。 这种时间融合策略使暹罗跟踪器能够处理较差的对象外观，例如运动模糊，遮挡等。此外，我们在暹罗网络中引入了对抗性dropout模块，以端到端的方式学习鲁棒的的目标特征。 

### 时间聚合模块

训练时，一个batch包括同一视频中的相邻几帧，并按时间排序，因此我们可以将batch维度视为时间维度。沿着时间维度移动通道，使得当前时刻的特征捕获相邻帧的信息。该模块可以直接插入到主干中，而无需更改网络的其他部分。

### 对抗性dropout

基于散度最大化来预测对抗性dropout模板。具体来说，就是寻找一个模板，对该模板进行尽量小的修改，使得施加该模板后的网络输出，与施加一个随机模板的网络输出相比，散度可以最大化。

寻找该对抗性模板的方法分为两步：第一步，为特征中的每一个元素计算一个影响值，该值与元素对散度的增加量成正比。

第二步，在不违反边界条件的情况下，调整模板的值，使得散度达到最大。

根据该模板对目标特征进行选择性删除。
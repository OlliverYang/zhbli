## 逻辑回归

$$
P(y=1|x;\theta)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}
$$

用于二分类任务。$g$是sigmoid函数。常用Binary Cross-Entropy优化。

## 线性回归

有$m$个样本，每个样本有$n$维特征和一个标签$y$：
$$
(x^1_1,x_2^1,...,x_n^1,y^1),\\(x^1_2,x_2^2,...,x_n^2,y^2),\\
...,\\
(x^m_1,x_m^1,...,x_m^1,y^m)
$$
我们可以假设一个多元一次函数来表示$y$和$\bf x$的关系：
$$
h_\theta(\textbf x)=\theta_0+\theta_1 x_1+\theta_2 x_2+...++\theta_n x_n
$$
损失函数：均方误差。

优化方法：最小二乘法或梯度下降法。

## 偏差与方差

拿射击打靶距离，如果都是瞄向中心，但分布很散，则方差大；如果弹孔很集中，但射偏了，则偏差大。

## 集成学习

集成学习（ensemble learning）通过构建并结合多个学习器来完成学习任务。

如果弱学习器之间存在强依赖该系且是串行生成，代表为boosting（如AdaBoost）;如果不存在强依赖关系且并行生成，则是bagging（如随机森林）。bagging减小方差，boosting减小偏差。

### boosting

训练集用初始样本权重训练出一个弱学习器1，根据学习误差率更新样本权重，提高误差率高的样本点的权重，基**于新权重训练集来训练弱学习器2**，如此反复达到要求。

### bagging

用自助法从训练集中经过T次采用得到T个采样集，用这T个采样集分别训练出T个弱分类器，通过结合策略生成强学习器。

### AdaBoost

首先训练第一个弱分类器。对于分错的样本，减小权重，对于分对的样本，提高权重。

### 决策树

决策树在每一个节点利用一个特征进行一次分类。需要设计选择哪个特征做根节点。选择**信息增益率**最大的节点当作根节点。

构造决策树的基本思想是，随着树的加深，节点的熵迅速降低（熵越低，意味着不确定性约小，即分类效果越好），且熵降低的速度越快越好。

### 随机森林

随机森林中的每棵树都会得出一个分类结果，然后把这些分类结果以投票的形式保存下来。随机森林选出票数最高的分类结果，令其为这个森林的分类结果。在回归问题中，把每一棵决策树的输出进行平均得到结果。

### 随机森林和GBDT的区别

随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回的均匀取样，而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重）

## SVM

支持向量机是一种二分类模型。SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。如下图所示， ![[公式]](https://www.zhihu.com/equation?tex=+%5Cboldsymbol%7Bw%7D%5Ccdot+x%2Bb%3D0+) 即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。

<img src="https://pic1.zhimg.com/80/v2-197913c461c1953c30b804b4a7eddfcc_1440w.jpg" alt="img" style="zoom:33%;" />

### SVM的损失函数

使用铰链损失函数。也就是说，数据点如果被正确分类，损失为0，如果没有被正确分类，损失为z。

<img src="https://upload-images.jianshu.io/upload_images/4155986-bc24e44cce20669b.png?imageMogr2/auto-orient/strip|imageView2/2/w/610/format/webp" alt="img" style="zoom: 50%;" />

### SVM的优化方法

对于线性可分数据集，SVM模型的求解最大分割超平面问题可以表示为以下约束最优化问题：

![[公式]](https://www.zhihu.com/equation?tex=+%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%7D%7B%5Cmin%7D%5C+%5Cfrac%7B1%7D%7B2%7D%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5E2+)

![[公式]](https://www.zhihu.com/equation?tex=+s.t.%5C+%5C+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+%5Cge+1%2C%5C+i%3D1%2C2%2C...%2CN+)

这是一个含有不等式约束的凸二次规划问题，可以对其使用拉格朗日乘子法得到其对偶问题（dual problem）。对于这个问题，我们有更高效的优化算法，即序列最小优化（SMO）算法。

对于线性不可分数据集，采用hinge损失，将原优化问题改写为：

![[公式]](https://www.zhihu.com/equation?tex=%5Cunderset%7B%5Cboldsymbol%7Bw%2C%7Db%2C%5Cxi+_i%7D%7B%5Cmin%7D%5C+%5Cfrac%7B1%7D%7B2%7D%5ClVert+%5Cboldsymbol%7Bw%7D+%5CrVert+%5E2%2BC%5Csum_%7Bi%3D1%7D%5Em%7B%5Cxi+_i%7D+)

![[公式]](https://www.zhihu.com/equation?tex=+s.t.%5C+%5C+y_i%5Cleft%28+%5Cboldsymbol%7Bw%7D%5Ccdot+%5Cboldsymbol%7Bx%7D_%7B%5Cboldsymbol%7Bi%7D%7D%2Bb+%5Cright%29+%5Cge+1-%5Cxi+_i+)

![[公式]](https://www.zhihu.com/equation?tex=+%5C+%5C+%5C+%5C+%5C+%5Cxi+_i%5Cge+0%5C+%2C%5C+i%3D1%2C2%2C...%2CN+)

跟线性可分求解的思路一致，同样先用拉格朗日乘子法得到拉格朗日函数，再求其对偶问题。
## 均方误差

$$
loss=(y-y')^2
$$

## 熵

### 信息量与信息熵

信息奠基人香农（Shannon）认为“信息是用来[消除](https://baike.baidu.com/item/消除)随机[不确定性](https://baike.baidu.com/item/不确定性)的东西”。也就是说衡量信息量大小就看这个信息消除不确定性的程度。

“太阳从东方升起了”这条信息没有减少不确定性。因为太阳肯定从东面升起。这是句废话，信息量为0。

“吐鲁番下中雨了”（吐鲁番年平均降水量日仅6天）这条信息比较有价值，为什么呢，因为按统计来看吐鲁番明天不下雨的概率为98%（1-6/300），对于吐鲁番下不下雨这件事，首先它是随机不去确定的，这条信息直接否定了发生概率为98%的事件------不下雨，把非常大概率的事情（不下雨）否定了，即消除不确定性的程度很大，所以这条信息的信息量比较大。这条信息的情形发生概率仅为2%但是它的信息量去很大，上面太阳从东方升起的发生概率很大为1，但信息量确很小。

从上面两个例子可以看出：**信息量的大小和事件发生的概率成反比**。

将事件$x_0$的**信息量**定义如下（其中$p(x_0)$表示事件$x_0$发生的概率）：
$$
I(x_0)=-log(p(x_0))
$$
<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMyLnpoaW1nLmNvbS92Mi0yMjBmNzg2NzA3MjFjYzRjYjIzOGE2NDUwNzBlMTk2NV9iLmpwZw?x-oss-process=image/format,png" alt="img" style="zoom: 33%;" />

信息量度量的是一个**具体事件发生所带来的信息**，而熵则是在结果出来之前对**可能产生的信息量的期望**——考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。即

### Entropy

$$
H(X)=-\sum_{i=1}^{N}p(x_i)log(p(x_i))
$$

$N$表示事件可能发生的情况总数。$p(x_0)$表示事件$x_0$发生的概率。

### Binary Cross-Entropy

<img src="C:\Users\zhbli\AppData\Roaming\Typora\typora-user-images\image-20201103141524330.png" alt="image-20201103141524330" style="zoom:50%;" />

用于训练二元分类器。$N$表示样本数。$y$表示样本的标签。$p(y)$表示样本的预测。

| <img src="C:\Users\zhbli\AppData\Roaming\Typora\typora-user-images\image-20201103142359325.png" alt="image-20201103142359325" style="zoom:50%;" /> | <img src="C:\Users\zhbli\AppData\Roaming\Typora\typora-user-images\image-20201103142200894.png" alt="image-20201103142200894" style="zoom:50%;" /> |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| 正样本 loss=-log(y)。预测接近1时，loss接近0。预测接近0时，loss接近1。 | 负样本 loss=-log(1-y)。预测接近1时，loss接近1。预测接近0时，loss接近0。 |

### KL散度（相对熵）

相对熵又称KL散度,如果我们对于同一个随机变量$X$有两个单独的概率分布$P(x)$和$Q(x)$，我们可以使用 KL 散度来衡量这两个分布的差异。

在机器学习中，$p$往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。$q$用来表示模型所预测的分布，比如[0.7,0.2,0.1]。

KL散度的计算公式：
$$
D_{KL}(p||q)=\sum_{i=1}^{N}p(x_i)log(\frac{p(x_i)}{q(x_i)})
$$
$D_{KL}$的值越小，表示$q$分布和$p$分布越接近。

### Cross Entropy

对KL散度公式变形可以得到：
$$
D_{KL}(p||q)=
\sum_{i=1}^{N}p(x_i)log(p(x_i))-\sum_{i=1}^{N}p(x_i)log(q(x_i))\\
=-H(X)+[-\sum_{i=1}^{N}p(x_i)log(q(x_i))]
$$

由于KL散度中的前一部分恰巧就是$p$的熵，$p$代表label或者叫ground truth，故$−H(X)$不变，故在优化过程中，只需要关注交叉熵就可以了，所以一般在机器学习中直接用用交叉熵做loss来评估模型：
$$
H(p,q)=-\sum_{i=1}^{N}p(x_i)log(q(x_i))
$$

注意，标签$p$是one-hot编码形式。
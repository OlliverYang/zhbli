## 最小二乘法与岭回归

对于一个回归问题$X\theta=y$，基于均方误差（$||X\theta-y||^2$）最小化来进行模型求解的方法称为“最小二乘法”。

最小二乘法分为线性最小二乘法和非线性最小二乘法，直接套闭式解的公式即可：
$$
\theta = (X^TX)^{-1}X^Ty。
$$
岭回归是一种改良的最小二乘估计法，对损失添加了一个正则化项，变为：
$$
||X\theta-y||^2+||\Gamma\theta||^2,
$$
其中$\Gamma=\alpha I$，于是：
$$
\theta(\alpha)=(X^TX+\alpha I)^{-1}X^Ty.
$$
$\theta(\alpha)$随$\alpha$变化的轨迹，称为岭迹。

## 牛顿法

牛顿法是常用来解决方程求解和最优化问题，求解问题通常是利用一阶导数求解方程为0的解，优化问题通常是利用二阶导数求解一阶导数为0的解。

从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。

## 拟牛顿法

拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。

## **SGD**

设$t$时刻网络的参数为$w_t$，学习率为$\alpha$。

$t$时刻损失函数关于当前参数的梯度为：
$$
g_t = \nabla loss = \frac{\part loss}{\part w_t}
$$
使用SGD更新网络参数：
$$
w_{t+1}=w_t-\alpha * g_t
$$
缺陷：对于某些变量，已经优化到了极小值附近，但是有的变量仍然在梯度很大的地方，这时候一个**统一的全局学习率**是可能出现问题的。如果学习率太小，则梯度很大的变量会收敛很慢，如果学习率太大，已经趋于收敛的变量可能会不稳定。

## 动量法

动量是物理学中的概念，一般指物体在它运动方向上保持运动的趋势，是该物体质量和速度的乘积。**动量法是用之前积累梯度来代替真正的梯度**，这样，每个参数实际更新差值取决于最近一段时间内梯度的加权平均值。**当某个参数在最近一段时间内的梯度方向不一致时，参数更新幅度变小；梯度方向一致时，更新幅度变大，起到加速作用。一般而言，在迭代初期，梯度方向比较一致，动量法会起到加速作用；在迭代后期，梯度方向会不一致，在收敛值附近震荡，动量法起减速作用，增加稳定性。**

## SGDM

与SGD相比，添加了一阶动量。一阶动量$m_t$表示了各时刻梯度方向的指数滑动平均值（xponential Moving Average,EMA）：
$$
m_t=\beta m_{t-1} + (1-\beta)g_t
$$
其中，$\beta$是动量因子，一般取0.9。

使用SGDM更新网络参数：
$$
w_{t+1}=w_t-\alpha * m_t
$$

## AdaGrad

在SGD的基础上增加了二阶动量，可以对模型中的每个参数分配自适应学习率，即对每个变量用不同的学习率。

二阶动量为从开始到$t$时刻梯度平方的累计和：
$$
V_t = \sum_{j=1}^t g_j^2
$$
使用AdaGrad更新网络参数：
$$
w_{t+1}=w_t-\alpha \cdot g_t / \sqrt{V_t}
$$
SGD以相同的学习率去更新$\theta$的各个分量。而深度学习模型中往往涉及大量的参数，不同参数的更新频率往往有所区别。对于更新不频繁的参数（典型例子：更新 word embedding 中的低频词），我们希望单次步长更大，多学习一些知识；对于更新频繁的参数，我们则希望步长较小，使得学习到的参数更稳定，不至于被单个样本影响太多。

与SGD的核心区别在于计算更新步长时，增加了分母：**梯度平方累积和的平方根**。此项能够累积各个参数的历史梯度平方，频繁更新的梯度，则累积的分母项逐渐偏大，那么更新的步长(stepsize)相对就会变小，而稀疏的梯度，则导致累积的分母项中对应值比较小，那么更新的步长则相对比较大。

这一方法在稀疏数据的场景下表现很好。

AdaGrad的**缺点**是来自分母项的对梯度平方不断累积，随之时间步地增加，分母项越来越大，最终导致学习率收缩到太小无法进行有效更新。

## RMSprop

在 Adagrad 中，$v_t$ 是单调递增的，使得学习率逐渐递减至 0，可能导致训练过程提前结束。为了改进这一缺点，可以考虑在计算二阶动量时不累积全部历史梯度，而只关注最近某一时间窗口内的下降梯度：
$$
V_t = \gamma(V_{t-1})+(1-\gamma)g^2_t
$$
其二阶动量采用指数移动平均公式计算，这样即可避免二阶动量持续累积的问题。

计算梯度平方的指数移动平均数。

优势：能够克服AdaGrad梯度急剧减小的问题。	

**Adam**：不仅存储了AdaDelta先前平方梯度的指数衰减平均值，而且保持了先前梯度M(t)的指数衰减平均值。

Adam可以认为是 RMSprop 和 Momentum 的结合。即：同时引入一阶动量和二阶动量。
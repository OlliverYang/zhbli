\chapter{暹罗跟踪者的端到端时间特征集合}\label{chap:end}

\section{摘要}
尽管暹罗网络已经证明了对象跟踪性能的显着提高，但是如何在暹罗跟踪器中利用时间信息尚未得到广泛研究。在本文中，我们介绍了一种新颖的暹罗跟踪体系结构，该体系结构配备了一个时间聚合模块，该模块通过聚合来自相邻帧的时间信息来改善每帧的特征。这种时间融合策略使暹罗跟踪器可以处理较差的对象外观，例如运动模糊，遮挡，\ textit {etc}。此外，我们在暹罗网络中整合了对抗性辍学模块，以端到端的方式计算出具有歧视性的目标特征。全面的实验表明，所提出的跟踪器的性能优于最新的跟踪器。

\section{简介}
\label{sec:intro}
视觉对象跟踪是估计视频序列每一帧中任意目标状态的任务。 
%Recently, siamese network [] is introduced into visual tracking community, which has already been applied in many applications [].
%The key idea of siamese tracking architecture is, the target and the search image share the same feature extractor, then use correlation to tell the target position.
最近，暹罗网络已经证明了对象跟踪性能的显着提高。但是，由于视频（图\ref{fig:visulization}）中的对象外观变差，例如运动模糊，遮挡等，因此学习到的通用表示可能没有那么大的判别力。
研究人员尝试了不同的方法来改进特征表示。
例如，
%RASNet \cite{wang2018learning} explores diverse attention mechanisms to adapt the offline learned feature representations to a specific tracking target.
SA-Siam \cite{he2018twofold} 分别训练两个分支，以保持语义/外观特征的异构性。
在 DaSiamRPN \cite{zhu2018distractor}中，设计了一个新颖的可感知干扰物的增量学习模块，该模块可以有效地将常规嵌入转移到当前视频域，并在推理过程中增量捕获目标外观变化。
SiamRPN++ \cite{SiamRPN++} 引入了一种简单而有效的采样策略，以更强大的深度架构来驱动暹罗跟踪器。
这些努力产生了一些影响，并提高了最新的准确性。
%These improvements are useful.
%However, all these effort ignores a important problem: they ignore to merge the inter-frame feature during the training phase.
%One important solution is use the temporal information.
%The use of temporal information is important because, when one frame is bad, the adjusting frame can be used. While several method trying to use the temporal information, the method is naive.
%(The bug is, I cannot find any temporal information used in siamese trackers. (I cannot believe it!!! Check it again.--Checked)) For example, A use a time memory to store the appearance information. B use a simple feature average during test. C use history positional information to predict the object position in current frame. However, all these method cannot handle the temporal information during training, which lead to sub-optimal result. To solve this problem, 
但是，上述所有暹罗算法都基于仅从当前帧裁剪的特征执行跟踪，这限制了暹罗跟踪器的功能。

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{Img/end/visulization.pdf}
    \caption{将我们的方法与SiamMask和SiamFCv2进行比较。示例框架来自GOT-10k测试装置。与现有方法相比，我们的方法可有效处理不良的对象外观。}
    \label{fig:visulization}
\end{figure}

实际上，视频具有关于目标的丰富信息，并且这种时间信息是视频理解和跟踪的重要基础。
例如，在视频对象检测中，FGFA \cite{zhu2017flow}在特征级别上利用了时间一致性。它通过沿运动路径聚集附近的特征来改善每帧特征，从而提高了视频识别精度。
在视频对象分割中，STCNN \cite{xu2019spatiotemporal}引入了时间相干模块，该模块着重于捕获动态外观和运动提示以提供对象分割的指导。
在基于区分性相关过滤器的对象跟踪中，FlowTrack \cite{zhu2018end} 专注于利用连续帧中的丰富流信息来改善特征表示和跟踪精度。
但是，如何在暹罗跟踪器中利用时间信息尚未得到广泛研究。

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.0\textwidth]{Img/end/net_v3.pdf}
    \caption{
    我们的两阶段SiamTFA概述。
    %At the first stage, a siamese FPN equipped with a temporal aggregation module provides features of the template image and the search image. Then the feature modulation module is used to merge the features and generate proposals.
    %proposals of the object given in the first-frame bounding box. 
    %At the second stage, 
    提案生成阶段旨在生成视觉上类似于给定模板目标的提案。精炼阶段旨在从候选提案中选择目标。}
    \label{fig:SiamTFA}
\end{figure*}

% key words: temporal modeling, model the temporal information, temporal fusion, inter-frame information, improve the feature representation, temporal aggregation, poor object appearance
在本文中，我们旨在充分利用暹罗跟踪器中的时间信息。
%(We introduce a novel siamese tracking architecture equipped with a temporal aggregation module to make full use of inter-frame information.)
%(In this work, to deal with the issues raised above, we propose a novel tracker to aggregate features of adjacent frames by ...)
%(In this paper, we develop an end-to-end temporal aggregation module in siamese architecture to utilize both the power of siamese trackers and the inter-frame information.)
我们介绍了一种新颖的暹罗跟踪体系结构，该体系结构配备了时间聚合模块，该模块通过聚合相邻帧中的特征来改善每帧的特征。
这种时间融合策略使暹罗跟踪器能够处理不良的对象外观，例如运动模糊，遮挡等。
为此，我们在暹罗网络的主干中沿时间维度 \cite{lin2019tsm} 移动频道。
请注意，由于视频运动 \cite{zhu2017flow}，同一对象的特征通常不会在整个帧上在空间上对齐，因此仅对残差层 \cite{lin2019tsm} 进行时间平移，以保持暹罗语的空间特征学习能力追踪器。
与其他时间融合方法 \cite{tao2016siamese, gladh2016deep}不同，引用的方法能够在大规模数据集上进行端到端的训练。
建议的时间聚合模块在暹罗网络中进行了端到端的培训。
%The proposed temporal aggregation module is trained end-to-end in the siamese network.
%(This module enables our module to be trained end-to-end on large-scale datasets.)
此外，我们的时间融合方法易于实施，而无需更改暹罗跟踪架构或使用光流 \cite{zhu2018end}。

%improve the generalization ability on bad object.
%Our insight is that, if we only rely on the traditional training method, the bad image in the training data will be over wheeled by the easy cases. So the tracker shows bad performance when meeting with bad image during testing. To solve this problem, we take advantage of the recent progress in adversarial learning to increase the robustness of object features.
%Our method employs adversarial dropout to increase the robustness of object features. 
%注意，由于该模块在文章中占据次要位置，所以不应该写目前方法存在的缺点，而应该直接写用了这个模块后起到的作用。
%感觉概括性的优点不太好想，所以可以多些几句实现细节。这就要求我们弄懂代码原理。
为了提高目标功能的鲁棒性，我们在暹罗跟踪网络中进一步整合了一个对战辍学 \cite{park2018adversarial} 模块。
% add adversarial dropout to enforce the tracker to learn difficult cases. 
% (We further propose a adversarial training module to enforce the tracker to learn difficult cases. The motivation is that ... This is achieved by computing the adversarial masks based on diversity maximum. This simple form of feature selection improves the discrimination power of the tracker.)
%Specifically, we calculate the diversity of two dropouts, and make the loss max to learn the difficult dropout mask.
具体来说，我们首先根据散度最大值预测对抗性辍学蒙版。
然后，我们旨在最大程度地减少随机删除的特征和对抗删除的特征之间的差异。
%The effect of this module can be interpreted from both the dropout and from the adversarial training perspectives.
该模块具有辍学和对抗训练的优势：辍学使我们的暹罗网络在训练过程中随机断开神经单元，以防止目标特征的共同适应；而对抗训练则使我们的跟踪器学习困难的情况。

\iffalse
Our contributions are three folds:
(1) We introduce a novel siamese tracking architecture equipped with a temporal aggregation module, which improves the per-frame features by aggregating features of adjacent frames.
(2) We incorporate the adversarial dropout module in our tracker to improve the discrimination power of the siamese tracking network.
(3) Experiments on GOT-10k and UAV20L tracking datasets show that the proposed method performs favorably against existing state-of-the-art methods (Fig. \ref{fig:visulization}).
\fi
\iffalse
\begin{itemize}
    \item We introduce a novel siamese tracking architecture equipped with a temporal aggregation module, which improves the per-frame features by aggregating features of adjacent frames.
    \item We utilize the adversarial dropout module to improve the discrimination power of the siamese tracking network.
    \item Experiments on GOT-10k and UAV20L tracking datasets show that the proposed method performs favorably against existing state-of-the-art methods.
\end{itemize}
\fi

\section{建议的方法}
\label{sec:method}
%应该提及我们的跟踪器的损失函数包括：对抗性损失，加上常规跟踪损失。
%In this section, we will introduce the proposed tracking method, SiamTFA (Fig. \ref{fig:SiamTFA}). 
%The main feature is:
%(1) A strong backbone. Use FPN and new-proposed feature modulation.
%(2) Equipped with 2 useful module: temporal aggregation module and adversarial dropout module to improve the robustness of features.
%In the following, three sub-section will be introduced: (1) Overview of tracking (2) Temporal aggregation module (3) Adversarial dropout module.
%网络结构怎么描述？可以参考ATOM。
%Our SiamTFA bases on a strong feature extractor -- FPN.
%We insert our temporal aggregation module in the backbone. Details of this module will be introduced at sec 3.2.
%Be robust to object size.
%FPN: handle different object size.
%modulator vector: handle different template object size.
%xcorr depth-wise: stage1 merge.
%element wise product: stage2 merge
% 介绍输入
%The input of the network is ... We use notations ... to denote ..., respectively. ... has a size of ...
%SiamTFA is composed of ... and ...
%Inspired by the great success of siamese trackers \cite{SiamRPN++, Wang2018SiamMask}, we also construct our SiamTFA based on the siamese architecture.
在本节中，我们将介绍拟议的基于暹罗体系结构的跟踪方法，即SiamTFA（图\ref{fig:SiamTFA}），其灵感来自暹罗跟踪器 \cite{SiamRPN++, Wang2018SiamMask}的巨大成功。
具体而言，SiamTFA将图像对作为输入，包括模板图像和搜索图像。模板图像是根据地面真实边界框从初始帧裁剪的图像块。搜索图像是视频其余部分中的一整帧。两个输入共享相同的特征提取器和参数。
受两阶段检测范例 \cite{ren2015faster}成功的启发，我们的暹罗跟踪器也是一种两阶段方法。第一阶段旨在生成在视觉上类似于给定模板目标的建议。在这个阶段，我们介绍一个时间聚合模块
%(Fig. \ref{fig:TSM}) 
增强时间信息（请参见\ref{sec:stage1})。第二阶段旨在从候选提案中确定目标。
% select the target from candidate proposals. 
在这一阶段，我们插入一个对抗性辍学模块以学习更强大的功能（第 \ref{sec:stage2}节）。

\subsection{临时汇总模块}
\label{sec:stage1}
% Note that 'stage1' and 'stage2' is a local term. You should introduce the global architecture which includes 'stage1' and 'stage2' first.
% You need to say something about ‘siamese' and the 'input'.
提案生成阶段包括3个组件：（1）特征提取器，（2）时间聚合模块和（3）特征调制模块。
% 需要综述三个模块在stage1中的作用。
特征提取器分别为搜索图像和模板图像生成搜索特征和模板特征。时间聚合模块被集成到特征提取器中以利用时间信息。特征调制模块合并搜索特征和模板特征以识别候选目标。

\textbf{特征提取器} 为了处理目标的比例变化，我们使用Res50-FPN \cite{lin2017feature} 作为特征提取器。
% You need to describe FPN here. What is FPN?
\textbf{F}eature \textbf{P}yramid \textbf{N}etwork (FPN) 利用深层卷积网络固有的多尺度金字塔层次结构来构建具有少量额外成本的特征金字塔。
我们的暹罗FPN将模板图像和搜索图像作为输入。对于搜索图像，FPN以完全卷积的方式在多个级别上输出按比例大小的特征图。
%The input of the feature extractor is an image pair consists of a template image and a search image. The search image is resized to min size 800. The template is cropped according to ground truth bounding box and resized into 400*400. According to the siamese architecture, the template/search image share the same parameters. 
%For the search image, we got 5 different features and they have strides of \{4, 8, 16, 32, 64\} pixels with respect to the input image. For template branch, we use the last layer of FPN with size 7*7. 
我们将搜索图像的输出表示为 $F_{x} = \{f_{x}^i\}_{i=1:5}$，并注意它们的步幅为 \{4, 8, 16, 32, 64\} 相对于输入搜索图像为像素。
对于模板图像，我们使用 FPN 最后阶段的输出作为模板特征，其空间大小为 $7 \times 7$。
%Different from traditional trackers, our template/search features have temporal information. The reason is because we insert the temporal shift module into the backbone, which will be introduced next.

\textbf{临时汇总模块}
%Different from traditional trackers, we use the rich information about the same object instance by 
最受欢迎的暹罗跟踪器 \cite{SiamRPN++, Wang2018SiamMask} 使用静止图像进行预测。% However, tracking on single frame generates unstable results and fails when appearance is pool \cite{zhu2017flow}. 
这限制了这些暹罗跟踪器的能力。
一方面，单帧跟踪会产生不稳定的结果，并且在外观不佳时会失败（图 \ref{fig:visulization})；另一方面，
时间相邻帧可以提供有关目标的更多信息。
%the video has rich information about the target. 
因此，我们旨在通过汇总相邻帧的特征来改善每帧特征。
具体来说，我们将一个时间聚合模块插入特征提取器的最后一个阶段。
% We have to cite TSM, because we use this idea.
为了对时间信息进行建模，一批图像是同一视频中的几个相邻帧，并按时间排序，因此我们可以将批次维度视为时间维度。假设特征提取器最后阶段的特征映射为 $f \in \mathbb R ^ {T \times C \times H \times W}$。
对于每次 $t \leq T$，我们首先将特征 $f^t \in \mathbb R ^ {C \times H \times W}$ 沿通道维度分为三部分：$f_{1:K}^t \in \mathbb R ^ {K \times H \times W}$，$f_{(K+1):2K}^t \in \mathbb R ^ {K \times H \times W}$ 和 $f_{(2K+1):C}^t \in \mathbb R ^ {(C-2K) \times H \times W}$。
然后，按照 \cite{lin2019tsm}沿时间维度移动通道：
% Then The aggregated function at time $t$ is:
\begin{equation}
    f_{agg}^t = \mathcal{C}(f_{{1:K}}^{t-1}, f_{(K+1):2K}^{t+1}, f_{(2K+1):C}^{t}),
\end{equation}
其中 $\mathcal{C}(\cdot)$ 是串联操作。根据 \cite{lin2019tsm}，移位操作仅在残差层执行，以保留暹罗跟踪器的空间特征学习能力。
请注意，聚合特征 $f_{agg}^t$ 具有与 $f^t$相同的形状，因此我们可以将此模块直接插入到主干中，而无需更改网络的其他部分。
而且，此操作仅需要进行数据移动，因此它在计算上是免费的，并且可以端到端进行训练。
% Say the effect of this module.
%After performing such channel fusion, rich target information in adjacent frames can be aggregated into the current frame. For example, if the current frame is blurred, target information that is not blurred in adjacent frames can be used.

\iffalse
\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{images/TSM_v1.pdf}
    \caption{Temporal aggregation module.}
    \label{fig:TSM}
\end{figure}
\fi

\textbf{功能调制模块} 在获得模板特征 $f_{z}$ 和搜索特征金字塔 $F_{x} = \{f_{x}^i\}_{i=1:5}$之后，被调制以生成特定于目标的特征。
具体而言，使用全局平均池生成生成调制矢量 $f_{avg}$ 从 $f_{z}$，该全局平均池携带目标特定的外观信息。
%The search feature pyramid is $F = \{f_1, f_2, f_3\}$. 
%For $f_{x}^{i} \in F_{x}$, t
调制特征金字塔 $F_{mod} = \{f_{mod}^i\}_{i=1:5}$ 生成如下：
\begin{equation}
    f_{mod}^i = \mathcal{M}(f_{avg}, f^i_x),
\end{equation}
其中 $\mathcal M(\cdot)$ 是深度相关 \cite{SiamRPN++}。
%So we get the modulated feature pyramid $F_{mod} = \{f_{mod}^i\}_{i=1:5}$. 
%Then we predict proposals on $F_{mod}$.
每个调制的特征图都馈入两个同级的全连接层中-通道尺寸为4$k$的盒子回归层和通道尺寸为2$k$的盒子分类层，其中 $k$ 是每个位置的最大建议数量。
% anchor 这个概念是必备的，所以是得讲一下。
相对于一组锚定义了对象/背景标准和边界框回归。
在 \cite{lin2017feature}之后，我们将具有相同比例的锚分配给每个不同的金字塔等级。有关锚点设置的详细信息，请参阅 \cite{lin2017feature}。
% For every modulated layer, we attach a classification layer and a regression layer to make predict.
\iffalse
The loss function of stage 1 is:
\begin{equation}
    \mathcal{L}_1 = 
\end{equation}
\fi
% you need to say how we get the candidates. Faster R-CNN already says it.
%The generated proposes with top-K classification scores is send to stage2.
在优化阶段，我们使用排名靠前的$N$ 个提案区域。

\subsection{对抗 dropout 模块}
%stage 1 的网络结构已经交代清楚了，就是一个FPN。和调制，和分类/回归层。那么stage的网络结构也应该交代清楚：roi align + 特征融合 + 对抗 + 分类/回归层。
\label{sec:stage2}
% 首先你要讲一下stage2在全局中的作用。
优化阶段旨在从候选提案中选择目标。
% 或许该讲一下stage2的输入包括：真实目标特征+候选区域。
% The input of stage 2 consists of: (1) The target feature is $f_{z} \in \mathbb R ^{256 \times 7 \times 7}$. (2) The candidate boxes generated from stage 1.
% Stage 2 compose of a feature merge module, a adversarial dropout module, a classifier layer $h^{cls}$ and a regression layer $h^{reg}$. 
这些候选投标的特征是使用RoIAlign \cite{he2017mask} 从搜索特征金字塔 $F_{x}$ 中裁剪出来的，然后与目标特征 $f_{z}$融合在一起：
\begin{equation}
    \mathcal{X} = \mathcal{R}(b, F_{x}) \odot f_{z},
\end{equation}
其中 $\mathcal{R}$表示RoIAlign，$\odot$表示按元素相乘，$b$表示候选投标书中的RoI，$\mathcal{X}$表示$b$的融合特征。
%where $x' \in \mathbb R ^{256 \times 7 \times 7}$.
%$x' \in \mathbb R ^{256 \times 7 \times 7}$ is the feature of an RoI cropped from unmodulated feature pyramid using RoIAlign \cite{he2017mask}.
%The merged feature of a candidate target is:$x = x' \odot x_0$, where $\odot$ represents the element-wise multiplication. $x$ is the merged feature of that RoI.
% Maybe you need to say why you do the merge again.
% The modulation in stage 1 and the feature merge in stage 2 play different part in the tracking process. The modulation aims to detect the similar appearance in the whole image. The

\textbf{对抗性 dropout} 功能融合之后，我们使用对抗性dropout\cite{park2018adversarial, lee2019drop}来提高$\mathcal{X}$的判别能力。
% 简短的描述你是怎么做的。这是很重要的，让读者先对你的东西有个大概的了解，才能进一步了解细节。
我们首先根据散度最大值预测对抗丢包率。将该掩码应用于 $\mathcal{X}$ 以获得对抗性删除的功能。
然后，我们旨在最大程度地减少随机删除的特征和对抗删除的特征之间的差异。
%Specifically, we donate the random mask as $m^s$, the adversarial mask as $m^{adv}$.
具体而言，令 $h^{cls}$ 和 $h^{reg}$ 分别表示阶段2中的分类层和回归层。
根据 \cite{lee2019drop}，按如下方式计算对抗辍学掩码：
\begin{equation}
\begin{split}
    & \mathbf{m}^{adv} = \mathop{\arg\max}\limits_{\mathbf{m}}D[h^{cls}(\mathcal{X} \odot \mathbf{m}^s), h^{cls}(\mathcal{X} \odot \mathbf{m})] \\
    & where~||\mathbf{m}^s - \mathbf{m}|| \leq \delta_e L,
\end{split}
\end{equation}
其中 $L$ 代表 $\mathbf{m} \in \mathbb R^L$的尺寸，
$\mathbf{m}^s$ 表示随机掩码， $\mathbf{m}^{adv}$ 表示对抗掩码。
$\delta_{e}$ 是一个超参数，用于控制相对于 $\mathbf{m}^{s}$ \cite{lee2019drop}的摄动幅度。
%and $\delta_{e}$ is a hyper parameter to control the perturbation magnitude with respect to $m^{s}$ \cite{lee2019drop}
$D[p, p']  \geq 0$ 测量两个分布 $p$ 和 $p'$之间的差异。

为了计算 $\mathbf{m}^{adv}$，\cite{park2018adversarial} 通过在过程中适当放松来优化0/1背包问题。有关详细信息，请参考 \cite{park2018adversarial}。
%Then, we use the mask to dropout the feature, then send it to the classifier.
%We also send the clean feature to the classifier, and wish the loss minimum:
在生成 $\mathbf{m}^{adv}$之后，我们的目标是使关于 $\mathcal{X}$的两个预测分布之间的差异最小化：一个带有随机辍学掩码 $\mathbf{m}^{s}$ 和另一个带有对抗辍学面具 $\mathbf{m}^{adv}$ \cite{lee2019drop}。
\begin{equation}
    \mathcal{L}_{adv} = \mathbb E[D_{KL}[h^{cls}(\mathcal{X} \odot\mathbf{m}^{s})||h^{cls}(\mathcal{X} \odot\mathbf{m}^{adv}))]],
\end{equation}
其中 $D_{KL}$ 是Kullback-Leibler散度。

最后，对于每个RoI，分类层在两个类别（前景或背景）上生成softmax概率估计，而回归层为前景类别输出四个实数值。这四个值编码RoI的精确边界框位置。
SiamTFA的损失是：
%The RoI Aligned features are fed into the global average pooling layer followed by two sibling output layers: one that produces softmax probability estimates over two classes (foreground or background) and another layer that outputs four real-valued numbers for the foreground class. These four values encode the refined bounding-box position for the RoI.
%We get the final tracking result as follows: The classification layer $h^{cls}$ predicts a score for a RoI, the regression layer $h^{reg}$ has shape * and predicts a 4-d vector for it.
%The RoI with the top score is the final box.
\begin{equation}
\mathcal{L} = \mathcal{L}_{cls}^{stage1} + \mathcal{L}_{cls}^{stage2} + \mathcal{L}_{reg}^{stage1}+\mathcal{L}_{reg}^{stage2} +  \lambda \mathcal{L}_{adv},
\end{equation}
其中 $\lambda$ 是用于平衡对抗损失和分类/回归损失的超参数。$\mathcal{L}_{cls}^{\cdot}$ 是交叉熵损失，$\mathcal{L}_{reg}^{\cdot}$ 是回归的标准平滑 $L1$ 损失。在测试过程中，具有最高分类得分的RoI被选择为预测目标。


\iffalse
{classification/regression} The structure of $f^{cls}$ is ... The structure of $f^{reg}$ is ...
Suppose $B^*_t$ is the target bounding box at frame $t$, which is estimated through:
\begin{equation}
B^*_t = h^{reg}(\mathop{\arg\max}\limits_{x \in \mathcal{X}}h^{cls}(x)), 
\end{equation}
where $\mathcal{B}_t^1$ is a set of proposals generated from stage 1.
\fi

\section{实验}
在本节中，我们首先介绍实现细节。
然后，我们对GOT-10K \cite{GOT-10k} 测试仪和UAV20L \cite{mueller2016benchmark} 数据集进行评估。
\subsection{实施细节}
拟议的网络在GOT-10k \cite{GOT-10k} 的训练集上进行训练，而骨干网在ImageNet上进行了预训练。
我们应用动量​​为0.9的随机梯度下降，并将权重衰减设置为0.0005。
学习率从 $10^{-2}$ 降至 $10^{-4}$.。
批次大小设置为2，并且对网络进行了90000次迭代训练。
我们的跟踪器是使用PyTorch在Python中实现的。
%First stage, we select 4000 proposals to send to stage2, the positive/negative rate is 1:3.
%To shift channels $K=2$.
%Because of the memory limit, the batch size is 2.
%To calculate the loss, $\lambda = 1$.
%For both training and testing, the size of a template image is 
% 其实有些对不上。按论文上讲，第一阶段是找到若干候选框。第二阶段是得到唯一结果。实际上是，第一阶段得到若干候选框，第二阶段进一步筛选得到约10个候选框（这才是真正的相似目标），然后按IoU和得分来选择最终结果。
% For test, we select top 100 proposals to send to stage2. The we use NMS to select top 10 object. 
\subsection{对GOT-10k数据集的评估}

\iffalse
\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{images/success_plot.png}
    \caption{Overall performance on GOT-10k, ranked by their average overlap (AO) scores.}
    \label{fig:got10k}
\end{figure}
\fi
\begin{table}
\centering
\caption{Performance of our algorithm with different components on GOT-10k test set.}
\begin{tabular}{c c c c c}
\bottomrule
\begin{tabular}[x]{@{}c@{}}Temporal\\aggregation\end{tabular} & \begin{tabular}[x]{@{}c@{}}Adversarial\\dropout\end{tabular} & $AO$ & $SR_{0.50}$ & $SR_{0.75}$ \\ 
\hline
          &           & 0.542 & 0.607 & 0.456 \\
\checkmark&           & 0.561 & 0.645 & 0.480 \\
\checkmark&\checkmark & 0.577 & 0.662 & 0.509 \\
\bottomrule
\label{tabel:ablation}
\end{tabular}
\end{table}
\begin{table}
\centering
\caption{将我们的方法的结果与GOT-10k测试集上的其他方法进行比较。}
\begin{tabular}{l l l l}
\bottomrule
Method   &  $AO$   &  $SR_{0.50}$ & $SR_{0.75}$  \\
\hline
Ours &  $\textbf{0.577}^\textbf{1}$ & $\textbf{0.662}^\textbf{1}$  & $\textbf{0.509}^\textbf{1}$  \\
SiamMask &  0.459&  0.560 &0.205 \\
SiamFCv2 &  0.374&  0.404 &0.144 \\
SiamFC   &  0.348&  0.353 &0.098 \\
GOTURN	 &  0.347&  0.375 &0.124 \\
CCOT	 &  0.325&  0.328 &0.107 \\
ECO	     &  0.316&  0.309 &0.111 \\
CF2	     &  0.315&  0.297 &0.088 \\
MDNet	 &  0.299&  0.303 &0.099 \\
%CFNetc2	 &  0.293&  0.265 &0.087 \\
%ECOhc	 &  0.286&  0.276 &0.096 \\
\bottomrule
\end{tabular}
\label{table:got}
\end{table}
在本小节中，我们在GOT-10k \cite{GOT-10k} 数据集上评估我们的方法。
% Say something about this dataset.
GOT-10k是最新的大规模高多样性数据集，包含10,000多个视频序列，目标序列由轴对齐的边界框注释。
%Note that the classes between training and test sets are zero-overlapped. 
%This one-shot protocol avoids the evaluation bias towards familiar objects. 
GOT-10k测试装置包括180种序列，具有84种不同的对象类别和32种运动模式。作为性能指标，我们使用 \cite{GOT-10k}中提出的平均重叠（AO）分数和成功率（SR）。AO表示所有地面和估计边界框之间重叠的平均值，而SR表示重叠超过0.5 / 0.75的成功跟踪帧的百分比。

\textbf{消融研究}
从表 \ref{tabel:ablation} (第 $1^{st}$ 行和第 $2^{nd}$ 行）中，我们看到，通过添加时间聚合模块，AO性能提高了1.9％。
这是因为时间聚集模块通过聚集来自相邻帧的时间信息来改善每帧特征。
%The RPN stage rapidly filters out most background samples, and the RoI head adopts a fixed foreground-to-background ratio to maintain a manageable balance between foreground and background.
从表 \ref{tabel:ablation} (第 $2^{nd}$ 行和第 $3^{rd}$ 行）中，我们看到，通过对抗性辍学模块，AO增加了1.6％。
%This is because the proposed motion model can effectively predict the position distribution of the target, effectively avoiding the adverse effects of distractors. 
这是因为对抗性退出模块提高了我们的暹罗跟踪网络的识别能力。

\textbf{总体效果}
我们将我们提出的方法与8个跟踪器（包括最新技术）进行了比较。
表 \ref{table:got}.
中显示了评估的跟踪器的性能。
与其他列出的方法相比，我们的方法实现了0.577的出色AO。
%ECO \cite{danelljan2017eco} is a state-of-the-art DCF-based tracker, which introduces a factorized convolution operator that dramatically reduces the number of parameters in the DCF model. In contrast, our tracker is based on the powerful siamese architecture. As a result, our tracker significantly outperforms ECO with a gain of 26.1\% in terms of AO score, which suggests the effectiveness of the siamese architecture for the object tracking task.
%SiamMask \cite{Wang2018SiamMask} is a recently proposed siamese tracker. It simultaneously trains a siamese network on three tasks, each corresponding to a different strategy to establish correspondences between the target object and candidate regions in the new frames.
% However, it is based on the local search mechanism: searching the target within a small neighborhood centered on the target position of the previous frame. In the contrast, our SiamTFA use the the global search mechanism. It is always able to perceive the target over the entire image. As a result, our tracker outperforms SiamMask by relative * in terms of AO, which highlights ...
与SiamMask相比，我们的跟踪器旨在充分利用时间信息。结果，我们的跟踪器在AO方面的性能优于SiamMask 11.8％，这凸显了建议的时间聚合模块的重要性。

\begin{figure}[t]
\begin{minipage}[b]{0.5\textwidth}
  \centering
  \centerline{\includegraphics[width=1.0\textwidth]{Img/end/quality_plot_overlap_OPE_AUC.png}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.5\textwidth}
  \centering
  \centerline{\includegraphics[width=1.0\textwidth]{Img/end/quality_plot_error_OPE_threshold.png}}
\end{minipage}
%
\caption{UAV20L数据集上的成功图和精度图。}
\label{fig:uav20l}
%
\end{figure}
\subsection{对UAV20L数据集的评估}
在本小节中，我们在UAV20L \cite{mueller2016benchmark} 长期跟踪数据集中评估跟踪器。
% say something about this dataset: length, 20段
它包含从低空鸟瞰视角捕获的20个高清视频序列，平均序列长度为2934帧。
% It has mean frames of 2934 and max frames of 5527.
% say something about the measures. 
在此实验中，使用两种方法对所有跟踪器进行了比较：精度和成功率。精度的度量是预测边界框的中心与相应的地面真值边界框的中心之间的距离。成功是通过预测边界框内的像素与地面真实边界框内的像素并集的交点来衡量的。
% say something about the accuracy.
在图 \ref{fig:uav20l}中，我们可以发现，与某些代表性跟踪器相比，该算法具有更好的跟踪性能。
在成功图中，我们的跟踪器获得的AUC得分为0.606。
% PTAV is ... Compared with it, our is higher. This demonstrates the effectiveness of our designed architecture.
在精度图中，所提算法的得分为0.804。
% MUSTER is ... Compared with it, our is higher.
它表明我们的跟踪器超越了其他最新算法，例如SiamRPN \cite{SiamRPN} 和 PTAV \cite{fan2018parallel}。这证明了我们的跟踪器在长期跟踪情况下的有效性。
%designed architecture.
%It shows that our tracker surpass another siamese tracker -- SiamRPN. This demonstrates the effectiveness of our designed architecture. Moreover, compared with other state-of-the-art algorithms, such as PTAV \cite{} and MUSTER \cite{}, our trackers are still superior in terms of precision.
\section{结论}
%一句话指出我们提出了什么。
在本文中，我们介绍了一种用于视觉对象跟踪的新颖暹罗架构。
具体来说，我们提出的算法包含两个主要模块，即时间聚合模块和对抗性删除模块。时间聚集模块通过聚集相邻帧的特征来改善每帧特征。对抗性辍学模块提高了暹罗跟踪网络的辨别能力。
大量的实验结果表明，所提出的算法的性能优于最新算法。
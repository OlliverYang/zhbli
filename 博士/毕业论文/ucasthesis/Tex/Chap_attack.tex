\chapter{对抗性信息在视频目标跟踪算法中的应用} \label{chap:attack}
在前面的章节中，我们展示了通过添加语义信息、时空信息和自适应信息，可以有效提高视频跟踪算法的性能。最近有研究显示，孪生追踪器容易受到对抗性攻击。但是，现有的攻击方法独立地为每个视频制作扰动，这在计算上具有不可忽略的成本。如果在线跟踪阶段无法访问有限的计算资源，则此类攻击方法难以在现实世界中发挥作用。

在本章中，我们展示了与视频无关的扰动的存在，从而对视频目标跟踪算法执行对抗性攻击。例如，强制跟踪器沿指定轨迹运动。所提出的对抗性扰动信息在不同视频间具有通用性，执行攻击时无需为不同视频重新生成扰动。具体来说，我们通过向模板图像添加通用的不可感知的扰动并将虚假目标（即小的通用对抗补丁）根据预定义轨迹粘贴到搜索图像中来攻击跟踪器，以便跟踪器输出虚假目标而非真实目标的位置和大小。本章提出的方法允许仅通过执行加法操作和图像粘贴操作就可以干扰基于孪生网络的视频目标跟踪算法，而无需进行梯度优化或网络推理。在多个数据集上的实验结果表明，我们的方法可以有效地欺骗孪生跟踪器。

\section{引文}

\begin{figure}[t]
\centering
\subfloat{\includegraphics[width=0.3\textwidth]{Img/attack/x.jpg}} \qquad 
\subfloat{\includegraphics[width=0.3\textwidth]{Img/attack/z.jpg}}
\caption{对抗性信息的可视化结果。}
\end{figure}

给定视频初始帧中的任意目标，视频目标跟踪的任务是在后续帧中对该目标进行识别并定位。
%在线跟踪阶段中从单个初始样本跟踪视觉对象的这种范式最近被广泛地描述为基于孪生网络的单发问题 \cite{SiamFC,SiamRPN,SiamRPN++,SiamFC++}，称为
最近提出的孪生网络框架 \cite{SiamFC,SiamRPN,SiamRPN++,SiamFC++} 可有效地处理视频目标跟踪任务。

\begin{figure}[thbp]
\centering
%\subfigure{\includegraphics[width=0.2\textwidth]{Img/attack/x.jpg}} \qquad
%\subfigure{\includegraphics[width=0.2\textwidth]{Img/attack/z.jpg}}
\includegraphics[width=1.0\textwidth]{Img/attack/1_v8.pdf}
\caption{在 GOT-10k 视频目标数据库中的一些示例跟踪序列上展示了我们对 SiamFC++ 的攻击效果。我们的方法会产生视频无关的通用扰动，用于迫使 SiamFC++ 遵循复杂的轨迹，而几乎不会增加任何计算量。对抗性信息通过离线训练获得。跟踪器错误地认为虚假目标区域包含要跟踪的目标（请参见左侧的热图）。此外，对抗性信息还同时误导了 SiamFC++ 中的质量评估分支（请参见右侧的热图）。注意，虚假目标的大小逐渐减小。} 
%Being universal, the generated perturbations can be conveniently exploited to perturb videos on-the-fly without extra computations. 
%Red represents the ground-truth bounding box, and green represents the predicted bounding box by the tracker.
%The purple dashed box marks the universal adversarial patch. The left heatmap represents the probability of each spatial position to contain the \textit{fake target}, and the right heatmap represents the target state estimation quality.}
\label{fig:1}
\end{figure}

近年来，孪生跟踪器的鲁棒性引起了很多关注，并且研究重点已转向设计更有效、计算成本更低的攻击方法 \cite{TTP,FAN,SPARK}。尽管很多攻击方法可以成功地攻击孪生网络跟踪器，但这些攻击方法仍不适合以有限的计算资源来攻击基于低功耗计算平台构建的在线跟踪系统。原因是这些攻击方法仍然需要基于迭代优化或对抗生成网络推理来独立地为每个视频制作扰动。因此，在计算密集型的跟踪系统中可能无法确保计算资源充足可用。

在文献 \cite{UAP} 中，作者提出的通用对抗性扰动（universarial adversarial perturbation，UAP）能够以与图像无关的方式欺骗同一数据分布中的大多数图像。UAP 具有通用性，可以用于快速扰动数据，而无需进行额外的计算。因此，攻击在计算资源有限的平台上部署的实时应用程序时，UAP 具有重要作用。然而，很少有工作研究如何使用 UAP 攻击孪生跟踪器，因为很难将现有的 UAP 直接应用于攻击孪生跟踪器。主要原因在于：
\begin{itemize}
\item 大多数 UAP 是为传统神经网络架构设计的，该架构接受一幅图像作为输入，而孪生网络同时接受模板图像和搜索图像作为输入。
\item 现有 UAP 方法的目标往往用于干扰模型的二进制输出，而我们需要使用通用扰动使得孪生追踪器遵循指定的轨迹。
\end{itemize}

在本章中，我们首次尝试了为最新的孪生跟踪器，即 SiamFC++ \cite{SiamFC++} 生成视频无关的通用对抗性扰动信息。具体来说，我们旨在通过向模板图像添加通用的不可感知的扰动并将虚假目标（即小的通用对抗补丁）粘贴到符合预定轨迹的搜索图像中来攻击跟踪器（如图 \ref{fig:1}），以便跟踪器输出虚假目标而非真实目标的位置和大小。利用本章提出的视频无关的通用扰动攻击一段新视频时，仅需执行加法操作和图像粘贴操作即可，而无需执行梯度优化或网络推理，因此几乎不会增加计算成本。在 OTB2015 \cite{OTB}，GOT-10k \cite{GOT-10k} 和 LaSOT \cite{LaSOT} 基准测试数据库中的实验结果证明了我们方法的有效性。

\section{相关工作}

\subsection{孪生视频目标跟踪}

基于模板匹配思想的跟踪算法主要包括基于相关过滤器的视频目标跟踪算法和基于孪生网络的视频目标跟踪算法。两者都旨在估计从初始视频帧裁剪的模板在后续帧中的位置。孪生跟踪器将视觉跟踪形式化为以卷积方式学习目标模板和搜索区域中候选目标之间的相似性。搜索图像区域中具有最高视觉相似度的位置被确定为目标位置。

最近，一些孪生跟踪器~\cite{SiamRPN,SiamRPN++,SiamFC++} 已证明，孪生网络架构可为视频目标跟踪带来显著的性能改进。
具体而言，SiamRPN \cite{SiamRPN} 由一个用于特征提取的孪生子网和一个分别包含分类和回归分支的区域生成子网组成。SiamRPN++ \cite{SiamRPN++} 基于通过对分类任务和状态估计任务的分解，通过一种简单而有效的空间感知采样策略突破了严格的平移不变性的限制，并成功地训练了 ResNet 驱动的孪生跟踪器，从而显着提高了性能。除了这些基于锚框的方法以外，Xu 等人进一步设计了无锚跟踪器 SiamFC++ \cite{SiamFC++}。在本章的实验中，我们将重点放在无锚的 SiamFC++ 跟踪器上，同时分析了对抗性信息在基于锚框的跟踪器上的移植性。

\subsection{对抗攻击}

图像分类的对抗性攻击在文献 \cite{intriguing} 中首次进行了研究，目的是指出现代深度卷积神经网络对不可察觉的扰动的脆弱性。
%Recent studies also emerge to investigate the adversarial attacks to other diverse types of tasks such as natural language processing \cite{generating} and object detection \cite{wei2019transferable}.
最近很多研究将对抗性信息的应用扩展到其他领域，如自然语言处理 \cite{generating} 和图像目标检测 \cite{wei2019transferable}。
可以按照不同的维度对对抗性信息进行分类。

\textbf{不可感知的扰动与对抗性补丁} 不可感知的扰动通常会使每个像素发生少量变化，并且可以使用多种优化策略进行求解，例如 BFGS \cite{intriguing} 和 PGD \cite{PGD}。
对抗性补丁可以放置在输入图像中的任何位置，以引起网络行为异常，因此常用于通用攻击 \cite{patch}。
据我们所知，我们是第一个同时利用不可感知的扰动和对抗性补丁来攻击目标跟踪器的方法，两种不同形式的扰动以端到端的方式共同训练。
请注意，我们的对抗性扰动信息在网络域而不是图像域中工作。在网络域设置中，扰动可以取任何值，并且不受图像像素取值范围的约束 \cite{karmon2018lavan}。

\textit{非针对性攻击与针对性攻击} 在非针对性攻击的情况下，攻击者的目标是使网络预测任何不正确的标签，而不关注标签的具体取值。例如，在视频目标跟踪中将目标位置估计远离真实目标位置。
有针对性攻击任务旨在将网络的预测更改为某个特定的标签。在视频目标跟踪任务中，针对性攻击旨在误导跟踪器按照预定的轨迹输出目标位置。

\subsection{目标跟踪中的对抗攻击}

最近，多项研究工作对视觉跟踪任务的对抗攻击进行了探索。例如，PAT \cite{PAT} 通过白盒攻击生成物理对抗纹理，以引导跟踪器在被跟踪目标移到纹理前面时锁定跟踪框。然而，PAT 仅通过攻击跟踪器 GOTURN \cite{GOTURN} 来验证其方法的有效性，该跟踪器与主流的目标跟踪算法相比在性能上有较大差距。在本章中，我们旨在攻击最先进的孪生跟踪器。
RTAA \cite{RTAA} 利用时间信息为视频的每帧图像生成微小扰动。然而，RTAA 仅对跟踪器执行非针对性攻击。相比之下，本章中的针对性攻击更具挑战性，因为我们的目标是在测试时令跟踪器遵循任意的复杂轨迹。
SPARK \cite{SPARK} 通过使用过去帧中的信息算增量扰动以对孪生跟踪器进行针对性攻击。然而，SPARK 需要通过繁琐的迭代计算为每个搜索图像生成独特的对抗样本，这对于实时在线跟踪算法而言非常耗时。在文献 \cite{TTP} 中，作者提出仅使用模板图像生成一个时间可传递的扰动，然后将其添加到每个搜索图像中，以进行实时攻击。然而，该方法针对每个视频都需要运行对抗生成网络以特定于视频的扰动。当计算资源有限或无法访问时，便不适用于攻击现实世界的在线跟踪系统。因此在本章中，我们提出了与视频无关的通用扰动，该扰动允许仅通过加法操作和图像粘贴操作即可对任意视频进行扰动，因此几乎不会增加计算成本。

\begin{figure}[t]
\centering
\includegraphics[width=0.75\textwidth]{Img/attack/network_v5.pdf}
\caption{所提出方法的训练流程。我们旨在为模板图像 $\textbf z$ 训练难以感知的扰动 $\delta$ ，为搜索图像 $\textbf x$ 训练对抗性补丁 $p$ 。在将 $\delta$ 添加到 $\textbf z$ 并将虚假目标 $p$ 粘贴到 $\textbf x$ 之后，跟踪器将输出虚假目标而非真实目标的位置和大小。}
\label{fig:net}
\end{figure}

\section{对抗性信息在孪生跟踪器中的应用}
在本节中，我们提出了针对孪生跟踪器的视频无关的针对性攻击框架，旨在通过向模板图像添加不可感知的扰动并将虚假目标（即对抗补丁）根据符合预定轨迹粘贴到搜索图像中来攻击跟踪器，以便跟踪器输出虚假目标而非真实目标的位置和大小。接下来，我们将形式化对孪生网络 \cite{SiamFC++} 的针对性攻击问题，然后详细介绍我们的扰动策略。

\subsection{问题定义}
令 $V=\{I_i\}_1^T$ 表示长度为 $T$ 的视频序列。
$B^{gt}=\{b^{gt}_i\}_1^T$ 用于表示目标在每一帧中的真实位置。
视频目标跟踪旨在给定目标初始状态的情况下，在随后的帧中预测此目标的位置 $B^{pred}=\{b^{pred}_i\}_1^T$。
在 SiamFC++ 中，跟踪器首先将参考帧 $I_1$ 根据对应的注释边界框 $b_1^{gt}$ 转换为模板图像 $\textbf z$，然后将搜索帧 $I_i$ 转换为以上一帧中估计的位置为中心的搜索图像 $\textbf x_i$。
在每个时刻，模板图像 $\textbf z$ 和搜索图像 $\textbf x_i$ 首先分别通过共享的骨干网络提取特征，其次利用若干非共享层处理所得的特征，并使用逐通道互相关操作对其进行融合。融合后的特征随后送入自网络中，以无锚的方式预测分类图 $\textbf{C}$，边界框回归图 $\textbf{R}$ 和质量评估图 $\textbf{Q}$。简而言之，$\textbf C$ 编码每个空间位置包含目标的概率，$\textbf R$ 回归目标的边界框，而 $\textbf Q$ 预测目标状态质量评估得分。最后，根据 $\textbf{C}$，$\textbf{R}$ 和 $\textbf{Q}$ 生成目标边界框。

我们的任务是为模板图像 $\textbf z$ 训练不可感知的扰动 $\delta$，为搜索图像 $\textbf x_i$ 训练对抗性补丁 $p$。在将 $\delta$ 添加到 $\textbf z$ 并将虚假目标 $p$ 粘贴到 $\textbf x_i$ 之后，跟踪器将输出对抗性补丁而非真实目标的位置和大小（请参见图 \ref{fig:net}）。
$\delta$ 和 $p$ 是通用的（即与视频无关），这意味着扰动一段新的视频仅需将扰动添加/粘贴到模板/搜索图像上，而无需进行梯度优化或网络推断。

\subsection{产生视频无关的对抗扰动}
在本节中，我们展示了如何为孪生跟踪器训练与视频无关的扰动 $(\delta, p)$。在训练的第 $k$ 次迭代中，从训练数据集 $V=\{I_i\}_1^T$ 中随机选择一段视频 $\mathcal V$。假设第 $k$ 次迭代的模板扰动是 $\delta_k \in \mathbb{R}^{127\times 127 \times 3}$，而对抗性补丁是 $p_k \in \mathbb{R}^{128\times 128\times 3}$。我们首先从 $V$ 中随机选择两帧 $I_t, I_s$。
干净的模板图像 $\textbf z\in \mathbb{R}^{127\times 127 \times 3}$ 是根据 $I_t$ 和 $b^{gt}_t$ 生成的，受干扰的模板图像为：
\begin{equation}
\tilde {\textbf z} = \textbf z + \delta_k.
\end{equation}
类似地，干净的搜索图像 $\textbf x \in \mathbb{R}^{303\times 303 \times 3}$ 是根据 $I_s$ 和 $b^{gt}_s$ 生成的。
如前所述，补丁被视为虚假目标并粘贴到搜索图像中。我们使虚假目标的中心位置靠近真实目标的中心位置（具有最大 64 个像素的随机平移）。
虚假目标的宽度/高度介于在 32 像素和 128 像素之间。
扰动的搜索图像生成如下：
\begin{equation}
\tilde{\textbf x} = A(\textbf x, p_k, (l^x, l^y), (w, h)),
\end{equation}
其中 $(l^x, l^y)$ 和 $(w, h)$ 分别表示虚假目标相对于搜索图像的位置和大小。$A$ 是补丁粘贴操作 \cite{patch}，该操作首先将补丁 $p_k \in \mathbb{R}^{128\times 128\times 3}$ 的大小调整为 $\hat{p}_k \in \mathbb{R}^{w\times h\times 3}$，然后将调整过大小的补丁 $\hat{p}_k$ 粘贴到搜索图像 $\textbf x$ 的 $(l^x,l^y)$ 位置。

随后，SiamFC++ 跟踪器 $\phi(\cdot)$ 将 $\tilde {\textbf x}$ 和 $\tilde{\textbf z}$ 作为输入，并进行如下预测：
\begin{equation}
\textbf{C, R, Q} = \phi(\tilde {\textbf x}, \tilde{\textbf z}).
\end{equation}

\textbf{训练目标} 损失函数的计算如下：
\begin{equation}
\begin{array}{l}
\begin{aligned}
L&=\frac{\alpha}{N_{\mathrm{pos}}} \sum_{x, y} L_{\mathrm{cls}}\left(\textbf{C}_{x, y}, \textbf{C}_{x, y}^{*}\right) \\
&+\frac{\beta}{N_{\mathrm{pos}}} \sum_{x, y} \textbf{1}_{\left\{\textbf{C}_{x, y}^{*}>0\right\}} L_{\mathrm{quality}}\left(\textbf{Q}_{x, y}, \textbf{Q}_{x, y}^{*}\right) \\
&+\frac{\gamma}{N_{\mathrm{pos}}} \sum_{x, y} \textbf{1}_{\left\{\textbf{C}_{x, y}^{*}>0\right\}} L_{\mathrm{reg}}\left(\textbf{R}_{x, y}, \textbf{R}_{x, y}^{*}\right) \\
&+\eta \cdot ||\delta_k||_2^2 +  \sigma \cdot ||p_k||^2_2,
\end{aligned}
\end{array}
\label{eq:loss}
\end{equation}
\textcolor{black} %(SiamFC++)
{其中 $\textbf{C}_{x, y}, \textbf{R}_{x, y}, \textbf{Q}_{x, y}$ 分别表示 $\textbf{C}, \textbf{R}, \textbf{Q}$ 在位置 $(x, y)$ 的值。$\textbf{C}^*, \textbf{R}^*, \textbf{Q}^*$ 是根据虚假目标的位置和大小生成的虚假标签。$\textbf 1$ 是指示函数，如果条件成立，则取 1，否则将取 0。$N_{\mathrm{pos}}$ 表示训练阶段正样本的数量，$L_{\mathrm{cls}}$ 表示分类的焦点损失 \cite{focal}，$L_{\mathrm{quality}}$ 表示质量评估的二元交叉熵（BCE）损失，$L_{\mathrm{reg}}$ 表示边界框回归的 IoU 损失 \cite{iou-loss}。根据文献 SiamFC++，如果将 $(x, y)$ 视为正样本，则将 1 分配给 $\textbf{C}_{x, y}^{*}$，如果视为负样本，则分配 0。}

\begin{algorithm}[tb]
\caption{训练过程}
\label{alg:algorithm}
\textbf{输入}: 训练数据集 $\mathcal{V}$，孪生跟踪器 $\phi$ 和最大迭代次数 $N$。\\
\textbf{输出}: 微小扰动 $\delta$ 和对抗补丁 $p$。
\begin{algorithmic}[1] %[1] enables line numbers
\State 令 $k = 0$.
\While{$k < N$}
\State 随机选择一段视频 $V\in \mathcal{V}$。对应的真实标注为 $B^{gt}=\{b^{gt}_i\}^T_1$。
\State 随机选择一对帧 $I_t, I_s$ 从 $V$ 中。
\State 生成模板图像 $\textbf{z}$ 根据 $I_t$ 和 $b^{gt}_t$。
\State $\tilde{\textbf{z}} = \textbf{z} + \delta_k.$。
\State 生成搜索图像 $\textbf{x}$ 根据 $I_s$ 和 $b^{gt}_s$。
\State 计算 虚假目标 相对于搜索图像的位置 $\{l^x, l^y, w, h\}$。
\State $\tilde{\textbf x} = A(\textbf x, p_k, (l^x, l^y), (w, h))$。
\State $\textbf{C, R, Q} = \phi(\tilde {\textbf x}, \tilde{\textbf z})$。
\State 生成虚假标签 $\textbf{C}^*,\textbf{R}^*,\textbf{Q}^*$ 使用 $\{l^x, l^y, w, h\}$。
\State 计算损失 $L(\textbf{C, R, Q}, \textbf{C}^*, \textbf{R}^*, \textbf{Q}^*)$ 使用等式 \ref{eq:loss}。
\State $\delta_{k+1} = \delta_{k} - \epsilon_1 \cdot \text{sign}(\nabla_{\delta_k}L)$。
\State $p_{k+1} = p_{k} - \epsilon_2 \cdot \text{sign}(\nabla_{p_k}L)$。
\State $k = k + 1$。
\EndWhile
\State \textbf{返回} $\delta_N, p_N.$
\end{algorithmic}
\label{alg}
\end{algorithm}

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{Img/attack/vis_v4.pdf}
\caption{我们的目标攻击结果遵循预定义的 虚假轨迹（由黄色的目标边界框指示）。}
\label{fig:vis}
\end{figure*}

\textit{优化} 在每个训练步骤中，扰动更新如下：
\begin{gather}
\delta_{k+1} = \delta_{k} - \epsilon_1 \cdot \text{sign}(\nabla_{\delta_k}L)\\
p_{k+1} = p_{k} - \epsilon_2 \cdot \text{sign}(\nabla_{p_k}L),
\end{gather}
其中 $\epsilon_1$ 用于确保添加到模板图像的扰动难以感知，而 $\epsilon_2$ 用于确保训练稳定性。
在训练期间，我们仅优化扰动 $(\delta, p)$ 的值，而孪生网络的参数保持不变。

我们在算法 \ref{alg} 中概述了该训练过程。

\subsection{在推理时攻击跟踪器}

一旦对扰动 $(\delta, p)$ 进行了训练，便可被用于对基于孪生网络的视频目标跟踪算法的攻击。$\delta$ 和 $p$ 都是通用的（即与视频无关），这意味着对一段新视频进行扰动仅需将扰动添加/粘贴到模板/搜索图像上，而无需进行梯度优化或网络推断。
假设 $B^{fake}=\{b^{fake}_i\}_1^{T}$ 是希望跟踪器输出的指定轨迹。
在跟踪视频 $V=\{I_i\}_1^T$ 的第 $i$ 帧期间，我们需要将相对于原始帧 $I_i$ 的边界框变换为 $b^{fake}_i$ 相对于搜索图像 $\textbf x_i$ 的边界框 $\hat b_i=\{l^x_i, l^y_i, w_i, h_i\}$ 中，然后根据 $\hat b_i$ 将 $p$ 粘贴到 $\textbf x_i$ ：
\begin{equation}
\tilde{\textbf x}_i = A(\textbf x_i, p, (l^x_i, l^y_i), (w_i, h_i)).
\end{equation}
跟踪器将 $\tilde{\textbf z}_i=\textbf z_i+\delta$ 和 $\tilde{\textbf x}_i$ 作为输入，随后的跟踪过程与 SiamFC++ 相同。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{实验评估与分析}

\begin{table}
\centering
\begin{tabular}{c c c c c}
\toprule
\multirow{2}{*}[-2pt]{数据库} & \multirow{2}{*}[-2pt]{评价指标} & 原始图像 & \multicolumn{2}{c}{对抗图像}  \\
\cmidrule(lr){3-3} \cmidrule(lr){4-5}
                          &                         & 真实轨迹 & 真实轨迹 & 虚假轨迹     \\ 
\midrule
\multirow{2}{*}{OTB-15} 
& AO   & 0.642 & 0.035 & 0.842\\
& Precision & 0.861 & 0.048 & 0.928\\
\midrule
\multirow{2}{*}{GOT-Val} 
& SR & 0.897 & 0.023 & 0.890\\
& AO 				   & 0.760 & 0.035 & 0.818 \\
\midrule
\multirow{3}{*}{LaSOT} 
& Precision       & 0.514 & 0.013 & 0.820\\
& Norm. Prec. & 0.551 & 0.015 & 0.788\\
& AO & 0.525 & 0.022 & 0.767\\
%& Succ. rate  & 0.626 & 0.016 & 0.834\\
\midrule
\multicolumn{2}{c}{FPS} & 58 & 58 & 58\\
\bottomrule
\end{tabular}
%}
\caption{评估基准上的总体攻击结果。}
\label{tab:benchmark results}
\end{table}

\subsection{实验设置}

\textit{评估基准} 我们在几种跟踪基准，即 OTB2015 \cite{OTB}，GOT-10k \cite{GOT-10k} 和 LaSOT \cite{LaSOT} 上评估针对跟踪器的对抗攻击效果。具体而言，OTB2015 是典型的跟踪基准数据库，已被广泛用于跟踪性能评估中。GOT-10k 具有更多的目标类别，LaSOT 具有更长的视频序列，平均持续时间为 84 秒。它们都遵循单次通过评估（OPE）协议，具有相似的评价指标，主要基于跟踪器在测试视频上的跟踪效果衡量其成功率与精确度。
%例如，以上数据库均根据序列的帧分数来衡量成功，在该序列中，预测矩形和地面真实矩形的交点重叠（IoU）重叠超过给定阈值，然后使用面积-曲线下（AUC）准则。
%由于最近证明所有测试视频帧上的 IoU 重叠平均值（AO）均等效于 AUC 标准，因此在下文中将成功度量表示为 AO。
%除了 AO 之外，成功率（SR）度量标准也可以直接用于测量在 GOT-10k 中给定阈值的成功跟踪帧的百分比。
%至于精度，它对帧的比例进行编码，对于这些比例，预测矩形的中心在地面真实中心的 20 个像素之内。由于精度度量对图像的分辨率和边界框的大小敏感，因此提出了在地面真实边界框的大小上进行归一化精度的度量，然后使用 AUC 对跟踪器进行排序，以使归一化精度介于0之间和0.5。

\textit{生成虚假轨迹} 我们需要为每个视频预先定义特定的轨迹 $B^{fake}=\{b^{fake}_i\}_1^{T}$，以实现在线跟踪中的针对性攻击，我们称之为虚假轨迹。我们将使用真实边界框表示的真实轨迹定义为 $B^{gt}=\{b^{gt}_i\}_1^T$。
我们可以为每个视频手动标记任意虚假轨迹 $B^{fake}$ ，然而这将非常耗时。因此，我们基于 $B^{gt}$ 生成 $B^{fake}$。具体来说，虚假轨迹沿 \textit{真实轨迹} 运动，且 $b^{fake}_i$ 和 $b^{gt}_i$ 的边界相距 16 像素。
%The size of $b^{fake}_0$ is the same as $B^{gt}_0$ and the size of $b^{fake}_i$
虚假轨迹的边框尺寸由 $b^{gt}_1$ 的尺寸逐渐变为 $64\times 64$。请注意，GOT-10k 测试数据的边界框注释信息未公开，因此我们仅使用其验证进行评估，并将其表示为 GOT-Val。

\textit{图像质量评估} 我们使用结构相似性（SSIM）\cite{SSIM} 来评估生成的不可感知的模板扰动 $\delta$ 的质量。当 SSIM 接近 1 说明生成的扰动难以感知（请参见表 \ref{tab:iter}）。

\subsection{实现细节}

在我们的评估中，基本跟踪器 SiamFC++ \cite{SiamFC++} 的主干孪生网络采用 GoogLeNet \cite{GoogLeNet}。
我们在 Pytorch 中实施我们的方法，并使用三个 GTX 1080Ti GPU 训练我们的扰动。
我们采用 COCO \cite{COCO}，ILSVRC-VID \cite{VID}，及 GOT-10k \cite{GOT-10k} 与 LaSOT \cite{LaSOT} 的训练分割作为我们的训练集。
我们以 96 个图像的微型批处理（每个GPU 32个图像）训练32768次迭代的扰动。
模板扰动 $\epsilon_1$ 的学习速率设置为0.1，而对抗性补丁的学习速率为 $\epsilon_2 = 0.5$。
我们按照SiamFC ++的实践生成培训样本。
在培训和在线跟踪阶段，SiamFC++ 跟踪网络模型 \footnote{由 SiamFC++ 的作者专门使用 GOT-10k 的训练集进行训练，可以从 \url{https://drive.google.com/file/d/1BevcIEZr_kgyFjhxayOFw08DFl2u5Zi7/view} 中下载。} 是固定的，并用于整个评估。模板图片的空间尺寸设置 $127\times 127$，搜索图片的价格为 $303\times 303$。
在等式 \ref{eq:loss} 中，我们设置 $\alpha=1, \beta=1, \gamma=1, \eta=0.005$ 和 $\sigma=10^{-5}$。

\begin{table}
\centering
\begin{tabular}{ccccccc} 
\toprule
\multirow{2}{*}[-2pt]{$L_{\text{cls}}$}     & \multirow{2}{*}[-2pt]{$L_{\text{quality}}$} & \multirow{2}{*}[-2pt]{$L_{\text{reg}}$} & \multicolumn{2}{c}{虚假轨迹}          & \multicolumn{2}{c}{真实轨迹}           \\ 
\cmidrule(lr){4-5} \cmidrule(lr){6-7}
                       &                    &                    & AO                    & SR                    & AO                    & SR                     \\ 
\midrule
\checkmark   &    &    & 0.718  & 0.824    & 0.086 & 0.083   \\
   & \checkmark   &    & 0.044  & 0.044    & 0.703 & 0.842   \\
   &    & \checkmark   & 0.664  & 0.726    & 0.165 & 0.184   \\
\checkmark   & \checkmark   & \checkmark   & 0.818  & 0.890    & 0.035 & 0.023   \\ \bottomrule
\end{tabular}
%}
\caption{每项损失对 GOT-Val 的贡献。}
\label{tab:loss}
\end{table}

\subsection{评估基准的攻击结果}

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{Img/attack/txt_visualize.pdf}
\caption{可视化结果}
\end{figure}

\textit{总体攻击结果} 我们在评估基准上测试了针对性攻击方法的性能，并将总体结果收集在表 \ref{tab:benchmark results}中。结果表明，基本跟踪器 SiamFC++ 可以在所有评估基准上达到最先进的性能，并且可以实时运行（在 GTX 1080Ti GPU 上约为 58 fps）。但是，这种实时性能要求计算量大的跟踪系统占用大部分计算资源，因此吸引人的是开发一种几乎无成本的攻击方法来欺骗跟踪系统而不会争用资源。如表 \ref{tab:benchmark results}所示，我们的攻击方法可以满足该要求，并通过误导跟踪器遵循预定义的 虚假轨迹 来有效地欺骗 SiamFC++ 跟踪器。此外，相对于 虚假轨迹 而言，较高的 AO 和 Precision 性能表明在没有引起任何怀疑的情况下更有效的攻击（请参见 \ref{fig:vis}）。

\begin{table*}[t]
\centering
\begin{tabular}{rrcccccccccccccccc} 
\toprule
\multicolumn{2}{r}{迭代次数}     & 1     & 2     & 4     & 8     & 16    & 32    & 64    & 128   & 256   & 512   & 1024  & 2048  & 4096  & 8192  & 16384 & 32768  \\ 
\midrule
\multirow{2}{*}{虚假轨迹} & AO    & 0.002 & 0.002 & 0.002 & 0.002 & 0.002 & 0.003 & 0.007 & 0.042 & 0.299 & 0.668 & 0.746 & 0.781 & 0.798 & 0.820 & 0.821 & 0.818  \\
                                 & SR    & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.001 & 0.005 & 0.044 & 0.335 & 0.749 & 0.822 & 0.855 & 0.872 & 0.895 & 0.897 & 0.890  \\ 
\midrule
\multirow{2}{*}{真实轨迹} & AO    & 0.757 & 0.756 & 0.757 & 0.757 & 0.758 & 0.759 & 0.753 & 0.720 & 0.474 & 0.150 & 0.095 & 0.071 & 0.041 & 0.032 & 0.032 & 0.035  \\
                                 & SR    & 0.894 & 0.891 & 0.893 & 0.891 & 0.893 & 0.896 & 0.888 & 0.852 & 0.559 & 0.164 & 0.098 & 0.066 & 0.031 & 0.021 & 0.022 & 0.023  \\ 
\midrule
\multicolumn{2}{r}{SSIM of $\delta$}                        & 1.00  & 1.00  & 1.00  & 1.00  & 0.99  & 0.99  & 0.97  & 0.93  & 0.86  & 0.86  & 0.87  & 0.88  & 0.88  & 0.88  & 0.88  & 0.88   \\
\multicolumn{2}{r}{MSE}                         & 0.51  & 0.26  & 0.32  & 0.37  & 0.48  & 0.84  & 2.03  & 5.65  & 15.10 & 25.43 & 23.70 & 21.89 & 20.69 & 20.49 & 20.03 & 20.87  \\
\bottomrule
\end{tabular}
\caption{训练迭代次数对 GOT-Val 的影响。}
\label{tab:iter}
\end{table*}

\begin{table}[t]
\centering
\begin{tabular}{c cc cc cc} 
\toprule
\multirow{3}{*}[-6pt]{主干网络} & \multicolumn{2}{c}{原始视频}    & \multicolumn{4}{c}{扰动视频}                                        \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-7}
                          & \multicolumn{2}{c}{真实轨迹} & \multicolumn{2}{c}{真实轨迹} & \multicolumn{2}{c}{虚假轨迹}  \\ 
\cmidrule(lr){2-3} \cmidrule(lr){4-7}
                          & AO    & SR                           & AO    & SR                           & AO    & SR                           \\ 
\midrule
GoogLeNet                 & 0.760 & 0.897                        & 0.035 & 0.023                        & 0.818 & 0.890                        \\
AlexNet                   & 0.720 & 0.850                        & 0.196 & 0.227                        & 0.572 & 0.640                        \\
ShuffleNet                & 0.766 & 0.888                        & 0.554 & 0.656                        & 0.135 & 0.134                        \\
\bottomrule
\end{tabular}
%}
\caption{可转移到 GOT-Val 上不同的主干。}
\label{tab:backbone}
\end{table}

\begin{table}
\centering
\begin{tabular}{ccccc} 
\toprule
\multirow{2}{*}[-2pt]{跟踪器} & \multicolumn{2}{c}{原始视频} & \multicolumn{2}{c}{对抗视频}  \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
                          & AO & Precision              & AO & Precision                   \\
\midrule
SiamRPN++                 & 0.676   & 0.879                  & 0.518   & 0.691                       \\
SiamRPN                   & 0.666   & 0.876                  & 0.379   & 0.506                       \\
\bottomrule
\end{tabular}
%}
\caption{在 OTB2015 上可转移到不同的跟踪体系结构。}
\label{tab:arch}
\end{table}

\textit{消融研究：培训损失的影响} 我们实施了一系列实验来分析和评估每个损失成分的贡献。
在表 \ref{tab:loss}中，我们报告了关于 GOT-Val 的结果，这些结果与在等式中用损失项的不同组合训练的不同扰动有关。（等式\ref{eq:loss}）
总而言之，所有损失项都是有益的，而分类项比质量评估项更重要。

\textit{消融研究：训练迭代的影响} 如表 \ref{tab:iter}所示，经过大约 30000 次训练迭代后，所产生的扰动会欺骗 GOT-Val 中的大多数目标。关于 虚假轨迹 的 AO 从 0.760 下降到 0.035。我们注意到，在训练周期开始时（训练迭代次数小于2048），AO 的降低明显快于后期。这证明了我们的端到端训练流水线的快速收敛能力，同时产生了对抗性扰动。

\subsection{传输性分析}

\textit{向不同骨干的传输能力} 当将扰动应用于 SiamFC++ 的另外两个不同的主干，即，ShuffleNet \cite{ShuffleNet} 和 AlexNet \cite{AlexNet}。
实验结果显示在表 \ref{tab:backbone} 中。对于 SiamFC++-AlexNet，关于 \textit{真实轨迹} 的AO从0.72下降到 0.196。但是，我们的扰动不能很好地推广到 SiamFC++-ShuffleNet，我们推测这是由于 ShuffleNet 中的特定组卷积和通道洗牌操作所致。

\textit{不同跟踪架构的迁移能力} 当将扰动应用于另外两个基于锚的最新跟踪器时，我们评估攻击的可传递性：基于 AlexNet 的 SiamRPN \cite{SiamRPN} 和基于 ResNet 的 SiamRPN++ \cite{SiamRPN++}，以验证对不同跟踪体系结构的可移植性。
SiamRPN 使用 RPN 网络在响应图上执行位置回归和目标分类。 SiamRPN++ 在使用 ResNet 主干进行有效训练的基础上，执行分层和深度聚合，以提高准确性。实验结果显示在表 \ref{tab:arch} 中。在 SiamRPN 的情况下，相对于 \textit{真实轨迹} 的 AO 从 0.666 降低到 0.379，而 SiamRPN++ 的性能从 0.676 降低到 0.518。结果表明，即使将生成的扰动应用于基于锚的跟踪器，我们的攻击也可以很好地转移到不同的跟踪体系结构。

\subsection{与其他方法的比较}

我们将我们的方法的攻击性能与最新的有前途的攻击方法进行比较，即基于 AlexNet 的非目标攻击方法 CSA \cite{CSA}，和其他两种有目标攻击方法：基于 AlexNet 的 FAN \cite{FAN} 和基于 ResNet-50 的 TTP \cite{TTP}。
我们在表 \ref{tab:untargeted} 中报告相对于 \textit{真实/虚假轨迹} 的精度得分。
除了单纯的添加和粘贴操作外，我们的方法在性能上明显优于其他方法，而且不需要梯度优化或网络推断。

\section{结论}

在本章中，我们提出了一种针对孪生跟踪器的视频不可知的有针对性的攻击方法。
我们旨在通过向模板图像添加不可察觉的扰动并将 虚假目标（即小的对抗补丁）粘贴到符合预定轨迹的搜索图像中来攻击跟踪器，以便跟踪器输出位置和 虚假目标 的大小，而不是实际目标。由于具有通用性，因此可以方便地利用生成的扰动来即时扰动视频，而无需进行额外的计算。
在几个流行的数据集上进行的实验表明，我们的方法可以有效地以有针对性的攻击方式欺骗孪生跟踪者。

\begin{table}[t]
\centering
\begin{tabular}{@{}ccccccc@{}}
\toprule
\multirow{2}{*}[-2pt]{方法} & \multirow{2}{*}[-2pt]{跟踪器} & \multirow{2}{*}[-2pt]{攻击时间（毫秒）} & \multirow{1}{*}[-2pt]{原始视频} && \multicolumn{2}{c}{对抗视频} \\
\cmidrule{4-4} \cmidrule{6-7}
 &  &  & 真实轨迹 & & 真实轨迹 & 虚假轨迹 \\ \midrule
CSA & SiamRPN & 4720 & 0.851 & & 0.458 & - \\
FAN & SiamFC & 10 & 0.720    & & 0.180&0.420 \\
TTP & SiamRPN++ & 8 & 0.910  && 0.080&0.692 \\
\midrule
Ours & SiamFC++ & 0 & 0.861  & & 0.048&0.928 \\ \bottomrule
\end{tabular}%
\caption{关于 OTB2015 上 \textit{真实/虚假轨迹} 的精确度得分。}
\label{tab:untargeted}
\end{table}
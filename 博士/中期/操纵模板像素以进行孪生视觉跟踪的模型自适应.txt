Manipulating Template Pixels for Model Adaptation of Siamese Visual Tracking

In this letter, we show that the challenging model adaptation task in visual object tracking can be handled by simply manipulating pixels of the template image in Siamese networks. For a target that is not included in the offline training set, a slight modification of the template image pixels will improve the prediction result of the offline trained Siamese network. The popular adversarial example generation methods can be used to perform template pixel manipulation for model adaptation. Different from current template update methods, which aim to combine the target features from previous frames, we focus on the initial adaptation using target ground-truth in the first frame. Our model adaptation method is pluggable, in the sense that it does not alter the overall architecture of its base tracker. To our knowledge, this work is the first attempt to directly manipulating template pixels for model adaptation in Siamese-based trackers. Extensive experiments on recent benchmarks demonstrate that our method achieves better performance than some other state-of-the-art trackers.

Object tracking refers to the task of sequentially locating a specified moving object in a video, given only its initial state. Recently, Siamese networks [1], [2] have demonstrated a significant improvement in object tracking performances. Siamese trackers formulate the visual object tracking problem as learning cross-correlation similarities between a target template and a search region. Tracking is then performed by finding the target object from the search image region by computing the highest visual similarity. Despite its recent success, the learned similarity measure of the Siamese network is not necessarily reliable for objects that are not included in the offline training set, leading to poor generalization [3]. Several recent works aim to adapt the model to the current target appearance. For example, TADT [4] identifies the importance of each convolutional filter according to the back-propagated gradients and selects the target-aware features based on activations for representing the targets. However, the feature extractor of TADT is pre-trained on ImageNet [5], not on large-scale visual tracking datasets. This limits the representation ability of its features on the object tracking task. GradNet [6] exploits the discriminative information in gradients and updates the template in the Siamese network through feed-forward and backward operations. However, the extra sub-network increases the computational cost and is prone to overfitting. UpdateNet [7] learns to combine the target features from previous frames. However, it does not use ground truth information to adaptively adjust the template features of the first frame. In this work, we show that the challenging model adaptation task in visual object tracking can be handled by simply manipulating pixels of the template image in Siamese networks. Given an object tracker, our algorithm modifies template pixels in only a few gradient-descent iterations using the target ground-truth in the first frame. For a target that is not included in the offline training set, we believe that a slight modification of the template image pixels can improve the prediction result of the offline trained Siamese network. We use the adversarial example generation method to achieve this, because it is commonly used to slightly modify the input image, and thereby impose an impact on the prediction result of the network. We depart from the purpose of adversarial sample generating in that the latter is aimed to make the prediction of the network worse, while we hope the prediction of the Siamese network is better. The proposed model adaptation method can be integrated with varieties of Siamese trackers like SiamFC++ [9]. Note that the parameters of the Siamese network remain intact to preserve the generative ability of offline-trained embedding space. We perform comprehensive experiments on 4 tracking benchmarks: VOT2018 [10], TrackingNet [11], GOT-10 k [8], and OTB2015 [12]. Our approach achieves state-of-the-art results while running at over 80 FPS (see Fig. 1).

PROPOSED ALGORITHM

In this section, we present a new model adaptation approach for Siamese trackers via directly manipulating template pixels. We start by revisiting the tracking process of popular template matching-based trackers, which is closely related to the proposed approach. Following [13], we formulate object tracking as a confidence-based regression problem, which learns a function sθ : Y×X → R, and predicts a scalar confidence score sθ(y, x) ∈ R given an output-input pair (y, x). The final estimate f(x) = y∗ is predicted as follows: f(x) = arg max y∈Y sθ(y, x), (1) where x is an input image. y usually represents the center 2D image coordinate of the target object. Currently, there are two prevalent template matching-based paradigms: discriminative correlation filter (DCF) approaches and Siamese tracking methods. In DCF-based methods, a circular correlation filter wθ is trained during tracking to predict a target confidence score: sθ(y, x)=(wθ ∗ φ(x))(y), (2) where φ(x) is the features extracted from the search image x. In contrast to DCF, Siamese trackers exploit a two-stream architecture. One stream extracts the targets featuresφθ(z) based on the template image z which is cropped from the first frame according to the ground truth bounding box. The other stream receives as input a large search image x and outputs the search features φθ(x). The two outputs are cross-correlated to predict a target confidence score: sθ(y, x)=(φθ(z) ∗ φθ(x))(y). (3) DCF-based trackers and Siamese trackers both have the advantage of utilizing large-scale visual tracking datasets to train the feature extractor φ(·) or the embedding network φθ(·) by their own right. This way the representation ability of features on the object tracking task can be enhanced. In contrast to Siamese trackers, DCF learns the filter wθ from example patches of the target appearance to discriminate it from the background. Despite the improved tracking efficiency using the circular correlation operation, its boundary effect and sophisticated optimization prevent itself from making a good trade-off between computational speed and tracking performance. Siamese trackers do better in this aspect, although the learned similarity measure in the cross-correlation is not necessarily reliable for objects that are not included in the offline training set, leading to poor generalization. In this letter, we aim to design a new Siamese tracking method that has the ability to make full use of the specific information of the current video for model adaptation like the DCF-based trackers, although the object in it is not included in the offline training set. This is achieved by utilizing the annotation information in the first frame to perform model adaptation. Note that Equ. (2) and Equ. (3) bear some similarities with each other, and the main difference lies in the kernel term for correlation: the kernel of DCF is the online-learned wθ, and the kernel of the Siamese network is φθ(z). In order for the Siamese network to have the model adaptation ability, we need to adapt φθ(z) using the first frame ground truth annotation of the current video. There are two design choices to adapt φθ(z): changing φθ(·) or changing z. However, changing φθ(·) may cause too much tedious meta-learning settings to ensure the generative ability of offline-trained embedding space [14], [15]. In contrast, our solution is in a simple way to perform model adaptation of Siamese trackers by changing z, i.e., modifying template pixels in only a few gradient-descent iterations using the target ground-truth at the first frame. Compared with current model adaptation methods of Siamese trackers, the proposed method has the following advantages. First, we never modify parameters of the Siamese network to preserve the representation ability of offline-trained embedding space. Second, different from current template update methods [7], [16], which aim to combine the target features from previous frames, we focus on the initial adaptation using target ground-truth at the first frame. Finally, our model adaptation method is pluggable, in the sense that it does not alter the overall architecture of the base tracker. In the next subsection, we will show how the popular adversarial example generation methods can be used to perform template pixel manipulation for model adaptation. A. Manipulating Template Pixels for Model Adaptation At first glance, there may be a contradiction between the model adaptation task and the adversarial example generation task, because these two tasks have different purposes. An adversarial example [17] is a sample of input data which has been modified very slightly in a way that is intended to perform an attack on machine learning systems, which means to cause a machine learning model to make wrong predictions. However, the purpose of model adaptation in our work is to make full use of the annotation information in the first frame to improve the tracking performance for the current video. We will point out in the following that there are some similarities between these two tasks and we can utilize adversarial example generation methods to perform the model adaptation task. Before introducing the proposed method, we first revisit the popular adversarial example generation methods. One of the simplest methods to generate adversarial images Iadv works by linearizing loss function in L∞ neighbourhood of a clean image and finds exact maximum of linearized function using following closed-form equation [18]: Iadv = I +  sign ∇IL(I,ytrue) , (4) where I is the input image, and the values of the pixels are integer numbers in the range [0, 255]. ytrue is the true label for the image I. L(I,y)is the cost function of the neural network for the attack purpose, given image I and label y.  is a hyper-parameter to be chosen. A straightforward way to extend the above method is applying it multiple times with small step size, and clipping pixel values of intermediate results after each step to ensure that they are in an -neighbourhood of the original image. This leads to the Basic Iterative Method (BIM) introduced in [17]: Iadv 0 = I, Iadv N+1 = ClipI,{Iadv N + α sign(∇IL(Iadv N , ytrue))}, (5) where ClipI,{I } is the function which performs per-pixel clipping of the image I , so that the result will be in L∞ -neighbourhood of the source image I. The BIM can be easily made into an attacker for a specific desired target class, called the Iterative Target Class Method [17]: Iadv 0 = I, Iadv N+1 = ClipI,{Iadv N − α sign(∇IL(Iadv N , ytarget))}. (6) Equ. (6) shows that manipulating pixels of the input image in only a few gradient-descent iterations is able to change the network’s prediction to the target class ytarget. Note that our purpose is manipulating pixels of the template image in the first frame, so that the prediction is closer to the ground truth bounding box. So we can perform model adaptation for Siamese networks using Equ. (6) with some modifications: z0 = z, zN+1 = Clipz,{zN − α sign(∇zL(zN , ybb))}, (7) where z is the template image in the first frame, and ybb is the label for Siamese trackers generated from the ground truth bounding box. In the next subsection, we will introduce the overall tracking process equipped with the proposed model adaptation module. B. Tracking Framework The proposed model adaptation method is integrated with the SiamFC++ tracker [9] in a plug-and-play manner. SiamFC++ is based on SiamFC [2] and progressively refined according to several guidelines proposed in [9]. The input of the original SiamFC++ network is composed of the template image z0 cropped from the first frame and search image xi cropped from the i-th frame. However, we expect to perform template manipulating such that, after N steps of pixel update on the input pair (z0, x0) to obtain z , the tracker performs well on (z, xi). To achieve this, we start by cropping the initial template image z0 ∈ R3×128×128 and the initial search image x0 ∈ R3×289×289 from the first frame using the ground truth bounding box. Then, z0 and x0 are sent to the SiamFC++ network to obtain the tracking prediction of the first frame. The tracking loss in SiamFC++ [9] is calculated as follows: L = Lcls + Lquality + Lreg, (8) where Lcls is the focal loss [19]. Lquality is the binary cross entropy (BCE) loss for quality assessment. Lreg is the IoU loss [20] for bounding box regression. The gradient with respect to the template z0 is used to generate z1 according to Equ. (7). The updated template z is obtained by applying a very small number of iterations of Equ. (7). Note that the template image is only updated with the first frame of given sequences and is kept fixed during the whole tracking process to ensure stability. The subsequent tracking procedure remains the same as SiamFC++.

In this section, we first present the implementation details. Then we compare our method with the state-of-the-art trackers on four tracking datasets: OTB2015 [12], VOT2018 [10], GOT10 k [8] and TrackingNet [11]. Specifically, OTB2015 [12] includes 100 sequences, which are labeled with different attributes for in-depth analysis of tracking performance. VOT2018 [10] uniquely applies a reset-based methodology and specially selects 60 sequences of various tracking scenarios for evaluation. GOT-10 k [8] and TrackingNet [11] are two recent large-scale high-diversity datasets. They both cover diverse object classes and scenes in the train and test splits. For GOT-10 k, there is no overlap in object classes between the train and test splits, promoting the importance of generalization to unseen object classes. A. Implementation Details We use SiamFC++ [9] as our base tracker, and the backbone Siamese network adopts GoogLeNet [21]. We do not perform any changes except for the model adaptation component. The parameterαin Equ. (7) is set to 0.05. Our method is implemented in Python with PyTorch. The proposed tracker runs at over 80 FPS on an NVIDIA RTX 2080Ti GPU. B. State-of-the-Art Comparison OTB2015: We use success rate to evaluate the performance of trackers on the OTB2015 dataset. Success rate relies on the intersection over union (IOU) of the predicted bounding box and the ground truth bounding box. We compare our method with various tracking algorithms, including ECO [22], MDNet [23], SiamRPN++ [24], ATOM [1] and SiamFC++_GoogLeNet [9]. The iteration number for template update is set to 16. The results are shown in Table I. Our tracker outperforms the online tracker ATOM by 2.8% in terms of the success score, which demonstrates the powerful model adaptation capability of our method. VOT2018: We compare our method with RCO [10], UPDT [25], SiamRPN [26], MFT [10], LADCF [10], ATOM [1], SiamRPN++ [24], SiamFC++_AlexNet [9] and SiamFC++_GoogLeNet [9] on VOT2018. The trackers are compared using the robustness and accuracy measures. Robustness indicates the number of tracking failures, while accuracy denotes the average overlap between tracker prediction and the groundtruth box. Both of the measures are combined into a single expected average overlap (EAO) score. The iteration number for template update is set to 2. As shown in Fig. 3, the performances of all the listed trackers are not as good as our algorithm. GOT-10 k: We use the average overlap (AO) score as performance measure following [8]. We compare our method with CF2 [27], ECO [22], CCOT [28], GOTURN [29], SiamFC [2], SiamFCv2 [30], ATOM [1], SiamFC++_AlexNet [9] and SiamFC++_GoogLeNet [9] on this dataset. The iteration number for template update is set to 2. In Fig. 2, we can find that the proposed algorithm achieves better tracking performance compared with the listed state-of-the-art trackers. TrackingNet: We compare our method with SiamFC [2], ECO [22], MDNet [23], SiamRPN++ [24], ATOM [1], SiamFC++_AlexNet [9] and SiamFC++_GoogLeNet [9]. The iteration number for template update is set to 32. Table II shows that our tracker performs best in terms of precision and the normalized precision while maintaining a very competitive success value. C. Ablation Study Robustness to Noisy Initial Frames: To study how robust the approach is if the first frame is noisy compared to the next frames in the sequence, we add three kinds of noise to the first frame by: changing the image brightness, applying Gaussian blur, and using non-accurate ground truth bounding box annotation. We denote γ1 ∈ [0.5, 1.5] as the coefficient of brightness variation, γ2 ∈ [0, 2] the blur radius, and γ3 ∈ [0.75, 1] the IoU between the non-accurate first frame bounding box annotation and the real bounding box annotation, respectively. We run both the baseline tracker and our tracker 10 times on OTB2015 with randomly sampled γ1, γ2 and γ3 (see Table III). Compared with SiamFC++_GoogLeNet [9], our tracker always performs better under different noise levels, which shows the robustness of our tracker to noisy initial frames. Attribute-based Analysis: For further analyses on the tracking performance, we also demonstrate the advantages of our algorithm through the attribute-based comparison on sequences of the OTB-2015 dataset (see Table IV). In OTB2015, each sequence is annotated with 11 different attributes, namely: background clutters (BC), deformation (DEF), fast motion (FM), illumination variation (IV), in-plane rotation (IPR), low resolution (LR), motion blur (MB), occlusion (OCC), out-of-plane rotation (OPR), out-of-view (OV) and scale variation (SV). Compared with SiamFC++_GoogLeNet [9], our tracker achieves better performance on 10 out of 11 attributes, which demonstrates its robustness in challenging tracking scenarios such as illumination variation and motion blur. IV. CONCLUSION In this letter, we propose a novel model adaptation method for Siamese trackers, achieving accurate tracking with high speed. We show that the challenging model adaptation task in visual object tracking can be handled by simply manipulating pixels of the template image using the target ground-truth in the first frame. Our model adaptation method is pluggable, in the sense that it does not alter the overall architecture of the base tracker. Numerous experimental results on four object tracking benchmarks demonstrate the effectiveness of the proposed model adaptation method.
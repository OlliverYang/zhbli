---
title: '[ICCV2019] Towards Adversarially Robust Object Detection'
date: 2020-08-20 14:38:33
tags:
- Object Detection
- Adversarial Attack
- ICCV2019
mathjax: true
---

##  Related Work

目标检测攻击： [57, 32, 6, 11, 55, 23, 22, 31]

> [57] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. Adversarial examples for semantic segmentation and object detection. In International Conference on Computer Vision, 2017.
>
> [32] Jiajun Lu, Hussein Sibai, and Evan Fabry. Adversarial examples that fool detectors. CoRR, abs/1712.02494, 2017.
>
> [6] Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Chau. ShapeShifter: Robust physical adversarial attack on Faster R-CNN object detector. CoRR, abs/1804.05810, 2018.
>
> [11] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Florian Tramer, Atul Prakash, Tadayoshi ` Kohno, and Dawn Song. Physical adversarial examples for object detectors. CoRR, abs/1807.07769, 2018.
>
> [55] Xingxing Wei, Siyuan Liang, Xiaochun Cao, and Jun Zhu. Transferable adversarial attacks for image and video object detection. CoRR, abs/1811.12641, 2018.
>
> [23] Yuezun Li, Daniel Tian, Ming-Ching Chang, Xiao Bian, and Siwei Lyu. Robust adversarial perturbation on deep proposal-based models. In British Machine Vision Conference, 2018.
>
> [22] Yuezun Li, Xian Bian, and Siwei Lyu. Attacking object detectors via imperceptible patches on background. CoRR, abs/1809.05966, 2018.
>
> [31] Xin Liu, Huanrui Yang, Linghao Song, Hai Li, and Yiran Chen. DPatch: Attacking object detectors with adversarial patches. CoRR, abs/1806.02299, 2018.

## Object Detection and Attacks Revisited

### Detection Attacks Guided by Task Losses

<img src="https://i.loli.net/2020/08/20/ry1PK6RujiUWw2n.png" alt="image-20200820144842483" style="zoom:50%;" />
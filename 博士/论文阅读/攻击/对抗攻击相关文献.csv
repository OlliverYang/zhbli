Key,Item Type,Publication Year,Author,Title,Publication Title,ISBN,ISSN,DOI,Url,Abstract Note,Date,Date Added,Date Modified,Access Date,Pages,Num Pages,Issue,Volume,Number Of Volumes,Journal Abbreviation,Short Title,Series,Series Number,Series Text,Series Title,Publisher,Place,Language,Rights,Type,Archive,Archive Location,Library Catalog,Call Number,Extra,Notes,File Attachments,Link Attachments,Manual Tags,Automatic Tags,Editor,Series Editor,Translator,Contributor,Attorney Agent,Book Author,Cast Member,Commenter,Composer,Cosponsor,Counsel,Interviewer,Producer,Recipient,Reviewed Author,Scriptwriter,Words By,Guest,Number,Edition,Running Time,Scale,Medium,Artwork Size,Filing Date,Application Number,Assignee,Issuing Authority,Country,Meeting Name,Conference Name,Court,References,Reporter,Legal Status,Priority Numbers,Programming Language,Version,System,Code,Code Number,Section,Session,Committee,History,Legislative Body
ISVTE5M5,conferencePaper,2019,"Wiyatno, Rey; Xu, Anqi",Physical Adversarial Textures That Fool Visual Object Tracking,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00492,https://ieeexplore.ieee.org/document/9009841/,"We present a method for creating inconspicuous-looking textures that, when displayed as posters in the physical world, cause visual object tracking systems to become confused. As a target being visually tracked moves in front of such a poster, its adversarial texture makes the tracker lock onto it, thus allowing the target to evade. This adversarial attack evaluates several optimization strategies for fooling seldom-targeted regression models: non-targeted, targeted, and a newly-coined family of guided adversarial losses. Also, while we use the Expectation Over Transformation (EOT) algorithm to generate physical adversaries that fool tracking models when imaged under diverse conditions, we compare the impacts of different scene variables to ﬁnd practical attack setups with high resulting adversarial strength and convergence speed. We further showcase that textures optimized using simulated scenes can confuse real-world tracking systems for cameras and robots.",Oct-19,2020/6/28 3:02,2020/6/28 3:02,2020/6/28 3:02,4821-4830,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\EEQYPDBN\Wiyatno 和 Xu - 2019 - Physical Adversarial Textures That Fool Visual Obj.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
KNLH7RRX,journalArticle,,"Addepalli, Sravanti",Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes,,,,,,"As humans, we inherently perceive images based on their predominant features, and ignore noise embedded within lower bit planes. On the contrary, Deep Neural Networks are known to conﬁdently misclassify images corrupted with meticulously crafted perturbations that are nearly imperceptible to the human eye. In this work, we attempt to address this problem by training networks to form coarse impressions based on the information in higher bit planes, and use the lower bit planes only to reﬁne their prediction. We demonstrate that, by imposing consistency on the representations learned across differently quantized images, the adversarial robustness of networks improves signiﬁcantly when compared to a normally trained model. Present stateof-the-art defenses against adversarial attacks require the networks to be explicitly trained using adversarial samples that are computationally expensive to generate. While such methods that use adversarial training continue to achieve the best results, this work paves the way towards achieving robustness without having to explicitly train on adversarial samples. The proposed approach is therefore faster, and also closer to the natural learning process in humans.",,2020/7/17 1:53,2020/7/17 1:53,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\42NZTJ48\Addepalli - Towards Achieving Adversarial Robustness by Enforc.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IDDUDENB,journalArticle,,,Single-Step Adversarial Training With Dropout Scheduling,,,,,,"Deep learning models have shown impressive performance across a spectrum of computer vision applications including medical diagnosis and autonomous driving. One of the major concerns that these models face is their susceptibility to adversarial attacks. Realizing the importance of this issue, more researchers are working towards developing robust models that are less affected by adversarial attacks. Adversarial training method shows promising results in this direction. In adversarial training regime, models are trained with mini-batches augmented with adversarial samples. Fast and simple methods (e.g., single-step gradient ascent) are used for generating adversarial samples, in order to reduce computational complexity. It is shown that models trained using single-step adversarial training method (adversarial samples are generated using noniterative method) are pseudo robust. Further, this pseudo robustness of models is attributed to the gradient masking effect. However, existing works fail to explain when and why gradient masking effect occurs during single-step adversarial training. In this work, (i) we show that models trained using single-step adversarial training method learn to prevent the generation of single-step adversaries, and this is due to over-ﬁtting of the model during the initial stages of training, and (ii) to mitigate this effect, we propose a singlestep adversarial training method with dropout scheduling. Unlike models trained using existing single-step adversarial training methods, models trained using the proposed single-step adversarial training method are robust against both single-step and multi-step adversarial attacks, and the performance is on par with models trained using computationally expensive multi-step adversarial training methods, in white-box and black-box settings.",,2020/7/17 1:54,2020/7/17 1:54,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\8S8TVI9Y\Single-Step Adversarial Training With Dropout Sche.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZPEVIVR2,journalArticle,,"Borkar, Tejas; Heide, Felix; Karam, Lina",Defending Against Universal Attacks Through Selective Feature Regeneration,,,,,,"Deep neural network (DNN) predictions have been shown to be vulnerable to carefully crafted adversarial perturbations. Speciﬁcally, image-agnostic (universal adversarial) perturbations added to any image can fool a target network into making erroneous predictions. Departing from existing defense strategies that work mostly in the image domain, we present a novel defense which operates in the DNN feature domain and effectively defends against such universal perturbations. Our approach identiﬁes pre-trained convolutional features that are most vulnerable to adversarial noise and deploys trainable feature regeneration units which transform these DNN ﬁlter activations into resilient features that are robust to universal perturbations. Regenerating only the top 50% adversarially susceptible activations in at most 6 DNN layers and leaving all remaining DNN activations unchanged, we outperform existing defense strategies across different network architectures by more than 10% in restored accuracy. We show that without any additional modiﬁcation, our defense trained on ImageNet with one type of universal attack examples effectively defends against other types of unseen universal attacks.",,2020/7/17 1:55,2020/7/17 1:55,,11,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\ILRMYR9W\Borkar 等。 - Defending Against Universal Attacks Through Select.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2NQI8P2C,journalArticle,,"Chen, Xuesong; Yan, Xiyu; Zheng, Feng; Jiang, Yong; Xia, Shu-Tao; Zhao, Yong; Ji, Rongrong",One-Shot Adversarial Attacks on Visual Tracking With Dual Attention,,,,,,"Almost all adversarial attacks in computer vision are aimed at pre-known object categories, which could be offline trained for generating perturbations. But as for visual object tracking, the tracked target categories are normally unknown in advance. However, the tracking algorithms also have potential risks of being attacked, which could be maliciously used to fool the surveillance systems. Meanwhile, it is still a challenging task that adversarial attacks on tracking since it has the free-model tracked target. Therefore, to help draw more attention to the potential risks, we study adversarial attacks on tracking algorithms. In this paper, we propose a novel one-shot adversarial attack method to generate adversarial examples for free-model single object tracking, where merely adding slight perturbations on the target patch in the initial frame causes state-of-the-art trackers to lose the target in subsequent frames. Speciﬁcally, the optimization objective of the proposed attack consists of two components and leverages the dual attention mechanisms. The ﬁrst component adopts a targeted attack strategy by optimizing the batch conﬁdence loss with conﬁdence attention while the second one applies a general perturbation strategy by optimizing the feature loss with channel attention. Experimental results show that our approach can signiﬁcantly lower the accuracy of the most advanced Siamese network-based trackers on three benchmarks.",,2020/7/17 1:58,2020/7/17 1:58,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\ZNLJYXGM\Chen 等。 - One-Shot Adversarial Attacks on Visual Tracking Wi.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WXAFVM7I,journalArticle,,"Cohen, Gilad; Sapiro, Guillermo; Giryes, Raja",Detecting Adversarial Samples Using Influence Functions and Nearest Neighbors,,,,,,"Deep neural networks (DNNs) are notorious for their vulnerability to adversarial attacks, which are small perturbations added to their input images to mislead their prediction. Detection of adversarial examples is, therefore, a fundamental requirement for robust classiﬁcation frameworks. In this work, we present a method for detecting such adversarial attacks, which is suitable for any pre-trained neural network classiﬁer. We use inﬂuence functions to measure the impact of every training sample on the validation set data. From the inﬂuence scores, we ﬁnd the most supportive training samples for any given validation example. A k-nearest neighbor (k-NN) model ﬁtted on the DNN’s activation layers is employed to search for the ranking of these supporting training samples. We observe that these samples are highly correlated with the nearest neighbors of the normal inputs, while this correlation is much weaker for adversarial inputs. We train an adversarial detector using the k-NN ranks and distances and show that it successfully distinguishes adversarial examples, getting state-of-the-art results on six attack methods with three datasets. Code is available at https:// github.com/giladcohen/NNIF_adv_defense.",,2020/7/17 1:59,2020/7/17 1:59,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\PPTJ6H94\Cohen 等。 - Detecting Adversarial Samples Using Influence Func.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2XPBS5J9,journalArticle,,"Dong, Yinpeng; Fu, Qi-An; Yang, Xiao; Pang, Tianyu; Su, Hang; Xiao, Zihao; Zhu, Jun",Benchmarking Adversarial Robustness on Image Classification,,,,,,"Deep neural networks are vulnerable to adversarial examples, which becomes one of the most important research problems in the development of deep learning. While a lot of efforts have been made in recent years, it is of great significance to perform correct and complete evaluations of the adversarial attack and defense algorithms. In this paper, we establish a comprehensive, rigorous, and coherent benchmark to evaluate adversarial robustness on image classiﬁcation tasks. After brieﬂy reviewing plenty of representative attack and defense methods, we perform large-scale experiments with two robustness curves as the fair-minded evaluation criteria to fully understand the performance of these methods. Based on the evaluation results, we draw several important ﬁndings that can provide insights for future research, including: 1) The relative robustness between models can change across different attack conﬁgurations, thus it is encouraged to adopt the robustness curves to evaluate adversarial robustness; 2) As one of the most effective defense techniques, adversarial training can generalize across different threat models; 3) Randomization-based defenses are more robust to query-based black-box attacks.",,2020/7/17 2:00,2020/7/17 2:00,,11,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\4ENLB3DG\Dong 等。 - Benchmarking Adversarial Robustness on Image Class.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M74DKFAZ,journalArticle,,"Duan, Ranjie; Ma, Xingjun; Wang, Yisen; Bailey, James; Qin, A K; Yang, Yun",Adversarial Camouflage: Hiding Physical-World Attacks With Natural Styles,,,,,,,,2020/7/17 2:01,2020/7/17 2:01,,9,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\T5RHFGRF\Duan 等。 - Adversarial Camouflage Hiding Physical-World Atta.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ADW4SLBY,journalArticle,,"Guo, Minghao; Yang, Yuzhe; Xu, Rui; Liu, Ziwei; Lin, Dahua",When NAS Meets Robustness: In Search of Robust Architectures Against Adversarial Attacks,,,,,,"Recent advances in adversarial attacks uncover the intrinsic vulnerability of modern deep neural networks. Since then, extensive efforts have been devoted to enhancing the robustness of deep networks via specialized learning algorithms and loss functions. In this work, we take an architectural perspective and investigate the patterns of network architectures that are resilient to adversarial attacks. To obtain the large number of networks needed for this study, we adopt one-shot neural architecture search, training a large network for once and then ﬁnetuning the sub-networks sampled therefrom. The sampled architectures together with the accuracies they achieve provide a rich basis for our study. Our “robust architecture Odyssey” reveals several valuable observations: 1) densely connected patterns result in improved robustness; 2) under computational budget, adding convolution operations to direct connection edge is effective; 3) ﬂow of solution procedure (FSP) matrix is a good indicator of network robustness. Based on these observations, we discover a family of robust architectures (RobNets). On various datasets, including CIFAR, SVHN, Tiny-ImageNet, and ImageNet, RobNets exhibit superior robustness performance to other widely used architectures. Notably, RobNets substantially improve the robust accuracy (∼5% absolute gains) under both white-box and blackbox attacks, even with fewer parameter numbers. Code is available at https://github.com/gmh14/RobNets.",,2020/7/17 2:04,2020/7/17 2:04,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\MVD3RLMM\Guo 等。 - When NAS Meets Robustness In Search of Robust Arc.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W4PNAXW2,journalArticle,,"He, Zhezhi; Rakin, Adnan Siraj; Li, Jingtao; Chakrabarti, Chaitali; Fan, Deliang",Defending and Harnessing the Bit-Flip Based Adversarial Weight Attack,,,,,,"Recently, a new paradigm of the adversarial attack on the quantized neural network weights has attracted great attention, namely, the Bit-Flip based adversarial weight attack, aka. Bit-Flip Attack (BFA). BFA has shown extraordinary attacking ability, where the adversary can malfunction a quantized Deep Neural Network (DNN) as a random guess, through malicious bit-ﬂips on a small set of vulnerable weight bits (e.g., 13 out of 93 millions bits of 8-bit quantized ResNet-18). However, there are no effective defensive methods to enhance the fault-tolerance capability of DNN against such BFA. In this work, we conduct comprehensive investigations on BFA and propose to leverage binarizationaware training and its relaxation – piece-wise clustering as simple and effective countermeasures to BFA. The experiments show that, for BFA to achieve the identical prediction accuracy degradation (e.g., below 11% on CIFAR-10), it requires 19.3× and 480.1× more effective malicious bitﬂips on ResNet-20 and VGG-11 respectively, compared to defend-free counterparts.",,2020/7/17 2:04,2020/7/17 2:04,,9,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\FCPVE8FM\He 等。 - Defending and Harnessing the Bit-Flip Based Advers.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDEQTLX3,journalArticle,,"Huang, Lifeng; Gao, Chengying; Zhou, Yuyin; Xie, Cihang; Yuille, Alan L; Zou, Changqing; Liu, Ning",Universal Physical Camouflage Attacks on Object Detectors,,,,,,"In this paper, we study physical adversarial attacks on object detectors in the wild. Previous works mostly craft instance-dependent perturbations only for rigid or planar objects. To this end, we propose to learn an adversarial pattern to effectively attack all instances belonging to the same object category, referred to as Universal Physical Camouﬂage Attack (UPC). Concretely, UPC crafts camouﬂage by jointly fooling the region proposal network, as well as misleading the classiﬁer and the regressor to output errors. In order to make UPC effective for non-rigid or nonplanar objects, we introduce a set of transformations for mimicking deformable properties. We additionally impose optimization constraint to make generated patterns look natural to human observers. To fairly evaluate the effectiveness of different physical-world attacks, we present the ﬁrst standardized virtual database, AttackScenes, which simulates the real 3D world in a controllable and reproducible environment. Extensive experiments suggest the superiority of our proposed UPC compared with existing physical adversarial attackers not only in virtual environments (AttackScenes), but also in real-world physical environments. Code and dataset are available at https:// mesunhlf.github.io/index_physical.html.",,2020/7/17 2:06,2020/7/17 2:06,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\KEFVTGPK\Huang 等。 - Universal Physical Camouflage Attacks on Object De.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3IVKL9YV,journalArticle,,,Attack to Explain Deep Representation,,,,,,"Deep visual models are susceptible to extremely low magnitude perturbations to input images. Though carefully crafted, the perturbation patterns generally appear noisy, yet they are able to perform controlled manipulation of model predictions. This observation is used to argue that deep representation is misaligned with human perception. This paper counter-argues and proposes the ﬁrst attack on deep learning that aims at explaining the learned representation instead of fooling it. By extending the input domain of the manipulative signal and employing a model faithful channelling, we iteratively accumulate adversarial perturbations for a deep model. The accumulated signal gradually manifests itself as a collection of visually salient features of the target label (in model fooling), casting adversarial perturbations as primitive features of the target label. Our attack provides the ﬁrst demonstration of systematically computing perturbations for adversarially nonrobust classiﬁers that comprise salient visual features of objects. We leverage the model explaining character of our algorithm to perform image generation, inpainting and interactive image manipulation by attacking adversarially robust classiﬁers. The visually appealing results across these applications demonstrate the utility of our attack (and perturbations in general) beyond model fooling.",,2020/7/17 2:07,2020/7/17 2:07,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\3LWSGWH8\Attack to Explain Deep Representation.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P9GNGM69,journalArticle,,"Jeddi, Ahmadreza; Shafiee, Mohammad Javad; Karg, Michelle; Scharfenberger, Christian; Wong, Alexander",Learn2Perturb: An End-to-End Feature Perturbation Learning to Improve Adversarial Robustness,,,,,,"While deep neural networks have been achieving stateof-the-art performance across a wide variety of applications, their vulnerability to adversarial attacks limits their widespread deployment for safety-critical applications. Alongside other adversarial defense approaches being investigated, there has been a very recent interest in improving adversarial robustness in deep neural networks through the introduction of perturbations during the training process. However, such methods leverage ﬁxed, pre-deﬁned perturbations and require signiﬁcant hyperparameter tuning that makes them very difﬁcult to leverage in a general fashion. In this study, we introduce Learn2Perturb, an end-to-end feature perturbation learning approach for improving the adversarial robustness of deep neural networks. More speciﬁcally, we introduce novel perturbation-injection modules that are incorporated at each layer to perturb the feature space and increase uncertainty in the network. This feature perturbation is performed at both the training and the inference stages. Furthermore, inspired by the Expectation-Maximization, an alternating back-propagation training algorithm is introduced to train the network and noise parameters consecutively. Experimental results on CIFAR-10 and CIFAR-100 datasets show that the proposed Learn2Perturb method can result in deep neural networks which are 4-7% more robust on l∞ FGSM and PDG adversarial attacks and signiﬁcantly outperforms the state-of-the-art against l2 C&W attack and a wide range of well-known black-box attacks1.",,2020/7/17 2:07,2020/7/17 2:07,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\SS4M7KS2\Jeddi 等。 - Learn2Perturb An End-to-End Feature Perturbation .pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U6XMYJDP,journalArticle,,"Kariyappa, Sanjay; Qureshi, Moinuddin K",Defending Against Model Stealing Attacks With Adaptive Misinformation,,,,,,"Deep Neural Networks (DNNs) are susceptible to model stealing attacks, which allows a data-limited adversary with no knowledge of the training dataset to clone the functionality of a target model, just by using black-box query access. Such attacks are typically carried out by querying the target model using inputs that are synthetically generated or sampled from a surrogate dataset to construct a labeled dataset. The adversary can use this labeled dataset to train a clone model, which achieves a classiﬁcation accuracy comparable to that of the target model. We propose “Adaptive Misinformation” to defend against such model stealing attacks. We identify that all existing model stealing attacks invariably query the target model with Out-OfDistribution (OOD) inputs. By selectively sending incorrect predictions for OOD queries, our defense substantially degrades the accuracy of the attacker’s clone model (by up to 40%), while minimally impacting the accuracy (< 0.5%) for benign users. Compared to existing defenses, our defense has a signiﬁcantly better security vs accuracy tradeoff and incurs minimal computational overhead.",,2020/7/17 2:08,2020/7/17 2:08,,9,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\4Z9KJC5V\Kariyappa 和 Qureshi - Defending Against Model Stealing Attacks With Adap.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QSSN8HTJ,journalArticle,,"Kim, Edward; Rego, Jocelyn; Watkins, Yijing; Kenyon, Garrett T",Modeling Biological Immunity to Adversarial Examples,,,,,,"While deep learning continues to permeate through all ﬁelds of signal processing and machine learning, a critical exploit in these frameworks exists and remains unsolved. These exploits, or adversarial examples, are a type of signal attack that can change the output class of a classiﬁer by perturbing the stimulus signal by an imperceptible amount. The attack takes advantage of statistical irregularities within the training data, where the added perturbations can “move” the image across deep learning decision boundaries. What is even more alarming is the transferability of these attacks to different deep learning models and architectures. This means a successful attack on one model has adversarial effects on other, unrelated models.",,2020/7/17 2:08,2020/7/17 2:08,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\X4UQ6LAF\Kim 等。 - Modeling Biological Immunity to Adversarial Exampl.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IQSWRT9D,journalArticle,,"Kolouri, Soheil; Saha, Aniruddha; Pirsiavash, Hamed; Hoffmann, Heiko",Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs,,,,,,"The unprecedented success of deep neural networks in many applications has made these networks a prime target for adversarial exploitation. In this paper, we introduce a benchmark technique for detecting backdoor attacks (aka Trojan attacks) on deep convolutional neural networks (CNNs). We introduce the concept of Universal Litmus Patterns (ULPs), which enable one to reveal backdoor attacks by feeding these universal patterns to the network and analyzing the output (i.e., classifying the network as ‘clean’ or ‘corrupted’). This detection is fast because it requires only a few forward passes through a CNN. We demonstrate the effectiveness of ULPs for detecting backdoor attacks on thousands of networks with different architectures trained on four benchmark datasets, namely the German Trafﬁc Sign Recognition Benchmark (GTSRB), MNIST, CIFAR10, and Tiny-ImageNet. The codes and train/test models for this paper can be found here: https://umbcvision. github.io/Universal-Litmus-Patterns/.",,2020/7/17 2:09,2020/7/17 2:09,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\Y89KCM9P\Kolouri 等。 - Universal Litmus Patterns Revealing Backdoor Atta.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RJIR87TW,journalArticle,,"Kong, Zelun; Guo, Junfeng; Li, Ang; Liu, Cong",PhysGAN: Generating Physical-World-Resilient Adversarial Examples for Autonomous Driving,,,,,,"Although Deep neural networks (DNNs) are being pervasively used in vision-based autonomous driving systems, they are found vulnerable to adversarial attacks where small-magnitude perturbations into the inputs during test time cause dramatic changes to the outputs. While most of the recent attack methods target at digital-world adversarial scenarios, it is unclear how they perform in the physical world, and more importantly, the generated perturbations under such methods would cover a whole driving scene including those ﬁxed background imagery such as the sky, making them inapplicable to physical world implementation. We present PhysGAN, which generates physical-world-resilient adversarial examples for misleading autonomous driving systems in a continuous manner. We show the effectiveness and robustness of PhysGAN via extensive digital- and real-world evaluations. We compare PhysGAN with a set of state-of-the-art baseline methods, which further demonstrate the robustness and efﬁcacy of our approach. We also show that PhysGAN outperforms state-of-the-art baseline methods. To the best of our knowledge, PhysGAN is probably the ﬁrst technique of generating realistic and physical-world-resilient adversarial examples for attacking common autonomous driving scenarios.",,2020/7/17 2:09,2020/7/17 2:09,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\YZQM4DK8\Kong 等。 - PhysGAN Generating Physical-World-Resilient Adver.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8S2NUJU5,journalArticle,,"Li, Guanlin; Ding, Shuya; Luo, Jun; Liu, Chang",Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder,,,,,,"Whereas adversarial training is employed as the main defence strategy against speciﬁc adversarial samples, it has limited generalization capability and incurs excessive time complexity. In this paper, we propose an attack-agnostic defence framework to enhance the intrinsic robustness of neural networks, without jeopardizing the ability of generalizing clean samples. Our Feature Pyramid Decoder (FPD) framework applies to all block-based convolutional neural networks (CNNs). It implants denoising and image restoration modules into a targeted CNN, and it also constraints the Lipschitz constant of the classiﬁcation layer. Moreover, we propose a two-phase strategy to train the FPD-enhanced CNN, utilizing ǫ-neighbourhood noisy images with multi-task and self-supervised learning. Evaluated against a variety of white-box and black-box attacks, we demonstrate that FPD-enhanced CNNs gain sufﬁcient robustness against general adversarial samples on MNIST, SVHN and CALTECH. In addition, if we further conduct adversarial training, the FPD-enhanced CNNs perform better than their non-enhanced versions.",,2020/7/17 2:11,2020/7/17 2:11,,9,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\2JJ6YD2X\Li 等。 - Enhancing Intrinsic Adversarial Robustness via Fea.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IXZ4TR6D,journalArticle,,"Li, Jie; Ji, Rongrong; Liu, Hong; Liu, Jianzhuang; Zhong, Bineng; Deng, Cheng; Tian, Qi",Projection & Probability-Driven Black-Box Attack,,,,,,,,2020/7/17 2:12,2020/7/17 2:12,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\GDQ3YDRM\Li 等。 - Projection & Probability-Driven Black-Box Attack.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KC56Q92R,journalArticle,,"Li, Huichen; Xu, Xiaojun; Zhang, Xiaolu; Yang, Shuang; Li, Bo",QEBA: Query-Efficient Boundary-Based Blackbox Attack,,,,,,"Machine learning (ML), especially deep neural networks (DNNs) have been widely used in various applications, including several safety-critical ones (e.g. autonomous driving). As a result, recent research about adversarial examples has raised great concerns. Such adversarial attacks can be achieved by adding a small magnitude of perturbation to the input to mislead model prediction. While several whitebox attacks have demonstrated their effectiveness, which assume that the attackers have full access to the machine learning models; blackbox attacks are more realistic in practice. In this paper, we propose a Query-Efﬁcient Boundary-based blackbox Attack (QEBA) based only on model’s ﬁnal prediction labels. We theoretically show why previous boundary-based attack with gradient estimation on the whole gradient space is not efﬁcient in terms of query numbers, and provide optimality analysis for our dimension reduction-based gradient estimation. On the other hand, we conducted extensive experiments on ImageNet and CelebA datasets to evaluate QEBA. We show that compared with the state-of-the-art blackbox attacks, QEBA is able to use a smaller number of queries to achieve a lower magnitude of perturbation with 100% attack success rate. We also show case studies of attacks on real-world APIs including MEGVII Face++ and Microsoft Azure.",,2020/7/17 2:12,2020/7/17 2:12,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\24P3I6WW\Li 等。 - QEBA Query-Efficient Boundary-Based Blackbox Atta.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EGVV73TS,journalArticle,,"Li, Maosen; Deng, Cheng; Li, Tengjiao; Yan, Junchi; Gao, Xinbo; Huang, Heng",Towards Transferable Targeted Attack,,,,,,"An intriguing property of adversarial examples is their transferability, which suggests that black-box attacks are feasible in real-world applications. Previous works mostly study the transferability on non-targeted setting. However, recent studies show that targeted adversarial examples are more difﬁcult to transfer than non-targeted ones. In this paper, we ﬁnd there exist two defects that lead to the difﬁculty in generating transferable examples. First, the magnitude of gradient is decreasing during iterative attack, causing excessive consistency between two successive noises in accumulation of momentum, which is termed as noise curing. Second, it is not enough for targeted adversarial examples to just get close to target class without moving away from true class. To overcome the above problems, we propose a novel targeted attack approach to effectively generate more transferable adversarial examples. Speciﬁcally, we ﬁrst introduce the Poincare´ distance as the similarity metric to make the magnitude of gradient self-adaptive during iterative attack to alleviate noise curing. Furthermore, we regularize the targeted attack process with metric learning to take adversarial examples away from true label and gain more transferable targeted adversarial examples. Experiments on ImageNet validate the superiority of our approach achieving 8% higher attack success rate over other state-ofthe-art methods on average in black-box targeted attack.",,2020/7/17 2:13,2020/7/17 2:13,,9,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\7ZV3N36X\Li 等。 - Towards Transferable Targeted Attack.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TBLYUBK3,journalArticle,,"Lu, Yantao; Jia, Yunhan; Wang, Jianyu; Li, Bai; Chai, Weiheng; Carin, Lawrence; Velipasalar, Senem",Enhancing Cross-Task Black-Box Transferability of Adversarial Examples With Dispersion Reduction,,,,,,"Neural networks are known to be vulnerable to carefully crafted adversarial examples, and these malicious samples often transfer, i.e., they remain adversarial even against other models. Although signiﬁcant effort has been devoted to the transferability across models, surprisingly little attention has been paid to cross-task transferability, which represents the real-world cybercriminal’s situation, where an ensemble of different defense/detection mechanisms need to be evaded all at once. We investigate the transferability of adversarial examples across a wide range of real-world computer vision tasks, including image classiﬁcation, object detection, semantic segmentation, explicit content detection, and text detection. Our proposed attack minimizes the “dispersion” of the internal feature map, overcoming the limitations of existing attacks, that require task-speciﬁc loss functions and/or probing a target model. We conduct evaluation on open-source detection and segmentation models, as well as four different computer vision tasks provided by Google Cloud Vision (GCV) APIs. We demonstrate that our approach outperforms existing attacks by degrading performance of multiple CV tasks by a large margin with only modest perturbations.",,2020/7/17 2:16,2020/7/17 2:16,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\6DSU4L9L\Lu 等。 - Enhancing Cross-Task Black-Box Transferability of .pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DB92UVU8,journalArticle,,"Mohapatra, Jeet; Weng, Tsui-Wei; Chen, Pin-Yu; Liu, Sijia; Daniel, Luca",Towards Verifying Robustness of Neural Networks Against A Family of Semantic Perturbations,,,,,,"Verifying robustness of neural networks given a speciﬁed threat model is a fundamental yet challenging task. While current veriﬁcation methods mainly focus on the ℓp-norm threat model of the input instances, robustness veriﬁcation against semantic adversarial attacks inducing large ℓp-norm perturbations, such as color shifting and lighting adjustment, are beyond their capacity. To bridge this gap, we propose Semantify-NN, a model-agnostic and generic robustness veriﬁcation approach against semantic perturbations for neural networks. By simply inserting our proposed semantic perturbation layers (SP-layers) to the input layer of any given model, Semantify-NN is model-agnostic, and any ℓp-norm based veriﬁcation tools can be used to verify the model robustness against semantic perturbations. We illustrate the principles of designing the SP-layers and provide examples including semantic perturbations to image classiﬁcation in the space of hue, saturation, lightness, brightness, contrast and rotation, respectively. In addition, an efﬁcient reﬁnement technique is proposed to further signiﬁcantly improve the semantic certiﬁcate. Experiments on various network architectures and different datasets demonstrate the superior veriﬁcation performance of Semantify-NN over ℓp-norm-based veriﬁcation frameworks that naively convert semantic perturbation to ℓp-norm. The results show that Semantify-NN can support robustness veriﬁcation against a wide range of semantic perturbations.",,2020/7/17 2:19,2020/7/17 2:19,,9,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\YPNT7B8D\Mohapatra 等。 - Towards Verifying Robustness of Neural Networks Ag.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A2CHGWI7,journalArticle,,"Naseer, Muzammal; Khan, Salman; Hayat, Munawar; Khan, Fahad Shahbaz; Porikli, Fatih",A Self-supervised Approach for Adversarial Robustness,,,,,,"Adversarial examples can cause catastrophic mistakes in Deep Neural Network (DNNs) based vision systems e.g., for classiﬁcation, segmentation and object detection. The vulnerability of DNNs against such attacks can prove a major roadblock towards their real-world deployment. Transferability of adversarial examples demand generalizable defenses that can provide cross-task protection. Adversarial training that enhances robustness by modifying target model’s parameters lacks such generalizability. On the other hand, different input processing based defenses fall short in the face of continuously evolving attacks. In this paper, we take the ﬁrst step to combine the beneﬁts of both approaches and propose a self-supervised adversarial training mechanism in the input space. By design, our defense is a generalizable approach and provides signiﬁcant robustness against the unseen adversarial attacks (e.g. by reducing the success rate of translation-invariant ensemble attack from 82.6% to 31.9% in comparison to previous stateof-the-art). It can be deployed as a plug-and-play solution to protect a variety of vision systems, as we demonstrate for the case of classiﬁcation, segmentation and detection. Code is available at: https://github.com/ Muzammal-Naseer/NRP.",,2020/7/17 2:19,2020/7/17 2:19,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\CIELAMQX\Naseer 等。 - A Self-supervised Approach for Adversarial Robustn.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9VSRJ2M2,journalArticle,,"Rahmati, Ali; Moosavi-Dezfooli, Seyed-Mohsen; Frossard, Pascal; Dai, Huaiyu",GeoDA: A Geometric Framework for Black-Box Adversarial Attacks,,,,,,,,2020/7/17 2:22,2020/7/17 2:22,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\NRL4KII8\Rahmati 等。 - GeoDA A Geometric Framework for Black-Box Adversa.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GMBXAK8Y,journalArticle,,"Rahnama, Arash; Nguyen, Andre T; Raff, Edward",Robust Design of Deep Neural Networks Against Adversarial Attacks Based on Lyapunov Theory,,,,,,"Deep neural networks (DNNs) are vulnerable to subtle adversarial perturbations applied to the input. These adversarial perturbations, though imperceptible, can easily mislead the DNN. In this work, we take a control theoretic approach to the problem of robustness in DNNs. We treat each individual layer of the DNN as a nonlinear system and use Lyapunov theory to prove stability and robustness locally. We then proceed to prove stability and robustness globally for the entire DNN. We develop empirically tight bounds on the response of the output layer, or any hidden layer, to adversarial perturbations added to the input, or to any preceding hidden layer. We show how the spectral norm of the weight matrix for an individual layer relates to Lyapunov properties of that layer, and consequently to the local and global stability and robustness of the DNN. Our results give new insights into how spectral norm regularization can mitigate the adversarial effects. Finally, we evaluate the power of our approach on a variety of data sets and network architectures and against some of the well-known adversarial attacks.",,2020/7/17 2:22,2020/7/17 2:22,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\59DXVKA8\Rahnama 等。 - Robust Design of Deep Neural Networks Against Adve.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
95Z53Z3R,journalArticle,,"Rakin, Adnan Siraj; He, Zhezhi; Fan, Deliang",TBT: Targeted Neural Network Attack With Bit Trojan,,,,,,,,2020/7/17 2:22,2020/7/17 2:22,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\CE8MU5HC\Rakin 等。 - TBT Targeted Neural Network Attack With Bit Troja.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDGW5TY9,journalArticle,,"Shamsabadi, Ali Shahin; Sanchez-Matilla, Ricardo; Cavallaro, Andrea",ColorFool: Semantic Adversarial Colorization,,,,,,"Adversarial attacks that generate small Lp-norm perturbations to mislead classiﬁers have limited success in black-box settings and with unseen classiﬁers. These attacks are also not robust to defenses that use denoising ﬁlters and to adversarial training procedures. Instead, adversarial attacks that generate unrestricted perturbations are more robust to defenses, are generally more successful in black-box settings and are more transferable to unseen classiﬁers. However, unrestricted perturbations may be noticeable to humans. In this paper, we propose a content-based black-box adversarial attack that generates unrestricted perturbations by exploiting image semantics to selectively modify colors within chosen ranges that are perceived as natural by humans. We show that the proposed approach, ColorFool, outperforms in terms of success rate, robustness to defense frameworks and transferability, ﬁve state-of-the-art adversarial attacks on two different tasks, scene and object classiﬁcation, when attacking three state-of-the-art deep neural networks using three standard datasets. The source code is available at https: //github.com/smartcameras/ColorFool.",,2020/7/17 2:24,2020/7/17 2:24,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\F9YFFV7L\Shamsabadi 等。 - ColorFool Semantic Adversarial Colorization.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KM87I36C,journalArticle,,"Shi, Yucheng; Han, Yahong; Tian, Qi",Polishing Decision-Based Adversarial Noise With a Customized Sampling,,,,,,"As an effective black-box adversarial attack, decisionbased methods polish adversarial noise by querying the target model. Among them, boundary attack is widely applied due to its powerful noise compression capability, especially when combined with transfer-based methods. Boundary attack splits the noise compression into several independent sampling processes, repeating each query with a constant sampling setting. In this paper, we demonstrate the advantage of using current noise and historical queries to customize the variance and mean of sampling in boundary attack to polish adversarial noise. We further reveal the relationship between the initial noise and the compressed noise in boundary attack. We propose Customized Adversarial Boundary (CAB) attack that uses the current noise to model the sensitivity of each pixel and polish adversarial noise of each image with a customized sampling setting. On the one hand, CAB uses current noise as a prior belief to customize the multivariate normal distribution. On the other hand, CAB keeps the new samplings away from historical failed queries to avoid similar mistakes. Experimental results measured on several image classiﬁcation datasets emphasizes the validity of our method.",,2020/7/17 2:25,2020/7/17 2:25,,9,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\7KPK92IH\Shi 等。 - Polishing Decision-Based Adversarial Noise With a .pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6RL57GC9,journalArticle,,"Stehouwer, Joel; Jourabloo, Amin; Liu, Yaojie; Liu, Xiaoming","Noise Modeling, Synthesis and Classification for Generic Object Anti-Spoofing",,,,,,"Using printed photograph and replaying videos of biometric modalities, such as iris, ﬁngerprint and face, are common attacks to fool the recognition systems for granting access as the genuine user. With the growing online personto-person shopping (e.g., Ebay and Craigslist), such attacks also threaten those services, where the online photo illustration might not be captured from real items but from paper or digital screen. Thus, the study of anti-spooﬁng should be extended from modality-speciﬁc solutions to generic-objectbased ones. In this work, we deﬁne and tackle the problem of Generic Object Anti-Spooﬁng (GOAS) for the ﬁrst time. One signiﬁcant cue to detect these attacks is the noise patterns introduced by the capture sensors and spoof mediums. Different sensor/medium combinations can result in diverse noise patterns. We propose a GAN-based architecture to synthesize and identify the noise patterns from seen and unseen medium/sensor combinations. We show that the procedure of synthesis and identiﬁcation are mutually beneﬁcial. We further demonstrate the learned GOAS models can directly contribute to modality-speciﬁc anti-spooﬁng without domain transfer. The code and GOSet dataset are available at cvlab.cse.msu.edu/project-goas.html.",,2020/7/17 2:26,2020/7/17 2:26,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,"C:\Users\zhbli\Zotero\storage\PIX7KEYI\Stehouwer 等。 - Noise Modeling, Synthesis and Classification for G.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CEGGET6G,journalArticle,,"Sun, Hao; Zhao, Zhiqun; He, Zhihai",Reciprocal Learning Networks for Human Trajectory Prediction,,,,,,"We observe that the human trajectory is not only forward predictable, but also backward predictable. Both forward and backward trajectories follow the same social norms and obey the same physical constraints with the only difference in their time directions. Based on this unique property, we develop a new approach, called reciprocal learning, for human trajectory prediction. Two networks, forward and backward prediction networks, are tightly coupled, satisfying the reciprocal constraint, which allows them to be jointly learned. Based on this constraint, we borrow the concept of adversarial attacks of deep neural networks, which iteratively modiﬁes the input of the network to match the given or forced network output, and develop a new method for network prediction, called reciprocal attack for matched prediction. It further improves the prediction accuracy. Our experimental results on benchmark datasets demonstrate that our new method outperforms the state-ofthe-art methods for human trajectory prediction.",,2020/7/17 2:26,2020/7/17 2:26,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\ZLUP3WJ5\Sun 等。 - Reciprocal Learning Networks for Human Trajectory .pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XCU4ZBY2,journalArticle,,"Tu, James; Ren, Mengye; Manivasagam, Sivabalan; Liang, Ming; Yang, Bin; Du, Richard; Cheng, Frank; Urtasun, Raquel",Physically Realizable Adversarial Examples for LiDAR Object Detection,,,,,,"Modern autonomous driving systems rely heavily on deep learning models to process point cloud sensory data; meanwhile, deep models have been shown to be susceptible to adversarial attacks with visually imperceptible perturbations. Despite the fact that this poses a security concern for the self-driving industry, there has been very little exploration in terms of 3D perception, as most adversarial attacks have only been applied to 2D ﬂat images. In this paper, we address this issue and present a method to generate universal 3D adversarial objects to fool LiDAR detectors. In particular, we demonstrate that placing an adversarial object on the rooftop of any target vehicle to hide the vehicle entirely from LiDAR detectors with a success rate of 80%. We report attack results on a suite of detectors using various input representation of point clouds. We also conduct a pilot study on adversarial defense using data augmentation. This is one step closer towards safer self-driving under unseen conditions from limited training data.",,2020/7/17 2:28,2020/7/17 2:28,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\AGSAI5XP\Tu 等。 - Physically Realizable Adversarial Examples for LiD.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2UUSW2N4,journalArticle,,"Wang, Guoqing; Han, Hu; Shan, Shiguang; Chen, Xilin",Cross-Domain Face Presentation Attack Detection via Multi-Domain Disentangled Representation Learning,,,,,,"Face presentation attack detection (PAD) has been an urgent problem to be solved in the face recognition systems. Conventional approaches usually assume the testing and training are within the same domain; as a result, they may not generalize well into unseen scenarios because the representations learned for PAD may overﬁt to the subjects in the training set. In light of this, we propose an efﬁcient disentangled representation learning for cross-domain face PAD. Our approach consists of disentangled representation learning (DR-Net) and multi-domain learning (MD-Net). DR-Net learns a pair of encoders via generative models that can disentangle PAD informative features from subject discriminative features. The disentangled features from different domains are fed to MD-Net which learns domainindependent features for the ﬁnal cross-domain face PAD task. Extensive experiments on several public datasets validate the effectiveness of the proposed approach for crossdomain PAD.",,2020/7/17 2:30,2020/7/17 2:30,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\MGCJS4QJ\Wang 等。 - Cross-Domain Face Presentation Attack Detection vi.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E5UZKSKK,journalArticle,,"Wang, Zezheng; Yu, Zitong; Zhao, Chenxu; Zhu, Xiangyu; Qin, Yunxiao; Zhou, Qiusheng; Zhou, Feng; Lei, Zhen",Deep Spatial Gradient and Temporal Depth Learning for Face Anti-Spoofing,,,,,,"Face anti-spooﬁng is critical to the security of face recognition systems. Depth supervised learning has been proven as one of the most effective methods for face antispooﬁng. Despite the great success, most previous works still formulate the problem as a single-frame multi-task one by simply augmenting the loss with depth, while neglecting the detailed ﬁne-grained information and the interplay between facial depths and moving patterns. In contrast, we design a new approach to detect presentation attacks from multiple frames based on two insights: 1) detailed discriminative clues (e.g., spatial gradient magnitude) between living and spooﬁng face may be discarded through stacked vanilla convolutions, and 2) the dynamics of 3D moving faces provide important clues in detecting the spooﬁng faces. The proposed method is able to capture discriminative details via Residual Spatial Gradient Block (RSGB) and encode spatio-temporal information from Spatio-Temporal Propagation Module (STPM) efﬁciently. Moreover, a novel Contrastive Depth Loss is presented for more accurate depth supervision. To assess the efﬁcacy of our method, we also collect a Double-modal Anti-spooﬁng Dataset (DMAD) which provides actual depth for each sample. The experiments demonstrate that the proposed approach achieves state-of-the-art results on ﬁve benchmark datasets including OULU-NPU, SiW, CASIAMFSD, Replay-Attack, and the new DMAD. Codes will be available at https://github.com/clks-wzz/ FAS-SGTD.",,2020/7/17 2:30,2020/7/17 2:30,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\3MXETYTM\Wang 等。 - Deep Spatial Gradient and Temporal Depth Learning .pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AZTU2LNP,journalArticle,,"Wang, Guangcong; Lai, Jian-Huang; Liang, Wenqi; Wang, Guangrun",Smoothing Adversarial Domain Attack and P-Memory Reconsolidation for Cross-Domain Person Re-Identification,,,,,,"Most of the existing person re-identiﬁcation (re-ID) methods achieve promising accuracy in a supervised manner, but they assume the identity labels of the target domain is available. This greatly limits the scalability of person re-ID in real-world scenarios. Therefore, the current person re-ID community focuses on the cross-domain person re-ID that aims to transfer the knowledge from a labeled source domain to an unlabeled target domain and exploits the speciﬁc knowledge from the data distribution of the target domain to further improve the performance. To reduce the gap between the source and target domains, we propose a Smoothing Adversarial Domain Attack (SADA) approach that guides the source domain images to align the target domain images by using a trained camera classiﬁer. To stabilize a memory trace of cross-domain knowledge transfer after its initial acquisition from the source domain, we propose a p-Memory Reconsolidation (pMR) method that reconsolidates the source knowledge with a small probability p during the self-training of the target domain. With both SADA and pMR, the proposed method signiﬁcantly improves the cross-domain person re-ID. Extensive experiments on Market-1501 and DukeMTMC-reID benchmarks show that our pMR-SADA outperforms all of the state-ofthe-arts by a large margin.",,2020/7/17 2:31,2020/7/17 2:31,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\6HKRCWML\Wang 等。 - Smoothing Adversarial Domain Attack and P-Memory R.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TM44D7AI,journalArticle,,"Wang, Hongjun; Wang, Guangrun; Li, Ya; Zhang, Dongyu; Lin, Liang","Transferable, Controllable, and Inconspicuous Adversarial Attacks on Person Re-identification With Deep Mis-Ranking",,,,,,"The success of DNNs has driven the extensive applications of person re-identiﬁcation (ReID) into a new era. However, whether ReID inherits the vulnerability of DNNs remains unexplored. To examine the robustness of ReID systems is rather important because the insecurity of ReID systems may cause severe losses, e.g., the criminals may use the adversarial perturbations to cheat the CCTV systems.",,2020/7/17 2:32,2020/7/17 2:32,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,"C:\Users\zhbli\Zotero\storage\FARX9QU8\Wang 等。 - Transferable, Controllable, and Inconspicuous Adve.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QS4UJIGX,journalArticle,,"Wu, Weibin; Su, Yuxin; Chen, Xixian; Zhao, Shenglin; King, Irwin; Lyu, Michael R; Tai, Yu-Wing",Boosting the Transferability of Adversarial Samples via Attention,,,,,,"The widespread deployment of deep models necessitates the assessment of model vulnerability in practice, especially for safety- and security-sensitive domains such as autonomous driving and medical diagnosis. Transfer-based attacks against image classiﬁers thus elicit mounting interest, where attackers are required to craft adversarial images based on local proxy models without the feedback information from remote target ones. However, under such a challenging but practical setup, the synthesized adversarial samples often achieve limited success due to overﬁtting to the local model employed. In this work, we propose a novel mechanism to alleviate the overﬁtting issue. It computes model attention over extracted features to regularize the search of adversarial examples, which prioritizes the corruption of critical features that are likely to be adopted by diverse architectures. Consequently, it can promote the transferability of resultant adversarial instances. Extensive experiments on ImageNet classiﬁers conﬁrm the effectiveness of our strategy and its superiority to state-of-the-art benchmarks in both white-box and black-box settings.",,2020/7/17 2:33,2020/7/17 2:33,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\6C5SBXSZ\Wu 等。 - Boosting the Transferability of Adversarial Sample.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9RLYGN9B,journalArticle,,"Wu, Weibin; Su, Yuxin; Chen, Xixian; Zhao, Shenglin; King, Irwin; Lyu, Michael R; Tai, Yu-Wing",Towards Global Explanations of Convolutional Neural Networks With Concept Attribution,,,,,,"With the growing prevalence of convolutional neural networks (CNNs), there is an urgent demand to explain their behaviors. Global explanations contribute to understanding model predictions on a whole category of samples, and thus have attracted increasing interest recently. However, existing methods overwhelmingly conduct separate input attribution or rely on local approximations of models, making them fail to offer faithful global explanations of CNNs. To overcome such drawbacks, we propose a novel two-stage framework, Attacking for Interpretability (AfI), which explains model decisions in terms of the importance of userdeﬁned concepts. AfI ﬁrst conducts a feature occlusion analysis, which resembles a process of attacking models to derive the category-wide importance of different features. We then map the feature importance to concept importance through ad-hoc semantic tasks. Experimental results conﬁrm the effectiveness of AfI and its superiority in providing more accurate estimations of concept importance than existing proposals.",,2020/7/17 2:34,2020/7/17 2:34,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\2MS63RG9\Wu 等。 - Towards Global Explanations of Convolutional Neura.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VPDGM5AB,journalArticle,,"Xu, Xing; Chen, Jiefu; Xiao, Jinhui; Gao, Lianli; Shen, Fumin; Shen, Heng Tao",What Machines See Is Not What They Get: Fooling Scene Text Recognition Models With Adversarial Text Images,,,,,,"The research on scene text recognition (STR) has made remarkable progress in recent years with the development of deep neural networks (DNNs). Recent studies on adversarial attack have veriﬁed that a DNN model designed for non-sequential tasks (e.g., classiﬁcation, segmentation and retrieval) can be easily fooled by adversarial examples. Actually, STR is an application highly related to security issues. However, there are few studies considering the safety and reliability of STR models that make sequential prediction. In this paper, we make the ﬁrst attempt in attacking the state-of-the-art DNN-based STR models. Speciﬁcally, we propose a novel and efﬁcient optimization-based method that can be naturally integrated to different sequential prediction schemes, i.e., connectionist temporal classiﬁcation (CTC) and attention mechanism. We apply our proposed method to ﬁve state-of-the-art STR models with both targeted and untargeted attack modes, the comprehensive results on 7 real-world datasets and 2 synthetic datasets consistently show the vulnerability of these STR models with a signiﬁcant performance drop. Finally, we also test our attack method on a real-world STR engine of Baidu OCR, which demonstrates the practical potentials of our method.",,2020/7/17 2:35,2020/7/17 2:35,,11,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\WPJZDULD\Xu 等。 - What Machines See Is Not What They Get Fooling Sc.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H8ASW4M7,journalArticle,,"Yan, Bin; Wang, Dong; Lu, Huchuan; Yang, Xiaoyun",Cooling-Shrinking Attack: Blinding the Tracker With Imperceptible Noises,,,,,,"Adversarial attack of CNN aims at deceiving models to misbehave by adding imperceptible perturbations to images. This feature facilitates to understand neural networks deeply and to improve the robustness of deep learning models. Although several works have focused on attacking image classiﬁers and object detectors, an effective and efﬁcient method for attacking single object trackers of any target in a model-free way remains lacking. In this paper, a cooling-shrinking attack method is proposed to deceive state-of-the-art SiameseRPN-based trackers. An effective and efﬁcient perturbation generator is trained with a carefully designed adversarial loss, which can simultaneously cool hot regions where the target exists on the heatmaps and force the predicted bounding box to shrink, making the tracked target invisible to trackers. Numerous experiments on OTB100, VOT2018, and LaSOT datasets show that our method can effectively fool the state-of-theart SiameseRPN++ tracker by adding small perturbations to the template or the search regions. Besides, our method has good transferability and is able to deceive other topperformance trackers such as DaSiamRPN, DaSiamRPNUpdateNet, and DiMP. The source codes are available at https://github.com/MasterBin-IIAU/CSA.",,2020/7/17 2:35,2020/7/17 2:35,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\H5KCCESR\Yan 等。 - Cooling-Shrinking Attack Blinding the Tracker Wit.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K6GSNUEZ,journalArticle,,"Yu, Zitong; Zhao, Chenxu; Wang, Zezheng; Qin, Yunxiao; Su, Zhuo; Li, Xiaobai; Zhou, Feng; Zhao, Guoying",Searching Central Difference Convolutional Networks for Face Anti-Spoofing,,,,,,"Face anti-spooﬁng (FAS) plays a vital role in face recognition systems. Most state-of-the-art FAS methods 1) rely on stacked convolutions and expert-designed network, which is weak in describing detailed ﬁne-grained information and easily being ineffective when the environment varies (e.g., different illumination), and 2) prefer to use long sequence as input to extract dynamic features, making them difﬁcult to deploy into scenarios which need quick response. Here we propose a novel frame level FAS method based on Central Difference Convolution (CDC), which is able to capture intrinsic detailed patterns via aggregating both intensity and gradient information. A network built with CDC, called the Central Difference Convolutional Network (CDCN), is able to provide more robust modeling capacity than its counterpart built with vanilla convolution. Furthermore, over a speciﬁcally designed CDC search space, Neural Architecture Search (NAS) is utilized to discover a more powerful network structure (CDCN++), which can be assembled with Multiscale Attention Fusion Module (MAFM) for further boosting performance. Comprehensive experiments are performed on six benchmark datasets to show that 1) the proposed method not only achieves superior performance on intra-dataset testing (especially 0.2% ACER in Protocol1 of OULU-NPU dataset), 2) it also generalizes well on cross-dataset testing (particularly 6.5% HTER from CASIAMFSD to Replay-Attack datasets). The codes are available at https://github.com/ZitongYu/CDCN.",,2020/7/17 2:39,2020/7/17 2:39,,11,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\KFANIK5T\Yu 等。 - Searching Central Difference Convolutional Network.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Y5IXAUJP,journalArticle,,"Yuan, Jianhe; He, Zhihai",Ensemble Generative Cleaning With Feedback Loops for Defending Adversarial Attacks,,,,,,"Effective defense of deep neural networks against adversarial attacks remains a challenging problem, especially under powerful white-box attacks. In this paper, we develop a new method called ensemble generative cleaning with feedback loops (EGC-FL) for effective defense of deep neural networks. The proposed EGC-FL method is based on two central ideas. First, we introduce a transformed deadzone layer into the defense network, which consists of an orthonormal transform and a deadzone-based activation function, to destroy the sophisticated noise pattern of adversarial attacks. Second, by constructing a generative cleaning network with a feedback loop, we are able to generate an ensemble of diverse estimations of the original clean image. We then learn a network to fuse this set of diverse estimations together to restore the original image. Our extensive experimental results demonstrate that our approach improves the state-of-art by large margins in both white-box and black-box attacks. It signiﬁcantly improves the classiﬁcation accuracy for white-box PGD attacks upon the second best method by more than 29% on the SVHN dataset and more than 39% on the challenging CIFAR-10 dataset.",,2020/7/17 2:39,2020/7/17 2:39,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\WLL6CTWW\Yuan 和 He - Ensemble Generative Cleaning With Feedback Loops f.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8R6AMMAA,journalArticle,,"Zhang, Yuheng; Jia, Ruoxi; Pei, Hengzhi; Wang, Wenxiao; Li, Bo; Song, Dawn",The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks,,,,,,"This paper studies model-inversion attacks, in which the access to a model is abused to infer information about the training data. Since its ﬁrst introduction by [7], such attacks have raised serious concerns given that training data usually contain privacy-sensitive information. Thus far, successful model-inversion attacks have only been demonstrated on simple models, such as linear regression and logistic regression. Previous attempts to invert neural networks, even the ones with simple architectures, have failed to produce convincing results. We present a novel attack method, termed the generative model-inversion attack, which can invert deep neural networks with high success rates. Rather than reconstructing private training data from scratch, we leverage partial public information, which can be very generic, to learn a distributional prior via generative adversarial networks (GANs) and use it to guide the inversion process. Moreover, we theoretically prove that a model’s predictive power and its vulnerability to inversion attacks are indeed two sides of the same coin—highly predictive models are able to establish a strong correlation between features and labels, which coincides exactly with what an adversary exploits to mount the attacks. Our extensive experiments demonstrate that the proposed attack improves identiﬁcation accuracy over the existing work by about 75% for reconstructing face images from a state-of-the-art face recognition classiﬁer. We also show that differential privacy, in its canonical form, is of little avail to defend against our attacks.",,2020/7/17 2:42,2020/7/17 2:42,,9,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\ILU45F5R\Zhang 等。 - The Secret Revealer Generative Model-Inversion At.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GDF8BTLV,journalArticle,,"Zhang, Chaoning; Benz, Philipp; Imtiaz, Tooba; Kweon, In So",Understanding Adversarial Examples From the Mutual Influence of Images and Perturbations,,,,,,"A wide variety of works have explored the reason for the existence of adversarial examples, but there is no consensus on the explanation. We propose to treat the DNN logits as a vector for feature representation, and exploit them to analyze the mutual inﬂuence of two independent inputs based on the Pearson correlation coefﬁcient (PCC). We utilize this vector representation to understand adversarial examples by disentangling the clean images and adversarial perturbations, and analyze their inﬂuence on each other. Our results suggest a new perspective towards the relationship between images and universal perturbations: Universal perturbations contain dominant features, and images behave like noise to them. This feature perspective leads to a new method for generating targeted universal adversarial perturbations using random source images. We are the ﬁrst to achieve the challenging task of a targeted universal attack without utilizing original training data. Our approach using a proxy dataset achieves comparable performance to the state-of-the-art baselines which utilize the original training dataset.",,2020/7/17 2:42,2020/7/17 2:42,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\M7LTPE7M\Zhang 等。 - Understanding Adversarial Examples From the Mutual.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F3AK7YSR,journalArticle,,"Zhao, Shihao; Ma, Xingjun; Zheng, Xiang; Bailey, James; Chen, Jingjing; Jiang, Yu-Gang",Clean-Label Backdoor Attacks on Video Recognition Models,,,,,,"Deep neural networks (DNNs) are vulnerable to backdoor attacks which can hide backdoor triggers in DNNs by poisoning training data. A backdoored model behaves normally on clean test images, yet consistently predicts a particular target class for any test examples that contain the trigger pattern. As such, backdoor attacks are hard to detect, and have raised severe security concerns in real-world applications. Thus far, backdoor research has mostly been conducted in the image domain with image classiﬁcation models. In this paper, we show that existing image backdoor attacks are far less effective on videos, and outline 4 strict conditions where existing attacks are likely to fail: 1) scenarios with more input dimensions (eg. videos), 2) scenarios with high resolution, 3) scenarios with a large number of classes and few examples per class (a “sparse dataset”), and 4) attacks with access to correct labels (eg. clean-label attacks). We propose the use of a universal adversarial trigger as the backdoor trigger to attack video recognition models, a situation where backdoor attacks are likely to be challenged by the above 4 strict conditions. We show on benchmark video datasets that our proposed backdoor attack can manipulate state-of-the-art video models with high success rates by poisoning only a small proportion of training data (without changing the labels). We also show that our proposed backdoor attack is resistant to state-of-the-art backdoor defense/detection methods, and can even be applied to improve image backdoor attacks. Our proposed video backdoor attack not only serves as a strong baseline for improving the robustness of video models, but also provides a new perspective for more understanding more powerful backdoor attacks.",,2020/7/17 2:42,2020/7/17 2:42,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\H432RS3X\Zhao 等。 - Clean-Label Backdoor Attacks on Video Recognition .pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CW2KEG98,journalArticle,,"Zhao, Yue; Wu, Yuwei; Chen, Caihua; Lim, Andrew",On Isometry Robustness of Deep 3D Point Cloud Models Under Adversarial Attacks,,,,,,"While deep learning in 3D domain has achieved revolutionary performance in many tasks, the robustness of these models has not been sufﬁciently studied or explored. Regarding the 3D adversarial samples, most existing works focus on manipulation of local points, which may fail to invoke the global geometry properties, like robustness under linear projection that preserves the Euclidean distance, i.e., isometry. In this work, we show that existing state-ofthe-art deep 3D models are extremely vulnerable to isometry transformations. Armed with the Thompson Sampling, we develop a black-box attack with success rate over 95% on ModelNet40 data set. Incorporating with the Restricted Isometry Property, we propose a novel framework of whitebox attack on top of spectral norm based perturbation. In contrast to previous works, our adversarial samples are experimentally shown to be strongly transferable. Evaluated on a sequence of prevailing 3D models, our white-box attack achieves success rates from 98.88% to 100%. It maintains a successful attack rate over 95% even within an imperceptible rotation range [±2.81◦].",,2020/7/17 2:42,2020/7/17 2:42,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\Q6IKFKQ2\Zhao 等。 - On Isometry Robustness of Deep 3D Point Cloud Mode.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RHG82MJE,journalArticle,,"Zheng, Haizhong; Zhang, Ziqi; Gu, Juncheng; Lee, Honglak; Prakash, Atul",Efficient Adversarial Training With Transferable Adversarial Examples,,,,,,"Adversarial training is an effective defense method to protect classiﬁcation models against adversarial attacks. However, one limitation of this approach is that it can require orders of magnitude additional training time due to high cost of generating strong adversarial examples during training. In this paper, we ﬁrst show that there is high transferability between models from neighboring epochs in the same training process, i.e., adversarial examples from one epoch continue to be adversarial in subsequent epochs. Leveraging this property, we propose a novel method, Adversarial Training with Transferable Adversarial Examples (ATTA), that can enhance the robustness of trained models and greatly improve the training efﬁciency by accumulating adversarial perturbations through epochs. Compared to state-of-the-art adversarial training methods, ATTA enhances adversarial accuracy by up to 7.2% on CIFAR10 and requires 12 ∼ 14× less training time on MNIST and CIFAR10 datasets with comparable model robustness.",,2020/7/17 2:43,2020/7/17 2:43,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\HWDAJTDL\Zheng 等。 - Efficient Adversarial Training With Transferable A.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4BV3M7WN,journalArticle,,"Zhou, Mingyi; Wu, Jing; Liu, Yipeng; Liu, Shuaicheng; Zhu, Ce",DaST: Data-Free Substitute Training for Adversarial Attacks,,,,,,"Machine learning models are vulnerable to adversarial examples. For the black-box setting, current substitute attacks need pre-trained models to generate adversarial examples. However, pre-trained models are hard to obtain in real-world tasks. In this paper, we propose a data-free substitute training method (DaST) to obtain substitute models for adversarial black-box attacks without the requirement of any real data. To achieve this, DaST utilizes specially designed generative adversarial networks (GANs) to train the substitute models. In particular, we design a multi-branch architecture and label-control loss for the generative model to deal with the uneven distribution of synthetic samples. The substitute model is then trained by the synthetic samples generated by the generative model, which are labeled by the attacked model subsequently. The experiments demonstrate the substitute models produced by DaST can achieve competitive performance compared with the baseline models which are trained by the same train set with attacked models. Additionally, to evaluate the practicability of the proposed method on the real-world task, we attack an online machine learning model on the Microsoft Azure platform. The remote model misclassiﬁes 98.35% of the adversarial examples crafted by our method. To the best of our knowledge, we are the ﬁrst to train a substitute model for adversarial attacks without any real data. Our codes are publicly available 1.",,2020/7/17 2:43,2020/7/17 2:43,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\HBCVQNC2\Zhou 等。 - DaST Data-Free Substitute Training for Adversaria.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SY9ZDRJB,journalArticle,,"Zhou, Hang; Chen, Dongdong; Liao, Jing; Chen, Kejiang; Dong, Xiaoyi; Liu, Kunlin; Zhang, Weiming; Hua, Gang; Yu, Nenghai",LG-GAN: Label Guided Adversarial Network for Flexible Targeted Attack of Point Cloud Based Deep Networks,,,,,,"Deep neural networks have made tremendous progress in 3D point-cloud recognition. Recent works have shown that these 3D recognition networks are also vulnerable to adversarial samples produced from various attack methods, including optimization-based 3D Carlini-Wagner attack, gradient-based iterative fast gradient method, and skeleton-detach based point-dropping. However, after a careful analysis, these methods are either extremely slow because of the optimization/iterative scheme, or not ﬂexible to support targeted attack of a speciﬁc category. To overcome these shortcomings, this paper proposes a novel label guided adversarial network (LG-GAN) for real-time ﬂexible targeted point cloud attack. To the best of our knowledge, this is the ﬁrst generation based 3D point cloud attack method. By feeding the original point clouds and target attack label into LG-GAN, it can learn how to deform the point clouds to mislead the recognition network into the speciﬁc label only with a single forward pass. In detail, LGGAN ﬁrst leverages one multi-branch adversarial network to extract hierarchical features of the input point clouds, then incorporates the speciﬁed label information into multiple intermediate features using the label encoder. Finally, the encoded features will be fed into the coordinate reconstruction decoder to generate the target adversarial sample. By evaluating different point-cloud recognition models (e.g., PointNet, PointNet++ and DGCNN), we demonstrate that the proposed LG-GAN can support ﬂexible targeted attack on the ﬂy while guaranteeing good attack performance and higher efﬁciency simultaneously.",,2020/7/17 2:44,2020/7/17 2:44,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\KEJUDXAC\Zhou 等。 - LG-GAN Label Guided Adversarial Network for Flexi.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZDSLAIBJ,journalArticle,,"Zoran, Daniel; Chrzanowski, Mike; Huang, Po-Sen; Gowal, Sven; Mott, Alex; Kohli, Pushmeet",Towards Robust Image Classification Using Sequential Attention Models,,,,,,"In this paper we propose to augment a modern neuralnetwork architecture with an attention model inspired by human perception. Speciﬁcally, we adversarially train and analyze a neural model incorporating a human inspired, visual attention component that is guided by a recurrent top-down sequential process. Our experimental evaluation uncovers several notable ﬁndings about the robustness and behavior of this new model. First, introducing attention to the model signiﬁcantly improves adversarial robustness resulting in state-of-the-art ImageNet accuracies under a wide range of random targeted attack strengths. Second, we show that by varying the number of attention steps (glances/ﬁxations) for which the model is unrolled, we are able to make its defense capabilities stronger, even in light of stronger attacks — resulting in a “computational race” between the attacker and the defender. Finally, we show that some of the adversarial examples generated by attacking our model are quite different from conventional adversarial examples — they contain global, salient and spatially coherent structures coming from the target class that would be recognizable even to a human, and work by distracting the attention of the model away from the main object in the original image.",,2020/7/17 2:45,2020/7/17 2:45,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\NQ6IFGKN\Zoran 等。 - Towards Robust Image Classification Using Sequenti.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NHDBZ6XI,conferencePaper,2019,"Bai, Yang; Feng, Yan; Wang, Yisen; Dai, Tao; Xia, Shutao; Jiang, Yong",Hilbert-Based Generative Defense for Adversarial Examples,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00488,https://ieeexplore.ieee.org/document/9010387/,"Adversarial perturbations of clean images are usually imperceptible for human eyes, but can conﬁdently fool deep neural networks (DNNs) to make incorrect predictions. Such vulnerability of DNNs raises serious security concerns about their practicability in security-sensitive applications. To defend against such adversarial perturbations, recently developed PixelDefend puriﬁes a perturbed image based on PixelCNN in a raster scan order (row/column by row/column). However, such scan mode insufﬁciently exploits the correlations between pixels, which further limits its robustness performance. Therefore, we propose a more advanced Hilbert curve scan order to model the pixel dependencies in this paper. Hilbert curve could well preserve local consistency when mapping from 2-D image to 1-D vector, thus the local features in neighboring pixels can be more effectively modeled. Moreover, the defensive power can be further improved via ensembles of Hilbert curve with different orientations. Experimental results demonstrate the superiority of our method over the state-of-the-art defenses against various adversarial attacks.",Oct-19,2020/7/19 1:58,2020/7/19 1:58,2020/7/19 1:58,4783-4792,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\KLESB84L\Bai 等。 - 2019 - Hilbert-Based Generative Defense for Adversarial E.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
JE5EC9R6,conferencePaper,2019,"Brunner, Thomas; Diehl, Frederik; Le, Michael Truong; Knoll, Alois",Guessing Smart: Biased Sampling for Efficient Black-Box Adversarial Attacks,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00506,https://ieeexplore.ieee.org/document/9008375/,"We consider adversarial examples for image classiﬁcation in the black-box decision-based setting. Here, an attacker cannot access conﬁdence scores, but only the ﬁnal label. Most attacks for this scenario are either unreliable or inefﬁcient. Focusing on the latter, we show that a speciﬁc class of attacks, Boundary Attacks, can be reinterpreted as a biased sampling framework that gains efﬁciency from domain knowledge. We identify three such biases, image frequency, regional masks and surrogate gradients, and evaluate their performance against an ImageNet classiﬁer. We show that the combination of these biases outperforms the state of the art by a wide margin. We also showcase an efﬁcient way to attack the Google Cloud Vision API, where we craft convincing perturbations with just a few hundred queries. Finally, the methods we propose have also been found to work very well against strong defenses: Our targeted attack won second place in the NeurIPS 2018 Adversarial Vision Challenge.",Oct-19,2020/7/19 2:01,2020/7/19 2:01,2020/7/19 2:01,4957-4965,,,,,,Guessing Smart,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\F33YL9SY\Brunner 等。 - 2019 - Guessing Smart Biased Sampling for Efficient Blac.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
M6FPHYTC,conferencePaper,2019,"Chen, Lisha; Su, Hui; Ji, Qiang",Face Alignment With Kernel Density Deep Neural Network,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00709,https://ieeexplore.ieee.org/document/9009569/,"Deep neural networks achieve good performance in many computer vision problems such as face alignment. However, when the testing image is challenging due to low resolution, occlusion or adversarial attacks, the accuracy of a deep neural network suffers greatly. Therefore, it is important to quantify the uncertainty in its predictions. A probabilistic neural network with Gaussian distribution over the target is typically used to quantify uncertainty for regression problems. However, in real-world problems especially computer vision tasks, the Gaussian assumption is too strong. To model more general distributions, such as multimodal or asymmetric distributions, we propose to develop a kernel density deep neural network. Speciﬁcally, for face alignment, we adapt state-of-the-art hourglass neural network into a probabilistic neural network framework with landmark probability map as its output. The model is trained by maximizing the conditional log likelihood. To exploit the output probability map, we extend the model to multistage so that the logits map from the previous stage can feed into the next stage to progressively improve the landmark detection accuracy. Extensive experiments on benchmark datasets against state-of-the-art unconstrained deep learning method demonstrate that the proposed kernel density network achieves comparable or superior performance in terms of prediction accuracy. It further provides aleatoric uncertainty estimation in predictions.",Oct-19,2020/7/19 2:04,2020/7/19 2:04,2020/7/19 2:04,6991-7001,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\TP2A5MJC\Chen 等。 - 2019 - Face Alignment With Kernel Density Deep Neural Net.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
GKK3ZA3G,conferencePaper,2019,"Chen, Hao-Yun; Liang, Jhao-Hong; Chang, Shih-Chieh; Pan, Jia-Yu; Chen, Yu-Ting; Wei, Wei; Juan, Da-Cheng",Improving Adversarial Robustness via Guided Complement Entropy,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00498,https://ieeexplore.ieee.org/document/9009563/,"Adversarial robustness has emerged as an important topic in deep learning as carefully crafted attack samples can signiﬁcantly disturb the performance of a model. Many recent methods have proposed to improve adversarial robustness by utilizing adversarial training or model distillation, which adds additional procedures to model training. In this paper, we propose a new training paradigm called Guided Complement Entropy (GCE) that is capable of achieving “adversarial defense for free,” which involves no additional procedures in the process of improving adversarial robustness. In addition to maximizing model probabilities on the ground-truth class like crossentropy, we neutralize its probabilities on the incorrect classes along with a “guided” term to balance between these two terms. We show in the experiments that our method achieves better model robustness with even better performance compared to the commonly used crossentropy training objective. We also show that our method can be used orthogonal to adversarial training across wellknown methods with noticeable robustness gain. To the best of our knowledge, our approach is the ﬁrst one that improves model robustness without compromising performance.",Oct-19,2020/7/19 2:04,2020/7/19 2:04,2020/7/19 2:04,4880-4888,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\93PCGHXD\Chen 等。 - 2019 - Improving Adversarial Robustness via Guided Comple.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
AYSUGLMR,conferencePaper,2019,"Choi, Jun-Ho; Zhang, Huan; Kim, Jun-Hyuk; Hsieh, Cho-Jui; Lee, Jong-Seok",Evaluating Robustness of Deep Image Super-Resolution Against Adversarial Attacks,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00039,https://ieeexplore.ieee.org/document/9010873/,"Single-image super-resolution aims to generate a highresolution version of a low-resolution image, which serves as an essential component in many computer vision applications. This paper investigates the robustness of deep learning-based super-resolution methods against adversarial attacks, which can signiﬁcantly deteriorate the superresolved images without noticeable distortion in the attacked low-resolution images. It is demonstrated that stateof-the-art deep super-resolution methods are highly vulnerable to adversarial attacks. Different levels of robustness of different methods are analyzed theoretically and experimentally. We also present analysis on transferability of attacks, and feasibility of targeted attacks and universal attacks.",Oct-19,2020/7/19 2:06,2020/7/19 2:06,2020/7/19 2:06,303-311,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\Y5BICZK3\Choi 等。 - 2019 - Evaluating Robustness of Deep Image Super-Resoluti.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
ZP5TWKHC,conferencePaper,2019,"Croce, Francesco; Hein, Matthias",Sparse and Imperceivable Adversarial Attacks,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00482,https://ieeexplore.ieee.org/document/9010626/,"Neural networks have been proven to be vulnerable to a variety of adversarial attacks. From a safety perspective, highly sparse adversarial attacks are particularly dangerous. On the other hand the pixelwise perturbations of sparse attacks are typically large and thus can be potentially detected. We propose a new black-box technique to craft adversarial examples aiming at minimizing l0distance to the original image. Extensive experiments show that our attack is better or competitive to the state of the art. Moreover, we can integrate additional bounds on the componentwise perturbation. Allowing pixels to change only in region of high variation and avoiding changes along axisaligned edges makes our adversarial examples almost nonperceivable. Moreover, we adapt the Projected Gradient Descent attack to the l0-norm integrating componentwise constraints. This allows us to do adversarial training to enhance the robustness of classiﬁers against sparse and imperceivable adversarial manipulations.",Oct-19,2020/7/19 2:07,2020/7/19 2:07,2020/7/19 2:07,4723-4731,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\Y9TVMYZG\Croce 和 Hein - 2019 - Sparse and Imperceivable Adversarial Attacks.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
YYXTUYWV,conferencePaper,2019,"Finlay, Chris; Pooladian, Aram-Alexandre; Oberman, Adam",The LogBarrier Adversarial Attack: Making Effective Use of Decision Boundary Information,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00496,https://ieeexplore.ieee.org/document/9010707/,"Adversarial attacks for image classiﬁcation are small perturbations to images that are designed to cause misclassiﬁcation by a model. Adversarial attacks formally correspond to an optimization problem: ﬁnd a minimum norm image perturbation, constrained to cause misclassiﬁcation. A number of effective attacks have been developed. However, to date, no gradient-based attacks have used best practices from the optimization literature to solve this constrained minimization problem. We design a new untargeted attack, based on these best practices, using the wellregarded logarithmic barrier method.",Oct-19,2020/7/19 2:12,2020/7/19 2:12,2020/7/19 2:12,4861-4869,,,,,,The LogBarrier Adversarial Attack,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\H26RAFYD\Finlay 等。 - 2019 - The LogBarrier Adversarial Attack Making Effectiv.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
9KFXG4PK,conferencePaper,2019,"Ganeshan, Aditya; S, Vivek B; Radhakrishnan, Venkatesh Babu",FDA: Feature Disruptive Attack,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00816,https://ieeexplore.ieee.org/document/9008248/,"Though Deep Neural Networks (DNN) show excellent performance across various computer vision tasks, several works show their vulnerability to adversarial samples, i.e., image samples with imperceptible noise engineered to manipulate the network’s prediction. Adversarial sample generation methods range from simple to complex optimization techniques. Majority of these methods generate adversaries through optimization objectives that are tied to the pre-softmax or softmax output of the network. In this work we, (i) show the drawbacks of such attacks, (ii) propose two new evaluation metrics: Old Label New Rank (OLNR) and New Label Old Rank (NLOR) in order to quantify the extent of damage made by an attack, and (iii) propose a new adversarial attack FDA: Feature Disruptive Attack, to address the drawbacks of existing attacks. FDA works by generating image perturbation that disrupt features at each layer of the network and causes deep-features to be highly corrupt. This allows FDA adversaries to severely reduce the performance of deep networks. We experimentally validate that FDA generates stronger adversaries than other state-of-theart methods for image classiﬁcation, even in the presence of various defense measures. More importantly, we show that FDA disrupts feature-representation based tasks even without access to the task-speciﬁc network or methodology.",Oct-19,2020/7/19 2:13,2020/7/19 2:13,2020/7/19 2:13,8068-8078,,,,,,FDA,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\6GH7KK57\Ganeshan 等。 - 2019 - FDA Feature Disruptive Attack.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
IAUDN99S,conferencePaper,2019,"Gupta, Puneet; Rahtu, Esa",CIIDefence: Defeating Adversarial Attacks by Fusing Class-Specific Image Inpainting and Image Denoising,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00681,https://ieeexplore.ieee.org/document/9010982/,"This paper presents a novel approach for protecting deep neural networks from adversarial attacks, i.e., methods that add well-crafted imperceptible modiﬁcations to the original inputs such that they are incorrectly classiﬁed with high conﬁdence. The proposed defence mechanism is inspired by the recent works mitigating the adversarial disturbances by the means of image reconstruction and denoising. However, unlike the previous works, we apply the reconstruction only for small and carefully selected image areas that are most inﬂuential to the current classiﬁcation outcome. The selection process is guided by the class activation map responses obtained for multiple top-ranking class labels. The same regions are also the most prominent for the adversarial perturbations and hence most important to purify. The resulting inpainting task is substantially more tractable than the full image reconstruction, while still being able to prevent the adversarial attacks.",Oct-19,2020/7/19 2:15,2020/7/19 2:15,2020/7/19 2:15,6707-6716,,,,,,CIIDefence,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\SEY2GTU7\Gupta 和 Rahtu - 2019 - CIIDefence Defeating Adversarial Attacks by Fusin.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
QQJ6WIZY,conferencePaper,2019,"Han, Jiangfan; Dong, Xiaoyi; Zhang, Ruimao; Chen, Dongdong; Zhang, Weiming; Yu, Nenghai; Luo, Ping; Wang, Xiaogang",Once a MAN: Towards Multi-Target Attack via Learning Multi-Target Adversarial Network Once,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00526,https://ieeexplore.ieee.org/document/9009819/,"Modern deep neural networks are often vulnerable to adversarial samples. Based on the ﬁrst optimization-based attacking method, many following methods are proposed to improve the attacking performance and speed. Recently, generation-based methods have received much attention since they directly use feed-forward networks to generate the adversarial samples, which avoid the time-consuming iterative attacking procedure in optimization-based and gradient-based methods. However, current generationbased methods are only able to attack one speciﬁc target (category) within one model, thus making them not applicable to real classiﬁcation systems that often have hundreds/thousands of categories. In this paper, we propose the ﬁrst Multi-target Adversarial Network (MAN), which can generate multi-target adversarial samples with a single model. By incorporating the speciﬁed category information into the intermediate features, it can attack any category of the target classiﬁcation model during runtime. Experiments show that the proposed MAN can produce stronger attack results and also have better transferability than previous state-of-the-art methods in both multi-target attack task and single-target attack task. We further use the adversarial samples generated by our MAN to improve the robustness of the classiﬁcation model. It can also achieve better classiﬁcation accuracy than other methods when attacked by various methods.",Oct-19,2020/7/19 2:16,2020/7/19 2:16,2020/7/19 2:16,5157-5166,,,,,,Once a MAN,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\9UR364CT\Han 等。 - 2019 - Once a MAN Towards Multi-Target Attack via Learni.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
EBY9DGIJ,conferencePaper,2019,"Huang, Qian; Katsman, Isay; Gu, Zeqi; He, Horace; Belongie, Serge; Lim, Ser-Nam",Enhancing Adversarial Example Transferability With an Intermediate Level Attack,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00483,https://ieeexplore.ieee.org/document/9008298/,"Neural networks are vulnerable to adversarial examples, malicious inputs crafted to fool trained models. Adversarial examples often exhibit black-box transfer, meaning that adversarial examples for one model can fool another model. However, adversarial examples are typically overﬁt to exploit the particular architecture and feature representation of a source model, resulting in sub-optimal black-box transfer attacks to other target models. We introduce the Intermediate Level Attack (ILA), which attempts to ﬁne-tune an existing adversarial example for greater black-box transferability by increasing its perturbation on a pre-speciﬁed layer of the source model, improving upon state-of-the-art methods. We show that we can select a layer of the source model to perturb without any knowledge of the target models while achieving high transferability. Additionally, we provide some explanatory insights regarding our method and the effect of optimizing for adversarial examples using intermediate feature maps.",Oct-19,2020/7/19 2:19,2020/7/19 2:19,2020/7/19 2:19,4732-4741,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\EED97GEN\Huang 等。 - 2019 - Enhancing Adversarial Example Transferability With.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
UU6P2BHX,conferencePaper,2019,"Jang, Yunseok; Zhao, Tianchen; Hong, Seunghoon; Lee, Honglak",Adversarial Defense via Learning to Generate Diverse Attacks,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00283,https://ieeexplore.ieee.org/document/9008544/,"With the remarkable success of deep learning, Deep Neural Networks (DNNs) have been applied as dominant tools to various machine learning domains. Despite this success, however, it has been found that DNNs are surprisingly vulnerable to malicious attacks; adding a small, perceptually indistinguishable perturbations to the data can easily degrade classiﬁcation performance. Adversarial training is an effective defense strategy to train a robust classiﬁer. In this work, we propose to utilize the generator to learn how to create adversarial examples. Unlike the existing approaches that create a one-shot perturbation by a deterministic generator, we propose a recursive and stochastic generator that produces much stronger and diverse perturbations that comprehensively reveal the vulnerability of the target classiﬁer. Our experiment results on MNIST and CIFAR-10 datasets show that the classiﬁer adversarially trained with our method yields more robust performance over various white-box and black-box attacks.",Oct-19,2020/7/19 2:21,2020/7/19 2:21,2020/7/19 2:21,2740-2749,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\B65JLY8X\Jang 等。 - 2019 - Adversarial Defense via Learning to Generate Diver.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
XUBUNHCP,conferencePaper,2019,"Jiang, Qing-Yuan; He, Yi; Li, Gen; Lin, Jian; Li, Lei; Li, Wu-Jun",SVD: A Large-Scale Short Video Dataset for Near-Duplicate Video Retrieval,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00538,https://ieeexplore.ieee.org/document/9008275/,"With the explosive growth of video data in real applications, near-duplicate video retrieval (NDVR) has become indispensable and challenging, especially for short videos. However, all existing NDVR datasets are introduced for long videos. Furthermore, most of them are small-scale and lack of diversity due to the high cost of collecting and labeling near-duplicate videos. In this paper, we introduce a large-scale short video dataset, called SVD, for the NDVR task. SVD contains over 500,000 short videos and over 30,000 labeled videos of near-duplicates. We use multiple video mining techniques to construct positive/negative pairs. Furthermore, we design temporal and spatial transformations to mimic user-attack behavior in real applications for constructing more difﬁcult variants of SVD. Experiments show that existing state-of-the-art NDVR methods, including real-value based and hashing based methods, fail to achieve satisfactory performance on this challenging dataset. The release of SVD dataset will foster research and system engineering in the NDVR area. The SVD dataset is available at https://svdbase.github.io.",Oct-19,2020/7/19 2:21,2020/7/19 2:21,2020/7/19 2:21,5280-5288,,,,,,SVD,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\7BQ8BLBW\Jiang 等。 - 2019 - SVD A Large-Scale Short Video Dataset for Near-Du.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
D3UXW9N6,conferencePaper,2019,"Joshi, Ameya; Mukherjee, Amitangshu; Sarkar, Soumik; Hegde, Chinmay",Semantic Adversarial Attacks: Parametric Transformations That Fool Deep Classifiers,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00487,https://ieeexplore.ieee.org/document/9010394/,"Deep neural networks have been shown to exhibit an intriguing vulnerability to adversarial input images corrupted with imperceptible perturbations. However, the majority of adversarial attacks assume global, ﬁne-grained control over the image pixel space. In this paper, we consider a different setting: what happens if the adversary could only alter speciﬁc attributes of the input image? These would generate inputs that might be perceptibly different, but still natural-looking and enough to fool a classiﬁer. We propose a novel approach to generate such “semantic” adversarial examples by optimizing a particular adversarial loss over the range-space of a parametric conditional generative model. We demonstrate implementations of our attacks on binary classiﬁers trained on face images, and show that such natural-looking semantic adversarial examples exist. We evaluate the effectiveness of our attack on synthetic and real data, and present detailed comparisons with existing attack methods. We supplement our empirical results with theoretical bounds that demonstrate the existence of such parametric adversarial examples.",Oct-19,2020/7/19 2:22,2020/7/19 2:22,2020/7/19 2:22,4772-4782,,,,,,Semantic Adversarial Attacks,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\AT98ITH4\Joshi 等。 - 2019 - Semantic Adversarial Attacks Parametric Transform.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
9U9L626L,journalArticle,,"Li, Jie; Ji, Rongrong; Liu, Hong; Hong, Xiaopeng; Gao, Yue; Tian, Qi",Universal Perturbation Attack Against Image Retrieval,,,,,,"Universal adversarial perturbations (UAPs), a.k.a. input-agnostic perturbations, has been proved to exist and be able to fool cutting-edge deep learning models on most of the data samples. Existing UAP methods mainly focus on attacking image classiﬁcation models. Nevertheless, little attention has been paid to attacking image retrieval systems. In this paper, we make the ﬁrst attempt in attacking image retrieval systems. Concretely, image retrieval attack is to make the retrieval system return irrelevant images to the query at the top ranking list. It plays an important role to corrupt the neighbourhood relationships among features in image retrieval attack. To this end, we propose a novel method to generate retrieval-against UAP to break the neighbourhood relationships of image features via degrading the corresponding ranking metric. To expand the attack method to scenarios with varying input sizes or untouchable network parameters, a multi-scale random resizing scheme and a ranking distillation strategy are proposed. We evaluate the proposed method on four widely-used image retrieval datasets, and report a signiﬁcant performance drop in terms of different metrics, such as mAP and mP@10. Finally, we test our attack methods on the real-world visual search engine, i.e., Google Images, which demonstrates the practical potentials of our methods.",,2020/7/19 2:32,2020/7/19 2:32,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\KKS34PRD\Li 等。 - Universal Perturbation Attack Against Image Retrie.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EXZKXJ5T,conferencePaper,2019,"Liu, Yujia; Moosavi-Dezfooli, Seyed-Mohsen; Frossard, Pascal",A Geometry-Inspired Decision-Based Attack,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00499,https://ieeexplore.ieee.org/document/9010924/,"Deep neural networks have recently achieved tremendous success in image classiﬁcation. Recent studies have however shown that they are easily misled into incorrect classiﬁcation decisions by adversarial examples. Adversaries can even craft attacks by querying the model in blackbox settings, where no information about the model is released except its ﬁnal decision. Such decision-based attacks usually require lots of queries, while real-world image recognition systems might actually restrict the number of queries. In this paper, we propose qFool, a novel decisionbased attack algorithm that can generate adversarial examples using a small number of queries. The qFool method can drastically reduce the number of queries compared to previous decision-based attacks while reaching the same quality of adversarial examples. We also enhance our method by constraining adversarial perturbations in low-frequency subspace, which can make qFool even more computationally efﬁcient. Altogether, we manage to fool commercial image recognition systems with a small number of queries, which demonstrates the actual effectiveness of our new algorithm in practice.",Oct-19,2020/7/19 2:33,2020/7/19 2:33,2020/7/19 2:33,4889-4897,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\63GT8QBI\Liu 等。 - 2019 - A Geometry-Inspired Decision-Based Attack.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
RVB8NXCG,conferencePaper,2019,"Liu, Hong; Ji, Rongrong; Li, Jie; Zhang, Baochang; Gao, Yue; Wu, Yongjian; Huang, Feiyue",Universal Adversarial Perturbation via Prior Driven Uncertainty Approximation,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00303,https://ieeexplore.ieee.org/document/9008259/,"Deep learning models have shown their vulnerabilities to universal adversarial perturbations (UAP), which are quasi-imperceptible. Compared to the conventional supervised UAPs that suffer from the knowledge of training data, the data-independent unsupervised UAPs are more applicable. Existing unsupervised methods fail to take advantage of the model uncertainty to produce robust perturbations. In this paper, we propose a new unsupervised universal adversarial perturbation method, termed as Prior Driven Uncertainty Approximation (PD-UA), to generate a robust UAP by fully exploiting the model uncertainty. Speciﬁcally, a Monte Carlo sampling method is deployed to activate more neurons to increase the model uncertainty for a better adversarial perturbation. Thereafter, a textural bias prior revealing a statistical uncertainty is proposed, which helps to improve the attacking performance. The UAP is crafted by the stochastic gradient descent algorithm with a boosted momentum optimizer, and a Laplacian pyramid frequency model is ﬁnally used to maintain the statistical uncertainty. Extensive experiments demonstrate that our method achieves well attacking performances on the ImageNet validation set, and signiﬁcantly improves the fooling rate compared with the state-of-the-art methods.",Oct-19,2020/7/19 2:35,2020/7/19 2:35,2020/7/19 2:35,2941-2949,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\M6UVHEAV\Liu 等。 - 2019 - Universal Adversarial Perturbation via Prior Drive.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
3674BJ5Q,conferencePaper,2019,"Mustafa, Aamir; Khan, Salman; Hayat, Munawar; Goecke, Roland; Shen, Jianbing; Shao, Ling",Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00348,https://ieeexplore.ieee.org/document/9010713/,"Deep neural networks are vulnerable to adversarial attacks, which can fool them by adding minuscule perturbations to the input images. The robustness of existing defenses suffers greatly under white-box attack settings, where an adversary has full knowledge about the network and can iterate several times to ﬁnd strong perturbations. We observe that the main reason for the existence of such perturbations is the close proximity of different class samples in the learned feature space. This allows model decisions to be totally changed by adding an imperceptible perturbation in the inputs. To counter this, we propose to class-wise disentangle the intermediate feature representations of deep networks. Speciﬁcally, we force the features for each class to lie inside a convex polytope that is maximally separated from the polytopes of other classes. In this manner, the network is forced to learn distinct and distant decision regions for each class. We observe that this simple constraint on the features greatly enhances the robustness of learned models, even against the strongest white-box attacks, without degrading the classiﬁcation performance on clean images. We report extensive evaluations in both black-box and whitebox attack scenarios and show signiﬁcant gains in comparison to state-of-the art defenses1.",Oct-19,2020/7/19 2:40,2020/7/19 2:40,2020/7/19 2:40,3384-3393,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\44CQ3TQJ\Mustafa 等。 - 2019 - Adversarial Defense by Restricting the Hidden Spac.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
PTAN3KDV,conferencePaper,2019,"Peterson, Joshua; Battleday, Ruairidh; Griffiths, Thomas; Russakovsky, Olga",Human Uncertainty Makes Classification More Robust,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00971,https://ieeexplore.ieee.org/document/9010969/,"The classiﬁcation performance of deep neural networks has begun to asymptote at near-perfect levels. However, their ability to generalize outside the training set and their robustness to adversarial attacks have not. In this paper, we make progress on this problem by training with full label distributions that reﬂect human perceptual uncertainty. We ﬁrst present a new benchmark dataset which we call CIFAR10H, containing a full distribution of human labels for each image of the CIFAR10 test set. We then show that, while contemporary classiﬁers fail to exhibit human-like uncertainty on their own, explicit training on our dataset closes this gap, supports improved generalization to increasingly out-of-training-distribution test datasets, and confers robustness to adversarial attacks.",Oct-19,2020/7/19 2:43,2020/7/19 2:43,2020/7/19 2:43,9616-9625,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\XAJQXE76\Peterson 等。 - 2019 - Human Uncertainty Makes Classification More Robust.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
6UTFU7PJ,conferencePaper,2019,"Rakin, Adnan Siraj; He, Zhezhi; Fan, Deliang",Bit-Flip Attack: Crushing Neural Network With Progressive Bit Search,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00130,https://ieeexplore.ieee.org/document/9009040/,,Oct-19,2020/7/19 2:45,2020/7/19 2:45,2020/7/19 2:45,1211-1220,,,,,,Bit-Flip Attack,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\XZDCDHVT\Rakin 等。 - 2019 - Bit-Flip Attack Crushing Neural Network With Prog.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
N4QWK2VT,conferencePaper,2019,"Ranjan, Anurag; Janai, Joel; Geiger, Andreas; Black, Michael",Attacking Optical Flow,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00249,https://ieeexplore.ieee.org/document/9010809/,"Deep neural nets achieve state-of-the-art performance on the problem of optical ﬂow estimation. Since optical ﬂow is used in several safety-critical applications like selfdriving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical ﬂow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical ﬂow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can signiﬁcantly affect optical ﬂow estimates. Our attacks lead to noisy ﬂow estimates that extend signiﬁcantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical ﬂow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes.",Oct-19,2020/7/19 2:45,2020/7/19 2:45,2020/7/19 2:45,2404-2413,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\YM98AAL7\Ranjan 等。 - 2019 - Attacking Optical Flow.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
96BLT2DP,conferencePaper,2019,"Sabokrou, Mohammad; Khalooei, Mohammad; Adeli, Ehsan",Self-Supervised Representation Learning via Neighborhood-Relational Encoding,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00810,https://ieeexplore.ieee.org/document/9010354/,"In this paper, we propose a novel self-supervised representation learning by taking advantage of a neighborhoodrelational encoding (NRE) among the training data. Conventional unsupervised learning methods only focused on training deep networks to understand the primitive characteristics of the visual data, mainly to be able to reconstruct the data from a latent space. They often neglected the relation among the samples, which can serve as an important metric for self-supervision. Different from the previous work, NRE aims at preserving the local neighborhood structure on the data manifold. Therefore, it is less sensitive to outliers. We integrate our NRE component with an encoder-decoder structure for learning to represent samples considering their local neighborhood information. Such discriminative and unsupervised representation learning scheme is adaptable to different computer vision tasks due to its independence from intense annotation requirements. We evaluate our proposed method for different tasks, including classiﬁcation, detection, and segmentation based on the learned latent representations. In addition, we adopt the auto-encoding capability of our proposed method for applications like defense against adversarial example attacks and video anomaly detection. Results conﬁrm the performance of our method is better or at least comparable with the state-of-the-art for each speciﬁc application, but with a generic and self-supervised approach.",Oct-19,2020/7/19 2:46,2020/7/19 2:46,2020/7/19 2:46,8009-8018,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\J4BV2ZE9\Sabokrou 等。 - 2019 - Self-Supervised Representation Learning via Neighb.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
YD2AMBZ3,conferencePaper,2019,"Subramanya, Akshayvarun; Pillai, Vipin; Pirsiavash, Hamed",Fooling Network Interpretation in Image Classification,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00211,https://ieeexplore.ieee.org/document/9010911/,"Deep neural networks have been shown to be fooled rather easily using adversarial attack algorithms. Practical methods such as adversarial patches have been shown to be extremely effective in causing misclassiﬁcation. However, these patches are highlighted using standard network interpretation algorithms, thus revealing the identity of the adversary. We show that it is possible to create adversarial patches which not only fool the prediction, but also change what we interpret regarding the cause of the prediction. Moreover, we introduce our attack as a controlled setting to measure the accuracy of interpretation algorithms. We show this using extensive experiments for Grad-CAM interpretation that transfers to occluding patch interpretation as well. We believe our algorithms can facilitate developing more robust network interpretation tools that truly explain the network’s underlying decision making process.",Oct-19,2020/7/19 2:51,2020/7/19 2:51,2020/7/19 2:51,2020-2029,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\GPELLND2\Subramanya 等。 - 2019 - Fooling Network Interpretation in Image Classifica.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
WU4Y6RK3,conferencePaper,2019,"Tolias, Giorgos; Radenovic, Filip; Chum, Ondrej",Targeted Mismatch Adversarial Attack: Query With a Flower to Retrieve the Tower,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00514,https://ieeexplore.ieee.org/document/9008294/,"Access to online visual search engines implies sharing of private user content – the query images. We introduce the concept of targeted mismatch attack for deep learning based retrieval systems to generate an adversarial image to conceal the query image. The generated image looks nothing like the user intended query, but leads to identical or very similar retrieval results. Transferring attacks to fully unseen networks is challenging. We show successful attacks to partially unknown systems, by designing various loss functions for the adversarial image construction. These include loss functions, for example, for unknown global pooling operation or unknown input resolution by the retrieval system. We evaluate the attacks on standard retrieval benchmarks and compare the results retrieved with the original and adversarial image.",Oct-19,2020/7/19 2:53,2020/7/19 2:53,2020/7/19 2:53,5036-5045,,,,,,Targeted Mismatch Adversarial Attack,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\SWPJPGWR\Tolias 等。 - 2019 - Targeted Mismatch Adversarial Attack Query With a.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
MHAZM997,conferencePaper,2019,"Wang, Zhibo; Zheng, Siyan; Song, Mengkai; Wang, Qian; Rahimpour, Alireza; Qi, Hairong",advPattern: Physical-World Attacks on Deep Person Re-Identification via Adversarially Transformable Patterns,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00843,https://ieeexplore.ieee.org/document/9008244/,"Person re-identiﬁcation (re-ID) is the task of matching person images across camera views, which plays an important role in surveillance and security applications. Inspired by great progress of deep learning, deep re-ID models began to be popular and gained state-of-the-art performance. However, recent works found that deep neural networks (DNNs) are vulnerable to adversarial examples, posing potential threats to DNNs based applications. This phenomenon throws a serious question about whether deep re-ID based systems are vulnerable to adversarial attacks.",Oct-19,2020/7/19 2:54,2020/7/19 2:54,2020/7/19 2:54,8340-8349,,,,,,advPattern,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\69VTB3N3\Wang 等。 - 2019 - advPattern Physical-World Attacks on Deep Person .pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
UMF2CUCR,conferencePaper,2019,"Wang, Jianyu; Zhang, Haichao",Bilateral Adversarial Training: Towards Fast Training of More Robust Models Against Adversarial Attacks,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00673,https://ieeexplore.ieee.org/document/9009088/,"In this paper, we study fast training of adversarially robust models. From the analyses of the state-of-the-art defense method, i.e., the multi-step adversarial training [34], we hypothesize that the gradient magnitude links to the model robustness. Motivated by this, we propose to perturb both the image and the label during training, which we call Bilateral Adversarial Training (BAT). To generate the adversarial label, we derive an closed-form heuristic solution. To generate the adversarial image, we use one-step targeted attack with the target label being the most confusing class. In the experiment, we ﬁrst show that random start and the most confusing target attack effectively prevent the label leaking and gradient masking problem. Then coupled with the adversarial label part, our model significantly improves the state-of-the-art results. For example, against PGD100 white-box attack with cross-entropy loss, on CIFAR10, we achieve 63.7% versus 47.2%; on SVHN, we achieve 59.1% versus 42.1%. At last, the experiment on the very (computationally) challenging ImageNet dataset further demonstrates the effectiveness of our fast method.",Oct-19,2020/7/19 2:55,2020/7/19 2:55,2020/7/19 2:55,6628-6637,,,,,,Bilateral Adversarial Training,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\5LW9EIAP\Wang 和 Zhang - 2019 - Bilateral Adversarial Training Towards Fast Train.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
MSCVPKI6,conferencePaper,2019,"Wiyatno, Rey; Xu, Anqi",Physical Adversarial Textures That Fool Visual Object Tracking,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00492,https://ieeexplore.ieee.org/document/9009841/,"We present a method for creating inconspicuous-looking textures that, when displayed as posters in the physical world, cause visual object tracking systems to become confused. As a target being visually tracked moves in front of such a poster, its adversarial texture makes the tracker lock onto it, thus allowing the target to evade. This adversarial attack evaluates several optimization strategies for fooling seldom-targeted regression models: non-targeted, targeted, and a newly-coined family of guided adversarial losses. Also, while we use the Expectation Over Transformation (EOT) algorithm to generate physical adversaries that fool tracking models when imaged under diverse conditions, we compare the impacts of different scene variables to ﬁnd practical attack setups with high resulting adversarial strength and convergence speed. We further showcase that textures optimized using simulated scenes can confuse real-world tracking systems for cameras and robots.",Oct-19,2020/7/19 2:58,2020/7/19 2:58,2020/7/19 2:58,4821-4830,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\UYCFQIV5\Wiyatno 和 Xu - 2019 - Physical Adversarial Textures That Fool Visual Obj.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
KRBEJFP9,conferencePaper,2019,"Xiao, Chaowei; Deng, Ruizhi; Li, Bo; Lee, Taesung; Edwards, Benjamin; Yi, Jinfeng; Song, Dawn; Liu, Mingyan; Molloy, Ian",AdvIT: Adversarial Frames Identifier Based on Temporal Consistency in Videos,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00407,https://ieeexplore.ieee.org/document/9010733/,"Deep neural networks (DNNs) have been widely applied in various applications. However, DNNs are found to be vulnerable to adversarial examples. While several defense and detection approaches are proposed for static image classiﬁcation, many security-critical tasks use videos as their input and require efﬁcient processing. In this paper, we propose an efﬁcient and effective method AdvIT to detect adversarial frames within videos against different types of attacks based on temporal consistency property of videos. In particular, we apply optical ﬂow estimation to the target and previous frames to generate pseudo frames and evaluate the consistency of the learner output between these pseudo frames and target. High inconsistency indicates that the target frame is adversarial. We conduct extensive experiments on various learning tasks including video semantic segmentation, human pose estimation, object detection, and action recognition, and demonstrate that we can achieve above 95% adversarial frame detection rate. To consider adaptive attackers, we show that even if an adversary has access to the detector and performs a strong adaptive attack based on the state of the art expectation of transformation method, the detection rate stays almost the same. We also tested the transferability among different optical ﬂow estimators and show that it is hard for attackers to attack one and transfer the perturbation to others. In addition, as efﬁciency is important in video analysis, we show that AdvIT can achieve real-time detection.",Oct-19,2020/7/19 2:59,2020/7/19 2:59,2020/7/19 2:59,3967-3976,,,,,,AdvIT,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\IR63WNQ7\Xiao 等。 - 2019 - AdvIT Adversarial Frames Identifier Based on Temp.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
N7TKA6EE,journalArticle,,"Ye, Shaokai; Xu, Kaidi; Liu, Sijia; Cheng, Hao; Lambrechts, Jan-Henrik; Zhang, Huan; Zhou, Aojun; Ma, Kaisheng; Wang, Yanzhi; Lin, Xue","Adversarial Robustness vs. Model Compression, or Both?",,,,,,"It is well known that deep neural networks (DNNs) are vulnerable to adversarial attacks, which are implemented by adding crafted perturbations onto benign examples. Min-max robust optimization based adversarial training can provide a notion of security against adversarial attacks. However, adversarial robustness requires a signiﬁcantly larger capacity of the network than that for the natural training with only benign examples. This paper proposes a framework of concurrent adversarial training and weight pruning that enables model compression while still preserving the adversarial robustness and essentially tackles the dilemma of adversarial training. Furthermore, this work studies two hypotheses about weight pruning in the conventional setting and ﬁnds that weight pruning is essential for reducing the network model size in the adversarial setting; training a small model from scratch even with inherited initialization from the large model cannot achieve neither adversarial robustness nor high standard accuracy. Code is available at https://github.com/yeshaokai/ Robustness-Aware-Pruning-ADMM .",,2020/7/19 3:03,2020/7/19 3:03,,10,,,,,,,,,,,,,en,,,,,Zotero,,,,"C:\Users\zhbli\Zotero\storage\MD3B47RA\Ye 等。 - Adversarial Robustness vs. Model Compression, or B.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DZ28PKAJ,conferencePaper,2019,"Zhang, Haichao; Wang, Jianyu",Towards Adversarially Robust Object Detection,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00051,https://ieeexplore.ieee.org/document/9009990/,"Object detection is an important vision task and has emerged as an indispensable component in many vision system, rendering its robustness as an increasingly important performance factor for practical applications. While object detection models have been demonstrated to be vulnerable against adversarial attacks by many recent works, very few efforts have been devoted to improving their robustness. In this work, we take an initial attempt towards this direction. We ﬁrst revisit and systematically analyze object detectors and many recently developed attacks from the perspective of model robustness. We then present a multi-task learning perspective of object detection and identify an asymmetric role of task losses. We further develop an adversarial training approach which can leverage the multiple sources of attacks for improving the robustness of detection models. Extensive experiments on PASCAL-VOC and MS-COCO veriﬁed the effectiveness of the proposed approach.",Oct-19,2020/7/19 3:06,2020/7/19 3:06,2020/7/19 3:06,421-430,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\2RV9ZZBU\Zhang 和 Wang - 2019 - Towards Adversarially Robust Object Detection.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
YDSEN7C4,conferencePaper,2019,"Zhao, Pu; Liu, Sijia; Chen, Pin-Yu; Hoang, Nghia; Xu, Kaidi; Kailkhura, Bhavya; Lin, Xue",On the Design of Black-Box Adversarial Examples by Leveraging Gradient-Free Optimization and Operator Splitting Method,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00021,https://ieeexplore.ieee.org/document/9009069/,"Robust machine learning is currently one of the most prominent topics which could potentially help shaping a future of advanced AI platforms that not only perform well in average cases but also in worst cases or adverse situations. Despite the long-term vision, however, existing studies on black-box adversarial attacks are still restricted to very speciﬁc settings of threat models (e.g., single distortion metric and restrictive assumption on target model’s feedback to queries) and/or suffer from prohibitively high query complexity. To push for further advances in this ﬁeld, we introduce a general framework based on an operator splitting method, the alternating direction method of multipliers (ADMM) to devise efﬁcient, robust black-box attacks that work with various distortion metrics and feedback settings without incurring high query complexity. Due to the black-box nature of the threat model, the proposed ADMM solution framework is integrated with zeroth-order (ZO) optimization and Bayesian optimization (BO), and thus is applicable to the gradient-free regime. This results in two new black-box adversarial attack generation methods, ZOADMM and BO-ADMM. Our empirical evaluations on image classiﬁcation datasets show that our proposed approaches have much lower function query complexities compared to state-of-the-art attack methods, but achieve very competitive attack success rates. Codes are available at https: //github.com/LinLabNEU/Blackbox_ADMM .",Oct-19,2020/7/19 3:07,2020/7/19 3:07,2020/7/19 3:07,121-130,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\YY2LHRPR\Zhao 等。 - 2019 - On the Design of Black-Box Adversarial Examples by.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
5NNVRMAR,conferencePaper,2019,"Zhong, Yaoyao; Deng, Weihong",Adversarial Learning With Margin-Based Triplet Embedding Regularization,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00665,https://ieeexplore.ieee.org/document/9010739/,"The Deep neural networks (DNNs) have achieved great success on a variety of computer vision tasks, however, they are highly vulnerable to adversarial attacks. To address this problem, we propose to improve the local smoothness of the representation space, by integrating a margin-based triplet embedding regularization term into the classiﬁcation objective, so that the obtained model learns to resist adversarial examples. The regularization term consists of two steps optimizations which ﬁnd potential perturbations and punish them by a large margin in an iterative way. Experimental results on MNIST, CASIA-WebFace, VGGFace2 and MSCeleb-1M reveal that our approach increases the robustness of the network against both feature and label adversarial attacks in simple object classiﬁcation and deep face recognition. The code is available at https://github.com/ zhongyy/Adversarial_MTER.",Oct-19,2020/7/19 3:08,2020/7/19 3:08,2020/7/19 3:08,6548-6557,,,,,,,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\9EVHRN5D\Zhong 和 Deng - 2019 - Adversarial Learning With Margin-Based Triplet Emb.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
AJT9QZFI,conferencePaper,2019,"Zhou, Hang; Chen, Kejiang; Zhang, Weiming; Fang, Han; Zhou, Wenbo; Yu, Nenghai",DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds Defense,2019 IEEE/CVF International Conference on Computer Vision (ICCV),978-1-72814-803-8,,10.1109/ICCV.2019.00205,https://ieeexplore.ieee.org/document/9010939/,"Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems. We propose a Denoiser and UPsampler Network (DUP-Net) structure as defenses for 3D adversarial point cloud classiﬁcation, where the two modules reconstruct surface smoothness by dropping or adding points. In this paper, statistical outlier removal (SOR) and a datadriven upsampling network are considered as denoiser and upsampler respectively. Compared with baseline defenses, DUP-Net has three advantages. First, with DUP-Net as a defense, the target model is more robust to white-box adversarial attacks. Second, the statistical outlier removal provides added robustness since it is a non-differentiable denoising operation. Third, the upsampler network can be trained on a small dataset and defends well against adversarial attacks generated from other point cloud datasets. We conduct various experiments to validate that DUP-Net is very effective as defense in practice. Our best defense eliminates 83.8% of C&W and l2 loss based attack (point shifting), 50.0% of C&W and Hausdorff distance loss based attack (point adding) and 9.0% of saliency map based attack (point dropping) under 200 dropped points on PointNet.",Oct-19,2020/7/19 3:08,2020/7/19 3:08,2020/7/19 3:08,1961-1970,,,,,,DUP-Net,,,,,IEEE,"Seoul, Korea (South)",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\4UMMS3HK\Zhou 等。 - 2019 - DUP-Net Denoiser and Upsampler Network for 3D Adv.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF International Conference on Computer Vision (ICCV),,,,,,,,,,,,,,,
Y2AMJ48U,conferencePaper,2019,"Zhang, Shifeng; Wang, Xiaobo; Liu, Ajian; Zhao, Chenxu; Wan, Jun; Escalera, Sergio; Shi, Hailin; Wang, Zezheng; Li, Stan Z.",A Dataset and Benchmark for Large-Scale Multi-Modal Face Anti-Spoofing,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00101,https://ieeexplore.ieee.org/document/8953793/,"Face anti-spooﬁng is essential to prevent face recognition systems from a security breach. Much of the progresses have been made by the availability of face antispooﬁng benchmark datasets in recent years. However, existing face anti-spooﬁng benchmarks have limited number of subjects (≤ 170) and modalities (≤ 2), which hinder the further development of the academic community. To facilitate face anti-spooﬁng research, we introduce a large-scale multi-modal dataset, namely CASIASURF, which is the largest publicly available dataset for face anti-spooﬁng in terms of both subjects and visual modalities. Speciﬁcally, it consists of 1, 000 subjects with 21, 000 videos and each sample has 3 modalities (i.e., RGB, Depth and IR). We also provide a measurement set, evaluation protocol and training/validation/testing subsets, developing a new benchmark for face anti-spooﬁng. Moreover, we present a new multi-modal fusion method as baseline, which performs feature re-weighting to select the more informative channel features while suppressing the less useful ones for each modal. Extensive experiments have been conducted on the proposed dataset to verify its signiﬁcance and generalization capability. The dataset is available at https://sites.google.com/qq. com/chalearnfacespoofingattackdete/.",Jun-19,2020/7/19 6:50,2020/7/19 6:50,2020/7/19 6:50,919-928,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\UIJEVJRN\Zhang 等。 - 2019 - A Dataset and Benchmark for Large-Scale Multi-Moda.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
P93NZW83,conferencePaper,2019,"Taghanaki, Saeid Asgari; Abhishek, Kumar; Azizi, Shekoofeh; Hamarneh, Ghassan",A Kernelized Manifold Mapping to Diminish the Effect of Adversarial Perturbations,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.01160,https://ieeexplore.ieee.org/document/8953528/,"The linear and non-ﬂexible nature of deep convolutional models makes them vulnerable to carefully crafted adversarial perturbations. To tackle this problem, we propose a non-linear radial basis convolutional feature mapping by learning a Mahalanobis-like distance function. Our method then maps the convolutional features onto a linearly well-separated manifold, which prevents small adversarial perturbations from forcing a sample to cross the decision boundary. We test the proposed method on three publicly available image classiﬁcation and segmentation datasets namely, MNIST, ISBI ISIC 2017 skin lesion segmentation, and NIH Chest X-Ray-14. We evaluate the robustness of our method to different gradient (targeted and untargeted) and non-gradient based attacks and compare it to several nongradient masking defense strategies. Our results demonstrate that the proposed method can increase the resilience of deep convolutional neural networks to adversarial perturbations without accuracy drop on clean data.",Jun-19,2020/7/19 6:50,2020/7/19 6:50,2020/7/19 6:50,11332-11341,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\2J3EA9WB\Taghanaki 等。 - 2019 - A Kernelized Manifold Mapping to Diminish the Effe.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
FVHYZQW5,conferencePaper,2019,"Zeng, Xiaohui; Liu, Chenxi; Wang, Yu-Siang; Qiu, Weichao; Xie, Lingxi; Tai, Yu-Wing; Tang, Chi-Keung; Yuille, Alan L.",Adversarial Attacks Beyond the Image Space,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00443,https://ieeexplore.ieee.org/document/8954178/,"Generating adversarial examples is an intriguing problem and an important way of understanding the working mechanism of deep neural networks. Most existing approaches generated perturbations in the image space, i.e., each pixel can be modiﬁed independently. However, in this paper we pay special attention to the subset of adversarial examples that correspond to meaningful changes in 3D physical properties (like rotation and translation, illumination condition, etc.). These adversaries arguably pose a more serious concern, as they demonstrate the possibility of causing neural network failure by easy perturbations of real-world 3D objects and scenes.",Jun-19,2020/7/19 6:53,2020/7/19 6:53,2020/7/19 6:53,4297-4306,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\JJBLDQLF\Zeng 等。 - 2019 - Adversarial Attacks Beyond the Image Space.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
ZNZ9FFRR,conferencePaper,2019,"Sun, Bo; Tsai, Nian-Hsuan; Liu, Fangchen; Yu, Ronald; Su, Hao",Adversarial Defense by Stratified Convolutional Sparse Coding,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.01171,https://ieeexplore.ieee.org/document/8953552/,"We propose an adversarial defense method that achieves state-of-the-art performance among attack-agnostic adversarial defense methods while also maintaining robustness to input resolution, scale of adversarial perturbation, and scale of dataset size. Based on convolutional sparse coding, we construct a stratiﬁed low-dimensional quasi-natural image space that faithfully approximates the natural image space while also removing adversarial perturbations. We introduce a novel Sparse Transformation Layer (STL) between the input image and the ﬁrst layer of the neural network to efﬁciently project images into our quasi-natural image space. Our experiments show state-of-the-art performance of our method compared to other attack-agnostic adversarial defense methods in various adversarial settings.",Jun-19,2020/7/19 6:53,2020/7/19 6:53,2020/7/19 6:53,11439-11448,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\ZEFVIQWI\Sun 等。 - 2019 - Adversarial Defense by Stratified Convolutional Sp.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
JW7RH8GL,conferencePaper,2019,"Qiu, Yuxian; Leng, Jingwen; Guo, Cong; Chen, Quan; Li, Chao; Guo, Minyi; Zhu, Yuhao",Adversarial Defense Through Network Profiling Based Path Extraction,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00491,https://ieeexplore.ieee.org/document/8953324/,"Recently, researchers have started decomposing deep neural network models according to their semantics or functions. Recent work has shown the effectiveness of decomposed functional blocks for defending adversarial attacks, which add small input perturbation to the input image to fool the DNN models. This work proposes a proﬁling-based method to decompose the DNN models to different functional blocks, which lead to the effective path as a new approach to exploring DNNs’ internal organization. Speciﬁcally, the per-image effective path can be aggregated to the class-level effective path, through which we observe that adversarial images activate effective path different from normal images. We propose an effective path similarity-based method to detect adversarial images with an interpretable model, which achieve better accuracy and broader applicability than the state-of-the-art technique.",Jun-19,2020/7/19 6:53,2020/7/19 6:53,2020/7/19 6:53,4772-4781,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\58XEUQ7K\Qiu 等。 - 2019 - Adversarial Defense Through Network Profiling Base.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
3LHYWXVP,conferencePaper,2019,"Raff, Edward; Sylvester, Jared; Forsyth, Steven; McLean, Mark",Barrage of Random Transforms for Adversarially Robust Defense,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00669,https://ieeexplore.ieee.org/document/8954476/,"Defenses against adversarial examples, when using the ImageNet dataset, are historically easy to defeat. The common understanding is that a combination of simple image transformations and other various defenses are insuﬃcient to provide the necessary protection when the obfuscated gradient is taken into account. In this paper, we explore the idea of stochastically combining a large number of individually weak defenses into a single barrage of randomized transformations to build a strong defense against adversarial attacks. We show that, even after accounting for obfuscated gradients, the Barrage of Random Transforms (BaRT) is a resilient defense against even the most diﬃcult attacks, such as PGD. BaRT achieves up to a 24⇥ improvement in accuracy compared to previous work, and has even extended eﬀectiveness out to a previously untested maximum adversarial perturbation of ✏ = 32.",Jun-19,2020/7/19 6:57,2020/7/19 6:57,2020/7/19 6:57,6521-6530,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\AIADLRG4\Raff 等。 - 2019 - Barrage of Random Transforms for Adversarially Rob.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
QHWKC92P,conferencePaper,2019,"Ho, Chih-Hui; Leung, Brandon; Sandstrom, Erik; Chang, Yen; Vasconcelos, Nuno","Catastrophic Child's Play: Easy to Perform, Hard to Defend Adversarial Attacks",2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00945,https://ieeexplore.ieee.org/document/8954169/,"The problem of adversarial CNN attacks is considered, with an emphasis on attacks that are trivial to perform but difﬁcult to defend. A framework for the study of such attacks is proposed, using real world object manipulations. Unlike most works in the past, this framework supports the design of attacks based on both small and large image perturbations, implemented by camera shake and pose variation. A setup is proposed for the collection of such perturbations and determination of their perceptibility. It is argued that perceptibility depends on context, and a distinction is made between imperceptible and semantically imperceptible perturbations. While the former survives image comparisons, the latter are perceptible but have no impact on human object recognition. A procedure is proposed to determine the perceptibility of perturbations using Turk experiments, and a dataset of both perturbation classes which enables replicable studies of object manipulation attacks, is assembled. Experiments using defenses based on many datasets, CNN models, and algorithms from the literature elucidate the difﬁculty of defending these attacks – in fact, none of the existing defenses is found effective against them. Better results are achieved with real world data augmentation, but even this is not foolproof. These results conﬁrm the hypothesis that current CNNs are vulnerable to attacks implementable even by a child, and that such attacks may prove difﬁcult to defend.",Jun-19,2020/7/19 6:58,2020/7/19 6:58,2020/7/19 6:58,9221-9229,,,,,,Catastrophic Child's Play,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,"C:\Users\zhbli\Zotero\storage\YFGTW2S5\Ho 等。 - 2019 - Catastrophic Child's Play Easy to Perform, Hard t.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
QAR6QPEM,conferencePaper,2019,"Jia, Xiaojun; Wei, Xingxing; Cao, Xiaochun; Foroosh, Hassan",ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00624,https://ieeexplore.ieee.org/document/8954282/,"Deep neural networks (DNNs) have been demonstrated to be vulnerable to adversarial examples. Speciﬁcally, adding imperceptible perturbations to clean images can fool the well trained deep neural networks. In this paper, we propose an end-to-end image compression model to defend adversarial examples: ComDefend. The proposed model consists of a compression convolutional neural network (ComCNN) and a reconstruction convolutional neural network (RecCNN). The ComCNN is used to maintain the structure information of the original image and purify adversarial perturbations. And the RecCNN is used to reconstruct the original image with high quality. In other words, ComDefend can transform the adversarial image to its clean version, which is then fed to the trained classiﬁer. Our method is a pre-processing module, and does not modify the classiﬁer’s structure during the whole process. Therefore, it can be combined with other model-speciﬁc defense models to jointly improve the classiﬁer’s robustness. A series of experiments conducted on MNIST, CIFAR10 and ImageNet show that the proposed method outperforms the state-of-the-art defense methods, and is consistently effective to protect classiﬁers against adversarial attacks.",Jun-19,2020/7/19 7:00,2020/7/19 7:00,2020/7/19 7:00,6077-6085,,,,,,ComDefend,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\FZWH5EU2\Jia 等。 - 2019 - ComDefend An Efficient Image Compression Model to.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
MLVFNMKE,conferencePaper,2019,"Shi, Yucheng; Wang, Siyu; Han, Yahong",Curls & Whey: Boosting Black-Box Adversarial Attacks,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00668,https://ieeexplore.ieee.org/document/8953945/,"Image classiﬁers based on deep neural networks suffer from harassment caused by adversarial examples. Two defects exist in black-box iterative attacks that generate adversarial examples by incrementally adjusting the noiseadding direction for each step. On the one hand, existing iterative attacks add noises monotonically along the direction of gradient ascent, resulting in a lack of diversity and adaptability of the generated iterative trajectories. On the other hand, it is trivial to perform adversarial attack by adding excessive noises, but currently there is no reﬁnement mechanism to squeeze redundant noises. In this work, we propose Curls & Whey black-box attack to ﬁx the above two defects. During Curls iteration, by combining gradient ascent and descent, we ‘curl’ up iterative trajectories to integrate more diversity and transferability into adversarial examples. Curls iteration also alleviates the diminishing marginal effect in existing iterative attacks. The Whey optimization further squeezes the ‘whey’ of noises by exploiting the robustness of adversarial perturbation. Extensive experiments on Imagenet and Tiny-Imagenet demonstrate that our approach achieves impressive decrease on noise magnitude in ℓ2 norm. Curls & Whey attack also shows promising transferability against ensemble models as well as adversarially trained models. In addition, we extend our attack to the targeted misclassiﬁcation, effectively reducing the difﬁculty of targeted attacks under black-box condition.",Jun-19,2020/7/19 7:02,2020/7/19 7:02,2020/7/19 7:02,6512-6520,,,,,,Curls & Whey,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\TJPNCSJP\Shi 等。 - 2019 - Curls & Whey Boosting Black-Box Adversarial Attac.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
C5LEBNYA,conferencePaper,2019,"Rony, Jerome; Hafemann, Luiz G.; Oliveira, Luiz S.; Ben Ayed, Ismail; Sabourin, Robert; Granger, Eric",Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00445,https://ieeexplore.ieee.org/document/8954314/,"Research on adversarial examples in computer vision tasks has shown that small, often imperceptible changes to an image can induce misclassiﬁcation, which has security implications for a wide range of image processing systems. Considering L2 norm distortions, the Carlini and Wagner attack is presently the most effective white-box attack in the literature. However, this method is slow since it performs a line-search for one of the optimization terms, and often requires thousands of iterations. In this paper, an efﬁcient approach is proposed to generate gradient-based attacks that induce misclassiﬁcations with low L2 norm, by decoupling the direction and the norm of the adversarial perturbation that is added to the image. Experiments conducted on the MNIST, CIFAR-10 and ImageNet datasets indicate that our attack achieves comparable results to the state-of-the-art (in terms of L2 norm) with considerably fewer iterations (as few as 100 iterations), which opens the possibility of using these attacks for adversarial training. Models trained with our attack achieve state-of-the-art robustness against whitebox gradient-based L2 attacks on the MNIST and CIFAR-10 datasets, outperforming the Madry defense when the attacks are limited to a maximum norm.",Jun-19,2020/7/19 7:03,2020/7/19 7:03,2020/7/19 7:03,4317-4325,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\HDC5Z26A\Rony 等。 - 2019 - Decoupling Direction and Norm for Efficient Gradie.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
DG6S9EI9,conferencePaper,2019,"Liu, Yaojie; Stehouwer, Joel; Jourabloo, Amin; Liu, Xiaoming",Deep Tree Learning for Zero-Shot Face Anti-Spoofing,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00481,https://ieeexplore.ieee.org/document/8953776/,"Face anti-spooﬁng is designed to prevent face recognition systems from recognizing fake faces as the genuine users. While advanced face anti-spooﬁng methods are developed, new types of spoof attacks are also being created and becoming a threat to all existing systems. We deﬁne the detection of unknown spoof attacks as Zero-Shot Face Anti-spooﬁng (ZSFA). Previous ZSFA works only study 12 types of spoof attacks, such as print/replay, which limits the insight of this problem. In this work, we investigate the ZSFA problem in a wide range of 13 types of spoof attacks, including print, replay, 3D mask, and so on. A novel Deep Tree Network (DTN) is proposed to partition the spoof samples into semantic sub-groups in an unsupervised fashion. When a data sample arrives, being know or unknown attacks, DTN routes it to the most similar spoof cluster, and makes the binary decision. In addition, to enable the study of ZSFA, we introduce the ﬁrst face anti-spooﬁng database that contains diverse types of spoof attacks. Experiments show that our proposed method achieves the state of the art on multiple testing protocols of ZSFA.",Jun-19,2020/7/19 7:04,2020/7/19 7:04,2020/7/19 7:04,4675-4684,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\J46FT2HQ\Liu 等。 - 2019 - Deep Tree Learning for Zero-Shot Face Anti-Spoofin.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
QNHRUPBG,conferencePaper,2019,"Taran, Olga; Rezaeifar, Shideh; Holotyak, Taras; Voloshynovskiy, Slava",Defending Against Adversarial Attacks by Randomized Diversification,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.01148,https://ieeexplore.ieee.org/document/8953443/,"The vulnerability of machine learning systems to adversarial attacks questions their usage in many applications. In this paper, we propose a randomized diversiﬁcation as a defense strategy. We introduce a multi-channel architecture in a gray-box scenario, which assumes that the architecture of the classiﬁer and the training data set are known to the attacker. The attacker does not only have access to a secret key and to the internal states of the system at the test time. The defender processes an input in multiple channels. Each channel introduces its own randomization in a special transform domain based on a secret key shared between the training and testing stages. Such a transform based randomization with a shared key preserves the gradients in key-deﬁned sub-spaces for the defender but it prevents gradient back propagation and the creation of various bypass systems for the attacker. An additional beneﬁt of multi-channel randomization is the aggregation that fuses soft-outputs from all channels, thus increasing the reliability of the ﬁnal score. The sharing of a secret key creates an information advantage to the defender. Experimental evaluation demonstrates an increased robustness of the proposed method to a number of known state-of-the-art attacks.",Jun-19,2020/7/19 7:05,2020/7/19 7:05,2020/7/19 7:05,11218-11225,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\WIGT7XZM\Taran 等。 - 2019 - Defending Against Adversarial Attacks by Randomize.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
BSQP28NF,conferencePaper,2019,"Dubey, Abhimanyu; Maaten, Laurens van der; Yalniz, Zeki; Li, Yixuan; Mahajan, Dhruv",Defense Against Adversarial Images Using Web-Scale Nearest-Neighbor Search,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00897,https://ieeexplore.ieee.org/document/8954397/,"A plethora of recent work has shown that convolutional networks are not robust to adversarial images: images that are created by perturbing a sample from the data distribution as to maximize the loss on the perturbed example. In this work, we hypothesize that adversarial perturbations move the image away from the image manifold in the sense that there exists no physical process that could have produced the adversarial image. This hypothesis suggests that a successful defense mechanism against adversarial images should aim to project the images back onto the image manifold. We study such defense mechanisms, which approximate the projection onto the unknown image manifold by a nearest-neighbor search against a web-scale image database containing tens of billions of images. Empirical evaluations of this defense strategy on ImageNet suggest that it is very effective in attack settings in which the adversary does not have access to the image database. We also propose two novel attack methods to break nearestneighbor defenses, and demonstrate conditions under which nearest-neighbor defense fails. We perform a series of ablation experiments, which suggest that there is a trade-off between robustness and accuracy in our defenses, that a large image database (with hundreds of millions of images) is crucial to get good performance, and that careful construction the image database is important to be robust against attacks tailored to circumvent our defenses.",Jun-19,2020/7/19 7:05,2020/7/19 7:05,2020/7/19 7:05,8759-8768,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\MNPJYUDU\Dubey 等。 - 2019 - Defense Against Adversarial Images Using Web-Scale.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
QKBAQ9TW,conferencePaper,2019,"Liu, Jiayang; Zhang, Weiming; Zhang, Yiwei; Hou, Dongdong; Liu, Yujia; Zha, Hongyue; Yu, Nenghai",Detection Based Defense Against Adversarial Examples From the Steganalysis Point of View,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00496,https://ieeexplore.ieee.org/document/8953710/,"Deep Neural Networks (DNNs) have recently led to signiﬁcant improvements in many ﬁelds. However, DNNs are vulnerable to adversarial examples which are samples with imperceptible perturbations while dramatically misleading the DNNs. Moreover, adversarial examples can be used to perform an attack on various kinds of DNN based systems, even if the adversary has no access to the underlying model. Many defense methods have been proposed, such as obfuscating gradients of the networks or detecting adversarial examples. However it is proved out that these defense methods are not effective or cannot resist secondary adversarial attacks. In this paper, we point out that steganalysis can be applied to adversarial examples detection, and propose a method to enhance steganalysis features by estimating the probability of modiﬁcations caused by adversarial attacks. Experimental results show that the proposed method can accurately detect adversarial examples. Moreover, secondary adversarial attacks are hard to be directly performed to our method because our method is not based on a neural network but based on high-dimensional artiﬁcial features and Fisher Linear Discriminant ensemble.",Jun-19,2020/7/19 7:06,2020/7/19 7:06,2020/7/19 7:06,4820-4829,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\ECHGK93I\Liu 等。 - 2019 - Detection Based Defense Against Adversarial Exampl.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
M4AXIRBD,conferencePaper,2019,"Dong, Jianfeng; Li, Xirong; Xu, Chaoxi; Ji, Shouling; He, Yuan; Yang, Gang; Wang, Xun",Dual Encoding for Zero-Example Video Retrieval,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00957,https://ieeexplore.ieee.org/document/8953675/,"This paper attacks the challenging problem of zeroexample video retrieval. In such a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc queries described in natural language text with no visual example provided. Given videos as sequences of frames and queries as sequences of words, an effective sequence-to-sequence cross-modal matching is required. The majority of existing methods are concept based, extracting relevant concepts from queries and videos and accordingly establishing associations between the two modalities. In contrast, this paper takes a concept-free approach, proposing a dual deep encoding network that encodes videos and queries into powerful dense representations of their own. Dual encoding is conceptually simple, practically effective and endto-end. As experiments on three benchmarks, i.e. MSRVTT, TRECVID 2016 and 2017 Ad-hoc Video Search show, the proposed solution establishes a new state-of-the-art for zero-example video retrieval.",Jun-19,2020/7/19 7:08,2020/7/19 7:08,2020/7/19 7:08,9338-9347,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\68IXQ45N\Dong 等。 - 2019 - Dual Encoding for Zero-Example Video Retrieval.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
6W6NT4NB,conferencePaper,2019,"Dong, Yinpeng; Su, Hang; Wu, Baoyuan; Li, Zhifeng; Liu, Wei; Zhang, Tong; Zhu, Jun",Efficient Decision-Based Black-Box Adversarial Attacks on Face Recognition,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00790,https://ieeexplore.ieee.org/document/8953400/,"Face recognition has obtained remarkable progress in recent years due to the great improvement of deep convolutional neural networks (CNNs). However, deep CNNs are vulnerable to adversarial examples, which can cause fateful consequences in real-world face recognition applications with security-sensitive purposes. Adversarial attacks are widely studied as they can identify the vulnerability of the models before they are deployed. In this paper, we evaluate the robustness of state-of-the-art face recognition models in the decision-based black-box attack setting, where the attackers have no access to the model parameters and gradients, but can only acquire hard-label predictions by sending queries to the target model. This attack setting is more practical in real-world face recognition systems. To improve the efﬁciency of previous methods, we propose an evolutionary attack algorithm, which can model the local geometry of the search directions and reduce the dimension of the search space. Extensive experiments demonstrate the effectiveness of the proposed method that induces a minimum perturbation to an input face image with fewer queries. We also apply the proposed method to attack a real-world face recognition system successfully.",Jun-19,2020/7/19 7:09,2020/7/19 7:09,2020/7/19 7:09,7706-7714,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\9U2VR2H4\Dong 等。 - 2019 - Efficient Decision-Based Black-Box Adversarial Att.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
L9SDECSM,conferencePaper,2019,"Dong, Yinpeng; Pang, Tianyu; Su, Hang; Zhu, Jun",Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00444,https://ieeexplore.ieee.org/document/8953425/,"Deep neural networks are vulnerable to adversarial examples, which can mislead classiﬁers by adding imperceptible perturbations. An intriguing property of adversarial examples is their good transferability, making black-box attacks feasible in real-world applications. Due to the threat of adversarial attacks, many methods have been proposed to improve the robustness. Several state-of-the-art defenses are shown to be robust against transferable adversarial examples. In this paper, we propose a translation-invariant attack method to generate more transferable adversarial examples against the defense models. By optimizing a perturbation over an ensemble of translated images, the generated adversarial example is less sensitive to the white-box model being attacked and has better transferability. To improve the efﬁciency of attacks, we further show that our method can be implemented by convolving the gradient at the untranslated image with a pre-deﬁned kernel. Our method is generally applicable to any gradient-based attack method. Extensive experiments on the ImageNet dataset validate the effectiveness of the proposed method. Our best attack fools eight state-of-the-art defenses at an 82% success rate on average based only on the transferability, demonstrating the insecurity of the current defense techniques.",Jun-19,2020/7/19 7:11,2020/7/19 7:11,2020/7/19 7:11,4307-4316,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\RR32IIDX\Dong 等。 - 2019 - Evading Defenses to Transferable Adversarial Examp.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
UHBXFBUG,conferencePaper,2019,"Xu, Yan; Wu, Baoyuan; Shen, Fumin; Fan, Yanbo; Zhang, Yong; Shen, Heng Tao; Liu, Wei",Exact Adversarial Attack to Image Captioning via Structured Output Learning With Latent Variables,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00426,https://ieeexplore.ieee.org/document/8954337/,"In this work, we study the robustness of a CNN+RNN based image captioning system being subjected to adversarial noises. We propose to fool an image captioning system to generate some targeted partial captions for an image polluted by adversarial noises, even the targeted captions are totally irrelevant to the image content. A partial caption indicates that the words at some locations in this caption are observed, while words at other locations are not restricted. It is the ﬁrst work to study exact adversarial attacks of targeted partial captions. Due to the sequential dependencies among words in a caption, we formulate the generation of adversarial noises for targeted partial captions as a structured output learning problem with latent variables. Both the generalized expectation maximization algorithm and structural SVMs with latent variables are then adopted to optimize the problem. The proposed methods generate very successful attacks to three popular CNN+RNN based image captioning models. Furthermore, the proposed attack methods are used to understand the inner mechanism of image captioning systems, providing the guidance to further improve automatic image captioning systems towards human captioning.",Jun-19,2020/7/19 7:11,2020/7/19 7:11,2020/7/19 7:11,4130-4139,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\GWVWB4HG\Xu 等。 - 2019 - Exact Adversarial Attack to Image Captioning via S.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
XJ2ZNY7M,conferencePaper,2019,"Yang, Xiao; Luo, Wenhan; Bao, Linchao; Gao, Yuan; Gong, Dihong; Zheng, Shibao; Li, Zhifeng; Liu, Wei","Face Anti-Spoofing: Model Matters, so Does Data",2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00362,https://ieeexplore.ieee.org/document/8953211/,"Face anti-spooﬁng is an important task in full-stack face applications including face detection, veriﬁcation, and recognition. Previous approaches build models on datasets which do not simulate the real-world data well (e.g., small scale, insigniﬁcant variance, etc.). Existing models may rely on auxiliary information, which prevents these antispooﬁng solutions from generalizing well in practice. In this paper, we present a data collection solution along with a data synthesis technique to simulate digital medium-based face spooﬁng attacks, which can easily help us obtain a large amount of training data well reﬂecting the real-world scenarios. Through exploiting a novel Spatio-Temporal Anti-Spoof Network (STASN), we are able to push the performance on public face anti-spooﬁng datasets over stateof-the-art methods by a large margin. Since the proposed model can automatically attend to discriminative regions, it makes analyzing the behaviors of the network possible. We conduct extensive experiments and show that the proposed model can distinguish spoof faces by extracting features from a variety of regions to seek out subtle evidences such as borders, moire patterns, reﬂection artifacts, etc.",Jun-19,2020/7/19 7:12,2020/7/19 7:12,2020/7/19 7:12,3502-3511,,,,,,Face Anti-Spoofing,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,"C:\Users\zhbli\Zotero\storage\YQSEKBX2\Yang 等。 - 2019 - Face Anti-Spoofing Model Matters, so Does Data.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
MULC28LV,journalArticle,,"Xie, Cihang; Wu, Yuxin",Feature Denoising for Improving Adversarial Robustness,,,,,,"Adversarial attacks to image classiﬁcation systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Speciﬁcally, our networks contain blocks that denoise the features using non-local means or other ﬁlters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked ﬁrst in Competition on Adversarial Attacks and Defenses (CAAD) 2018 — it achieved 50.6% classiﬁcation accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by ∼10%. Code is available at https://github.com/facebookresearch/ ImageNet-Adversarial-Training.",,2020/7/19 7:13,2020/7/19 7:13,,9,,,,,,,,,,,,,en,,,,,Zotero,,,,C:\Users\zhbli\Zotero\storage\UXZ7PHLE\Xie 和 Wu - Feature Denoising for Improving Adversarial Robust.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LBN5DTWW,conferencePaper,2019,"Inkawhich, Nathan; Wen, Wei; Li, Hai Helen; Chen, Yiran",Feature Space Perturbations Yield More Transferable Adversarial Examples,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00723,https://ieeexplore.ieee.org/document/8953700/,"Many recent works have shown that deep learning models are vulnerable to quasi-imperceptible input perturbations, yet practitioners cannot fully explain this behavior. This work describes a transfer-based blackbox targeted adversarial attack of deep feature space representations that also provides insights into cross-model class representations of deep CNNs. The attack is explicitly designed for transferability and drives feature space representation of a source image at layer L towards the representation of a target image at L. The attack yields highly transferable targeted examples, which outperform competition winning methods by over 30% in targeted attack metrics. We also show the choice of L to generate examples from is important, transferability characteristics are blackbox model agnostic, and indicate that well trained deep models have similar highly-abstract representations.",Jun-19,2020/7/19 7:13,2020/7/19 7:13,2020/7/19 7:13,7059-7067,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\FCFTYJU3\Inkawhich 等。 - 2019 - Feature Space Perturbations Yield More Transferabl.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
MF55K8FT,conferencePaper,2019,"Xiang, Chong; Qi, Charles R.; Li, Bo",Generating 3D Adversarial Point Clouds,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00935,https://ieeexplore.ieee.org/document/8953645/,"Deep neural networks are known to be vulnerable to adversarial examples which are carefully crafted instances to cause the models to make wrong predictions. While adversarial examples for 2D images and CNNs have been extensively studied, less attention has been paid to 3D data such as point clouds. Given many safety-critical 3D applications such as autonomous driving, it is important to study how adversarial point clouds could affect current deep 3D models. In this work, we propose several novel algorithms to craft adversarial point clouds against PointNet, a widely used deep neural network for point cloud processing. Our algorithms work in two ways: adversarial point perturbation and adversarial point generation. For point perturbation, we shift existing points negligibly. For point generation, we generate either a set of independent and scattered points or a small number (1-3) of point clusters with meaningful shapes such as balls and airplanes which could be hidden in the human psyche. In addition, we formulate six perturbation measurement metrics tailored to the attacks in point clouds and conduct extensive experiments to evaluate the proposed algorithms on the ModelNet40 3D shape classiﬁcation dataset. Overall, our attack algorithms achieve a success rate higher than 99% for all targeted attacks 1.",Jun-19,2020/7/19 7:15,2020/7/19 7:15,2020/7/19 7:15,9128-9136,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\G67FJUDQ\Xiang 等。 - 2019 - Generating 3D Adversarial Point Clouds.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
PY59RLDH,conferencePaper,2019,"Xie, Cihang; Zhang, Zhishuai; Zhou, Yuyin; Bai, Song; Wang, Jianyu; Ren, Zhou; Yuille, Alan L.",Improving Transferability of Adversarial Examples With Input Diversity,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00284,https://ieeexplore.ieee.org/document/8953423/,"Though CNNs have achieved the state-of-the-art performance on various vision tasks, they are vulnerable to adversarial examples — crafted by adding human-imperceptible perturbations to clean images. However, most of the existing adversarial attacks only achieve relatively low success rates under the challenging black-box setting, where the attackers have no knowledge of the model structure and parameters. To this end, we propose to improve the transferability of adversarial examples by creating diverse input patterns. Instead of only using the original images to generate adversarial examples, our method applies random transformations to the input images at each iteration. Extensive experiments on ImageNet show that the proposed attack method can generate adversarial examples that transfer much better to different networks than existing baselines. By evaluating our method against top defense solutions and ofﬁcial baselines from NIPS 2017 adversarial competition, the enhanced attack reaches an average success rate of 73.0%, which outperforms the top-1 attack submission in the NIPS competition by a large margin of 6.6%. We hope that our proposed attack strategy can serve as a strong benchmark baseline for evaluating the robustness of networks to adversaries and the effectiveness of different defense methods in the future. Code is available at https: //github.com/cihangxie/DI-2-FGSM .",Jun-19,2020/7/19 7:19,2020/7/19 7:19,2020/7/19 7:19,2725-2734,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\JNYZCHVI\Xie 等。 - 2019 - Improving Transferability of Adversarial Examples .pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
CPDWE9IU,conferencePaper,2019,"Xiao, Chaowei; Yang, Dawei; Li, Bo; Deng, Jia; Liu, Mingyan",MeshAdv: Adversarial Meshes for Visual Recognition,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00706,https://ieeexplore.ieee.org/document/8953197/,"Highly expressive models such as deep neural networks (DNNs) have been widely applied to various applications. However, recent studies show that DNNs are vulnerable to adversarial examples, which are carefully crafted inputs aiming to mislead the predictions. Currently, the majority of these studies have focused on perturbation added to image pixels, while such manipulation is not physically realistic. Some works have tried to overcome this limitation by attaching printable 2D patches or painting patterns onto surfaces, but can be potentially defended because 3D shape features are intact. In this paper, we propose meshAdv to generate “adversarial 3D meshes” from objects that have rich shape features but minimal textural variation. To manipulate the shape or texture of the objects, we make use of a differentiable renderer to compute accurate shading on the shape and propagate the gradient. Extensive experiments show that the generated 3D meshes are effective in attacking both classiﬁers and object detectors. We evaluate the attack under different viewpoints. In addition, we design a pipeline to perform black-box attack on a photorealistic renderer with unknown rendering parameters.",Jun-19,2020/7/19 7:29,2020/7/19 7:29,2020/7/19 7:29,6891-6900,,,,,,MeshAdv,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\Z3YQH6MG\Xiao 等。 - 2019 - MeshAdv Adversarial Meshes for Visual Recognition.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
QF6VCNE7,conferencePaper,2019,"Shao, Rui; Lan, Xiangyuan; Li, Jiawei; Yuen, Pong C.",Multi-Adversarial Discriminative Deep Domain Generalization for Face Presentation Attack Detection,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.01026,https://ieeexplore.ieee.org/document/8953226/,"Face presentation attacks have become an increasingly critical issue in the face recognition community. Many face anti-spooﬁng methods have been proposed, but they cannot generalize well on ”unseen” attacks. This work focuses on improving the generalization ability of face antispooﬁng methods from the perspective of the domain generalization. We propose to learn a generalized feature space via a novel multi-adversarial discriminative deep domain generalization framework. In this framework, a multiadversarial deep domain generalization is performed under a dual-force triplet-mining constraint. This ensures that the learned feature space is discriminative and shared by multiple source domains, and thus is more generalized to new face presentation attacks. An auxiliary face depth supervision is incorporated to further enhance the generalization ability. Extensive experiments on four public datasets validate the effectiveness of the proposed method.",Jun-19,2020/7/19 7:30,2020/7/19 7:30,2020/7/19 7:30,10015-10023,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\VBYE67GL\Shao 等。 - 2019 - Multi-Adversarial Discriminative Deep Domain Gener.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
7YMMMMW3,conferencePaper,2019,"He, Zhezhi; Rakin, Adnan Siraj; Fan, Deliang",Parametric Noise Injection: Trainable Randomness to Improve Deep Neural Network Robustness Against Adversarial Attack,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00068,https://ieeexplore.ieee.org/document/8954187/,"Recent developments in the ﬁeld of Deep Learning have exposed the underlying vulnerability of Deep Neural Network (DNN) against adversarial examples. In image classiﬁcation, an adversarial example is a carefully modiﬁed image that is visually imperceptible to the original image but can cause DNN model to misclassify it. Training the network with Gaussian noise is an effective technique to perform model regularization, thus improving model robustness against input variation. Inspired by this classical method, we explore to utilize the regularization characteristic of noise injection to improve DNN’s robustness against adversarial attack. In this work, we propose ParametricNoise-Injection (PNI) 1 which involves trainable Gaussian noise injection at each layer on either activation or weights through solving the Min-Max optimization problem, embedded with adversarial training. These parameters are trained explicitly to achieve improved robustness. The extensive results show that our proposed PNI technique effectively improves the robustness against a variety of powerful whitebox and black-box attacks such as PGD, C & W, FGSM, transferable attack, and ZOO attack. Last but not the least, PNI method improves both clean- and perturbed-data accuracy in comparison to the state-of-the-art defense methods, which outperforms current unbroken PGD defense by 1.1 % and 6.8 % on clean- and perturbed- test data respectively, using ResNet-20 architecture.",Jun-19,2020/7/19 7:35,2020/7/19 7:35,2020/7/19 7:35,588-597,,,,,,Parametric Noise Injection,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\VEVSZTBS\He 等。 - 2019 - Parametric Noise Injection Trainable Randomness t.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
JRUETX65,conferencePaper,2019,"Zhao, Jake Junbo; Cho, Kyunghyun",Retrieval-Augmented Convolutional Neural Networks Against Adversarial Examples,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.01183,https://ieeexplore.ieee.org/document/8953314/,"We propose a retrieval-augmented convolutional network (RaCNN) and propose to train it with local mixup, a novel variant of the recently proposed mixup algorithm. The proposed hybrid architecture combining a convolutional network and an off-the-shelf retrieval engine was designed to mitigate the adverse effect of off-manifold adversarial examples, while the proposed local mixup addresses on-manifold ones by explicitly encouraging the classiﬁer to locally behave linearly on the data manifold. Our evaluation of the proposed approach against seven readilyavailable adversarial attacks on three datasets–CIFAR-10, SVHN and ImageNet–demonstrate the improved robustness compared to a vanilla convolutional network, and comparable performance with the state-of-the-art reactive defense approaches.",Jun-19,2020/7/19 7:40,2020/7/19 7:40,2020/7/19 7:40,11555-11563,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\32Z5AI6F\Zhao 和 Cho - 2019 - Retrieval-Augmented Convolutional Neural Networks .pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
LESJ9H64,conferencePaper,2019,"Pittaluga, Francesco; Koppal, Sanjeev J.; Kang, Sing Bing; Sinha, Sudipta N.",Revealing Scenes by Inverting Structure From Motion Reconstructions,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00023,https://ieeexplore.ieee.org/document/8953773/,"Many 3D vision systems localize cameras within a scene using 3D point clouds. Such point clouds are often obtained using structure from motion (SfM), after which the images are discarded to preserve privacy. In this paper, we show, for the ﬁrst time, that such point clouds retain enough information to reveal scene appearance and compromise privacy. We present a privacy attack that reconstructs color images of the scene from the point cloud. Our method is based on a cascaded U-Net that takes as input, a 2D multichannel image of the points rendered from a speciﬁc viewpoint containing point depth and optionally color and SIFT descriptors and outputs a color image of the scene from that viewpoint. Unlike previous feature inversion methods [46, 9], we deal with highly sparse and irregular 2D point distributions and inputs where many point attributes are missing, namely keypoint orientation and scale, the descriptor image source and the 3D point visibility. We evaluate our attack algorithm on public datasets [24, 39] and analyze the signiﬁcance of the point cloud attributes. Finally, we show that novel views can also be generated thereby enabling compelling virtual tours of the underlying scene.",Jun-19,2020/7/19 7:40,2020/7/19 7:40,2020/7/19 7:40,145-154,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\FSKTBH4R\Pittaluga 等。 - 2019 - Revealing Scenes by Inverting Structure From Motio.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
2JAN8HNP,conferencePaper,2019,"Liu, Xuanqing; Hsieh, Cho-Jui","Rob-GAN: Generator, Discriminator, and Adversarial Attacker",2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.01149,https://ieeexplore.ieee.org/document/8953327/,"We study two important concepts in adversarial deep learning—adversarial training and generative adversarial network (GAN). Adversarial training is the technique used to improve the robustness of discriminator by combining adversarial attacker and discriminator in the training phase. GAN is commonly used for image generation by jointly optimizing discriminator and generator. We show these two concepts are indeed closely related and can be used to strengthen each other—adding a generator to the adversarial training procedure can improve the robustness of discriminators, and adding an adversarial attack to GAN training can improve the convergence speed and lead to better generators. Combining these two insights, we develop a framework called Rob-GAN to jointly optimize generator and discriminator in the presence of adversarial attacks—the generator generates fake images to fool discriminator; the adversarial attacker perturbs real images to fool discriminator, and the discriminator wants to minimize loss under fake and adversarial images. Through this end-to-end training procedure, we are able to simultaneously improve the convergence speed of GAN training, the quality of synthetic images, and the robustness of discriminator under strong adversarial attacks. Experimental results demonstrate that the obtained classiﬁer is more robust than state-of-the-art adversarial training approach [23], and the generator outperforms SN-GAN on ImageNet-143.",Jun-19,2020/7/19 7:41,2020/7/19 7:41,2020/7/19 7:41,11226-11235,,,,,,Rob-GAN,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,"C:\Users\zhbli\Zotero\storage\N6YWKV2N\Liu 和 Hsieh - 2019 - Rob-GAN Generator, Discriminator, and Adversarial.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
RR8RQ6AK,conferencePaper,2019,"Wicker, Matthew; Kwiatkowska, Marta",Robustness of 3D Deep Learning in an Adversarial Setting,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.01204,https://ieeexplore.ieee.org/document/8953599/,"Understanding the spatial arrangement and nature of real-world objects is of paramount importance to many complex engineering tasks, including autonomous navigation. Deep learning has revolutionized state-of-the-art performance for tasks in 3D environments; however, relatively little is known about the robustness of these approaches in an adversarial setting. The lack of comprehensive analysis makes it difﬁcult to justify deployment of 3D deep learning models in real-world, safety-critical applications. In this work, we develop an algorithm for analysis of pointwise robustness of neural networks that operate on 3D data. We show that current approaches presented for understanding the resilience of state-of-the-art models vastly overestimate their robustness. We then use our algorithm to evaluate an array of state-of-the-art models in order to demonstrate their vulnerability to occlusion attacks. We show that, in the worst case, these networks can be reduced to 0% classiﬁcation accuracy after the occlusion of at most 6.5% of the occupied input space.",Jun-19,2020/7/19 7:41,2020/7/19 7:41,2020/7/19 7:41,11759-11767,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\PEBENM7M\Wicker 和 Kwiatkowska - 2019 - Robustness of 3D Deep Learning in an Adversarial S.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
XZ8LDY8I,conferencePaper,2019,"He, Zecheng; Zhang, Tianwei; Lee, Ruby",Sensitive-Sample Fingerprinting of Deep Neural Networks,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00486,https://ieeexplore.ieee.org/document/8953972/,"Numerous cloud-based services are provided to help customers develop and deploy deep learning applications. When a customer deploys a deep learning model in the cloud and serves it to end-users, it is important to be able to verify that the deployed model has not been tampered with. In this paper, we propose a novel and practical methodology to verify the integrity of remote deep learning models, with only black-box access to the target models. Speciﬁcally, we deﬁne Sensitive-Sample ﬁngerprints, which are a small set of human unnoticeable transformed inputs that make the model outputs sensitive to the model’s parameters. Even small model changes can be clearly reﬂected in the model outputs. Experimental results on different types of model integrity attacks show that the proposed approach is both effective and efﬁcient. It can detect model integrity breaches with high accuracy (>99.95%) and guaranteed zero false positives on all evaluated attacks. Meanwhile, it only requires up to 103× fewer model inferences, compared to non-sensitive samples.",Jun-19,2020/7/19 7:44,2020/7/19 7:44,2020/7/19 7:44,4724-4732,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\TDQQVBP8\He 等。 - 2019 - Sensitive-Sample Fingerprinting of Deep Neural Net.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
3S6ME5XB,conferencePaper,2019,"Theagarajan, Rajkumar; Chen, Ming; Bhanu, Bir; Zhang, Jing",ShieldNets: Defending Against Adversarial Attacks Using Probabilistic Adversarial Robustness,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00715,https://ieeexplore.ieee.org/document/8953209/,"Defending adversarial attack is a critical step towards reliable deployment of deep learning empowered solutions for industrial applications. Probabilistic adversarial robustness (PAR), as a theoretical framework, is introduced to neutralize adversarial attacks by concentrating sample probability to adversarial-free zones. Distinct to most of the existing defense mechanisms that require modifying the architecture/training of the target classiﬁer which is not feasible in the real-world scenario, e.g., when a model has already been deployed, PAR is designed in the ﬁrst place to provide proactive protection to an existing ﬁxed model. ShieldNet is implemented as a demonstration of PAR in this work by using PixelCNN. Experimental results show that this approach is generalizable, robust against adversarial transferability and resistant to a wide variety of attacks on the Fashion-MNIST and CIFAR10 datasets, respectively.",Jun-19,2020/7/19 7:45,2020/7/19 7:45,2020/7/19 7:45,6981-6989,,,,,,ShieldNets,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\JM8Z3ZXE\Theagarajan 等。 - 2019 - ShieldNets Defending Against Adversarial Attacks .pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
UYIYEWBL,conferencePaper,2019,"Yao, Zhewei; Gholami, Amir; Xu, Peng; Keutzer, Kurt; Mahoney, Michael W.",Trust Region Based Adversarial Attack on Neural Networks,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.01161,https://ieeexplore.ieee.org/document/8953547/,"Deep Neural Networks are quite vulnerable to adversarial perturbations. Current state-of-the-art adversarial attack methods typically require very time consuming hyper-parameter tuning, or require many iterations to solve an optimization based adversarial attack. To address this problem, we present a new family of trust region based adversarial attacks, with the goal of computing adversarial perturbations eﬃciently. We propose several attacks based on variants of the trust region optimization method. We test the proposed methods on Cifar-10 and ImageNet datasets using several diﬀerent models including AlexNet, ResNet-50, VGG-16, and DenseNet-121 models. Our methods achieve comparable results with the Carlini-Wagner (CW) attack, but with signiﬁcant speed up of up to 37×, for the VGG-16 model on a Titan Xp GPU. For the case of ResNet-50 on ImageNet, we can bring down its classiﬁcation accuracy to less than 0.1% with at most 1.5% relative L∞ (or L2) perturbation requiring only 1.02 seconds as compared to 27.04 seconds for the CW attack. We have open sourced our method which can be accessed at [1].",Jun-19,2020/7/19 7:51,2020/7/19 7:51,2020/7/19 7:51,11342-11351,,,,,,,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\RJCB4AY6\Yao 等。 - 2019 - Trust Region Based Adversarial Attack on Neural Ne.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
AXFJP2MT,conferencePaper,2019,"Wu, Hao; Mao, Jiayuan; Zhang, Yufeng; Jiang, Yuning; Li, Lei; Sun, Weiwei; Ma, Wei-Ying",Unified Visual-Semantic Embeddings: Bridging Vision and Language With Structured Meaning Representations,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),978-1-72813-293-8,,10.1109/CVPR.2019.00677,https://ieeexplore.ieee.org/document/8954449/,"We propose the Uniﬁed Visual-Semantic Embeddings (Uniﬁed VSE) for learning a joint space of visual representation and textual semantics. The model uniﬁes the embeddings of concepts at different levels: objects, attributes, relations, and full scenes. We view the sentential semantics as a combination of different semantic components such as objects and relations; their embeddings are aligned with different image regions. A contrastive learning approach is proposed for the effective learning of this ﬁne-grained alignment from only image-caption pairs. We also present a simple yet effective approach that enforces the coverage of caption embeddings on the semantic components that appear in the sentence. We demonstrate that the Uniﬁed VSE outperforms baselines on cross-modal retrieval tasks; the enforcement of the semantic coverage improves the model’s robustness in defending text-domain adversarial attacks. Moreover, our model empowers the use of visual cues to accurately resolve word dependencies in novel sentences.",Jun-19,2020/7/19 7:52,2020/7/19 7:52,2020/7/19 7:52,6602-6611,,,,,,Unified Visual-Semantic Embeddings,,,,,IEEE,"Long Beach, CA, USA",en,,,,,DOI.org (Crossref),,,,C:\Users\zhbli\Zotero\storage\GMQPQV9N\Wu 等。 - 2019 - Unified Visual-Semantic Embeddings Bridging Visio.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,,,,,,,,,,,,,
